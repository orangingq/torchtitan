
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Wed Sep 24 06:47:43 UTC 2025
âœ”ï¸SERVER: a4b73cb5e01f (172.18.0.2),  GPUs: 0,1,2,3,4,5,6,7
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod8/0922_main/0922_gpipe_nofreeze.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs
âœ”ï¸Running with nofreeze x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=8 --local-ranks-filter=0,1,2,3,4,5,6,7 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod8/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0924 06:47:45.015000 3607 torch/distributed/run.py:815] 
W0924 06:47:45.015000 3607 torch/distributed/run.py:815] *****************************************
W0924 06:47:45.015000 3607 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0924 06:47:45.015000 3607 torch/distributed/run.py:815] *****************************************
[rank4]:[titan] 2025-09-24 06:47:53,962 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank1]:[titan] 2025-09-24 06:47:54,009 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank7]:[titan] 2025-09-24 06:47:53,978 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank5]:[titan] 2025-09-24 06:47:54,128 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-24 06:47:54,237 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank3]:[titan] 2025-09-24 06:47:54,252 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank6]:[titan] 2025-09-24 06:47:54,241 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank2]:[titan] 2025-09-24 06:47:54,308 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank4]:[titan] 2025-09-24 06:47:54,715 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank4]:[titan] 2025-09-24 06:47:54,720 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank7]:[titan] 2025-09-24 06:47:54,829 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank7]:[titan] 2025-09-24 06:47:54,833 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank1]:[titan] 2025-09-24 06:47:55,080 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-09-24 06:47:55,084 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-24 06:47:55,552 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-24 06:47:55,555 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-24 06:47:55,562 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank5]:[titan] 2025-09-24 06:47:55,499 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank5]:[titan] 2025-09-24 06:47:55,502 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank3]:[titan] 2025-09-24 06:47:55,661 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-24 06:47:55,652 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-24 06:47:55,654 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank2]:[titan] 2025-09-24 06:47:55,678 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-09-24 06:47:55,681 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank3]:[titan] 2025-09-24 06:47:55,665 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-24 06:47:56,835 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-24 06:47:57,166 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank4]:
[rank4]:Generating train split:   0%|          | 0/517982 [00:00<?, ? examples/s][rank4]:
[rank4]:Generating train split:   2%|â–         | 10643/517982 [00:00<00:05, 85306.87 examples/s][rank4]:
[rank4]:Generating train split:   6%|â–‹         | 32413/517982 [00:00<00:04, 101992.02 examples/s][rank4]:
[rank4]:Generating train split:   8%|â–Š         | 43053/517982 [00:00<00:04, 103011.63 examples/s][rank4]:
[rank4]:Generating train split:  10%|â–ˆ         | 53956/517982 [00:00<00:04, 104853.53 examples/s][rank4]:
[rank4]:Generating train split:  14%|â–ˆâ–Ž        | 70240/517982 [00:00<00:04, 106284.86 examples/s][rank4]:
[rank4]:Generating train split:  16%|â–ˆâ–Œ        | 81091/517982 [00:00<00:04, 106881.48 examples/s][rank4]:
[rank4]:Generating train split:  19%|â–ˆâ–‰        | 97496/517982 [00:00<00:03, 107923.09 examples/s][rank4]:
[rank4]:Generating train split:  22%|â–ˆâ–ˆâ–       | 113660/517982 [00:01<00:03, 107755.45 examples/s][rank4]:
[rank4]:Generating train split:  25%|â–ˆâ–ˆâ–Œ       | 129997/517982 [00:01<00:03, 108197.58 examples/s][rank4]:
[rank4]:Generating train split:  27%|â–ˆâ–ˆâ–‹       | 140926/517982 [00:01<00:03, 108125.17 examples/s][rank4]:
[rank4]:Generating train split:  29%|â–ˆâ–ˆâ–‰       | 151807/517982 [00:01<00:03, 108045.18 examples/s][rank4]:
[rank4]:Generating train split:  31%|â–ˆâ–ˆâ–ˆâ–      | 162730/517982 [00:01<00:03, 108272.35 examples/s][rank4]:
[rank4]:Generating train split:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 173632/517982 [00:01<00:03, 108044.94 examples/s][rank4]:
[rank4]:Generating train split:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 189989/517982 [00:01<00:03, 107529.68 examples/s][rank4]:
[rank4]:Generating train split:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 201144/517982 [00:01<00:02, 108223.75 examples/s][rank4]:
[rank4]:Generating train split:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 212034/517982 [00:01<00:02, 107854.12 examples/s][rank4]:
[rank4]:Generating train split:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 223001/517982 [00:02<00:02, 108172.73 examples/s][rank4]:
[rank4]:Generating train split:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 234106/517982 [00:02<00:02, 108646.04 examples/s][rank4]:
[rank4]:Generating train split:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 255708/517982 [00:02<00:02, 108282.12 examples/s][rank4]:
[rank4]:Generating train split:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 272065/517982 [00:02<00:02, 108419.79 examples/s][rank4]:
[rank4]:Generating train split:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 282931/517982 [00:02<00:02, 107701.38 examples/s][rank4]:
[rank4]:Generating train split:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 299515/517982 [00:02<00:02, 108657.32 examples/s][rank4]:
[rank4]:Generating train split:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 310650/517982 [00:02<00:01, 108934.05 examples/s][rank4]:
[rank4]:Generating train split:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 326963/517982 [00:03<00:01, 108157.45 examples/s][rank4]:
[rank4]:Generating train split:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 337959/517982 [00:03<00:01, 108345.80 examples/s][rank4]:
[rank4]:Generating train split:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 349025/517982 [00:03<00:01, 108573.46 examples/s][rank4]:
[rank4]:Generating train split:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 365576/517982 [00:03<00:01, 109182.25 examples/s][rank4]:
[rank4]:Generating train split:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 376507/517982 [00:03<00:01, 108720.44 examples/s][rank4]:
[rank4]:Generating train split:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 392954/517982 [00:03<00:01, 108509.16 examples/s][rank4]:
[rank4]:Generating train split:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 403988/517982 [00:03<00:01, 108435.48 examples/s][rank4]:
[rank4]:Generating train split:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 415401/517982 [00:03<00:00, 109152.96 examples/s][rank4]:
[rank4]:Generating train split:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 426793/517982 [00:03<00:00, 108794.80 examples/s][rank4]:
[rank4]:Generating train split:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 438292/517982 [00:04<00:00, 109824.24 examples/s][rank4]:
[rank4]:Generating train split:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 449667/517982 [00:04<00:00, 110011.17 examples/s][rank4]:
[rank4]:Generating train split:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 461032/517982 [00:04<00:00, 110467.03 examples/s][rank4]:
[rank4]:Generating train split:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 472481/517982 [00:04<00:00, 110845.14 examples/s][rank4]:
[rank4]:Generating train split:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 483906/517982 [00:04<00:00, 110847.59 examples/s][rank4]:
[rank4]:Generating train split:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 495362/517982 [00:04<00:00, 110936.67 examples/s][rank4]:
[rank4]:Generating train split:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 506755/517982 [00:04<00:00, 111331.99 examples/s][rank4]:
[rank4]:Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 517982/517982 [00:04<00:00, 108405.63 examples/s]
[rank0]:[titan] 2025-09-24 06:48:08,273 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:[titan] 2025-09-24 06:48:08,565 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank4]:[titan] 2025-09-24 06:48:08,543 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank1]:[titan] 2025-09-24 06:48:08,594 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank2]:[titan] 2025-09-24 06:48:08,577 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank0]:[titan] 2025-09-24 06:48:08,578 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank5]:[titan] 2025-09-24 06:48:08,583 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank7]:[titan] 2025-09-24 06:48:08,595 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank4]:[titan] 2025-09-24 06:48:08,614 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank4]:[titan] 2025-09-24 06:48:08,646 - root - INFO - Applied FSDP to the model
[rank4]:[titan] 2025-09-24 06:48:08,647 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-09-24 06:48:08,715 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank1]:[titan] 2025-09-24 06:48:08,755 - root - INFO - Applied FSDP to the model
[rank2]:[titan] 2025-09-24 06:48:08,706 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:[titan] 2025-09-24 06:48:08,755 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-09-24 06:48:08,746 - root - INFO - Applied FSDP to the model
[rank2]:[titan] 2025-09-24 06:48:08,746 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-24 06:48:08,682 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-24 06:48:08,709 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-24 06:48:08,745 - root - INFO - Applied FSDP to the model
[rank0]:[titan] 2025-09-24 06:48:08,745 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-24 06:48:08,684 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:[titan] 2025-09-24 06:48:08,725 - root - INFO - Applied FSDP to the model
[rank3]:[titan] 2025-09-24 06:48:08,726 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank5]:[titan] 2025-09-24 06:48:08,714 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank5]:[titan] 2025-09-24 06:48:08,748 - root - INFO - Applied FSDP to the model
[rank5]:[titan] 2025-09-24 06:48:08,749 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-24 06:48:08,716 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank7]:[titan] 2025-09-24 06:48:08,749 - root - INFO - Applied FSDP to the model
[rank7]:[titan] 2025-09-24 06:48:08,749 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank4]:[titan] 2025-09-24 06:48:08,957 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank4]:[titan] 2025-09-24 06:48:08,957 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank3]:[titan] 2025-09-24 06:48:09,088 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-24 06:48:09,089 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank5]:[titan] 2025-09-24 06:48:09,122 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank5]:[titan] 2025-09-24 06:48:09,122 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank1]:[titan] 2025-09-24 06:48:09,197 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-09-24 06:48:09,197 - root - INFO - CUDA memory usage for model: 4.24GiB(5.35%)
[rank2]:[titan] 2025-09-24 06:48:09,180 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-09-24 06:48:09,180 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank0]:[titan] 2025-09-24 06:48:09,197 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-24 06:48:09,197 - root - INFO - CUDA memory usage for model: 4.24GiB(5.35%)
[rank7]:[titan] 2025-09-24 06:48:09,201 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank7]:[titan] 2025-09-24 06:48:09,202 - root - INFO - CUDA memory usage for model: 3.83GiB(4.83%)
[rank6]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank6]:wandb: Tracking run with wandb version 0.22.0
[rank6]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_nofreeze_runpod/20250924-0648/wandb/run-20250924_064809-tw0986tn
[rank6]:wandb: Run `wandb offline` to turn off syncing.
[rank6]:wandb: Syncing run 0922_gpipe_nofreeze_runpod
[rank6]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank6]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/tw0986tn
[rank6]:[titan] 2025-09-24 06:48:10,333 - root - INFO - WandB logging enabled
[rank6]:[titan] 2025-09-24 06:48:10,334 - root - INFO - CUDA capacity: NVIDIA A100-SXM4-80GB with 79.25GiB memory
[rank6]:[titan] 2025-09-24 06:48:10,400 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank6]:[titan] 2025-09-24 06:48:10,435 - root - INFO - Applied FSDP to the model
[rank6]:[titan] 2025-09-24 06:48:10,435 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank6]:[titan] 2025-09-24 06:48:10,663 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank6]:[titan] 2025-09-24 06:48:10,664 - root - INFO - CUDA memory usage for model: 3.83GiB(4.83%)
[rank0]:[titan] 2025-09-24 06:48:10,687 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_nofreeze_runpod
[rank0]:[titan] 2025-09-24 06:48:10,687 - root - INFO - Mixed precision training is handled by fully_shard
[rank0]:[titan] 2025-09-24 06:48:10,691 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 20)
[rank0]:[titan] 2025-09-24 06:48:10,692 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:[titan] 2025-09-24 06:48:25,146 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 31.52GiB(39.77%) [34m tps: 995 [36m tflops: 46.40 [35m mfu: 14.87%[39m
[rank2]:[titan] 2025-09-24 06:48:25,147 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank0]:[titan] 2025-09-24 06:48:25,146 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 35.46GiB(44.74%) [34m tps: 995 [36m tflops: 46.41 [35m mfu: 14.87%[39m
[rank0]:[titan] 2025-09-24 06:48:25,146 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank6]:[titan] 2025-09-24 06:48:25,137 - root - INFO - [31m step:  1 [32m loss: 12.2451 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 38.21GiB(48.22%) [34m tps: 1,110 [36m tflops: 51.76 [35m mfu: 16.59%[39m
[rank6]:[titan] 2025-09-24 06:48:25,137 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank7]:[titan] 2025-09-24 06:48:25,136 - root - INFO - [31m step:  1 [32m loss: 12.2451 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 38.21GiB(48.22%) [34m tps: 996 [36m tflops: 46.46 [35m mfu: 14.89%[39m
[rank7]:[titan] 2025-09-24 06:48:25,137 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank3]:[titan] 2025-09-24 06:48:25,146 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 31.52GiB(39.77%) [34m tps: 994 [36m tflops: 46.34 [35m mfu: 14.85%[39m
[rank3]:[titan] 2025-09-24 06:48:25,147 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank1]:[titan] 2025-09-24 06:48:25,146 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 35.46GiB(44.74%) [34m tps: 995 [36m tflops: 46.43 [35m mfu: 14.88%[39m
[rank1]:[titan] 2025-09-24 06:48:25,147 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank5]:[titan] 2025-09-24 06:48:25,136 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 28.25GiB(35.65%) [34m tps: 996 [36m tflops: 46.45 [35m mfu: 14.89%[39m
[rank5]:[titan] 2025-09-24 06:48:25,136 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank4]:[titan] 2025-09-24 06:48:25,136 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.6778 [38;2;54;234;195m memory: 28.25GiB(35.65%) [34m tps: 990 [36m tflops: 46.18 [35m mfu: 14.80%[39m
[rank4]:[titan] 2025-09-24 06:48:25,136 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
