
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Tue Sep 23 09:04:17 UTC 2025
‚úîÔ∏èSERVER: 85e4aa33f65d (172.18.0.2),  GPUs: 0,1,2,3,4,5,6,7
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod8/0922_main/0922_gpipe_fullrand6.ans
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs
‚úîÔ∏èRunning with fullrand6 x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=8 --local-ranks-filter=0,1,2,3,4,5,6,7 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod8/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
[rank2]:[titan] 2025-09-23 09:04:24,790 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank7]:[titan] 2025-09-23 09:04:24,854 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank4]:[titan] 2025-09-23 09:04:25,138 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank2]:[titan] 2025-09-23 09:04:25,195 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-09-23 09:04:25,199 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank7]:[titan] 2025-09-23 09:04:25,270 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank7]:[titan] 2025-09-23 09:04:25,274 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank6]:[titan] 2025-09-23 09:04:25,338 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-23 09:04:25,457 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank4]:[titan] 2025-09-23 09:04:25,462 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank4]:[titan] 2025-09-23 09:04:25,467 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank3]:[titan] 2025-09-23 09:04:25,568 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank5]:[titan] 2025-09-23 09:04:25,795 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank6]:[titan] 2025-09-23 09:04:25,836 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-23 09:04:25,841 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank1]:[titan] 2025-09-23 09:04:25,929 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-23 09:04:25,966 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-23 09:04:25,969 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 09:04:25,977 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-23 09:04:26,177 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-23 09:04:26,182 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank5]:[titan] 2025-09-23 09:04:26,483 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank5]:[titan] 2025-09-23 09:04:26,497 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank1]:[titan] 2025-09-23 09:04:26,589 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-09-23 09:04:26,594 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 09:04:27,660 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-23 09:04:27,985 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-23 09:04:28,560 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:[titan] 2025-09-23 09:04:28,722 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank0]:[titan] 2025-09-23 09:04:28,724 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank7]:[titan] 2025-09-23 09:04:28,753 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank2]:[titan] 2025-09-23 09:04:28,789 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank2]:[titan] 2025-09-23 09:04:28,822 - root - INFO - Applied FSDP to the model
[rank2]:[titan] 2025-09-23 09:04:28,822 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-23 09:04:28,771 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-23 09:04:28,789 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-23 09:04:28,821 - root - INFO - Applied FSDP to the model
[rank0]:[titan] 2025-09-23 09:04:28,822 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-23 09:04:28,807 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank7]:[titan] 2025-09-23 09:04:28,834 - root - INFO - Applied FSDP to the model
[rank7]:[titan] 2025-09-23 09:04:28,834 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank5]:[titan] 2025-09-23 09:04:28,813 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank3]:[titan] 2025-09-23 09:04:28,947 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank1]:[titan] 2025-09-23 09:04:28,894 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank1]:[titan] 2025-09-23 09:04:28,963 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank5]:[titan] 2025-09-23 09:04:28,884 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank5]:[titan] 2025-09-23 09:04:28,916 - root - INFO - Applied FSDP to the model
[rank5]:[titan] 2025-09-23 09:04:28,916 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-09-23 09:04:29,048 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-09-23 09:04:29,048 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank4]:[titan] 2025-09-23 09:04:28,970 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank4]:[titan] 2025-09-23 09:04:29,052 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank3]:[titan] 2025-09-23 09:04:29,012 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:[titan] 2025-09-23 09:04:29,048 - root - INFO - Applied FSDP to the model
[rank3]:[titan] 2025-09-23 09:04:29,048 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-23 09:04:29,050 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-23 09:04:29,050 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank1]:[titan] 2025-09-23 09:04:28,991 - root - INFO - Applied FSDP to the model
[rank1]:[titan] 2025-09-23 09:04:28,991 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-23 09:04:29,054 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank7]:[titan] 2025-09-23 09:04:29,055 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank4]:[titan] 2025-09-23 09:04:29,091 - root - INFO - Applied FSDP to the model
[rank4]:[titan] 2025-09-23 09:04:29,091 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank5]:[titan] 2025-09-23 09:04:29,110 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank5]:[titan] 2025-09-23 09:04:29,111 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank1]:[titan] 2025-09-23 09:04:29,195 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-09-23 09:04:29,195 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank3]:[titan] 2025-09-23 09:04:29,301 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-23 09:04:29,301 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank4]:[titan] 2025-09-23 09:04:29,330 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank4]:[titan] 2025-09-23 09:04:29,330 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank6]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank6]:wandb: Tracking run with wandb version 0.22.0
[rank6]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250923-0904/wandb/run-20250923_090429-fe91shai
[rank6]:wandb: Run `wandb offline` to turn off syncing.
[rank6]:wandb: Syncing run 0922_gpipe_fullrand6_dm4
[rank6]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank6]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/fe91shai
[rank6]:[titan] 2025-09-23 09:04:30,464 - root - INFO - WandB logging enabled
[rank6]:[titan] 2025-09-23 09:04:30,465 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank6]:[titan] 2025-09-23 09:04:30,497 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank6]:[titan] 2025-09-23 09:04:30,527 - root - INFO - Applied FSDP to the model
[rank6]:[titan] 2025-09-23 09:04:30,527 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-23 09:04:30,747 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-23 09:04:30,747 - root - INFO - Mixed precision training is handled by fully_shard
[rank0]:[titan] 2025-09-23 09:04:30,748 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 10)
[rank0]:[titan] 2025-09-23 09:04:30,749 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank6]:[titan] 2025-09-23 09:04:30,737 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank6]:[titan] 2025-09-23 09:04:30,737 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank0]:[titan] 2025-09-23 09:04:58,333 - root - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:[titan] 2025-09-23 09:04:58,333 - root - INFO - Finished loading the checkpoint in 27.58 seconds.
[rank0]:[titan] 2025-09-23 09:04:58,333 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:[titan] 2025-09-23 09:05:11,214 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 402 [36m tflops: 18.76 [35m mfu: 6.01%[39m
[rank6]:[titan] 2025-09-23 09:05:11,214 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank4]:[titan] 2025-09-23 09:05:11,247 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 388 [36m tflops: 18.10 [35m mfu: 5.80%[39m
[rank4]:[titan] 2025-09-23 09:05:11,247 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:[titan] 2025-09-23 09:05:11,215 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 388 [36m tflops: 18.08 [35m mfu: 5.79%[39m
[rank1]:[titan] 2025-09-23 09:05:11,215 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-23 09:05:11,215 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 386 [36m tflops: 18.00 [35m mfu: 5.77%[39m
[rank0]:[titan] 2025-09-23 09:05:11,215 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank7]:[titan] 2025-09-23 09:05:11,213 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 386 [36m tflops: 18.01 [35m mfu: 5.77%[39m
[rank7]:[titan] 2025-09-23 09:05:11,214 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank5]:[titan] 2025-09-23 09:05:11,247 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 387 [36m tflops: 18.03 [35m mfu: 5.78%[39m
[rank5]:[titan] 2025-09-23 09:05:11,247 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:[titan] 2025-09-23 09:05:11,256 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 386 [36m tflops: 17.99 [35m mfu: 5.76%[39m
[rank2]:[titan] 2025-09-23 09:05:11,257 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-23 09:05:11,256 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 388 [36m tflops: 18.08 [35m mfu: 5.79%[39m
[rank3]:[titan] 2025-09-23 09:05:11,257 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-23 09:06:35,180 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,757 [36m tflops: 81.95 [35m mfu: 26.27%[39m
[rank0]:[titan] 2025-09-23 09:06:35,184 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,756 [36m tflops: 81.90 [35m mfu: 26.25%[39m
[rank4]:[titan] 2025-09-23 09:06:35,177 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,757 [36m tflops: 81.94 [35m mfu: 26.26%[39m
[rank1]:[titan] 2025-09-23 09:06:35,184 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,756 [36m tflops: 81.90 [35m mfu: 26.25%[39m
[rank6]:[titan] 2025-09-23 09:06:35,182 - root - INFO - [31m step: 10 [32m loss: 10.0448 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,756 [36m tflops: 81.91 [35m mfu: 26.25%[39m
[rank7]:[titan] 2025-09-23 09:06:35,182 - root - INFO - [31m step: 10 [32m loss: 10.0448 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,756 [36m tflops: 81.91 [35m mfu: 26.25%[39m
[rank5]:[titan] 2025-09-23 09:06:35,178 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,757 [36m tflops: 81.94 [35m mfu: 26.26%[39m
[rank2]:[titan] 2025-09-23 09:06:35,180 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 166.2299 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,757 [36m tflops: 81.95 [35m mfu: 26.27%[39m
[rank2]:[titan] 2025-09-23 09:08:08,084 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank3]:[titan] 2025-09-23 09:08:08,084 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank1]:[titan] 2025-09-23 09:08:08,089 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank7]:[titan] 2025-09-23 09:08:08,087 - root - INFO - [31m step: 20 [32m loss: 10.8537 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank0]:[titan] 2025-09-23 09:08:08,088 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank4]:[titan] 2025-09-23 09:08:08,083 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank6]:[titan] 2025-09-23 09:08:08,091 - root - INFO - [31m step: 20 [32m loss: 10.8537 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank5]:[titan] 2025-09-23 09:08:08,082 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  9.2308 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,764 [36m tflops: 82.25 [35m mfu: 26.36%[39m
[rank2]:[titan] 2025-09-23 09:09:40,538 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank1]:[titan] 2025-09-23 09:09:40,543 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank3]:[titan] 2025-09-23 09:09:40,538 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank0]:[titan] 2025-09-23 09:09:40,542 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank5]:[titan] 2025-09-23 09:09:40,536 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank7]:[titan] 2025-09-23 09:09:40,541 - root - INFO - [31m step: 30 [32m loss:  7.8963 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank4]:[titan] 2025-09-23 09:09:40,536 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,772 [36m tflops: 82.65 [35m mfu: 26.49%[39m
[rank6]:[titan] 2025-09-23 09:09:40,541 - root - INFO - [31m step: 30 [32m loss:  7.8963 [38;2;180;60;0m grad_norm:  0.8070 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,772 [36m tflops: 82.66 [35m mfu: 26.49%[39m
[rank5]:[titan] 2025-09-23 09:11:13,087 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank7]:[titan] 2025-09-23 09:11:13,092 - root - INFO - [31m step: 40 [32m loss:  8.2507 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank2]:[titan] 2025-09-23 09:11:13,090 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank3]:[titan] 2025-09-23 09:11:13,090 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank4]:[titan] 2025-09-23 09:11:13,087 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank0]:[titan] 2025-09-23 09:11:13,094 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank1]:[titan] 2025-09-23 09:11:13,094 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank6]:[titan] 2025-09-23 09:11:13,093 - root - INFO - [31m step: 40 [32m loss:  8.2507 [38;2;180;60;0m grad_norm:  3.2374 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,770 [36m tflops: 82.57 [35m mfu: 26.46%[39m
[rank0]:[titan] 2025-09-23 09:12:36,622 - root - INFO - [GC] Peforming periodical GC collection 0.32 seconds
[rank2]:[titan] 2025-09-23 09:12:45,894 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank4]:[titan] 2025-09-23 09:12:45,892 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank0]:[titan] 2025-09-23 09:12:45,898 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank1]:[titan] 2025-09-23 09:12:45,898 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank7]:[titan] 2025-09-23 09:12:45,896 - root - INFO - [31m step: 50 [32m loss:  7.5565 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank5]:[titan] 2025-09-23 09:12:45,892 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank6]:[titan] 2025-09-23 09:12:45,897 - root - INFO - [31m step: 50 [32m loss:  7.5565 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
[rank3]:[titan] 2025-09-23 09:12:45,894 - root - INFO - [31m step: 50 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3610 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,765 [36m tflops: 82.34 [35m mfu: 26.39%[39m
