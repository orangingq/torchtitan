
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 10:51:27 UTC 2025
‚úîÔ∏èSERVER: 4530a92f8810 (172.18.0.2),  GPUs: 0,1,2,3,4,5,6,7
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod8/0922_main/0922_gpipe_fullrand6.ans
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs
‚úîÔ∏èRunning with fullrand6 x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=8 --local-ranks-filter=0,6,7 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod8/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
[rank7]:[titan] 2025-09-22 10:51:34,598 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank7]:[titan] 2025-09-22 10:51:34,865 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank7]:[titan] 2025-09-22 10:51:34,869 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank7]:[titan] 2025-09-22 10:51:34,878 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank6]:[titan] 2025-09-22 10:51:36,556 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-22 10:51:36,620 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-22 10:51:37,091 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 10:51:37,095 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-22 10:51:37,105 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank6]:[titan] 2025-09-22 10:51:37,087 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-22 10:51:37,094 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank6]:[titan] 2025-09-22 10:51:37,105 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank6]:[titan] 2025-09-22 10:51:38,281 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 10:51:38,281 - root - INFO - Loading tokenizer from tokenizer.json
[rank7]:[titan] 2025-09-22 10:51:38,281 - root - INFO - Loading tokenizer from tokenizer.json
[rank6]:[titan] 2025-09-22 10:51:38,561 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 10:51:38,544 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank7]:[titan] 2025-09-22 10:51:38,544 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 10:51:39,213 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank6]:[titan] 2025-09-22 10:51:39,199 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank7]:[titan] 2025-09-22 10:51:39,298 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 10:51:39,519 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank0]:[titan] 2025-09-22 10:51:39,578 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 10:51:39,606 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank7]:[titan] 2025-09-22 10:51:39,548 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank7]:[titan] 2025-09-22 10:51:39,613 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 10:51:39,644 - root - INFO - Applied FSDP to the model
[rank0]:[titan] 2025-09-22 10:51:39,644 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-22 10:51:39,643 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank7]:[titan] 2025-09-22 10:51:39,679 - root - INFO - Applied FSDP to the model
[rank7]:[titan] 2025-09-22 10:51:39,679 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 10:51:39,925 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 10:51:39,926 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank7]:[titan] 2025-09-22 10:51:39,930 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank7]:[titan] 2025-09-22 10:51:39,930 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank6]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank6]:wandb: Tracking run with wandb version 0.22.0
[rank6]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250922-1051/wandb/run-20250922_105140-ylbutk3f
[rank6]:wandb: Run `wandb offline` to turn off syncing.
[rank6]:wandb: Syncing run 0922_gpipe_fullrand6_dm4
[rank6]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank6]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/ylbutk3f
[rank6]:[titan] 2025-09-22 10:51:41,130 - root - INFO - WandB logging enabled
[rank6]:[titan] 2025-09-22 10:51:41,131 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank6]:[titan] 2025-09-22 10:51:41,145 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank6]:[titan] 2025-09-22 10:51:41,164 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank6]:[titan] 2025-09-22 10:51:41,193 - root - INFO - Applied FSDP to the model
[rank6]:[titan] 2025-09-22 10:51:41,193 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 10:51:41,416 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-22 10:51:41,417 - root - INFO - Mixed precision training is handled by fully_shard
[rank0]:[titan] 2025-09-22 10:51:41,418 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 10)
[rank0]:[titan] 2025-09-22 10:51:41,420 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank6]:[titan] 2025-09-22 10:51:41,406 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank6]:[titan] 2025-09-22 10:51:41,406 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank6]:[titan] 2025-09-22 10:51:41,417 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank6]:[titan] 2025-09-22 10:51:41,417 - root - INFO - Mixed precision training is handled by fully_shard
[rank6]:[titan] 2025-09-22 10:51:41,418 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 10)
[rank6]:[titan] 2025-09-22 10:51:41,419 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank7]:[titan] 2025-09-22 10:51:41,417 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank7]:[titan] 2025-09-22 10:51:41,417 - root - INFO - Mixed precision training is handled by fully_shard
[rank7]:[titan] 2025-09-22 10:51:41,418 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 10)
[rank7]:[titan] 2025-09-22 10:51:41,419 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 10:51:49,830 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 10:51:49,830 - root - INFO - Finished loading the checkpoint in 8.41 seconds.
[rank0]:[titan] 2025-09-22 10:51:49,831 - root - INFO - Training starts at step 1
[rank7]:[titan] 2025-09-22 10:51:49,829 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank7]:[titan] 2025-09-22 10:51:49,829 - root - INFO - Finished loading the checkpoint in 8.41 seconds.
[rank7]:[titan] 2025-09-22 10:51:49,830 - root - INFO - Training starts at step 1
[rank6]:[titan] 2025-09-22 10:51:49,842 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank6]:[titan] 2025-09-22 10:51:49,842 - root - INFO - Finished loading the checkpoint in 8.42 seconds.
[rank6]:[titan] 2025-09-22 10:51:49,843 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:[titan] 2025-09-22 10:52:02,812 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 756 [36m tflops: 35.27 [35m mfu: 11.30%[39m
[rank6]:[titan] 2025-09-22 10:52:02,813 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 10:52:02,843 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 704 [36m tflops: 32.84 [35m mfu: 10.53%[39m
[rank0]:[titan] 2025-09-22 10:52:02,843 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank7]:[titan] 2025-09-22 10:52:02,811 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1738 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 706 [36m tflops: 32.94 [35m mfu: 10.56%[39m
[rank7]:[titan] 2025-09-22 10:52:02,811 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank6]:[titan] 2025-09-22 10:53:26,806 - root - INFO - [31m step: 10 [32m loss:  6.5835 [38;2;180;60;0m grad_norm: 18.9712 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,756 [36m tflops: 81.88 [35m mfu: 26.24%[39m
[rank0]:[titan] 2025-09-22 10:53:26,808 - root - INFO - [31m step: 10 [32m loss: -8.0000 [38;2;180;60;0m grad_norm: 18.9712 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,756 [36m tflops: 81.91 [35m mfu: 26.25%[39m
[rank7]:[titan] 2025-09-22 10:53:26,806 - root - INFO - [31m step: 10 [32m loss:  6.5835 [38;2;180;60;0m grad_norm: 18.9712 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,756 [36m tflops: 81.88 [35m mfu: 26.24%[39m
[rank0]:[titan] 2025-09-22 10:54:59,249 - root - INFO - [31m step: 20 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  2.2888 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,772 [36m tflops: 82.66 [35m mfu: 26.49%[39m
[rank7]:[titan] 2025-09-22 10:54:59,247 - root - INFO - [31m step: 20 [32m loss:  8.8160 [38;2;180;60;0m grad_norm:  2.2888 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,772 [36m tflops: 82.67 [35m mfu: 26.50%[39m
[rank6]:[titan] 2025-09-22 10:54:59,247 - root - INFO - [31m step: 20 [32m loss:  8.8160 [38;2;180;60;0m grad_norm:  2.2888 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,772 [36m tflops: 82.67 [35m mfu: 26.50%[39m
[rank7]:[titan] 2025-09-22 10:56:31,293 - root - INFO - [31m step: 30 [32m loss:  7.8407 [38;2;180;60;0m grad_norm:  0.7959 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,780 [36m tflops: 83.02 [35m mfu: 26.61%[39m
[rank6]:[titan] 2025-09-22 10:56:31,293 - root - INFO - [31m step: 30 [32m loss:  7.8407 [38;2;180;60;0m grad_norm:  0.7959 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,780 [36m tflops: 83.02 [35m mfu: 26.61%[39m
[rank0]:[titan] 2025-09-22 10:56:31,295 - root - INFO - [31m step: 30 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7959 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,780 [36m tflops: 83.02 [35m mfu: 26.61%[39m
[rank0]:[titan] 2025-09-22 10:58:03,555 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.3849 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,776 [36m tflops: 82.83 [35m mfu: 26.55%[39m
[rank7]:[titan] 2025-09-22 10:58:03,553 - root - INFO - [31m step: 40 [32m loss:  7.7956 [38;2;180;60;0m grad_norm:  1.3849 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,776 [36m tflops: 82.83 [35m mfu: 26.55%[39m
[rank6]:[titan] 2025-09-22 10:58:03,553 - root - INFO - [31m step: 40 [32m loss:  7.7956 [38;2;180;60;0m grad_norm:  1.3849 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,776 [36m tflops: 82.83 [35m mfu: 26.55%[39m
