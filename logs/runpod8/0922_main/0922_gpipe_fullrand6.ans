
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Tue Sep 23 10:16:22 UTC 2025
âœ”ï¸SERVER: 85e4aa33f65d (172.18.0.2),  GPUs: 0,1,2,3,4,5,6,7
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod8/0922_main/0922_gpipe_fullrand6.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs
âœ”ï¸Running with fullrand6 x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=8 --local-ranks-filter=0,1,2,3,4,5,6,7 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod8/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:[titan] 2025-09-23 10:16:29,715 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank6]:[titan] 2025-09-23 10:16:29,830 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank2]:[titan] 2025-09-23 10:16:30,024 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-09-23 10:16:30,029 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank1]:[titan] 2025-09-23 10:16:30,192 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank6]:[titan] 2025-09-23 10:16:30,182 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-23 10:16:30,186 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank4]:[titan] 2025-09-23 10:16:30,178 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-23 10:16:30,271 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank7]:[titan] 2025-09-23 10:16:30,357 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank5]:[titan] 2025-09-23 10:16:30,315 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank3]:[titan] 2025-09-23 10:16:30,466 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank1]:[titan] 2025-09-23 10:16:30,869 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-09-23 10:16:30,873 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank4]:[titan] 2025-09-23 10:16:30,930 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank4]:[titan] 2025-09-23 10:16:30,935 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank5]:[titan] 2025-09-23 10:16:31,276 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank5]:[titan] 2025-09-23 10:16:31,281 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 10:16:31,223 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-23 10:16:31,230 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 10:16:31,240 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank7]:[titan] 2025-09-23 10:16:31,339 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank7]:[titan] 2025-09-23 10:16:31,343 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank3]:[titan] 2025-09-23 10:16:31,438 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-23 10:16:31,443 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 10:16:32,507 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-23 10:16:32,872 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-23 10:16:33,592 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:[titan] 2025-09-23 10:16:33,640 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank7]:[titan] 2025-09-23 10:16:33,622 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank7]:[titan] 2025-09-23 10:16:33,694 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-23 10:16:33,643 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank2]:[titan] 2025-09-23 10:16:33,712 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank2]:[titan] 2025-09-23 10:16:33,738 - root - INFO - Applied FSDP to the model
[rank2]:[titan] 2025-09-23 10:16:33,738 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-23 10:16:33,726 - root - INFO - Applied FSDP to the model
[rank7]:[titan] 2025-09-23 10:16:33,727 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-23 10:16:33,712 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:[titan] 2025-09-23 10:16:33,740 - root - INFO - Applied FSDP to the model
[rank3]:[titan] 2025-09-23 10:16:33,740 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-09-23 10:16:33,812 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank1]:[titan] 2025-09-23 10:16:33,873 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank5]:[titan] 2025-09-23 10:16:33,858 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank0]:[titan] 2025-09-23 10:16:33,902 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank4]:[titan] 2025-09-23 10:16:33,900 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank1]:[titan] 2025-09-23 10:16:33,904 - root - INFO - Applied FSDP to the model
[rank1]:[titan] 2025-09-23 10:16:33,904 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-09-23 10:16:33,965 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-09-23 10:16:33,965 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank7]:[titan] 2025-09-23 10:16:33,956 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank7]:[titan] 2025-09-23 10:16:33,957 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank5]:[titan] 2025-09-23 10:16:33,929 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank5]:[titan] 2025-09-23 10:16:33,968 - root - INFO - Applied FSDP to the model
[rank5]:[titan] 2025-09-23 10:16:33,968 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-23 10:16:33,965 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-23 10:16:33,965 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank0]:[titan] 2025-09-23 10:16:33,956 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-23 10:16:33,974 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-23 10:16:34,003 - root - INFO - Applied FSDP to the model
[rank0]:[titan] 2025-09-23 10:16:34,003 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank4]:[titan] 2025-09-23 10:16:33,975 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank4]:[titan] 2025-09-23 10:16:34,002 - root - INFO - Applied FSDP to the model
[rank4]:[titan] 2025-09-23 10:16:34,003 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-09-23 10:16:34,158 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-09-23 10:16:34,158 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank5]:[titan] 2025-09-23 10:16:34,210 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank5]:[titan] 2025-09-23 10:16:34,210 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank0]:[titan] 2025-09-23 10:16:34,224 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-23 10:16:34,225 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank4]:[titan] 2025-09-23 10:16:34,214 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank4]:[titan] 2025-09-23 10:16:34,215 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank6]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank6]:wandb: Tracking run with wandb version 0.22.0
[rank6]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250923-1016/wandb/run-20250923_101634-cc9p1o8r
[rank6]:wandb: Run `wandb offline` to turn off syncing.
[rank6]:wandb: Syncing run 0922_gpipe_fullrand6_dm4
[rank6]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank6]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/cc9p1o8r
[rank6]:[titan] 2025-09-23 10:16:35,392 - root - INFO - WandB logging enabled
[rank6]:[titan] 2025-09-23 10:16:35,393 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank6]:[titan] 2025-09-23 10:16:35,426 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank6]:[titan] 2025-09-23 10:16:35,455 - root - INFO - Applied FSDP to the model
[rank6]:[titan] 2025-09-23 10:16:35,455 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-23 10:16:35,760 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-23 10:16:35,760 - root - INFO - Mixed precision training is handled by fully_shard
[rank0]:[titan] 2025-09-23 10:16:35,761 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 20)
[rank0]:[titan] 2025-09-23 10:16:35,763 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank6]:[titan] 2025-09-23 10:16:35,748 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank6]:[titan] 2025-09-23 10:16:35,748 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank0]:[titan] 2025-09-23 10:17:04,440 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-23 10:17:04,440 - root - INFO - Finished loading the checkpoint in 28.68 seconds.
[rank0]:[titan] 2025-09-23 10:17:04,440 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:[titan] 2025-09-23 10:17:17,197 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 392 [36m tflops: 18.29 [35m mfu: 5.86%[39m
[rank6]:[titan] 2025-09-23 10:17:17,197 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank1]:[titan] 2025-09-23 10:17:17,202 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 378 [36m tflops: 17.63 [35m mfu: 5.65%[39m
[rank1]:[titan] 2025-09-23 10:17:17,202 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank2]:[titan] 2025-09-23 10:17:17,201 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 377 [36m tflops: 17.56 [35m mfu: 5.63%[39m
[rank2]:[titan] 2025-09-23 10:17:17,201 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank5]:[titan] 2025-09-23 10:17:17,232 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 378 [36m tflops: 17.63 [35m mfu: 5.65%[39m
[rank5]:[titan] 2025-09-23 10:17:17,232 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank7]:[titan] 2025-09-23 10:17:17,197 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 376 [36m tflops: 17.56 [35m mfu: 5.63%[39m
[rank7]:[titan] 2025-09-23 10:17:17,197 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank3]:[titan] 2025-09-23 10:17:17,201 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 377 [36m tflops: 17.56 [35m mfu: 5.63%[39m
[rank3]:[titan] 2025-09-23 10:17:17,202 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank0]:[titan] 2025-09-23 10:17:17,202 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 379 [36m tflops: 17.67 [35m mfu: 5.66%[39m
[rank0]:[titan] 2025-09-23 10:17:17,202 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank4]:[titan] 2025-09-23 10:17:17,232 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1739 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 379 [36m tflops: 17.66 [35m mfu: 5.66%[39m
[rank4]:[titan] 2025-09-23 10:17:17,232 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank6]:[titan] 2025-09-23 10:19:28,688 - root - INFO - > Batch Time: 714.05 ms (Average Freeze Ratio: 0.78, Time Reduction Rate: 0.36)
[rank6]:[titan] 2025-09-23 10:19:28,692 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 10:19:28,636 - root - INFO - [Step 120] Setting Upperbound
[rank4]:[titan] 2025-09-23 10:19:28,717 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.26/0.65, [MB1] 0.30/0.75, [MB2] 0.01/0.02, [MB3] 0.39/0.97, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.00/0.00, [MB7] 0.40/1.00
[rank7]:[titan] 2025-09-23 10:19:28,706 - root - INFO - > Batch Time: 712.97 ms (Average Freeze Ratio: 0.78, Time Reduction Rate: 0.36)
[rank7]:[titan] 2025-09-23 10:19:28,709 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank3]:[titan] 2025-09-23 10:19:28,714 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.32/0.81, [MB1] 0.10/0.25, [MB2] 0.07/0.17, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank5]:[titan] 2025-09-23 10:19:28,699 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.27/0.66, [MB1] 0.31/0.77, [MB2] 0.02/0.06, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank1]:[titan] 2025-09-23 10:19:28,708 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.04/0.11, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 10:19:28,701 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.33/0.81, [MB1] 0.11/0.27, [MB2] 0.11/0.27, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 10:19:28,710 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.06/0.15, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:[titan] 2025-09-23 10:20:12,649 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank7]:[titan] 2025-09-23 10:20:12,650 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank4]:[titan] 2025-09-23 10:20:12,734 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.26/0.65, [MB1] 0.30/0.75, [MB2] 0.01/0.02, [MB3] 0.39/0.97, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.00/0.00, [MB7] 0.40/1.00
[rank5]:[titan] 2025-09-23 10:20:12,734 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.27/0.66, [MB1] 0.31/0.77, [MB2] 0.02/0.06, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 10:20:12,827 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.33/0.81, [MB1] 0.11/0.27, [MB2] 0.11/0.27, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank3]:[titan] 2025-09-23 10:20:12,827 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.32/0.81, [MB1] 0.10/0.25, [MB2] 0.07/0.17, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank1]:[titan] 2025-09-23 10:20:12,925 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.04/0.11, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 10:20:12,922 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.06/0.15, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank7]:/workspace/torchtitan/timelyfreeze/core/schedule.py:374: RankWarning: Polyfit may be poorly conditioned
[rank7]:  a, b = np.polyfit(monitored_values_dict[stage][0], monitored_values_dict[stage][1], 1)
[rank6]:/workspace/torchtitan/timelyfreeze/core/schedule.py:374: RankWarning: Polyfit may be poorly conditioned
[rank6]:  a, b = np.polyfit(monitored_values_dict[stage][0], monitored_values_dict[stage][1], 1)
[rank4]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 10:20:57,176 - root - INFO - > Batch Time: 869.75 ms (Average Freeze Ratio: 0.00)
[rank7]:[titan] 2025-09-23 10:20:57,176 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 10:20:57,177 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 10:20:57,174 - root - INFO - > Batch Time: 866.71 ms (Average Freeze Ratio: 0.00)
[rank6]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 10:20:57,176 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 10:20:57,174 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 10:20:57,178 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 10:20:57,178 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank3]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank3]:[titan] 2025-09-23 10:20:57,176 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 10:20:57,174 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 10:20:57,175 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 10:20:57,176 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 10:21:43,575 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 10:21:43,576 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 10:21:43,665 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 10:21:43,666 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 10:21:43,765 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank3]:[titan] 2025-09-23 10:21:43,765 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 10:21:43,869 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 10:21:43,870 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 10:23:17,300 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,775 [36m tflops: 82.77 [35m mfu: 26.53%[39m
[rank6]:[titan] 2025-09-23 10:23:17,305 - root - INFO - [31m step: 40 [32m loss:  4.9237 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank0]:[titan] 2025-09-23 10:23:17,307 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank5]:[titan] 2025-09-23 10:23:17,300 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,775 [36m tflops: 82.77 [35m mfu: 26.53%[39m
[rank3]:[titan] 2025-09-23 10:23:17,303 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank2]:[titan] 2025-09-23 10:23:17,303 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank1]:[titan] 2025-09-23 10:23:17,307 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank7]:[titan] 2025-09-23 10:23:17,305 - root - INFO - [31m step: 40 [32m loss:  4.9237 [38;2;180;60;0m grad_norm:  7.7683 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,774 [36m tflops: 82.76 [35m mfu: 26.53%[39m
[rank0]:[titan] 2025-09-23 10:24:41,617 - root - INFO - [GC] Peforming periodical GC collection 0.32 seconds
[rank3]:[titan] 2025-09-23 10:24:50,987 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.33/0.82, [MB7] 0.38/0.95
[rank5]:[titan] 2025-09-23 10:24:50,987 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.24/0.61, [MB2] 0.09/0.22, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 10:24:50,985 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 10:24:50,986 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.32/0.79, [MB7] 0.38/0.95
[rank4]:[titan] 2025-09-23 10:24:50,986 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.20/0.51, [MB2] 0.01/0.02, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 10:24:50,986 - root - INFO - > Batch Time: 900.46 ms (Average Freeze Ratio: 0.78)
[rank7]:[titan] 2025-09-23 10:24:50,987 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.29/0.72, [MB7] 0.30/0.74
[rank1]:[titan] 2025-09-23 10:24:50,986 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:[titan] 2025-09-23 10:24:50,986 - root - INFO - > Batch Time: 902.76 ms (Average Freeze Ratio: 0.77)
[rank6]:[titan] 2025-09-23 10:24:50,986 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.32/0.79, [MB7] 0.20/0.50
[rank4]:[titan] 2025-09-23 10:28:31,377 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.25/0.62, [MB2] 0.25/0.62, [MB3] 0.24/0.60, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.10/0.25
[rank7]:[titan] 2025-09-23 10:28:31,377 - root - INFO - > Batch Time: 836.63 ms (Average Freeze Ratio: 0.83)
[rank7]:[titan] 2025-09-23 10:28:31,378 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.37/0.92
[rank1]:[titan] 2025-09-23 10:28:31,378 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.26/0.66, [MB1] 0.13/0.31, [MB2] 0.40/1.00, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.39/0.97, [MB6] 0.26/0.66, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 10:28:31,376 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.26/0.65, [MB1] 0.12/0.30, [MB2] 0.40/1.00, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.38/0.94, [MB6] 0.26/0.65, [MB7] 0.40/1.00
[rank5]:[titan] 2025-09-23 10:28:31,377 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.35/0.89, [MB2] 0.31/0.77, [MB3] 0.26/0.66, [MB4] 0.22/0.55, [MB5] 0.17/0.43, [MB6] 0.13/0.32, [MB7] 0.11/0.29
[rank3]:[titan] 2025-09-23 10:28:31,378 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 10:28:31,377 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:[titan] 2025-09-23 10:28:31,377 - root - INFO - > Batch Time: 838.14 ms (Average Freeze Ratio: 0.73)
[rank6]:[titan] 2025-09-23 10:28:31,377 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.04/0.11, [MB7] 0.22/0.56
[rank4]:[titan] 2025-09-23 10:29:15,435 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank7]:[titan] 2025-09-23 10:29:15,440 - root - INFO - [31m step: 80 [32m loss:  7.7486 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank1]:[titan] 2025-09-23 10:29:15,442 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 44.64GiB(56.41%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank5]:[titan] 2025-09-23 10:29:15,435 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank3]:[titan] 2025-09-23 10:29:15,438 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 39.13GiB(49.45%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank0]:[titan] 2025-09-23 10:29:15,442 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 44.64GiB(56.41%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank6]:[titan] 2025-09-23 10:29:15,440 - root - INFO - [31m step: 80 [32m loss:  7.7486 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank2]:[titan] 2025-09-23 10:29:15,438 - root - INFO - [31m step: 80 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.3773 [38;2;54;234;195m memory: 39.13GiB(49.45%) [34m tps: 1,830 [36m tflops: 85.35 [35m mfu: 27.36%[39m
[rank6]:[rank6]:[E923 11:22:02.942077927 ProcessGroupNCCL.cpp:683] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000003 milliseconds before timing out.
[rank6]:[rank6]:[E923 11:22:02.942221031 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 1(mesh_pp) Rank 3]  failure detected by watchdog at work sequence id: 12675 PG status: last enqueued work: 104, last completed work: 12674
[rank6]:[rank6]:[E923 11:22:02.942861786 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank6]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank6]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank6]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank6]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank6]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank6]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:768
[rank6]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank6]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank6]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank6]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank6]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank6]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank6]:#12 _run_code from <frozen runpy>:88
[rank6]:#13 _run_module_as_main from <frozen runpy>:198
[rank6]:
[rank6]:[rank6]:[E923 11:22:02.942955677 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 1(mesh_pp) Rank 3] First PG on this rank to signal dumping.
[rank6]:[rank6]:[E923 11:22:02.955971484 ProcessGroupNCCL.cpp:683] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12776, OpType=_REDUCE_SCATTER_BASE, NumelIn=525340672, NumelOut=262670336, Timeout(ms)=3000000) ran for 3000018 milliseconds before timing out.
[rank6]:[rank6]:[E923 11:22:02.956076322 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 6(mesh_dp_shard) Rank 0]  failure detected by watchdog at work sequence id: 12776 PG status: last enqueued work: 12777, last completed work: 12775
[rank6]:[rank6]:[E923 11:22:02.956448121 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank6]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank6]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank6]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank6]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank6]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank6]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank6]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank6]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank6]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank6]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank6]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank6]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank6]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank6]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank6]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank6]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank6]:#16 _run_code from <frozen runpy>:88
[rank6]:#17 _run_module_as_main from <frozen runpy>:198
[rank6]:
[rank4]:[rank4]:[E923 11:22:02.980110822 ProcessGroupNCCL.cpp:683] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000038 milliseconds before timing out.
[rank4]:[rank4]:[E923 11:22:02.980251153 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 1(mesh_pp) Rank 2]  failure detected by watchdog at work sequence id: 25348 PG status: last enqueued work: 25349, last completed work: 25347
[rank4]:[rank4]:[E923 11:22:02.980901532 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank4]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank4]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank4]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank4]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank4]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank4]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:756
[rank4]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank4]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank4]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank4]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank4]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank4]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank4]:#12 _run_code from <frozen runpy>:88
[rank4]:#13 _run_module_as_main from <frozen runpy>:198
[rank4]:
[rank4]:[rank4]:[E923 11:22:02.980998938 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 1(mesh_pp) Rank 2] First PG on this rank to signal dumping.
[rank7]:[rank7]:[E923 11:22:02.032197557 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12777, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=3000000) ran for 3000088 milliseconds before timing out.
[rank7]:[rank7]:[E923 11:22:02.032349095 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 6(mesh_dp_shard) Rank 1]  failure detected by watchdog at work sequence id: 12777 PG status: last enqueued work: 12777, last completed work: 12776
[rank7]:[rank7]:[E923 11:22:02.032981998 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank7]:#0 __call__ from /usr/local/lib/python3.11/dist-packages/torch/_ops.py:1255
[rank7]:#1 all_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/_functional_collectives.py:174
[rank7]:#2 _reduce_value from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/placement_types.py:684
[rank7]:#3 _reduce_value from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/_ops/_math_ops.py:127
[rank7]:#4 redistribute_local_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/_redistribute.py:206
[rank7]:#5 forward from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/_redistribute.py:317
[rank7]:#6 apply from /usr/local/lib/python3.11/dist-packages/torch/autograd/function.py:581
[rank7]:#7 redistribute from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/_api.py:541
[rank7]:#8 full_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/tensor/_api.py:571
[rank7]:#9 clip_grad_norm_ from /workspace/torchtitan/torchtitan/distributed/utils.py:373
[rank7]:#10 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank7]:#11 train_step from /workspace/torchtitan/timelyfreeze/train.py:488
[rank7]:#12 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank7]:#13 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank7]:#14 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank7]:#15 _run_code from <frozen runpy>:88
[rank7]:#16 _run_module_as_main from <frozen runpy>:198
[rank7]:
[rank7]:[rank7]:[E923 11:22:02.033071762 ProcessGroupNCCL.cpp:2573] [PG ID 2 PG GUID 6(mesh_dp_shard) Rank 1] First PG on this rank to signal dumping.
[rank7]:[rank7]:[E923 11:22:02.034509695 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=104, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=3000000) ran for 3000090 milliseconds before timing out.
[rank7]:[rank7]:[E923 11:22:02.034585059 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 2(mesh_pp) Rank 3]  failure detected by watchdog at work sequence id: 104 PG status: last enqueued work: 104, last completed work: 12675
[rank7]:[rank7]:[E923 11:22:02.035058890 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank7]:#0 all_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2925
[rank7]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank7]:#2 clip_grad_norm_ from /workspace/torchtitan/torchtitan/distributed/utils.py:382
[rank7]:#3 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank7]:#4 train_step from /workspace/torchtitan/timelyfreeze/train.py:488
[rank7]:#5 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank7]:#6 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank7]:#7 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank7]:#8 _run_code from <frozen runpy>:88
[rank7]:#9 _run_module_as_main from <frozen runpy>:198
[rank7]:
[rank3]:[rank3]:[E923 11:22:02.031543434 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000034 milliseconds before timing out.
[rank3]:[rank3]:[E923 11:22:02.031680068 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 2(mesh_pp) Rank 1]  failure detected by watchdog at work sequence id: 25348 PG status: last enqueued work: 25349, last completed work: 25347
[rank3]:[rank3]:[E923 11:22:02.032434673 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank3]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank3]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank3]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank3]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank3]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank3]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:756
[rank3]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank3]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank3]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank3]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank3]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank3]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank3]:#12 _run_code from <frozen runpy>:88
[rank3]:#13 _run_module_as_main from <frozen runpy>:198
[rank3]:
[rank3]:[rank3]:[E923 11:22:02.032530468 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 2(mesh_pp) Rank 1] First PG on this rank to signal dumping.
[rank4]:[rank4]:[E923 11:22:02.019390915 ProcessGroupNCCL.cpp:683] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12769, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000022 milliseconds before timing out.
[rank4]:[rank4]:[E923 11:22:02.019487360 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 5(mesh_dp_shard) Rank 0]  failure detected by watchdog at work sequence id: 12769 PG status: last enqueued work: 12776, last completed work: 12768
[rank4]:[rank4]:[E923 11:22:02.019984297 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank4]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank4]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank4]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank4]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank4]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank4]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank4]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank4]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank4]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank4]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank4]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank4]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank4]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank4]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank4]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank4]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank4]:#16 _run_code from <frozen runpy>:88
[rank4]:#17 _run_module_as_main from <frozen runpy>:198
[rank4]:
[rank2]:[rank2]:[E923 11:22:02.056267802 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000056 milliseconds before timing out.
[rank2]:[rank2]:[E923 11:22:02.056368242 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 1(mesh_pp) Rank 1]  failure detected by watchdog at work sequence id: 25348 PG status: last enqueued work: 25349, last completed work: 25347
[rank2]:[rank2]:[E923 11:22:02.057007924 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank2]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank2]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank2]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank2]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank2]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank2]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:756
[rank2]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank2]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank2]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank2]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank2]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank2]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank2]:#12 _run_code from <frozen runpy>:88
[rank2]:#13 _run_module_as_main from <frozen runpy>:198
[rank2]:
[rank2]:[rank2]:[E923 11:22:02.057086564 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 1(mesh_pp) Rank 1] First PG on this rank to signal dumping.
[rank2]:[rank2]:[E923 11:22:02.084780048 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=14352, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000027 milliseconds before timing out.
[rank2]:[rank2]:[E923 11:22:02.084857524 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 4(mesh_dp_shard) Rank 0]  failure detected by watchdog at work sequence id: 14352 PG status: last enqueued work: 14360, last completed work: 14351
[rank2]:[rank2]:[E923 11:22:02.085211717 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank2]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank2]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank2]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank2]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank2]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank2]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank2]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank2]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank2]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank2]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank2]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank2]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank2]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank2]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank2]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank2]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank2]:#16 _run_code from <frozen runpy>:88
[rank2]:#17 _run_module_as_main from <frozen runpy>:198
[rank2]:
[rank1]:[rank1]:[E923 11:22:02.080413550 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000043 milliseconds before timing out.
[rank1]:[rank1]:[E923 11:22:02.080529234 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 2(mesh_pp) Rank 0]  failure detected by watchdog at work sequence id: 12675 PG status: last enqueued work: 12675, last completed work: 12674
[rank1]:[rank1]:[E923 11:22:02.081192071 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank1]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank1]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank1]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank1]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank1]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank1]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:756
[rank1]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank1]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:441
[rank1]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank1]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank1]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank1]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank1]:#12 _run_code from <frozen runpy>:88
[rank1]:#13 _run_module_as_main from <frozen runpy>:198
[rank1]:
[rank1]:[rank1]:[E923 11:22:02.081276518 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 2(mesh_pp) Rank 0] First PG on this rank to signal dumping.
[rank1]:[rank1]:[E923 11:22:02.146157727 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=14344, OpType=_REDUCE_SCATTER_BASE, NumelIn=525336576, NumelOut=262668288, Timeout(ms)=3000000) ran for 3000053 milliseconds before timing out.
[rank1]:[rank1]:[E923 11:22:02.146261613 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 3(mesh_dp_shard) Rank 1]  failure detected by watchdog at work sequence id: 14344 PG status: last enqueued work: 14352, last completed work: 14343
[rank1]:[rank1]:[E923 11:22:02.146752091 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank1]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank1]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank1]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank1]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank1]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank1]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank1]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank1]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank1]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank1]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank1]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank1]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:441
[rank1]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank1]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank1]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank1]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank1]:#16 _run_code from <frozen runpy>:88
[rank1]:#17 _run_module_as_main from <frozen runpy>:198
[rank1]:
[rank5]:[rank5]:[E923 11:22:02.091144220 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12769, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000097 milliseconds before timing out.
[rank5]:[rank5]:[E923 11:22:02.091281796 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 5(mesh_dp_shard) Rank 1]  failure detected by watchdog at work sequence id: 12769 PG status: last enqueued work: 12776, last completed work: 12768
[rank5]:[rank5]:[E923 11:22:02.092051355 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank5]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank5]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank5]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank5]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank5]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank5]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank5]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank5]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank5]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank5]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank5]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank5]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank5]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank5]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank5]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank5]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank5]:#16 _run_code from <frozen runpy>:88
[rank5]:#17 _run_module_as_main from <frozen runpy>:198
[rank5]:
[rank5]:[rank5]:[E923 11:22:02.092157073 ProcessGroupNCCL.cpp:2573] [PG ID 2 PG GUID 5(mesh_dp_shard) Rank 1] First PG on this rank to signal dumping.
[rank5]:[rank5]:[E923 11:22:02.096211062 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25349, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000096 milliseconds before timing out.
[rank5]:[rank5]:[E923 11:22:02.096295358 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 2(mesh_pp) Rank 2]  failure detected by watchdog at work sequence id: 25349 PG status: last enqueued work: 25349, last completed work: 25348
[rank5]:[rank5]:[E923 11:22:02.096605444 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank5]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank5]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank5]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank5]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank5]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank5]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:768
[rank5]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank5]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank5]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank5]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank5]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank5]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank5]:#12 _run_code from <frozen runpy>:88
[rank5]:#13 _run_module_as_main from <frozen runpy>:198
[rank5]:
[rank3]:[rank3]:[E923 11:22:02.137653316 ProcessGroupNCCL.cpp:683] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=14352, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000083 milliseconds before timing out.
[rank3]:[rank3]:[E923 11:22:02.137737772 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 4(mesh_dp_shard) Rank 1]  failure detected by watchdog at work sequence id: 14352 PG status: last enqueued work: 14360, last completed work: 14351
[rank3]:[rank3]:[E923 11:22:02.138171585 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank3]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank3]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank3]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank3]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank3]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank3]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank3]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank3]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank3]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank3]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank3]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank3]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:445
[rank3]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank3]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank3]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank3]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank3]:#16 _run_code from <frozen runpy>:88
[rank3]:#17 _run_module_as_main from <frozen runpy>:198
[rank3]:
[rank0]:[rank0]:[E923 11:22:02.119603596 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000080 milliseconds before timing out.
[rank0]:[rank0]:[E923 11:22:02.119606040 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=14344, OpType=_REDUCE_SCATTER_BASE, NumelIn=525336576, NumelOut=262668288, Timeout(ms)=3000000) ran for 3000021 milliseconds before timing out.
[rank0]:[rank0]:[E923 11:22:02.120691182 ProcessGroupNCCL.cpp:2241] [PG ID 2 PG GUID 3(mesh_dp_shard) Rank 0]  failure detected by watchdog at work sequence id: 14344 PG status: last enqueued work: 14352, last completed work: 14343
[rank0]:[rank0]:[E923 11:22:02.120694517 ProcessGroupNCCL.cpp:2241] [PG ID 1 PG GUID 1(mesh_pp) Rank 0]  failure detected by watchdog at work sequence id: 12675 PG status: last enqueued work: 12675, last completed work: 12674
[rank0]:[rank0]:[E923 11:22:02.121313578 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank0]:#0 _coalescing_manager from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2642
[rank0]:#1 __exit__ from /usr/lib/python3.11/contextlib.py:144
[rank0]:#2 batch_isend_irecv from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:2758
[rank0]:#3 _batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:490
[rank0]:#4 _sorted_batch_p2p from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:515
[rank0]:#5 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:756
[rank0]:#6 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank0]:#7 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:441
[rank0]:#8 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank0]:#9 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank0]:#10 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank0]:#11 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank0]:#12 _run_code from <frozen runpy>:88
[rank0]:#13 _run_module_as_main from <frozen runpy>:198
[rank0]:
[rank0]:[rank0]:[E923 11:22:02.121382962 ProcessGroupNCCL.cpp:2573] [PG ID 1 PG GUID 1(mesh_pp) Rank 0] First PG on this rank to signal dumping.
[rank0]:[rank0]:[E923 11:22:02.121468691 ProcessGroupNCCL.cpp:727] Stack trace of the failed collective: 
[rank0]:#0 reduce_scatter_tensor from /usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py:4507
[rank0]:#1 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py:81
[rank0]:#2 __call__ from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:125
[rank0]:#3 foreach_reduce from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py:538
[rank0]:#4 decorate_context from /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:120
[rank0]:#5 post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py:544
[rank0]:#6 run_post_backward from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:660
[rank0]:#7 backward_maybe_with_nosync from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:667
[rank0]:#8 backward_one_chunk from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py:817
[rank0]:#9 _step_microbatches from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:761
[rank0]:#10 step from /usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py:620
[rank0]:#11 forward_backward_step from /workspace/torchtitan/timelyfreeze/train.py:441
[rank0]:#12 train_step from /workspace/torchtitan/timelyfreeze/train.py:485
[rank0]:#13 train from /workspace/torchtitan/timelyfreeze/train.py:574
[rank0]:#14 wrapper from /usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:357
[rank0]:#15 <module> from /workspace/torchtitan/timelyfreeze/train.py:727
[rank0]:#16 _run_code from <frozen runpy>:88
[rank0]:#17 _run_module_as_main from <frozen runpy>:198
[rank0]:
[rank6]:[rank6]:[E923 11:22:02.239460675 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 6] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank6]:[rank6]:[E923 11:22:02.239717241 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 6] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank6]:[rank6]:[W923 11:22:03.541781242 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank3]:[rank3]:[E923 11:22:03.573834544 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 3] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank3]:[rank3]:[E923 11:22:03.574071470 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 3] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank4]:[rank4]:[E923 11:22:03.574078631 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 4] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank4]:[rank4]:[E923 11:22:03.574337791 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 4] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[rank2]:[E923 11:22:03.573977148 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 2] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank2]:[rank2]:[E923 11:22:03.574208866 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 2] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[rank1]:[E923 11:22:03.574142216 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[rank1]:[E923 11:22:03.574366845 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank7]:[rank7]:[E923 11:22:03.573708825 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 7] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank7]:[rank7]:[E923 11:22:03.573955105 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 7] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank5]:[rank5]:[E923 11:22:03.573896347 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 5] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank5]:[rank5]:[E923 11:22:03.574154204 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 5] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[rank0]:[E923 11:22:03.593519402 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 9, last completed NCCL work: 9.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[rank0]:[E923 11:22:03.593725712 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank2]:[rank2]:[W923 11:22:03.882214394 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank7]:[rank7]:[W923 11:22:03.878933398 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank3]:[rank3]:[W923 11:22:03.883017752 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank0]:[rank0]:[W923 11:22:03.903828304 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank4]:[rank4]:[W923 11:22:03.943025451 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank1]:[rank1]:[W923 11:22:03.989358521 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank5]:[rank5]:[W923 11:22:03.993985851 Utils.hpp:112] Warning: Environment variable TORCH_NCCL_DEBUG_INFO_TEMP_FILE is deprecated; use TORCH_FR_DUMP_TEMP_FILE instead (function operator())
[rank6]:[rank6]:[E923 11:23:02.943057143 ProcessGroupNCCL.cpp:744] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[rank6]:[E923 11:23:02.943155090 ProcessGroupNCCL.cpp:758] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[rank6]:[E923 11:23:02.943869345 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 1(mesh_pp) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000003 milliseconds before timing out.
[rank6]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank6]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fe5f597a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank6]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fe598212357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fe598216f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fe59821818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #4: <unknown function> + 0xdc253 (0x7fe5f6c86253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank6]:frame #5: <unknown function> + 0x94ac3 (0x7fe5f91ceac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:frame #6: clone + 0x44 (0x7fe5f925fa04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:
[rank6]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank6]:  what():  [PG ID 1 PG GUID 1(mesh_pp) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000003 milliseconds before timing out.
[rank6]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank6]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fe5f597a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank6]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fe598212357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fe598216f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fe59821818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #4: <unknown function> + 0xdc253 (0x7fe5f6c86253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank6]:frame #5: <unknown function> + 0x94ac3 (0x7fe5f91ceac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:frame #6: clone + 0x44 (0x7fe5f925fa04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:
[rank6]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank6]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fe5f597a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank6]:frame #1: <unknown function> + 0xe86bb1 (0x7fe5981edbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #2: <unknown function> + 0x954001 (0x7fe597cbb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank6]:frame #3: <unknown function> + 0xdc253 (0x7fe5f6c86253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank6]:frame #4: <unknown function> + 0x94ac3 (0x7fe5f91ceac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:frame #5: clone + 0x44 (0x7fe5f925fa04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank6]:
[rank6]:Fatal Python error: Aborted
[rank6]:
[rank6]:Thread 0x00007fe373fff640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe3888dd640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe3898df640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe3890de640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe38a0e0640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe394c66640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe395467640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe396469640 (most recent call first):
[rank6]:  <no Python frame>
[rank6]:
[rank6]:Thread 0x00007fe3d5961640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank6]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank6]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank6]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe401fff640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank6]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 266 in _loop_check_status
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 335 in check_internal_messages
[rank6]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe424b35640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank6]:  File "/usr/lib/python3.11/concurrent/futures/_base.py", line 451 in result
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/asyncio_manager.py", line 136 in run
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/response_handle.py", line 71 in wait_or
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/mailbox_handle.py", line 127 in wait_or
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 250 in _loop_check_status
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 283 in check_network_status
[rank6]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe425336640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank6]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 266 in _loop_check_status
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py", line 308 in check_stop_status
[rank6]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe4657fa640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/selectors.py", line 468 in select
[rank6]:  File "/usr/lib/python3.11/asyncio/base_events.py", line 1898 in _run_once
[rank6]:  File "/usr/lib/python3.11/asyncio/base_events.py", line 608 in run_forever
[rank6]:  File "/usr/lib/python3.11/asyncio/base_events.py", line 641 in run_until_complete
[rank6]:  File "/usr/lib/python3.11/asyncio/runners.py", line 118 in run
[rank6]:  File "/usr/lib/python3.11/asyncio/runners.py", line 190 in run
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 75 in run
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/asyncio_manager.py", line 233 in _main
[rank6]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe425b37640 (most recent call first):
[rank6]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank6]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank6]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank6]:
[rank6]:Thread 0x00007fe5f9139000 (most recent call first):
[rank6]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 408 in batch_generator
[rank6]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 484 in train_step
[rank6]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank6]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank6]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank6]:  File "<frozen runpy>", line 88 in _run_code
[rank6]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank6]:
[rank6]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151)
[rank7]:[rank7]:[E923 11:23:02.033170274 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[rank7]:[E923 11:23:02.033241921 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[rank7]:[E923 11:23:02.033959061 ProcessGroupNCCL.cpp:2057] [PG ID 2 PG GUID 6(mesh_dp_shard) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12777, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=3000000) ran for 3000088 milliseconds before timing out.
[rank7]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank7]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f270cf7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank7]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f26af812357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f26af816f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f26af81818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #4: <unknown function> + 0xdc253 (0x7f270e27b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank7]:frame #5: <unknown function> + 0x94ac3 (0x7f27107c3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:frame #6: clone + 0x44 (0x7f2710854a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:
[rank7]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank7]:  what():  [PG ID 2 PG GUID 6(mesh_dp_shard) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12777, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=3000000) ran for 3000088 milliseconds before timing out.
[rank7]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank7]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f270cf7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank7]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f26af812357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f26af816f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f26af81818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #4: <unknown function> + 0xdc253 (0x7f270e27b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank7]:frame #5: <unknown function> + 0x94ac3 (0x7f27107c3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:frame #6: clone + 0x44 (0x7f2710854a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:
[rank7]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank7]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f270cf7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank7]:frame #1: <unknown function> + 0xe86bb1 (0x7f26af7edbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #2: <unknown function> + 0x954001 (0x7f26af2bb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #3: <unknown function> + 0xdc253 (0x7f270e27b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank7]:frame #4: <unknown function> + 0x94ac3 (0x7f27107c3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:frame #5: clone + 0x44 (0x7f2710854a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:
[rank7]:Fatal Python error: Aborted
[rank7]:
[rank7]:Thread 0x00007f247f7fe640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f247ffff640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f249cff9640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f249d7fa640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f249dffb640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f249e7fc640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f249effd640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f2513fff640 (most recent call first):
[rank7]:  <no Python frame>
[rank7]:
[rank7]:Thread 0x00007f2518ffd640 (most recent call first):
[rank7]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank7]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank7]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank7]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank7]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank7]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank7]:
[rank7]:Thread 0x00007f253c8d3640 (most recent call first):
[rank7]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank7]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank7]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank7]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank7]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank7]:
[rank7]:Thread 0x00007f271072e000 (most recent call first):
[rank7]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 408 in batch_generator
[rank7]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 484 in train_step
[rank7]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank7]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank7]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank7]:  File "<frozen runpy>", line 88 in _run_code
[rank7]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank7]:
[rank7]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg[rank7]:[E923 11:23:02.035252205 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[rank7]:[E923 11:23:02.035297063 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank7]:, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special[rank7]:[E923 11:23:02.035969674 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 2(mesh_pp) Rank 3] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=104, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=3000000) ran for 3000090 milliseconds before timing out.
[rank7]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank7]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f270cf7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank7]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f26af812357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f26af816f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f26af81818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank7]:frame #4: <unknown function> + 0xdc253 (0x7f270e27b253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank7]:frame #5: <unknown function> + 0x94ac3 (0x7f27107c3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:frame #6: clone + 0x44 (0x7f2710854a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank7]:
[rank7]:terminate called recursively
[rank4]:[rank4]:[E923 11:23:02.981124611 ProcessGroupNCCL.cpp:744] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[rank4]:[E923 11:23:02.981202688 ProcessGroupNCCL.cpp:758] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[rank4]:[E923 11:23:02.981950473 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 1(mesh_pp) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000038 milliseconds before timing out.
[rank4]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank4]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7ff3beae1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank4]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7ff3bfa12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:[rank3]:[E923 11:23:02.032637993 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7ff3bfa16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7ff3bfa1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:[rank3]:[E923 11:23:02.032712044 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank4]:frame #4: <unknown function> + 0xdc253 (0x7ff41e30e253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank4]:frame #5: <unknown function> + 0x94ac3 (0x7ff420856ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:frame #6: clone + 0x44 (0x7ff4208e7a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:
[rank3]:[rank3]:[E923 11:23:02.033436004 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 2(mesh_pp) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000034 milliseconds before timing out.
[rank4]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank3]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank3]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fd56437a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank4]:  what():  [PG ID 1 PG GUID 1(mesh_pp) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000038 milliseconds before timing out.
[rank3]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fd506c12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank4]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7ff3beae1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank3]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fd506c16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fd506c1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7ff3bfa12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #4: <unknown function> + 0xdc253 (0x7fd565648253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank4]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7ff3bfa16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #5: <unknown function> + 0x94ac3 (0x7fd567b90ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7ff3bfa1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #6: clone + 0x44 (0x7fd567c21a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:
[rank4]:frame #4: <unknown function> + 0xdc253 (0x7ff41e30e253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank3]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank3]:  what():  [PG ID 1 PG GUID 2(mesh_pp) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000034 milliseconds before timing out.
[rank4]:frame #5: <unknown function> + 0x94ac3 (0x7ff420856ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank4]:frame #6: clone + 0x44 (0x7ff4208e7a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fd56437a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank4]:
[rank3]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fd506c12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank3]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fd506c16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fd506c1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #4: <unknown function> + 0xdc253 (0x7fd565648253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank4]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7ff3beae1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank3]:frame #5: <unknown function> + 0x94ac3 (0x7fd567b90ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:frame #1: <unknown function> + 0xe86bb1 (0x7ff3bf9edbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:frame #2: <unknown function> + 0x954001 (0x7ff3bf4bb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #6: clone + 0x44 (0x7fd567c21a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:frame #3: <unknown function> + 0xdc253 (0x7ff41e30e253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank3]:
[rank4]:frame #4: <unknown function> + 0x94ac3 (0x7ff420856ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank4]:frame #5: clone + 0x44 (0x7ff4208e7a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank4]:
[rank3]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fd56437a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank4]:Fatal Python error: Aborted
[rank4]:
[rank3]:frame #1: <unknown function> + 0xe86bb1 (0x7fd506bedbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank3]:frame #2: <unknown function> + 0x954001 (0x7fd5066bb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank4]:Thread 0x00007ff1ab7fe640 (most recent call first):
[rank4]:  <no Python frame>
[rank3]:frame #3: <unknown function> + 0xdc253 (0x7fd565648253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank3]:frame #4: <unknown function> + 0x94ac3 (0x7fd567b90ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:frame #5: clone + 0x44 (0x7fd567c21a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank3]:
[rank4]:
[rank3]:Fatal Python error: Aborted
[rank3]:
[rank4]:Thread 0x00007ff1abfff640 (most recent call first):
[rank3]:Thread 0x00007fd2e6ffd640 (most recent call first):
[rank4]:  <no Python frame>
[rank4]:
[rank3]:  <no Python frame>
[rank4]:Thread 0x00007ff1c08dd640 (most recent call first):
[rank3]:
[rank4]:  <no Python frame>
[rank3]:Thread 0x00007fd2e7fff640 (most recent call first):
[rank3]:  <no Python frame>
[rank3]:
[rank3]:Thread 0x00007fd2f50de640 (most recent call first):
[rank4]:
[rank3]:  <no Python frame>
[rank3]:
[rank4]:Thread 0x00007ff1c10de640 (most recent call first):
[rank4]:  <no Python frame>
[rank3]:Thread 0x00007fd2e77fe640 (most recent call first):
[rank4]:
[rank3]:  <no Python frame>
[rank4]:Thread 0x00007ff1c18df640 (most recent call first):
[rank3]:
[rank4]:  <no Python frame>
[rank3]:Thread 0x00007fd2d7fff640 (most recent call first):
[rank4]:
[rank3]:  <no Python frame>
[rank4]:Thread 0x00007ff1c20e0640 (most recent call first):
[rank3]:
[rank3]:Thread 0x00007fd2f48dd640 (most recent call first):
[rank4]:  <no Python frame>
[rank3]:  <no Python frame>
[rank4]:
[rank3]:
[rank3]:Thread 0x00007fd2f58df640 (most recent call first):
[rank3]:  <no Python frame>
[rank3]:
[rank3]:Thread 0x00007fd370ffd640 (most recent call first):
[rank3]:  <no Python frame>
[rank4]:Thread 0x00007ff1ccb35640 (most recent call first):
[rank4]:  <no Python frame>
[rank4]:
[rank4]:Thread 0x00007ff1cdb37640 (most recent call first):
[rank4]:  <no Python frame>
[rank4]:
[rank4]:Thread 0x00007ff2297fe640 (most recent call first):
[rank4]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank3]:
[rank3]:Thread 0x00007fd3717fe640 (most recent call first):
[rank4]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank4]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank3]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank4]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank3]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank4]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank4]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank3]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank4]:
[rank3]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank4]:Thread 0x00007ff24d205640 (most recent call first):
[rank4]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank4]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank4]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank4]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank4]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank4]:
[rank3]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank4]:Thread 0x00007ff4207c1000 (most recent call first):
[rank4]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank3]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank3]:
[rank4]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank3]:Thread 0x00007fd395205640 (most recent call first):
[rank4]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank3]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank4]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank3]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank4]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank3]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank3]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank4]:  File "<frozen runpy>", line 88 in _run_code
[rank4]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank3]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank4]:
[rank3]:
[rank3]:Thread 0x00007fd567afb000 (most recent call first):
[rank3]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank4]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151)
[rank3]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank3]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank3]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank3]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank3]:  File "<frozen runpy>", line 88 in _run_code
[rank3]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank3]:
[rank3]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151)
[rank2]:[rank2]:[E923 11:23:02.057177374 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[rank2]:[E923 11:23:02.057220038 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[rank2]:[E923 11:23:02.057902384 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 1(mesh_pp) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000056 milliseconds before timing out.
[rank2]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank2]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fb08bb7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank2]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fb02e412357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fb02e416f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fb02e41818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #4: <unknown function> + 0xdc253 (0x7fb08ce93253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank2]:frame #5: <unknown function> + 0x94ac3 (0x7fb08f3dbac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:frame #6: clone + 0x44 (0x7fb08f46ca04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:
[rank2]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank2]:  what():  [PG ID 1 PG GUID 1(mesh_pp) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25348, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000056 milliseconds before timing out.
[rank2]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank2]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fb08bb7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank2]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fb02e412357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fb02e416f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fb02e41818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #4: <unknown function> + 0xdc253 (0x7fb08ce93253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank2]:frame #5: <unknown function> + 0x94ac3 (0x7fb08f3dbac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:frame #6: clone + 0x44 (0x7fb08f46ca04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:
[rank2]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank2]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fb08bb7a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank2]:frame #1: <unknown function> + 0xe86bb1 (0x7fb02e3edbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #2: <unknown function> + 0x954001 (0x7fb02debb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank2]:frame #3: <unknown function> + 0xdc253 (0x7fb08ce93253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank2]:frame #4: <unknown function> + 0x94ac3 (0x7fb08f3dbac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:frame #5: clone + 0x44 (0x7fb08f46ca04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank2]:
[rank2]:Fatal Python error: Aborted
[rank2]:
[rank2]:Thread 0x00007fae22ffd640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae237fe640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae23fff640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae308dd640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae310de640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae320e0640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae318df640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae3d205640 (most recent call first):
[rank2]:  <no Python frame>
[rank2]:
[rank2]:Thread 0x00007fae98ffd640 (most recent call first):
[rank2]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank2]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank2]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank2]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank2]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank2]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank2]:
[rank2]:Thread 0x00007faeb88d3640 (most recent call first):
[rank2]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank2]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank2]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank2]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank2]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank2]:
[rank2]:Thread 0x00007fb08f346000 (most recent call first):
[rank2]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank2]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank2]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank2]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank2]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank2]:  File "<frozen runpy>", line 88 in _run_code
[rank2]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank2]:
[rank2]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151)
[rank1]:[rank1]:[E923 11:23:02.081390413 ProcessGroupNCCL.cpp:744] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[rank1]:[E923 11:23:02.081463132 ProcessGroupNCCL.cpp:758] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[rank1]:[E923 11:23:02.082206941 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 2(mesh_pp) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000043 milliseconds before timing out.
[rank1]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank1]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fc56d6e1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank1]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fc56e612357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fc56e616f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fc56e61818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #4: <unknown function> + 0xdc253 (0x7fc5ccf12253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank1]:frame #5: <unknown function> + 0x94ac3 (0x7fc5cf45aac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:frame #6: clone + 0x44 (0x7fc5cf4eba04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:
[rank1]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank1]:  what():  [PG ID 1 PG GUID 2(mesh_pp) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000043 milliseconds before timing out.
[rank1]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank1]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fc56d6e1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank1]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fc56e612357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fc56e616f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fc56e61818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #4: <unknown function> + 0xdc253 (0x7fc5ccf12253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank1]:frame #5: <unknown function> + 0x94ac3 (0x7fc5cf45aac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:frame #6: clone + 0x44 (0x7fc5cf4eba04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:
[rank1]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank1]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fc56d6e1680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank1]:frame #1: <unknown function> + 0xe86bb1 (0x7fc56e5edbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #2: <unknown function> + 0x954001 (0x7fc56e0bb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank1]:frame #3: <unknown function> + 0xdc253 (0x7fc5ccf12253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank1]:frame #4: <unknown function> + 0x94ac3 (0x7fc5cf45aac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:frame #5: clone + 0x44 (0x7fc5cf4eba04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank1]:
[rank1]:Fatal Python error: Aborted
[rank1]:
[rank1]:Thread 0x00007fc3527fc640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3537fe640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc352ffd640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc353fff640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3648dd640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3650de640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3658df640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3d37fe640 (most recent call first):
[rank1]:  <no Python frame>
[rank1]:
[rank1]:Thread 0x00007fc3d8ffd640 (most recent call first):
[rank1]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank1]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank1]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank1]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank1]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank1]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank1]:
[rank1]:Thread 0x00007fc3f88d3640 (most recent call first):
[rank1]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank1]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank1]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank1]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank1]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank1]:
[rank1]:Thread 0x00007fc5cf3c5000 (most recent call first):
[rank1]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank1]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank1]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank1]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank1]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank1]:  File "<frozen runpy>", line 88 in _run_code
[rank1]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank1]:
[rank1]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrap, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151)
[rank5]:[rank5]:[E923 11:23:02.092265580 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[rank5]:[E923 11:23:02.092336037 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[rank5]:[E923 11:23:02.093086005 ProcessGroupNCCL.cpp:2057] [PG ID 2 PG GUID 5(mesh_dp_shard) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12769, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000097 milliseconds before timing out.
[rank5]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank5]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fdbc877a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank5]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fdb6b012357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fdb6b016f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fdb6b01818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #4: <unknown function> + 0xdc253 (0x7fdbc9a29253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank5]:frame #5: <unknown function> + 0x94ac3 (0x7fdbcbf71ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:frame #6: clone + 0x44 (0x7fdbcc002a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:
[rank5]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank5]:  what():  [PG ID 2 PG GUID 5(mesh_dp_shard) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12769, OpType=_REDUCE_SCATTER_BASE, NumelIn=218112000, NumelOut=109056000, Timeout(ms)=3000000) ran for 3000097 milliseconds before timing out.
[rank5]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank5]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fdbc877a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank5]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fdb6b012357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fdb6b016f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fdb6b01818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #4: <unknown function> + 0xdc253 (0x7fdbc9a29253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank5]:frame #5: <unknown function> + 0x94ac3 (0x7fdbcbf71ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:frame #6: clone + 0x44 (0x7fdbcc002a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:
[rank5]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank5]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fdbc877a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank5]:frame #1: <unknown function> + 0xe86bb1 (0x7fdb6afedbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #2: <unknown function> + 0x954001 (0x7fdb6aabb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #3: <unknown function> + 0xdc253 (0x7fdbc9a29253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank5]:frame #4: <unknown function> + 0x94ac3 (0x7fdbcbf71ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:frame #5: clone + 0x44 (0x7fdbcc002a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:
[rank5]:Fatal Python error: Aborted
[rank5]:
[rank5]:Thread 0x00007fd9517fa640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd9527fc640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd951ffb640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd952ffd640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd9537fe640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd953fff640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd958fad640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd9cffff640 (most recent call first):
[rank5]:  <no Python frame>
[rank5]:
[rank5]:Thread 0x00007fd9d4ffd640 (most recent call first):
[rank5]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank5]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank5]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank5]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank5]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank5]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank5]:
[rank5]:Thread 0x00007fd9f88d3640 (most recent call first):
[rank5]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank5]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank5]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank5]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank5]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank5]:
[rank5]:Thread 0x00007fdbcbedc000 (most recent call first):
[rank5]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank5]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank5]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank5]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank5]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 727 in <module>
[rank5]:  File "<frozen runpy>", line 88 in _run_code
[rank5]:  File "<frozen runpy>", line 198 in _run_module_as_main
[rank5]:
[rank5]:Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md[rank5]:[E923 11:23:02.096780742 ProcessGroupNCCL.cpp:744] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[rank5]:[E923 11:23:02.096815594 ProcessGroupNCCL.cpp:758] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank5]:, yaml._yaml, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, yarl._quoting_c, propcache._helpers_c, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket.mask, aiohttp._websocket.reader_c, frozenlist._frozenlist, xxhash._xxhash, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._substrait, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, google._upb._message, markupsafe._speedups, PIL._imaging, kiwisolver._cext, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, _cyutility, scipy._cyutility, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_schur_sqrtm, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._slsqplib, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy._lib._uarray._uarray, scipy.special._ufuncs_cxx, scipy.special._ellip_harm_2, scipy.special._special_ufuncs, scipy.special._gufuncs, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, [rank5]:[E923 11:23:02.097507194 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 2(mesh_pp) Rank 2] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=25349, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000096 milliseconds before timing out.
[rank5]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank5]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fdbc877a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank5]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7fdb6b012357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7fdb6b016f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7fdb6b01818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank5]:frame #4: <unknown function> + 0xdc253 (0x7fdbc9a29253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank5]:frame #5: <unknown function> + 0x94ac3 (0x7fdbcbf71ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:frame #6: clone + 0x44 (0x7fdbcc002a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank5]:
[rank5]:scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._hausdorff, scipy.spatial._distance_wrapterminate called recursively
[rank5]:, scipy.spatial.transform._rotation, scipy.spatial.transform._rigid_transform, scipy.optimize._direct, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils (total: 151[rank0]:[rank0]:[E923 11:23:02.121503998 ProcessGroupNCCL.cpp:744] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[rank0]:[E923 11:23:02.121565310 ProcessGroupNCCL.cpp:758] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[rank0]:[E923 11:23:02.121621715 ProcessGroupNCCL.cpp:744] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[rank0]:[E923 11:23:02.121657829 ProcessGroupNCCL.cpp:758] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[rank0]:[E923 11:23:02.122480407 ProcessGroupNCCL.cpp:2057] [PG ID 1 PG GUID 1(mesh_pp) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000080 milliseconds before timing out.
[rank0]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank0]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f124557a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank0]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f11e7e12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f11e7e16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f11e7e1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #4: <unknown function> + 0xdc253 (0x7f124684f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank0]:frame #5: <unknown function> + 0x94ac3 (0x7f1248d97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:frame #6: clone + 0x44 (0x7f1248e28a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:
[rank0]:terminate called after throwing an instance of 'c10::DistBackendError'
[rank0]:[rank0]:[E923 11:23:02.123105067 ProcessGroupNCCL.cpp:2057] [PG ID 2 PG GUID 3(mesh_dp_shard) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=14344, OpType=_REDUCE_SCATTER_BASE, NumelIn=525336576, NumelOut=262668288, Timeout(ms)=3000000) ran for 3000021 milliseconds before timing out.
[rank0]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank0]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f124557a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank0]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f11e7e12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f11e7e16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f11e7e1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #4: <unknown function> + 0xdc253 (0x7f124684f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank0]:frame #5: <unknown function> + 0x94ac3 (0x7f1248d97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:frame #6: clone + 0x44 (0x7f1248e28a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:
[rank0]:terminate called recursively
[rank0]:Fatal Python error: Aborted
[rank0]:
[rank0]:Thread 0x00007f0fdaffd640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fda7fc640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fdb7fe640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fdbfff640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fe48dd640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fe50de640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fe58df640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f0fece7c640 (most recent call first):
[rank0]:  <no Python frame>
[rank0]:
[rank0]:Thread 0x00007f10517fe640 (most recent call first):
[rank0]:  File "/usr/lib/python3.11/threading.py", line 327 in wait
[rank0]:  File "/usr/lib/python3.11/queue.py", line 171 in get
[rank0]:  File "/workspace/torchtitan/torchtitan/components/checkpoint.py", line 111 in purge_thread
[rank0]:  File "/usr/lib/python3.11/threading.py", line 982 in run
[rank0]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank0]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank0]:
[rank0]:Thread 0x00007f1071205640 (most recent call first):
[rank0]:  File "/usr/lib/python3.11/threading.py", line 331 in wait
[rank0]:  File "/usr/lib/python3.11/threading.py", line 629 in wait
[rank0]:  File "/usr/local/lib/python3.11/dist-packages/tqdm/_monitor.py", line 60 in run
[rank0]:  File "/usr/lib/python3.11/threading.py", line 1045 in _bootstrap_inner
[rank0]:  File "/usr/lib/python3.11/threading.py", line 1002 in _bootstrap
[rank0]:
[rank0]:Thread 0x00007f1248d02000 (most recent call first):
[rank0]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 454 in forward_backward_step
[rank0]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 485 in train_step
[rank0]:  File "/workspace/torchtitan/timelyfreeze/train.py", line 574 in train
[rank0]:  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357 in wrapper
[rank0]:  File   what():  "[PG ID 1 PG GUID 1(mesh_pp) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=12675, OpType=COALESCED, NumelIn=0, NumelOut=0, Timeout(ms)=3000000) ran for 3000080 milliseconds before timing out.
[rank0]:Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
[rank0]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f124557a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank0]:frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7f11e7e12357 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x16b1 (0x7f11e7e16f21 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xfa (0x7f11e7e1818a in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #4: <unknown function> + 0xdc253 (0x7f124684f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank0]:frame #5: <unknown function> + 0x94ac3 (0x7f1248d97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:frame #6: clone + 0x44 (0x7f1248e28a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:
[rank0]:Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
[rank0]:frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f124557a680 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)
[rank0]:frame #1: <unknown function> + 0xe86bb1 (0x7f11e7dedbb1 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #2: <unknown function> + 0x954001 (0x7f11e78bb001 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)
[rank0]:frame #3: <unknown function> + 0xdc253 (0x7f124684f253 in /lib/x86_64-linux-gnu/libstdc++.so.6)
[rank0]:frame #4: <unknown function> + 0x94ac3 (0x7f1248d97ac3 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:frame #5: clone + 0x44 (0x7f1248e28a04 in /lib/x86_64-linux-gnu/libc.so.6)
[rank0]:/workspace/torchtitan/timelyfreeze/train.py
[rank0]:", line 727 in <module>
[rank0]:  File "<frozen runpy>", line 88 in _run_code
[rank0]:  File "<frozen runpy>", line 198 in _run_module_as_main
W0923 11:23:03.873000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36462 closing signal SIGTERM
W0923 11:23:03.873000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36463 closing signal SIGTERM
W0923 11:23:03.873000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36464 closing signal SIGTERM
W0923 11:23:03.874000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36465 closing signal SIGTERM
W0923 11:23:03.874000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36466 closing signal SIGTERM
W0923 11:23:03.874000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36467 closing signal SIGTERM
W0923 11:23:03.874000 36436 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 36469 closing signal SIGTERM
E0923 11:23:05.675000 36436 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: -6) local_rank: 6 (pid: 36468) of binary: /usr/bin/python
[rank7]:Stage 3: Modules to keep: {'layers.31', 'layers.28', 'norm', 'layers.27', 'layers.25', 'layers.29', 'output', 'layers.26', 'layers.30'}
[rank3]:Stage 1: Modules to keep: {'layers.16', 'layers.13', 'layers.11', 'layers.10', 'layers.8', 'layers.15', 'layers.14', 'layers.12', 'layers.9'}
[rank2]:Stage 1: Modules to keep: {'layers.11', 'layers.8', 'layers.15', 'layers.13', 'layers.10', 'layers.9', 'layers.16', 'layers.12', 'layers.14'}
[rank1]:Stage 0: Modules to keep: {'layers.0', 'layers.4', 'layers.1', 'layers.5', 'layers.7', 'tok_embeddings', 'layers.2', 'layers.3', 'layers.6'}
[rank0]:Stage 0: Modules to keep: {'layers.6', 'layers.5', 'layers.3', 'layers.7', 'tok_embeddings', 'layers.2', 'layers.1', 'layers.4', 'layers.0'}
[rank4]:Stage 2: Modules to keep: {'layers.21', 'layers.19', 'layers.18', 'layers.24', 'layers.20', 'layers.23', 'layers.17', 'layers.22'}
[rank5]:Stage 2: Modules to keep: {'layers.18', 'layers.24', 'layers.17', 'layers.22', 'layers.19', 'layers.23', 'layers.21', 'layers.20'}
[rank6]:Stage 3: Modules to keep: {'layers.25', 'layers.26', 'norm', 'layers.28', 'output', 'layers.27', 'layers.31', 'layers.30', 'layers.29'}
[rank0]:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank5]:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank2]:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank4]:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank1]:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank3]:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank6]:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[rank7]:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
timelyfreeze.train FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 36462)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36462
[2]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 36463)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36463
[3]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 36464)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36464
[4]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 36465)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36465
[5]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 4 (local_rank: 4)
  exitcode  : -6 (pid: 36466)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36466
[6]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 5 (local_rank: 5)
  exitcode  : -6 (pid: 36467)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36467
[7]:
  time      : 2025-09-23_11:23:05
  host      : 85e4aa33f65d
  rank      : 7 (local_rank: 7)
  exitcode  : -6 (pid: 36469)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36469
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-23_11:23:03
  host      : 85e4aa33f65d
  rank      : 6 (local_rank: 6)
  exitcode  : -6 (pid: 36468)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 36468
======================================================
[W923 11:23:05.286203914 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Tue Sep 23 12:10:35 UTC 2025
âœ”ï¸SERVER: 85e4aa33f65d (172.18.0.2),  GPUs: 0,1,2,3,4,5,6,7
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod8/0922_main/0922_gpipe_fullrand6.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs
âœ”ï¸Running with fullrand6 x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=8 --local-ranks-filter=0,1,2,3,4,5,6,7 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod8/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:[titan] 2025-09-23 12:10:42,583 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank4]:[titan] 2025-09-23 12:10:42,835 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank2]:[titan] 2025-09-23 12:10:42,853 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-09-23 12:10:42,857 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank4]:[titan] 2025-09-23 12:10:43,120 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank4]:[titan] 2025-09-23 12:10:43,124 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank7]:[titan] 2025-09-23 12:10:43,127 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-23 12:10:43,378 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank1]:[titan] 2025-09-23 12:10:43,436 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank7]:[titan] 2025-09-23 12:10:43,447 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank7]:[titan] 2025-09-23 12:10:43,451 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank5]:[titan] 2025-09-23 12:10:43,461 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank6]:[titan] 2025-09-23 12:10:43,603 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank0]:[titan] 2025-09-23 12:10:43,941 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-23 12:10:43,946 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 12:10:43,954 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:[titan] 2025-09-23 12:10:44,110 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:[titan] 2025-09-23 12:10:44,123 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank5]:[titan] 2025-09-23 12:10:44,154 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-23 12:10:44,203 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod with 8 GPUs"
[rank5]:[titan] 2025-09-23 12:10:44,166 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank6]:[titan] 2025-09-23 12:10:44,276 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank6]:[titan] 2025-09-23 12:10:44,280 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank3]:[titan] 2025-09-23 12:10:44,629 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-23 12:10:44,633 - root - INFO - Building 2-D device mesh with ['pp', 'dp_shard'], [4, 2]
[rank0]:[titan] 2025-09-23 12:10:45,661 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-23 12:10:46,004 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-23 12:10:46,728 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:[titan] 2025-09-23 12:10:46,831 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank2]:[titan] 2025-09-23 12:10:46,864 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank2]:[titan] 2025-09-23 12:10:46,934 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank0]:[titan] 2025-09-23 12:10:46,894 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank7]:[titan] 2025-09-23 12:10:46,900 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank3]:[titan] 2025-09-23 12:10:46,887 - root - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:[titan] 2025-09-23 12:10:46,922 - root - INFO - Applied FSDP to the model
[rank3]:[titan] 2025-09-23 12:10:46,922 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank4]:[titan] 2025-09-23 12:10:46,971 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank2]:[titan] 2025-09-23 12:10:46,964 - root - INFO - Applied FSDP to the model
[rank2]:[titan] 2025-09-23 12:10:46,964 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-23 12:10:46,952 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-23 12:10:46,971 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-23 12:10:46,998 - root - INFO - Applied FSDP to the model
[rank0]:[titan] 2025-09-23 12:10:46,998 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank7]:[titan] 2025-09-23 12:10:46,975 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank7]:[titan] 2025-09-23 12:10:47,005 - root - INFO - Applied FSDP to the model
[rank7]:[titan] 2025-09-23 12:10:47,005 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank4]:[titan] 2025-09-23 12:10:47,047 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank4]:[titan] 2025-09-23 12:10:47,076 - root - INFO - Applied FSDP to the model
[rank4]:[titan] 2025-09-23 12:10:47,076 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank1]:[titan] 2025-09-23 12:10:47,097 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank5]:[titan] 2025-09-23 12:10:47,084 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank5]:[titan] 2025-09-23 12:10:47,146 - root - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank3]:[titan] 2025-09-23 12:10:47,127 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-23 12:10:47,127 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank1]:[titan] 2025-09-23 12:10:47,169 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank1]:[titan] 2025-09-23 12:10:47,209 - root - INFO - Applied FSDP to the model
[rank1]:[titan] 2025-09-23 12:10:47,209 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank2]:[titan] 2025-09-23 12:10:47,176 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:[titan] 2025-09-23 12:10:47,176 - root - INFO - CUDA memory usage for model: 3.67GiB(4.64%)
[rank0]:[titan] 2025-09-23 12:10:47,224 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-23 12:10:47,224 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank7]:[titan] 2025-09-23 12:10:47,226 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank7]:[titan] 2025-09-23 12:10:47,226 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank5]:[titan] 2025-09-23 12:10:47,182 - root - INFO - Applied FSDP to the model
[rank5]:[titan] 2025-09-23 12:10:47,182 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank4]:[titan] 2025-09-23 12:10:47,279 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank4]:[titan] 2025-09-23 12:10:47,280 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank5]:[titan] 2025-09-23 12:10:47,433 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank5]:[titan] 2025-09-23 12:10:47,433 - root - INFO - CUDA memory usage for model: 3.26GiB(4.12%)
[rank1]:[titan] 2025-09-23 12:10:47,461 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:[titan] 2025-09-23 12:10:47,461 - root - INFO - CUDA memory usage for model: 4.24GiB(5.36%)
[rank6]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank6]:wandb: Tracking run with wandb version 0.22.0
[rank6]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250923-1210/wandb/run-20250923_121047-dr522zts
[rank6]:wandb: Run `wandb offline` to turn off syncing.
[rank6]:wandb: Syncing run 0922_gpipe_fullrand6_dm4
[rank6]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank6]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/dr522zts
[rank6]:[titan] 2025-09-23 12:10:48,595 - root - INFO - WandB logging enabled
[rank6]:[titan] 2025-09-23 12:10:48,596 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.14GiB memory
[rank6]:[titan] 2025-09-23 12:10:48,629 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank6]:[titan] 2025-09-23 12:10:48,657 - root - INFO - Applied FSDP to the model
[rank6]:[titan] 2025-09-23 12:10:48,658 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank6]:[titan] 2025-09-23 12:10:48,871 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank6]:[titan] 2025-09-23 12:10:48,871 - root - INFO - CUDA memory usage for model: 3.83GiB(4.84%)
[rank0]:[titan] 2025-09-23 12:10:48,881 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-23 12:10:48,881 - root - INFO - Mixed precision training is handled by fully_shard
[rank0]:[titan] 2025-09-23 12:10:48,882 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 8, sequence length 1024, total steps 1000 (warmup 20)
[rank0]:[titan] 2025-09-23 12:10:48,883 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-23 12:11:15,016 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-23 12:11:15,016 - root - INFO - Finished loading the checkpoint in 26.13 seconds.
[rank0]:[titan] 2025-09-23 12:11:15,016 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:[titan] 2025-09-23 12:11:27,944 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 400 [36m tflops: 18.68 [35m mfu: 5.99%[39m
[rank4]:[titan] 2025-09-23 12:11:27,944 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank0]:[titan] 2025-09-23 12:11:27,940 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 400 [36m tflops: 18.64 [35m mfu: 5.98%[39m
[rank0]:[titan] 2025-09-23 12:11:27,940 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank6]:[titan] 2025-09-23 12:11:27,965 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 416 [36m tflops: 19.42 [35m mfu: 6.22%[39m
[rank6]:[titan] 2025-09-23 12:11:27,966 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank7]:[titan] 2025-09-23 12:11:27,965 - root - INFO - [31m step:  1 [32m loss:  1.8985 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 38.21GiB(48.29%) [34m tps: 400 [36m tflops: 18.63 [35m mfu: 5.97%[39m
[rank7]:[titan] 2025-09-23 12:11:27,965 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank1]:[titan] 2025-09-23 12:11:27,940 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 35.46GiB(44.81%) [34m tps: 402 [36m tflops: 18.73 [35m mfu: 6.00%[39m
[rank1]:[titan] 2025-09-23 12:11:27,940 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank5]:[titan] 2025-09-23 12:11:27,944 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 28.25GiB(35.70%) [34m tps: 401 [36m tflops: 18.72 [35m mfu: 6.00%[39m
[rank5]:[titan] 2025-09-23 12:11:27,944 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank2]:[titan] 2025-09-23 12:11:27,940 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 399 [36m tflops: 18.63 [35m mfu: 5.97%[39m
[rank2]:[titan] 2025-09-23 12:11:27,940 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank3]:[titan] 2025-09-23 12:11:27,940 - root - INFO - [31m step:  1 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  1.1759 [38;2;54;234;195m memory: 31.52GiB(39.82%) [34m tps: 399 [36m tflops: 18.61 [35m mfu: 5.96%[39m
[rank3]:[titan] 2025-09-23 12:11:27,940 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:50:00
[rank0]:[titan] 2025-09-23 12:13:38,596 - root - INFO - [Step 120] Setting Upperbound
[rank4]:[titan] 2025-09-23 12:13:38,666 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.26/0.65, [MB1] 0.30/0.75, [MB2] 0.01/0.02, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.39/0.96, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank1]:[titan] 2025-09-23 12:13:38,662 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.05/0.12, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 12:13:38,660 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.33/0.81, [MB1] 0.11/0.27, [MB2] 0.11/0.27, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 12:13:38,661 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.06/0.15, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank5]:[titan] 2025-09-23 12:13:38,657 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.27/0.67, [MB1] 0.31/0.77, [MB2] 0.02/0.06, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.01/0.02, [MB7] 0.40/1.00
[rank3]:[titan] 2025-09-23 12:13:38,664 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.32/0.81, [MB1] 0.10/0.26, [MB2] 0.06/0.16, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:[titan] 2025-09-23 12:13:38,782 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_max_batch_time_rank6.svg
[rank6]:> Batch Time: 1119.86 ms, GPU Bubble Ratio: 40.02%, 37.79%, 46.36%, 34.59%
[rank7]:[titan] 2025-09-23 12:13:38,780 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_max_batch_time_rank7.svg
[rank7]:> Batch Time: 1118.78 ms, GPU Bubble Ratio: 40.06%, 38.37%, 45.62%, 34.62%
[rank6]:[titan] 2025-09-23 12:13:38,909 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_min_batch_time_rank6.svg
[rank6]:> Batch Time: 712.78 ms, GPU Bubble Ratio: 39.51%, 37.31%, 45.92%, 34.66%
[rank7]:[titan] 2025-09-23 12:13:38,906 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_min_batch_time_rank7.svg
[rank7]:> Batch Time: 711.53 ms, GPU Bubble Ratio: 39.56%, 37.85%, 45.12%, 34.73%
[rank6]:[titan] 2025-09-23 12:13:39,053 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_frozen_pipeline_schedule_rank6.svg
[rank6]:> Batch Time: 712.78 ms, GPU Bubble Ratio: 28.93%, 30.69%, 36.86%, 34.66%
[rank6]:[titan] 2025-09-23 12:13:39,053 - root - INFO - > Batch Time: 712.78 ms (Average Freeze Ratio: 0.78, Time Reduction Rate: 0.36)
[rank6]:[titan] 2025-09-23 12:13:39,056 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank7]:[titan] 2025-09-23 12:13:39,051 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1213_frozen_pipeline_schedule_rank7.svg
[rank7]:> Batch Time: 711.53 ms, GPU Bubble Ratio: 28.87%, 30.80%, 36.35%, 34.73%
[rank7]:[titan] 2025-09-23 12:13:39,051 - root - INFO - > Batch Time: 711.53 ms (Average Freeze Ratio: 0.78, Time Reduction Rate: 0.36)
[rank7]:[titan] 2025-09-23 12:13:39,054 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank7]:[titan] 2025-09-23 12:14:22,906 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:[titan] 2025-09-23 12:14:22,906 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank5]:[titan] 2025-09-23 12:14:22,990 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.27/0.67, [MB1] 0.31/0.77, [MB2] 0.02/0.06, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.01/0.02, [MB7] 0.40/1.00
[rank4]:[titan] 2025-09-23 12:14:22,990 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.26/0.65, [MB1] 0.30/0.75, [MB2] 0.01/0.02, [MB3] 0.00/0.00, [MB4] 0.40/1.00, [MB5] 0.39/0.96, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 12:14:23,085 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.33/0.81, [MB1] 0.11/0.27, [MB2] 0.11/0.27, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank3]:[titan] 2025-09-23 12:14:23,086 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.32/0.81, [MB1] 0.10/0.26, [MB2] 0.06/0.16, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank1]:[titan] 2025-09-23 12:14:23,187 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.05/0.12, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank0]:[titan] 2025-09-23 12:14:23,185 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.06/0.15, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank6]:/workspace/torchtitan/timelyfreeze/core/schedule.py:344: RankWarning: Polyfit may be poorly conditioned
[rank6]:  a, b = np.polyfit(afrs, times, 1)
[rank7]:/workspace/torchtitan/timelyfreeze/core/schedule.py:344: RankWarning: Polyfit may be poorly conditioned
[rank7]:  a, b = np.polyfit(afrs, times, 1)
[rank0]:[titan] 2025-09-23 12:15:07,462 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank0_trend_line.svg
[rank0]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 12:15:07,456 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank5_trend_line.svg
[rank5]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 12:15:07,456 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank7_trend_line.svg
[rank4]:[titan] 2025-09-23 12:15:07,457 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank4_trend_line.svg
[rank4]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 12:15:07,455 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank6_trend_line.svg
[rank3]:[titan] 2025-09-23 12:15:07,460 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank3_trend_line.svg
[rank3]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank3]:[titan] 2025-09-23 12:15:07,468 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 12:15:07,463 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank1_trend_line.svg
[rank1]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 12:15:07,468 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 12:15:07,460 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1215_rank2_trend_line.svg
[rank2]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 12:15:07,467 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 12:15:07,601 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1215_adjusted_frozen_pipeline_schedule_rank7.svg
[rank7]:> Batch Time: 868.82 ms, GPU Bubble Ratio: 29.67%, 27.84%, 34.79%, 49.09%
[rank7]:[titan] 2025-09-23 12:15:07,601 - root - INFO - > Batch Time: 868.82 ms (Average Freeze Ratio: 0.00)
[rank7]:[titan] 2025-09-23 12:15:07,601 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 12:15:07,602 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 12:15:07,600 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1215_adjusted_frozen_pipeline_schedule_rank6.svg
[rank6]:> Batch Time: 865.68 ms, GPU Bubble Ratio: 29.33%, 27.77%, 35.44%, 48.79%
[rank6]:[titan] 2025-09-23 12:15:07,600 - root - INFO - > Batch Time: 865.68 ms (Average Freeze Ratio: 0.00)
[rank6]:[titan] 2025-09-23 12:15:07,601 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 12:15:07,601 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank6]:[titan] 2025-09-23 12:15:53,907 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 12:15:53,907 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 12:15:53,996 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 12:15:53,997 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank2]:[titan] 2025-09-23 12:15:54,097 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank3]:[titan] 2025-09-23 12:15:54,098 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank1]:[titan] 2025-09-23 12:15:54,202 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank0]:[titan] 2025-09-23 12:15:54,201 - root - INFO - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank4]:[titan] 2025-09-23 12:17:27,019 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,780 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank5]:[titan] 2025-09-23 12:17:27,019 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 34.68GiB(43.82%) [34m tps: 1,780 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank0]:[titan] 2025-09-23 12:17:27,025 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,779 [36m tflops: 82.99 [35m mfu: 26.60%[39m
[rank7]:[titan] 2025-09-23 12:17:27,023 - root - INFO - [31m step: 40 [32m loss:  7.9307 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,780 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank2]:[titan] 2025-09-23 12:17:27,021 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,779 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank1]:[titan] 2025-09-23 12:17:27,025 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 43.86GiB(55.42%) [34m tps: 1,779 [36m tflops: 82.99 [35m mfu: 26.60%[39m
[rank6]:[titan] 2025-09-23 12:17:27,024 - root - INFO - [31m step: 40 [32m loss:  7.9307 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 45.85GiB(57.94%) [34m tps: 1,780 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank3]:[titan] 2025-09-23 12:17:27,021 - root - INFO - [31m step: 40 [32m loss: -8.0000 [38;2;180;60;0m grad_norm:  0.7114 [38;2;54;234;195m memory: 38.74GiB(48.95%) [34m tps: 1,779 [36m tflops: 83.00 [35m mfu: 26.60%[39m
[rank6]:[titan] 2025-09-23 12:17:27,154 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1217_real_step40_rank6.svg
[rank6]:> Batch Time: 1117.19 ms, GPU Bubble Ratio: 39.84%, 37.62%, 46.20%, 34.69%
[rank7]:[titan] 2025-09-23 12:17:27,155 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1217_real_step40_rank7.svg
[rank7]:> Batch Time: 1115.22 ms, GPU Bubble Ratio: 39.91%, 38.09%, 45.43%, 34.72%
[rank7]:[titan] 2025-09-23 12:17:27,205 - root - INFO - Frozen Ratio History of Rank 7 (Stage 3)  is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/freeze_ratio_history/rank7/250923_1217_stage3_step40.svg
[rank6]:[titan] 2025-09-23 12:17:27,204 - root - INFO - Frozen Ratio History of Rank 6 (Stage 3)  is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/freeze_ratio_history/rank6/250923_1217_stage3_step40.svg
[rank0]:[titan] 2025-09-23 12:18:50,655 - root - INFO - [GC] Peforming periodical GC collection 0.31 seconds
[rank0]:[titan] 2025-09-23 12:19:00,002 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank0_trend_line.svg
[rank0]:[titan] 2025-09-23 12:19:00,006 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank1]:[titan] 2025-09-23 12:19:00,004 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank1_trend_line.svg
[rank1]:[titan] 2025-09-23 12:19:00,009 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.40/1.00, [MB7] 0.40/1.00
[rank2]:[titan] 2025-09-23 12:19:00,001 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank2_trend_line.svg
[rank2]:[titan] 2025-09-23 12:19:00,007 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.30/0.75, [MB7] 0.37/0.94
[rank6]:[titan] 2025-09-23 12:19:00,001 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank6_trend_line.svg
[rank3]:[titan] 2025-09-23 12:19:00,000 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank3_trend_line.svg
[rank3]:[titan] 2025-09-23 12:19:00,009 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.34/0.84, [MB7] 0.37/0.93
[rank4]:[titan] 2025-09-23 12:18:59,999 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank4_trend_line.svg
[rank4]:[titan] 2025-09-23 12:19:00,007 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.23/0.57, [MB2] 0.05/0.13, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank5]:[titan] 2025-09-23 12:18:59,997 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank5_trend_line.svg
[rank5]:[titan] 2025-09-23 12:19:00,009 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.30/0.74, [MB2] 0.19/0.48, [MB3] 0.09/0.22, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
[rank7]:[titan] 2025-09-23 12:19:00,001 - root - INFO - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule_adjustment/250923_1218_rank7_trend_line.svg
[rank6]:[titan] 2025-09-23 12:19:00,138 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1218_adjusted_frozen_pipeline_schedule_rank6.svg
[rank6]:> Batch Time: 903.89 ms, GPU Bubble Ratio: 30.15%, 29.59%, 34.53%, 29.93%
[rank6]:[titan] 2025-09-23 12:19:00,138 - root - INFO - > Batch Time: 903.89 ms (Average Freeze Ratio: 0.77)
[rank6]:[titan] 2025-09-23 12:19:00,138 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.38/0.94, [MB7] 0.18/0.46
[rank7]:[titan] 2025-09-23 12:19:00,141 - root - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/0922_gpipe_fullrand6_dm4/pipeline_schedule/250923_1218_adjusted_frozen_pipeline_schedule_rank7.svg
[rank7]:> Batch Time: 899.53 ms, GPU Bubble Ratio: 30.23%, 29.89%, 33.90%, 30.08%
[rank7]:[titan] 2025-09-23 12:19:00,141 - root - INFO - > Batch Time: 899.53 ms (Average Freeze Ratio: 0.81)
[rank7]:[titan] 2025-09-23 12:19:00,141 - root - INFO - Adjusted Freeze Ratio per Block: [MB0] 0.40/1.00, [MB1] 0.40/1.00, [MB2] 0.40/1.00, [MB3] 0.40/1.00, [MB4] 0.40/1.00, [MB5] 0.40/1.00, [MB6] 0.36/0.89, [MB7] 0.34/0.85
