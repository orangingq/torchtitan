2025-10-22 13:36:48,472 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 13:36:48,633 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 13:36:48,649 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 13:36:48,652 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 13:36:48,656 - [GC] Initial GC collection 0.00 seconds
2025-10-22 13:36:48,803 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 13:36:48,807 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 13:36:49,484 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 13:36:49,595 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 13:36:50,147 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 13:36:50,151 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 13:36:50,161 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 13:36:50,165 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 13:36:50,451 - Loading tokenizer from tokenizer.json
2025-10-22 13:36:50,841 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 13:36:53,806 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 13:36:54,248 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 13:36:54,290 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:54,304 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 13:36:54,350 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 13:36:54,350 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 13:36:54,525 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:54,525 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 13:36:54,527 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 13:36:55,341 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 13:36:55,382 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:55,444 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 13:36:55,444 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 13:36:55,617 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:55,617 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 13:36:55,618 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 13:36:56,087 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 13:36:56,130 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:56,192 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 13:36:56,193 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 13:36:56,377 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:56,378 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 13:36:56,379 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 13:36:58,161 - WandB logging enabled
2025-10-22 13:36:58,164 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 13:36:58,206 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:58,272 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 13:36:58,273 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 13:36:58,487 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 13:36:58,490 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 13:36:58,493 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 13:36:58,529 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 13:36:58,529 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 13:36:58,529 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 13:36:58,529 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 13:36:58,530 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 13:36:58,530 - Mixed precision training is disabled
2025-10-22 13:36:58,530 - Mixed precision training is disabled
2025-10-22 13:36:58,530 - Mixed precision training is disabled
2025-10-22 13:36:58,531 - Mixed precision training is disabled
2025-10-22 13:36:58,536 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 200)
2025-10-22 13:36:58,536 - Training starts at step 1
2025-10-22 13:37:16,900 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1660 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,578 [36m tflops: 12.33 [35m mfu: 3.95%[39m
2025-10-22 13:37:16,901 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 13:37:16,902 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1660 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,523 [36m tflops: 11.90 [35m mfu: 3.82%[39m
2025-10-22 13:37:16,903 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 13:37:16,911 - [31m step:  1 [32m loss: 12.2291 [38;2;180;60;0m grad_norm:  0.1660 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,752 [36m tflops: 13.70 [35m mfu: 4.39%[39m
2025-10-22 13:37:16,914 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 13:37:16,934 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1660 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,447 [36m tflops: 11.31 [35m mfu: 3.63%[39m
2025-10-22 13:37:16,935 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 13:51:09,137 - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 14:05:34,939 - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 14:05:52,322 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1335 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 14:05:52,325 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1335 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 14:05:52,329 - Destroying the purge thread.
2025-10-22 14:05:52,332 - Destroying the purge thread.
2025-10-22 14:05:52,335 - [31m step: 100 [32m loss:  6.1244 [38;2;180;60;0m grad_norm:  0.1335 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 14:05:52,338 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1335 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 14:05:52,342 - Destroying the purge thread.
2025-10-22 14:05:52,353 - Destroying the purge thread.
2025-10-22 14:18:23,947 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:18:23,975 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:18:24,022 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:18:24,076 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:18:24,606 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:18:24,610 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:18:24,614 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:18:24,618 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:18:24,619 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:18:24,622 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:18:24,626 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:18:24,630 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:18:24,634 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:18:24,935 - Loading tokenizer from tokenizer.json
2025-10-22 14:18:25,336 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:18:28,277 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:18:28,716 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:18:28,758 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:28,774 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:18:28,792 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:18:28,821 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:18:28,822 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:18:28,835 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:28,895 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:18:28,896 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:18:29,039 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:29,039 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:18:29,048 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:18:29,066 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:29,066 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:18:29,067 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:18:29,372 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:18:29,415 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:29,478 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:18:29,479 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:18:29,889 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:29,890 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:18:29,892 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:18:32,440 - WandB logging enabled
2025-10-22 14:18:32,442 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:18:32,485 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:32,551 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:18:32,552 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:18:32,744 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:18:32,746 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:18:32,748 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:18:32,775 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:18:32,775 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:18:32,776 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:18:32,776 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:18:32,776 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:18:32,776 - Mixed precision training is disabled
2025-10-22 14:18:32,777 - Mixed precision training is disabled
2025-10-22 14:18:32,777 - Mixed precision training is disabled
2025-10-22 14:18:32,777 - Mixed precision training is disabled
2025-10-22 14:18:32,782 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 200)
2025-10-22 14:18:32,783 - Training starts at step 1
2025-10-22 14:18:50,955 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1639 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,481 [36m tflops: 11.58 [35m mfu: 3.71%[39m
2025-10-22 14:18:50,956 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:18:50,958 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1639 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,521 [36m tflops: 11.89 [35m mfu: 3.81%[39m
2025-10-22 14:18:50,959 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:18:50,967 - [31m step:  1 [32m loss: 12.2555 [38;2;180;60;0m grad_norm:  0.1639 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,773 [36m tflops: 13.86 [35m mfu: 4.44%[39m
2025-10-22 14:18:50,969 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:18:50,991 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1639 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,474 [36m tflops: 11.52 [35m mfu: 3.69%[39m
2025-10-22 14:18:50,992 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:22:17,144 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:22:17,340 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:22:17,343 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:22:17,347 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:22:17,972 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:22:18,156 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:22:18,241 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:22:18,623 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:22:18,627 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:22:18,636 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:22:18,639 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:22:18,642 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:22:18,647 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:22:18,921 - Loading tokenizer from tokenizer.json
2025-10-22 14:22:19,314 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:22:22,286 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:22:22,703 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:22:22,742 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:22,757 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:22:22,805 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:22:22,805 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:22:22,907 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:22:22,924 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:22:22,949 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:22,964 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:22,985 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:22,986 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:22:22,986 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:22:23,014 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:22:23,014 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:22:23,024 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:22:23,025 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:22:23,196 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:23,196 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:22:23,198 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:22:23,204 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:23,204 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:22:23,206 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:22:26,853 - WandB logging enabled
2025-10-22 14:22:26,959 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:22:27,001 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:27,066 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:22:27,067 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:22:27,633 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:22:27,635 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:22:27,637 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:22:27,664 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:22:27,664 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:22:27,665 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:22:27,665 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:22:27,665 - Mixed precision training is disabled
2025-10-22 14:22:27,665 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:22:27,666 - Mixed precision training is disabled
2025-10-22 14:22:27,666 - Mixed precision training is disabled
2025-10-22 14:22:27,666 - Mixed precision training is disabled
2025-10-22 14:22:27,672 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:22:27,673 - Training starts at step 1
2025-10-22 14:22:46,135 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1608 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,413 [36m tflops: 11.05 [35m mfu: 3.54%[39m
2025-10-22 14:22:46,136 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:22:46,140 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1608 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,414 [36m tflops: 11.05 [35m mfu: 3.54%[39m
2025-10-22 14:22:46,141 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:22:46,149 - [31m step:  1 [32m loss: 12.2402 [38;2;180;60;0m grad_norm:  0.1608 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,712 [36m tflops: 13.38 [35m mfu: 4.29%[39m
2025-10-22 14:22:46,151 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:22:46,173 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1608 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,399 [36m tflops: 10.93 [35m mfu: 3.50%[39m
2025-10-22 14:22:46,174 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:38:43,602 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:38:43,789 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:38:43,793 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:38:43,797 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:38:44,058 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:38:44,061 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:38:44,283 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:38:44,287 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:38:44,288 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:38:44,292 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:38:44,777 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:38:45,337 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:38:45,341 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:38:45,626 - Loading tokenizer from tokenizer.json
2025-10-22 14:38:46,013 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:38:49,231 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:38:49,652 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:38:49,669 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:38:49,700 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:49,703 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:49,716 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:38:49,734 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:38:49,758 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:38:49,759 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:38:49,763 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:38:49,764 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:38:49,774 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:49,835 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:38:49,836 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:38:49,948 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:49,949 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:38:49,950 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:38:50,007 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:50,008 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:38:50,009 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:38:50,028 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:50,028 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:38:50,030 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:38:53,386 - WandB logging enabled
2025-10-22 14:38:53,388 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:38:53,430 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:53,499 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:38:53,501 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:38:53,693 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:38:53,696 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:38:53,704 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:38:53,734 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:38:53,734 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:38:53,735 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:38:53,735 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:38:53,735 - Mixed precision training is disabled
2025-10-22 14:38:53,735 - Mixed precision training is disabled
2025-10-22 14:38:53,736 - Mixed precision training is disabled
2025-10-22 14:38:53,736 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:38:53,736 - Mixed precision training is disabled
2025-10-22 14:38:53,742 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:38:53,743 - Training starts at step 1
2025-10-22 14:38:55,352 - Destroying the purge thread.
2025-10-22 14:38:55,495 - Destroying the purge thread.
2025-10-22 14:38:55,565 - Destroying the purge thread.
2025-10-22 14:38:55,597 - Destroying the purge thread.
2025-10-22 14:42:53,914 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:42:54,107 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:42:54,111 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:42:54,115 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:42:54,201 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:42:54,363 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:42:54,367 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:42:54,457 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:42:54,629 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:42:54,633 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:42:55,090 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:42:55,671 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:42:55,675 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:42:55,959 - Loading tokenizer from tokenizer.json
2025-10-22 14:42:56,349 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:42:59,373 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:42:59,789 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:42:59,795 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:42:59,837 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:42:59,841 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:42:59,856 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:42:59,898 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:42:59,899 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:42:59,902 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:42:59,903 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:43:00,091 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:00,092 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:43:00,092 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:00,093 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:43:00,094 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:43:00,094 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:43:00,449 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:43:00,488 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:00,548 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:43:00,549 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:43:01,163 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:01,164 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:43:01,165 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:43:03,672 - WandB logging enabled
2025-10-22 14:43:03,675 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:43:03,715 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:03,779 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:43:03,781 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:43:03,976 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:43:03,978 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:43:03,981 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:43:04,008 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:43:04,009 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:43:04,009 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:43:04,009 - Mixed precision training is disabled
2025-10-22 14:43:04,009 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:43:04,010 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:43:04,010 - Mixed precision training is disabled
2025-10-22 14:43:04,010 - Mixed precision training is disabled
2025-10-22 14:43:04,010 - Mixed precision training is disabled
2025-10-22 14:43:04,016 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:43:04,017 - Training starts at step 1
2025-10-22 14:43:39,374 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1661 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 843 [36m tflops: 6.59 [35m mfu: 2.11%[39m
2025-10-22 14:43:39,376 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:43:40,635 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1661 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 803 [36m tflops: 6.28 [35m mfu: 2.01%[39m
2025-10-22 14:43:40,637 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:43:42,244 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1661 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 773 [36m tflops: 6.04 [35m mfu: 1.94%[39m
2025-10-22 14:43:42,245 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:43:43,727 - [31m step:  1 [32m loss: 12.2663 [38;2;180;60;0m grad_norm:  0.1661 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 823 [36m tflops: 6.43 [35m mfu: 2.06%[39m
2025-10-22 14:43:43,731 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:46:32,286 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:46:32,464 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:46:32,468 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:46:32,472 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:46:33,067 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:46:33,108 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:46:33,196 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:46:33,740 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:46:33,744 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:46:33,753 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:46:33,757 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:46:33,763 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:46:33,766 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:46:34,055 - Loading tokenizer from tokenizer.json
2025-10-22 14:46:34,441 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:46:37,905 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:46:37,946 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:46:37,948 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:46:37,990 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:37,990 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:38,050 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:46:38,050 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:46:38,050 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:46:38,050 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:46:38,236 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:38,236 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:38,237 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:46:38,237 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:46:38,238 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:46:38,238 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:46:38,434 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:46:38,472 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:38,488 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:46:38,534 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:46:38,534 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:46:39,117 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:39,118 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:46:39,119 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:46:41,725 - WandB logging enabled
2025-10-22 14:46:41,728 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:46:41,768 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:41,830 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:46:41,831 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:46:42,006 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:46:42,008 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:46:42,010 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:46:42,036 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:46:42,036 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:46:42,037 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:46:42,037 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:46:42,037 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:46:42,037 - Mixed precision training is disabled
2025-10-22 14:46:42,037 - Mixed precision training is disabled
2025-10-22 14:46:42,037 - Mixed precision training is disabled
2025-10-22 14:46:42,038 - Mixed precision training is disabled
2025-10-22 14:46:42,044 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:46:42,045 - Training starts at step 1
2025-10-22 14:46:54,802 - Avg. fwd time: 36.1316 / Avg. bwd time: 69.2487 / Avg. batch time: 885.7101 (ms) / GPU bubble ratio: 4.82%
2025-10-22 14:46:54,835 - Avg. fwd time: 17.3323 / Avg. bwd time: 28.5443 / Avg. batch time: 933.0112 (ms) / GPU bubble ratio: 60.66%
2025-10-22 14:46:54,876 - Avg. fwd time: 28.6408 / Avg. bwd time: 37.4148 / Avg. batch time: 993.1932 (ms) / GPU bubble ratio: 46.79%
2025-10-22 14:46:54,913 - Avg. fwd time: 22.3911 / Avg. bwd time: 34.1194 / Avg. batch time: 1046.8657 (ms) / GPU bubble ratio: 56.82%
2025-10-22 14:49:32,430 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1656 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 188 [36m tflops: 1.47 [35m mfu: 0.47%[39m
2025-10-22 14:49:32,432 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:49:36,020 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1656 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 185 [36m tflops: 1.44 [35m mfu: 0.46%[39m
2025-10-22 14:49:36,022 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:49:38,674 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1656 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 181 [36m tflops: 1.42 [35m mfu: 0.45%[39m
2025-10-22 14:49:38,677 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:49:41,094 - [31m step:  1 [32m loss: 12.2227 [38;2;180;60;0m grad_norm:  0.1656 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 183 [36m tflops: 1.43 [35m mfu: 0.46%[39m
2025-10-22 14:49:41,097 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 14:55:00,623 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:55:00,659 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:55:00,754 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:55:00,795 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:55:01,203 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:55:01,207 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:55:01,213 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:55:01,217 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:55:01,222 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:55:01,225 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:55:01,227 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:55:01,228 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:55:01,230 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:55:01,524 - Loading tokenizer from tokenizer.json
2025-10-22 14:55:01,917 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:55:05,186 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:55:05,412 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:55:05,440 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:55:05,455 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,482 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,519 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:55:05,520 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:55:05,545 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:55:05,546 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:55:05,615 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:55:05,655 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,669 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:55:05,700 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,701 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:55:05,702 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:55:05,717 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:55:05,718 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:55:05,748 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,748 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:55:05,750 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:55:05,896 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:05,897 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:55:05,898 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:55:09,227 - WandB logging enabled
2025-10-22 14:55:09,229 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:55:09,271 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:09,337 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:55:09,338 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:55:09,524 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:55:09,527 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:55:09,529 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:55:09,558 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:55:09,558 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:55:09,558 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:55:09,558 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:55:09,559 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:55:09,559 - Mixed precision training is disabled
2025-10-22 14:55:09,559 - Mixed precision training is disabled
2025-10-22 14:55:09,560 - Mixed precision training is disabled
2025-10-22 14:55:09,560 - Mixed precision training is disabled
2025-10-22 14:55:09,565 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:55:09,567 - Training starts at step 1
2025-10-22 14:56:43,237 - Avg. fwd time: 35.1967 / Avg. bwd time: 69.9598 / Avg. batch time: 915.6367 (ms) / GPU bubble ratio: 8.12%
2025-10-22 14:56:43,271 - Avg. fwd time: 17.3306 / Avg. bwd time: 28.6258 / Avg. batch time: 963.2880 (ms) / GPU bubble ratio: 61.83%
2025-10-22 14:56:43,314 - Avg. fwd time: 32.6051 / Avg. bwd time: 37.2896 / Avg. batch time: 1024.1140 (ms) / GPU bubble ratio: 45.40%
2025-10-22 14:56:43,351 - Avg. fwd time: 22.8988 / Avg. bwd time: 34.1352 / Avg. batch time: 1078.0341 (ms) / GPU bubble ratio: 57.68%
2025-10-22 14:56:48,721 - Destroying the purge thread.
2025-10-22 14:56:48,722 - Destroying the purge thread.
2025-10-22 14:56:48,723 - Destroying the purge thread.
2025-10-22 14:56:48,748 - Destroying the purge thread.
2025-10-22 14:59:46,650 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:59:46,842 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:59:46,849 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:59:46,853 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:59:46,856 - [GC] Initial GC collection 0.00 seconds
2025-10-22 14:59:47,003 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:59:47,008 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:59:47,020 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:59:47,051 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 14:59:47,212 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:59:47,217 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:59:47,265 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 14:59:47,269 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 14:59:48,383 - Loading tokenizer from tokenizer.json
2025-10-22 14:59:48,772 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 14:59:52,063 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 14:59:52,182 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:59:52,212 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:59:52,224 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,253 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,287 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 14:59:52,288 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:59:52,313 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 14:59:52,314 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:59:52,492 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:59:52,499 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,500 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:59:52,501 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 14:59:52,512 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,513 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:59:52,514 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 14:59:52,530 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,544 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 14:59:52,592 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 14:59:52,592 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:59:52,769 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:52,770 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:59:52,771 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 14:59:55,568 - WandB logging enabled
2025-10-22 14:59:55,570 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 14:59:55,612 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:55,678 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 14:59:55,679 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 14:59:55,855 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 14:59:55,857 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 14:59:55,859 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 14:59:55,885 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:59:55,886 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 14:59:55,887 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:59:55,887 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:59:55,887 - Mixed precision training is disabled
2025-10-22 14:59:55,887 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 14:59:55,887 - Mixed precision training is disabled
2025-10-22 14:59:55,887 - Mixed precision training is disabled
2025-10-22 14:59:55,887 - Mixed precision training is disabled
2025-10-22 14:59:55,893 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 14:59:55,894 - Training starts at step 1
2025-10-22 15:05:27,319 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:05:27,456 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:05:27,464 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:05:27,924 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:05:28,105 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:05:28,108 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:05:28,111 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:05:28,112 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:05:28,114 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:05:28,123 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:05:28,127 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:05:28,246 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:05:28,251 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:05:28,532 - Loading tokenizer from tokenizer.json
2025-10-22 15:05:28,936 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:05:32,427 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:05:32,466 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:32,495 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:05:32,526 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:05:32,526 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:05:32,537 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:32,599 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:05:32,600 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:05:32,738 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:05:33,259 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:05:33,301 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:33,317 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:05:33,365 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:05:33,366 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:05:33,487 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:33,487 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:33,487 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:05:33,488 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:05:33,489 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:05:33,489 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:05:33,552 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:33,553 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:05:33,555 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:05:36,296 - WandB logging enabled
2025-10-22 15:05:36,395 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:05:36,436 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:36,505 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:05:36,507 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:05:37,110 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:05:37,113 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:05:37,115 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:05:37,141 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:05:37,141 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:05:37,141 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:05:37,142 - Mixed precision training is disabled
2025-10-22 15:05:37,142 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:05:37,142 - Mixed precision training is disabled
2025-10-22 15:05:37,143 - Mixed precision training is disabled
2025-10-22 15:05:37,148 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:05:37,149 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:05:37,150 - Training starts at step 1
2025-10-22 15:05:37,151 - Mixed precision training is disabled
2025-10-22 15:06:19,940 - Avg. fwd time: 35.8832 / Avg. bwd time: 69.5072 / Avg. batch time: 865.5333 (ms) / GPU bubble ratio: 2.59%
2025-10-22 15:06:19,975 - Avg. fwd time: 17.3969 / Avg. bwd time: 28.5157 / Avg. batch time: 951.2890 (ms) / GPU bubble ratio: 61.39%
2025-10-22 15:06:20,015 - Avg. fwd time: 25.1369 / Avg. bwd time: 37.3124 / Avg. batch time: 1011.6348 (ms) / GPU bubble ratio: 50.62%
2025-10-22 15:06:20,052 - Avg. fwd time: 22.8566 / Avg. bwd time: 34.0920 / Avg. batch time: 1065.5680 (ms) / GPU bubble ratio: 57.24%
2025-10-22 15:06:25,471 - Destroying the purge thread.
2025-10-22 15:06:25,473 - Destroying the purge thread.
2025-10-22 15:06:25,473 - Destroying the purge thread.
2025-10-22 15:06:25,497 - Destroying the purge thread.
2025-10-22 15:07:53,396 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:07:53,467 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:07:53,477 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:07:53,797 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:07:54,040 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:07:54,043 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:07:54,063 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:07:54,066 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:07:54,069 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:07:54,070 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:07:54,073 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:07:54,113 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:07:54,117 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:07:54,411 - Loading tokenizer from tokenizer.json
2025-10-22 15:07:54,803 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:07:58,452 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:07:58,483 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:07:58,494 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:58,556 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:07:58,557 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:07:58,728 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:58,729 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:07:58,730 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:07:59,100 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:07:59,101 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:07:59,145 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:59,145 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:59,159 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:07:59,204 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:07:59,204 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:07:59,205 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:07:59,205 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:07:59,504 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:59,505 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:07:59,506 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:07:59,512 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:07:59,513 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:07:59,514 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:08:02,082 - WandB logging enabled
2025-10-22 15:08:02,085 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:08:02,128 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:08:02,193 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:08:02,194 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:08:02,368 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:08:02,371 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:08:02,373 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:08:02,400 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:08:02,401 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:08:02,401 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:08:02,401 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:08:02,402 - Mixed precision training is disabled
2025-10-22 15:08:02,402 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:08:02,402 - Mixed precision training is disabled
2025-10-22 15:08:02,402 - Mixed precision training is disabled
2025-10-22 15:08:02,402 - Mixed precision training is disabled
2025-10-22 15:08:02,410 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:08:02,411 - Training starts at step 1
2025-10-22 15:08:15,019 - Avg. fwd time: 34.9250 / Avg. bwd time: 69.2970 / Avg. batch time: 873.2632 (ms) / GPU bubble ratio: 4.52%
2025-10-22 15:08:15,052 - Avg. fwd time: 17.4923 / Avg. bwd time: 28.5138 / Avg. batch time: 920.3801 (ms) / GPU bubble ratio: 60.01%
2025-10-22 15:08:15,094 - Avg. fwd time: 28.0679 / Avg. bwd time: 37.5339 / Avg. batch time: 980.9198 (ms) / GPU bubble ratio: 46.50%
2025-10-22 15:08:15,132 - Avg. fwd time: 22.8761 / Avg. bwd time: 34.1937 / Avg. batch time: 1034.8132 (ms) / GPU bubble ratio: 55.88%
2025-10-22 15:08:20,465 - Destroying the purge thread.
2025-10-22 15:08:20,466 - Destroying the purge thread.
2025-10-22 15:08:20,467 - Destroying the purge thread.
2025-10-22 15:08:20,491 - Destroying the purge thread.
2025-10-22 15:09:58,039 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:09:58,090 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:09:58,183 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:09:58,237 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:09:58,527 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:09:58,528 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:09:58,532 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:09:58,532 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:09:58,533 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:09:58,536 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:09:58,537 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:09:58,620 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:09:58,624 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:09:58,916 - Loading tokenizer from tokenizer.json
2025-10-22 15:09:59,304 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:10:02,653 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:10:02,658 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:10:02,701 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:02,705 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:02,761 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:10:02,761 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:10:02,765 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:10:02,766 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:10:02,940 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:02,941 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:10:02,942 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:10:02,955 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:02,956 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:10:02,957 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:10:03,248 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:10:03,675 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:10:03,715 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:03,728 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:10:03,774 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:10:03,774 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:10:03,948 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:03,948 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:10:03,949 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:10:06,832 - WandB logging enabled
2025-10-22 15:10:06,976 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:10:07,015 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:07,081 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:10:07,083 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:10:07,521 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:10:07,523 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:10:07,525 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:10:07,552 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:10:07,552 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:10:07,553 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:10:07,553 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:10:07,553 - Mixed precision training is disabled
2025-10-22 15:10:07,554 - Mixed precision training is disabled
2025-10-22 15:10:07,553 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:10:07,554 - Mixed precision training is disabled
2025-10-22 15:10:07,554 - Mixed precision training is disabled
2025-10-22 15:10:07,566 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:10:07,567 - Training starts at step 1
2025-10-22 15:10:07,593 - [Step 1] 🚦  Starting step
2025-10-22 15:10:07,600 - [Step 1] 🚦  Starting step
2025-10-22 15:10:07,604 - [Step 1] 🚦  Starting step
2025-10-22 15:10:07,605 - [Step 1] 🚦  Starting step
2025-10-22 15:10:07,780 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:07,898 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:07,998 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:08,098 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:09,569 - [Step 1] 🚦  Starting step
2025-10-22 15:10:09,573 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:09,599 - [Step 1] 🚦  Starting step
2025-10-22 15:10:09,600 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:09,659 - [Step 1] 🚦  Starting step
2025-10-22 15:10:09,660 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:09,684 - [Step 1] 🚦  Starting step
2025-10-22 15:10:09,685 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:10,629 - [Step 1] 🚦  Starting step
2025-10-22 15:10:10,632 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:10,668 - [Step 1] 🚦  Starting step
2025-10-22 15:10:10,670 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:10,706 - [Step 1] 🚦  Starting step
2025-10-22 15:10:10,708 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:10,743 - [Step 1] 🚦  Starting step
2025-10-22 15:10:10,744 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:11,696 - [Step 1] 🚦  Starting step
2025-10-22 15:10:11,698 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:11,734 - [Step 1] 🚦  Starting step
2025-10-22 15:10:11,735 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:11,782 - [Step 1] 🚦  Starting step
2025-10-22 15:10:11,783 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:11,811 - [Step 1] 🚦  Starting step
2025-10-22 15:10:11,812 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:12,761 - [Step 1] 🚦  Starting step
2025-10-22 15:10:12,765 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:12,800 - [Step 1] 🚦  Starting step
2025-10-22 15:10:12,801 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:12,840 - [Step 1] 🚦  Starting step
2025-10-22 15:10:12,842 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:12,877 - [Step 1] 🚦  Starting step
2025-10-22 15:10:12,878 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:13,809 - [Step 1] 🚦  Starting step
2025-10-22 15:10:13,811 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:13,849 - [Step 1] 🚦  Starting step
2025-10-22 15:10:13,850 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:13,889 - [Step 1] 🚦  Starting step
2025-10-22 15:10:13,890 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:13,927 - [Step 1] 🚦  Starting step
2025-10-22 15:10:13,928 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:14,860 - [Step 1] 🚦  Starting step
2025-10-22 15:10:14,864 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:14,897 - [Step 1] 🚦  Starting step
2025-10-22 15:10:14,898 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:14,936 - [Step 1] 🚦  Starting step
2025-10-22 15:10:14,937 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:14,972 - [Step 1] 🚦  Starting step
2025-10-22 15:10:14,973 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:15,919 - [Step 1] 🚦  Starting step
2025-10-22 15:10:15,921 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:15,958 - [Step 1] 🚦  Starting step
2025-10-22 15:10:15,959 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:15,996 - [Step 1] 🚦  Starting step
2025-10-22 15:10:15,997 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:16,032 - [Step 1] 🚦  Starting step
2025-10-22 15:10:16,033 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:16,969 - [Step 1] 🚦  Starting step
2025-10-22 15:10:16,971 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:17,008 - [Step 1] 🚦  Starting step
2025-10-22 15:10:17,009 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:17,045 - [Step 1] 🚦  Starting step
2025-10-22 15:10:17,046 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:17,080 - [Step 1] 🚦  Starting step
2025-10-22 15:10:17,081 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:17,998 - [Step 1] 🚦  Starting step
2025-10-22 15:10:18,000 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:18,036 - [Step 1] 🚦  Starting step
2025-10-22 15:10:18,037 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:18,074 - [Step 1] 🚦  Starting step
2025-10-22 15:10:18,075 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:18,113 - [Step 1] 🚦  Starting step
2025-10-22 15:10:18,114 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:19,047 - [Step 1] 🚦  Starting step
2025-10-22 15:10:19,050 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:19,089 - [Step 1] 🚦  Starting step
2025-10-22 15:10:19,091 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:19,126 - [Step 1] 🚦  Starting step
2025-10-22 15:10:19,127 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:19,164 - [Step 1] 🚦  Starting step
2025-10-22 15:10:19,165 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:20,082 - Avg. fwd time: 35.2860 / Avg. bwd time: 69.7723 / Avg. batch time: 862.9925 (ms) / GPU bubble ratio: 2.61%
2025-10-22 15:10:20,107 - [Step 1] 🚦  Starting step
2025-10-22 15:10:20,109 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:20,115 - Avg. fwd time: 17.4874 / Avg. bwd time: 28.5091 / Avg. batch time: 910.5798 (ms) / GPU bubble ratio: 59.59%
2025-10-22 15:10:20,140 - [Step 1] 🚦  Starting step
2025-10-22 15:10:20,141 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:20,156 - Avg. fwd time: 26.5361 / Avg. bwd time: 37.2284 / Avg. batch time: 970.7860 (ms) / GPU bubble ratio: 47.45%
2025-10-22 15:10:20,181 - [Step 1] 🚦  Starting step
2025-10-22 15:10:20,182 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:20,193 - Avg. fwd time: 22.9496 / Avg. bwd time: 34.2185 / Avg. batch time: 1024.6338 (ms) / GPU bubble ratio: 55.37%
2025-10-22 15:10:20,221 - [Step 1] 🚦  Starting step
2025-10-22 15:10:20,222 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:21,158 - [Step 1] 🚦  Starting step
2025-10-22 15:10:21,161 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:21,197 - [Step 1] 🚦  Starting step
2025-10-22 15:10:21,198 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:21,235 - [Step 1] 🚦  Starting step
2025-10-22 15:10:21,236 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:21,272 - [Step 1] 🚦  Starting step
2025-10-22 15:10:21,273 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:22,208 - [Step 1] 🚦  Starting step
2025-10-22 15:10:22,210 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:22,248 - [Step 1] 🚦  Starting step
2025-10-22 15:10:22,249 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:22,285 - [Step 1] 🚦  Starting step
2025-10-22 15:10:22,286 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:22,321 - [Step 1] 🚦  Starting step
2025-10-22 15:10:22,322 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:23,255 - [Step 1] 🚦  Starting step
2025-10-22 15:10:23,258 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:23,294 - [Step 1] 🚦  Starting step
2025-10-22 15:10:23,295 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:23,333 - [Step 1] 🚦  Starting step
2025-10-22 15:10:23,334 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:23,371 - [Step 1] 🚦  Starting step
2025-10-22 15:10:23,372 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:24,318 - [Step 1] 🚦  Starting step
2025-10-22 15:10:24,322 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:10:24,355 - [Step 1] 🚦  Starting step
2025-10-22 15:10:24,357 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:10:24,394 - [Step 1] 🚦  Starting step
2025-10-22 15:10:24,395 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:10:24,432 - [Step 1] 🚦  Starting step
2025-10-22 15:10:24,433 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:10:25,523 - Destroying the purge thread.
2025-10-22 15:10:25,524 - Destroying the purge thread.
2025-10-22 15:10:25,524 - Destroying the purge thread.
2025-10-22 15:10:25,549 - Destroying the purge thread.
2025-10-22 15:14:15,980 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:14:15,998 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:14:16,075 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:14:16,167 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:14:16,171 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:14:16,175 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:14:16,219 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:14:16,223 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:14:16,264 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:14:16,310 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:14:16,314 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:14:16,439 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:14:16,442 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:14:16,716 - Loading tokenizer from tokenizer.json
2025-10-22 15:14:17,120 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:14:20,496 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:14:20,938 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:14:20,938 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:14:20,940 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:14:21,002 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,002 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,002 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,025 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:14:21,062 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:14:21,063 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:14:21,073 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:14:21,073 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:14:21,073 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:14:21,073 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:14:21,701 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,702 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:14:21,703 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:14:21,711 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,712 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:14:21,713 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:14:21,715 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:21,715 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:14:21,717 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:14:24,302 - WandB logging enabled
2025-10-22 15:14:24,305 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:14:24,344 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:24,414 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:14:24,415 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:14:25,254 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:14:25,257 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:14:25,260 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:14:25,288 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:14:25,289 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:14:25,289 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:14:25,289 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:14:25,290 - Mixed precision training is disabled
2025-10-22 15:14:25,290 - Mixed precision training is disabled
2025-10-22 15:14:25,290 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:14:25,290 - Mixed precision training is disabled
2025-10-22 15:14:25,290 - Mixed precision training is disabled
2025-10-22 15:14:25,296 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:14:25,297 - Training starts at step 1
2025-10-22 15:14:30,785 - [Step 1] 🚦  Starting step
2025-10-22 15:14:33,476 - [Step 1] 🚦  Starting step
2025-10-22 15:14:35,278 - [Step 1] 🚦  Starting step
2025-10-22 15:14:36,954 - [Step 1] 🚦  Starting step
2025-10-22 15:14:37,761 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:37,874 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:38,034 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:38,136 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:39,067 - [Step 1] 🚦  Starting step
2025-10-22 15:14:39,076 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:39,082 - [Step 1] 🚦  Starting step
2025-10-22 15:14:39,084 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:39,123 - [Step 1] 🚦  Starting step
2025-10-22 15:14:39,124 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:39,160 - [Step 1] 🚦  Starting step
2025-10-22 15:14:39,161 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:40,307 - [Step 1] 🚦  Starting step
2025-10-22 15:14:40,311 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:40,347 - [Step 1] 🚦  Starting step
2025-10-22 15:14:40,348 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:40,387 - [Step 1] 🚦  Starting step
2025-10-22 15:14:40,388 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:40,421 - [Step 1] 🚦  Starting step
2025-10-22 15:14:40,422 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:41,374 - [Step 1] 🚦  Starting step
2025-10-22 15:14:41,378 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:41,413 - [Step 1] 🚦  Starting step
2025-10-22 15:14:41,414 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:41,453 - [Step 1] 🚦  Starting step
2025-10-22 15:14:41,454 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:41,486 - [Step 1] 🚦  Starting step
2025-10-22 15:14:41,487 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:42,415 - [Step 1] 🚦  Starting step
2025-10-22 15:14:42,417 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:42,454 - [Step 1] 🚦  Starting step
2025-10-22 15:14:42,455 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:42,495 - [Step 1] 🚦  Starting step
2025-10-22 15:14:42,496 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:42,529 - [Step 1] 🚦  Starting step
2025-10-22 15:14:42,530 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:43,480 - [Step 1] 🚦  Starting step
2025-10-22 15:14:43,482 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:43,519 - [Step 1] 🚦  Starting step
2025-10-22 15:14:43,521 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:43,561 - [Step 1] 🚦  Starting step
2025-10-22 15:14:43,563 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:43,597 - [Step 1] 🚦  Starting step
2025-10-22 15:14:43,598 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:44,510 - [Step 1] 🚦  Starting step
2025-10-22 15:14:44,514 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:44,547 - [Step 1] 🚦  Starting step
2025-10-22 15:14:44,548 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:44,587 - [Step 1] 🚦  Starting step
2025-10-22 15:14:44,589 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:44,621 - [Step 1] 🚦  Starting step
2025-10-22 15:14:44,622 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:45,575 - [Step 1] 🚦  Starting step
2025-10-22 15:14:45,577 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:45,613 - [Step 1] 🚦  Starting step
2025-10-22 15:14:45,615 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:45,659 - [Step 1] 🚦  Starting step
2025-10-22 15:14:45,661 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:45,690 - [Step 1] 🚦  Starting step
2025-10-22 15:14:45,691 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:46,609 - [Step 1] 🚦  Starting step
2025-10-22 15:14:46,612 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:46,645 - [Step 1] 🚦  Starting step
2025-10-22 15:14:46,646 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:46,691 - [Step 1] 🚦  Starting step
2025-10-22 15:14:46,693 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:46,720 - [Step 1] 🚦  Starting step
2025-10-22 15:14:46,721 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:47,673 - [Step 1] 🚦  Starting step
2025-10-22 15:14:47,676 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:47,712 - [Step 1] 🚦  Starting step
2025-10-22 15:14:47,713 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:47,754 - [Step 1] 🚦  Starting step
2025-10-22 15:14:47,755 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:47,787 - [Step 1] 🚦  Starting step
2025-10-22 15:14:47,789 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:48,735 - [Step 1] 🚦  Starting step
2025-10-22 15:14:48,737 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:48,774 - [Step 1] 🚦  Starting step
2025-10-22 15:14:48,775 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:48,814 - [Step 1] 🚦  Starting step
2025-10-22 15:14:48,816 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:48,849 - [Step 1] 🚦  Starting step
2025-10-22 15:14:48,850 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:49,787 - Avg. fwd time: 35.8776 / Avg. bwd time: 69.2669 / Avg. batch time: 886.6556 (ms) / GPU bubble ratio: 5.13%
2025-10-22 15:14:49,814 - [Step 1] 🚦  Starting step
2025-10-22 15:14:49,816 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:49,820 - Avg. fwd time: 17.3234 / Avg. bwd time: 28.5711 / Avg. batch time: 933.8177 (ms) / GPU bubble ratio: 60.68%
2025-10-22 15:14:49,845 - [Step 1] 🚦  Starting step
2025-10-22 15:14:49,846 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:49,860 - Avg. fwd time: 27.5164 / Avg. bwd time: 37.1613 / Avg. batch time: 994.1128 (ms) / GPU bubble ratio: 47.95%
2025-10-22 15:14:49,887 - [Step 1] 🚦  Starting step
2025-10-22 15:14:49,888 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:49,897 - Avg. fwd time: 22.4842 / Avg. bwd time: 34.0733 / Avg. batch time: 1047.9037 (ms) / GPU bubble ratio: 56.82%
2025-10-22 15:14:49,923 - [Step 1] 🚦  Starting step
2025-10-22 15:14:49,924 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:50,852 - [Step 1] 🚦  Starting step
2025-10-22 15:14:50,854 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:50,889 - [Step 1] 🚦  Starting step
2025-10-22 15:14:50,890 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:50,929 - [Step 1] 🚦  Starting step
2025-10-22 15:14:50,930 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:50,966 - [Step 1] 🚦  Starting step
2025-10-22 15:14:50,967 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:51,912 - [Step 1] 🚦  Starting step
2025-10-22 15:14:51,914 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:51,950 - [Step 1] 🚦  Starting step
2025-10-22 15:14:51,951 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:51,993 - [Step 1] 🚦  Starting step
2025-10-22 15:14:51,994 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:52,027 - [Step 1] 🚦  Starting step
2025-10-22 15:14:52,028 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:52,970 - [Step 1] 🚦  Starting step
2025-10-22 15:14:52,973 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:53,008 - [Step 1] 🚦  Starting step
2025-10-22 15:14:53,010 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:53,051 - [Step 1] 🚦  Starting step
2025-10-22 15:14:53,052 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:53,086 - [Step 1] 🚦  Starting step
2025-10-22 15:14:53,087 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:54,042 - [Step 1] 🚦  Starting step
2025-10-22 15:14:54,045 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:14:54,080 - [Step 1] 🚦  Starting step
2025-10-22 15:14:54,081 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:14:54,118 - [Step 1] 🚦  Starting step
2025-10-22 15:14:54,119 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:14:54,153 - [Step 1] 🚦  Starting step
2025-10-22 15:14:54,154 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:14:55,262 - Destroying the purge thread.
2025-10-22 15:14:55,262 - Destroying the purge thread.
2025-10-22 15:14:55,263 - Destroying the purge thread.
2025-10-22 15:14:55,290 - Destroying the purge thread.
2025-10-22 15:17:20,866 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:17:20,877 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:17:21,217 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:17:21,343 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:17:21,359 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:17:21,363 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:17:21,365 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:17:21,370 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:17:21,373 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:17:21,529 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:17:21,533 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:17:21,588 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:17:21,593 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:17:21,886 - Loading tokenizer from tokenizer.json
2025-10-22 15:17:22,285 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:17:26,956 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:17:27,105 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:17:27,144 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:27,203 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:17:27,204 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:17:27,370 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:17:27,377 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:27,377 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:17:27,379 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:17:27,409 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:27,423 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:17:27,470 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:17:27,471 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:17:27,648 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:27,648 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:17:27,650 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:17:27,679 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:17:27,720 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:27,783 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:17:27,784 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:17:28,625 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:28,626 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:17:28,627 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:17:30,214 - WandB logging enabled
2025-10-22 15:17:30,216 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:17:30,258 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:30,326 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:17:30,327 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:17:30,501 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:17:30,503 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:17:30,506 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:17:30,534 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:17:30,534 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:17:30,535 - Mixed precision training is disabled
2025-10-22 15:17:30,535 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:17:30,535 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:17:30,536 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:17:30,536 - Mixed precision training is disabled
2025-10-22 15:17:30,536 - Mixed precision training is disabled
2025-10-22 15:17:30,538 - Mixed precision training is disabled
2025-10-22 15:17:30,542 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:17:30,543 - Training starts at step 1
2025-10-22 15:17:33,643 - [Step 1] 🚦  Starting step
2025-10-22 15:17:35,592 - [Step 1] 🚦  Starting step
2025-10-22 15:17:37,514 - [Step 1] 🚦  Starting step
2025-10-22 15:17:39,482 - [Step 1] 🚦  Starting step
2025-10-22 15:17:39,713 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:39,822 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:39,931 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:40,030 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:41,608 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:41,627 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:41,666 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:41,704 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:42,656 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:42,698 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:42,734 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:42,771 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:43,698 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:43,735 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:43,774 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:43,811 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:44,758 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:44,796 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:44,839 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:44,877 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:45,779 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:45,820 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:45,857 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:45,892 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:46,839 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:46,877 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:46,918 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:46,954 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:47,859 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:47,898 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:47,937 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:47,972 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:48,924 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:48,961 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:48,999 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:49,037 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:49,959 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:49,997 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:50,036 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:50,072 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:51,024 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:51,063 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:51,100 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:51,139 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:52,080 - Avg. fwd time: 35.2169 / Avg. bwd time: 69.5030 / Avg. batch time: 860.9545 (ms) / GPU bubble ratio: 2.69%
2025-10-22 15:17:52,109 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:52,113 - Avg. fwd time: 17.3396 / Avg. bwd time: 28.4690 / Avg. batch time: 908.0706 (ms) / GPU bubble ratio: 59.64%
2025-10-22 15:17:52,141 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:52,154 - Avg. fwd time: 26.2332 / Avg. bwd time: 37.4893 / Avg. batch time: 969.2614 (ms) / GPU bubble ratio: 47.41%
2025-10-22 15:17:52,181 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:52,191 - Avg. fwd time: 22.8510 / Avg. bwd time: 34.1935 / Avg. batch time: 1023.1400 (ms) / GPU bubble ratio: 55.40%
2025-10-22 15:17:52,220 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:53,161 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:53,200 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:53,237 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:53,274 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:54,217 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:54,259 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:54,295 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:54,335 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:55,290 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:55,329 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:55,368 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:55,403 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:56,360 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:17:56,397 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:17:56,437 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:17:56,472 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:17:57,549 - Destroying the purge thread.
2025-10-22 15:17:57,549 - Destroying the purge thread.
2025-10-22 15:17:57,550 - Destroying the purge thread.
2025-10-22 15:17:57,575 - Destroying the purge thread.
2025-10-22 15:21:15,378 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:21:15,617 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:21:15,730 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:21:15,734 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:21:15,749 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:21:15,751 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:21:15,829 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:21:15,833 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:21:15,976 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:21:15,979 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:21:15,992 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:21:15,997 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:21:16,002 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:21:16,299 - Loading tokenizer from tokenizer.json
2025-10-22 15:21:16,703 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:21:20,247 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:21:20,280 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:21:20,322 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:20,393 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:21:20,393 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:21:20,628 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:20,628 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:21:20,629 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:21:20,808 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:21:20,808 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:21:20,856 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:20,858 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:20,871 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:21:20,916 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:21:20,917 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:21:20,918 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:21:20,918 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:21:21,436 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:21,436 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:21:21,438 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:21:21,440 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:21,440 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:21:21,442 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:21:24,303 - WandB logging enabled
2025-10-22 15:21:24,543 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:21:24,584 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:24,649 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:21:24,650 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:21:25,018 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:21:25,020 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:21:25,023 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:21:25,051 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:21:25,051 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:21:25,051 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:21:25,051 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:21:25,052 - Mixed precision training is disabled
2025-10-22 15:21:25,052 - Mixed precision training is disabled
2025-10-22 15:21:25,052 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:21:25,053 - Mixed precision training is disabled
2025-10-22 15:21:25,053 - Mixed precision training is disabled
2025-10-22 15:21:25,059 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:21:25,060 - Training starts at step 1
2025-10-22 15:21:29,829 - [Step 1] 🚦  Starting step
2025-10-22 15:21:32,163 - [Step 1] 🚦  Starting step
2025-10-22 15:21:33,784 - [Step 1] 🚦  Starting step
2025-10-22 15:21:35,450 - [Step 1] 🚦  Starting step
2025-10-22 15:21:35,878 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:36,006 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:36,127 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:36,228 - [Step 1] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:37,151 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:37,168 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:37,204 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:37,244 - [Step 2] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:38,392 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:38,434 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:38,472 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:38,508 - [Step 3] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:39,457 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:39,495 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:39,534 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:39,570 - [Step 4] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:40,471 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:40,509 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:40,547 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:40,584 - [Step 5] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:41,532 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:41,573 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:41,610 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:41,647 - [Step 6] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:42,566 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:42,604 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:42,643 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:42,680 - [Step 7] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:43,628 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:43,666 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:43,706 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:43,741 - [Step 8] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:44,685 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:44,721 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:44,763 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:44,798 - [Step 9] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:45,742 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:45,780 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:45,821 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:45,856 - [Step 10] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:46,793 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:46,830 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:46,870 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:46,905 - [Step 11] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:47,845 - Avg. fwd time: 35.5073 / Avg. bwd time: 69.4531 / Avg. batch time: 884.1375 (ms) / GPU bubble ratio: 5.03%
2025-10-22 15:21:47,871 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:47,877 - Avg. fwd time: 17.4985 / Avg. bwd time: 28.4737 / Avg. batch time: 931.1828 (ms) / GPU bubble ratio: 60.50%
2025-10-22 15:21:47,903 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:47,919 - Avg. fwd time: 28.6985 / Avg. bwd time: 37.3428 / Avg. batch time: 991.6689 (ms) / GPU bubble ratio: 46.72%
2025-10-22 15:21:47,947 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:47,956 - Avg. fwd time: 22.5386 / Avg. bwd time: 34.0602 / Avg. batch time: 1045.6282 (ms) / GPU bubble ratio: 56.70%
2025-10-22 15:21:47,983 - [Step 12] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:48,921 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:48,958 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:48,999 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:49,034 - [Step 13] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:49,985 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:50,023 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:50,063 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:50,099 - [Step 14] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:51,053 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:51,091 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:51,132 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:51,167 - [Step 15] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:52,134 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 3)
2025-10-22 15:21:52,169 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 2)
2025-10-22 15:21:52,208 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 1)
2025-10-22 15:21:52,243 - [Step 16] 🚦  Starting batch (microbatch: 0, stage: 0)
2025-10-22 15:21:53,336 - Destroying the purge thread.
2025-10-22 15:21:53,336 - Destroying the purge thread.
2025-10-22 15:21:53,338 - Destroying the purge thread.
2025-10-22 15:21:53,363 - Destroying the purge thread.
2025-10-22 15:26:10,875 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:26:11,029 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:26:11,063 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:26:11,067 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:26:11,129 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:26:11,201 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:26:11,205 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:26:11,212 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:26:11,316 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:26:11,319 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:26:12,273 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:26:12,475 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:26:12,479 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:26:12,765 - Loading tokenizer from tokenizer.json
2025-10-22 15:26:13,159 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:26:16,720 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:26:16,899 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:26:16,941 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,006 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:26:17,007 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:26:17,184 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,184 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:26:17,186 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:26:17,237 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:26:17,239 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:26:17,283 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,287 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,298 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:26:17,344 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:26:17,345 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:26:17,345 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:26:17,346 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:26:17,994 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,995 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:26:17,996 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:17,996 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:26:17,997 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:26:17,998 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:26:20,524 - WandB logging enabled
2025-10-22 15:26:20,532 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:26:20,579 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:20,647 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:26:20,649 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:26:20,838 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:26:20,841 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:26:20,843 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:26:20,871 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:26:20,871 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:26:20,871 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:26:20,871 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:26:20,872 - Mixed precision training is disabled
2025-10-22 15:26:20,872 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:26:20,872 - Mixed precision training is disabled
2025-10-22 15:26:20,872 - Mixed precision training is disabled
2025-10-22 15:26:20,872 - Mixed precision training is disabled
2025-10-22 15:26:20,877 - [Step 1] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:20,878 - [Step 1] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:20,879 - [Step 1] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:20,884 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:26:20,885 - Training starts at step 1
2025-10-22 15:26:20,886 - [Step 1] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:21,630 - [Step 1] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:21,752 - [Step 1] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:21,854 - [Step 1] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:21,954 - [Step 1] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:22,877 - [Step 2] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:22,894 - [Step 2] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:22,931 - [Step 2] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:22,966 - [Step 2] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:24,186 - [Step 3] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:24,228 - [Step 3] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:24,263 - [Step 3] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:24,302 - [Step 3] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:25,250 - [Step 4] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:25,288 - [Step 4] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:25,327 - [Step 4] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:25,365 - [Step 4] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:26,259 - [Step 5] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:26,298 - [Step 5] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:26,338 - [Step 5] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:26,373 - [Step 5] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:27,322 - [Step 6] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:27,366 - [Step 6] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:27,403 - [Step 6] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:27,438 - [Step 6] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:28,343 - [Step 7] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:28,382 - [Step 7] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:28,418 - [Step 7] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:28,457 - [Step 7] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:29,395 - [Step 8] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:29,434 - [Step 8] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:29,476 - [Step 8] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:29,512 - [Step 8] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:30,450 - [Step 9] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:30,486 - [Step 9] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:30,524 - [Step 9] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:30,561 - [Step 9] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:31,518 - [Step 10] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:31,557 - [Step 10] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:31,595 - [Step 10] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:31,632 - [Step 10] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:32,579 - [Step 11] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:32,616 - [Step 11] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:32,656 - [Step 11] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:32,692 - [Step 11] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:33,627 - Avg. fwd time: 35.0465 / Avg. bwd time: 69.4364 / Avg. batch time: 889.3051 (ms) / GPU bubble ratio: 6.01%
2025-10-22 15:26:33,656 - [Step 12] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:33,660 - Avg. fwd time: 17.2328 / Avg. bwd time: 28.5557 / Avg. batch time: 936.6503 (ms) / GPU bubble ratio: 60.89%
2025-10-22 15:26:33,687 - [Step 12] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:33,700 - Avg. fwd time: 29.5373 / Avg. bwd time: 37.6318 / Avg. batch time: 997.4893 (ms) / GPU bubble ratio: 46.13%
2025-10-22 15:26:33,726 - [Step 12] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:33,737 - Avg. fwd time: 22.5677 / Avg. bwd time: 34.1873 / Avg. batch time: 1051.3440 (ms) / GPU bubble ratio: 56.81%
2025-10-22 15:26:33,768 - [Step 12] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:34,708 - [Step 13] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:34,745 - [Step 13] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:34,784 - [Step 13] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:34,819 - [Step 13] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:35,757 - [Step 14] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:35,796 - [Step 14] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:35,835 - [Step 14] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:35,871 - [Step 14] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:36,812 - [Step 15] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:36,854 - [Step 15] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:36,891 - [Step 15] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:36,940 - [Step 15] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:37,883 - [Step 16] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:37,922 - [Step 16] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:37,958 - [Step 16] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:37,995 - [Step 16] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:39,109 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1618 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,478 [36m tflops: 11.56 [35m mfu: 3.70%[39m
2025-10-22 15:26:39,109 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:26:39,112 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1618 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,501 [36m tflops: 11.74 [35m mfu: 3.76%[39m
2025-10-22 15:26:39,113 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:26:39,121 - [31m step:  1 [32m loss: 12.2543 [38;2;180;60;0m grad_norm:  0.1618 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,768 [36m tflops: 13.82 [35m mfu: 4.43%[39m
2025-10-22 15:26:39,123 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:26:39,146 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1618 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,499 [36m tflops: 11.72 [35m mfu: 3.76%[39m
2025-10-22 15:26:39,147 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:26:39,148 - [Step 2] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:39,148 - [Step 2] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:39,148 - [Step 2] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:39,148 - [Step 2] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:39,169 - [Step 17] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:39,171 - [Step 17] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:39,171 - [Step 17] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:39,172 - [Step 17] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:40,319 - [Step 18] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:40,357 - [Step 18] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:40,394 - [Step 18] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:40,433 - [Step 18] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:41,386 - [Step 19] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:41,426 - [Step 19] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:41,464 - [Step 19] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:41,501 - [Step 19] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:42,421 - [Step 20] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:42,461 - [Step 20] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:42,506 - [Step 20] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:42,535 - [Step 20] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:43,491 - [Step 21] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:43,531 - [Step 21] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:43,573 - [Step 21] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:43,609 - [Step 21] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:44,545 - Avg. fwd time: 35.3488 / Avg. bwd time: 69.6405 / Avg. batch time: 889.5846 (ms) / GPU bubble ratio: 5.58%
2025-10-22 15:26:44,575 - [Step 22] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:44,579 - Avg. fwd time: 17.3436 / Avg. bwd time: 28.5230 / Avg. batch time: 936.9276 (ms) / GPU bubble ratio: 60.84%
2025-10-22 15:26:44,607 - [Step 22] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:44,620 - Avg. fwd time: 27.5082 / Avg. bwd time: 37.5477 / Avg. batch time: 997.7162 (ms) / GPU bubble ratio: 47.84%
2025-10-22 15:26:44,648 - [Step 22] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:44,657 - Avg. fwd time: 24.9109 / Avg. bwd time: 34.1274 / Avg. batch time: 1051.6244 (ms) / GPU bubble ratio: 55.09%
2025-10-22 15:26:44,697 - [Step 22] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:45,658 - [Step 23] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:45,695 - [Step 23] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:45,732 - [Step 23] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:45,770 - [Step 23] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:46,741 - [Step 24] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:46,774 - [Step 24] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:46,811 - [Step 24] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:46,848 - [Step 24] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:47,800 - [Step 25] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:47,838 - [Step 25] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:47,876 - [Step 25] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:47,913 - [Step 25] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:48,880 - [Step 26] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:48,918 - [Step 26] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:48,956 - [Step 26] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:48,992 - [Step 26] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:49,969 - [Step 27] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:50,008 - [Step 27] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:50,047 - [Step 27] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:50,084 - [Step 27] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:51,049 - [Step 28] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:51,087 - [Step 28] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:51,127 - [Step 28] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:51,167 - [Step 28] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:52,119 - [Step 29] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:52,155 - [Step 29] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:52,192 - [Step 29] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:52,229 - [Step 29] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:53,169 - [Step 30] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:53,212 - [Step 30] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:53,250 - [Step 30] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:53,289 - [Step 30] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:54,231 - [Step 31] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:54,269 - [Step 31] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:54,305 - [Step 31] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:54,341 - [Step 31] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:55,273 - Avg. fwd time: 35.8230 / Avg. bwd time: 69.9543 / Avg. batch time: 886.9542 (ms) / GPU bubble ratio: 4.59%
2025-10-22 15:26:55,301 - [Step 32] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:55,306 - Avg. fwd time: 17.3454 / Avg. bwd time: 28.5673 / Avg. batch time: 934.3904 (ms) / GPU bubble ratio: 60.69%
2025-10-22 15:26:55,335 - [Step 32] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:55,347 - Avg. fwd time: 26.7467 / Avg. bwd time: 37.5816 / Avg. batch time: 995.2122 (ms) / GPU bubble ratio: 48.29%
2025-10-22 15:26:55,375 - [Step 32] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:55,385 - Avg. fwd time: 24.1622 / Avg. bwd time: 34.1730 / Avg. batch time: 1049.1555 (ms) / GPU bubble ratio: 55.52%
2025-10-22 15:26:55,415 - [Step 32] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:55,962 - [Step 3] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:56,353 - [Step 3] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:56,391 - [Step 3] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:56,428 - [Step 3] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:26:56,442 - [Step 33] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:56,445 - [Step 33] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:56,453 - [Step 33] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:56,459 - [Step 33] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:57,416 - [Step 34] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:57,457 - [Step 34] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:57,495 - [Step 34] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:57,531 - [Step 34] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:58,507 - [Step 35] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:58,545 - [Step 35] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:58,584 - [Step 35] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:58,621 - [Step 35] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:59,587 - [Step 36] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:59,625 - [Step 36] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:59,665 - [Step 36] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:26:59,703 - [Step 36] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:00,667 - [Step 37] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:00,712 - [Step 37] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:00,750 - [Step 37] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:00,790 - [Step 37] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:01,766 - [Step 38] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:01,799 - [Step 38] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:01,838 - [Step 38] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:01,878 - [Step 38] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:02,826 - [Step 39] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:02,863 - [Step 39] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:02,902 - [Step 39] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:02,941 - [Step 39] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:03,896 - [Step 40] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:03,936 - [Step 40] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:03,973 - [Step 40] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:04,010 - [Step 40] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:04,977 - [Step 41] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:05,016 - [Step 41] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:05,054 - [Step 41] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:05,091 - [Step 41] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:06,060 - Avg. fwd time: 36.1518 / Avg. bwd time: 70.1368 / Avg. batch time: 886.5571 (ms) / GPU bubble ratio: 4.09%
2025-10-22 15:27:06,088 - [Step 42] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:06,093 - Avg. fwd time: 17.4022 / Avg. bwd time: 28.5979 / Avg. batch time: 934.0067 (ms) / GPU bubble ratio: 60.60%
2025-10-22 15:27:06,120 - [Step 42] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:06,135 - Avg. fwd time: 26.4930 / Avg. bwd time: 37.6284 / Avg. batch time: 994.9373 (ms) / GPU bubble ratio: 48.44%
2025-10-22 15:27:06,161 - [Step 42] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:06,172 - Avg. fwd time: 23.7594 / Avg. bwd time: 34.1824 / Avg. batch time: 1048.9729 (ms) / GPU bubble ratio: 55.81%
2025-10-22 15:27:06,202 - [Step 42] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:07,166 - [Step 43] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:07,206 - [Step 43] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:07,244 - [Step 43] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:07,282 - [Step 43] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:08,235 - [Step 44] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:08,272 - [Step 44] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:08,311 - [Step 44] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:08,351 - [Step 44] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:09,324 - [Step 45] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:09,364 - [Step 45] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:09,404 - [Step 45] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:09,443 - [Step 45] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:10,364 - [Step 46] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:10,403 - [Step 46] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:10,443 - [Step 46] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:10,481 - [Step 46] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:11,447 - [Step 47] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:11,484 - [Step 47] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:11,524 - [Step 47] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:11,558 - [Step 47] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:12,513 - [Step 48] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:12,556 - [Step 48] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:12,593 - [Step 48] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:12,629 - [Step 48] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:13,226 - [Step 4] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:13,615 - [Step 4] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:13,655 - [Step 4] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:13,691 - [Step 4] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:13,705 - [Step 49] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:13,709 - [Step 49] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:13,716 - [Step 49] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:13,723 - [Step 49] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:14,671 - [Step 50] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:14,710 - [Step 50] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:14,749 - [Step 50] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:14,786 - [Step 50] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:15,765 - [Step 51] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:15,802 - [Step 51] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:15,841 - [Step 51] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:15,879 - [Step 51] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:16,807 - Avg. fwd time: 36.2325 / Avg. bwd time: 70.2674 / Avg. batch time: 885.4782 (ms) / GPU bubble ratio: 3.78%
2025-10-22 15:27:16,835 - [Step 52] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:16,840 - Avg. fwd time: 17.4211 / Avg. bwd time: 28.6283 / Avg. batch time: 932.9533 (ms) / GPU bubble ratio: 60.51%
2025-10-22 15:27:16,867 - [Step 52] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:16,880 - Avg. fwd time: 26.2719 / Avg. bwd time: 37.6575 / Avg. batch time: 993.9203 (ms) / GPU bubble ratio: 48.54%
2025-10-22 15:27:16,907 - [Step 52] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:16,919 - Avg. fwd time: 23.6757 / Avg. bwd time: 34.2039 / Avg. batch time: 1048.0004 (ms) / GPU bubble ratio: 55.82%
2025-10-22 15:27:16,948 - [Step 52] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:17,902 - [Step 53] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:17,940 - [Step 53] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:17,979 - [Step 53] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:18,017 - [Step 53] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:18,985 - [Step 54] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:19,025 - [Step 54] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:19,063 - [Step 54] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:19,100 - [Step 54] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:20,041 - [Step 55] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:20,079 - [Step 55] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:20,118 - [Step 55] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:20,158 - [Step 55] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:21,109 - [Step 56] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:21,148 - [Step 56] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:21,186 - [Step 56] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:21,224 - [Step 56] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:22,190 - [Step 57] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:22,228 - [Step 57] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:22,267 - [Step 57] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:22,304 - [Step 57] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:23,280 - [Step 58] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:23,318 - [Step 58] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:23,361 - [Step 58] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:23,400 - [Step 58] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:24,369 - [Step 59] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:24,407 - [Step 59] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:24,446 - [Step 59] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:24,483 - [Step 59] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:25,460 - [Step 60] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:25,499 - [Step 60] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:25,539 - [Step 60] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:25,577 - [Step 60] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:26,525 - [Step 61] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:26,563 - [Step 61] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:26,601 - [Step 61] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:26,638 - [Step 61] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:27,602 - Avg. fwd time: 36.4148 / Avg. bwd time: 70.3391 / Avg. batch time: 885.7008 (ms) / GPU bubble ratio: 3.58%
2025-10-22 15:27:27,627 - [Step 62] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:27,634 - Avg. fwd time: 17.4610 / Avg. bwd time: 28.6611 / Avg. batch time: 933.1983 (ms) / GPU bubble ratio: 60.46%
2025-10-22 15:27:27,658 - [Step 62] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:27,674 - Avg. fwd time: 26.1641 / Avg. bwd time: 37.6833 / Avg. batch time: 994.2112 (ms) / GPU bubble ratio: 48.62%
2025-10-22 15:27:27,699 - [Step 62] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:27,712 - Avg. fwd time: 23.5822 / Avg. bwd time: 34.2421 / Avg. batch time: 1048.4088 (ms) / GPU bubble ratio: 55.88%
2025-10-22 15:27:27,738 - [Step 62] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:28,700 - [Step 63] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:28,739 - [Step 63] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:28,776 - [Step 63] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:28,813 - [Step 63] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:29,777 - [Step 64] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:29,816 - [Step 64] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:29,856 - [Step 64] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:29,892 - [Step 64] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:30,476 - [Step 5] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:30,870 - [Step 5] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:30,908 - [Step 5] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:30,944 - [Step 5] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:27:30,958 - [Step 65] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:30,961 - [Step 65] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:30,969 - [Step 65] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:30,975 - [Step 65] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:31,943 - [Step 66] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:31,990 - [Step 66] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:32,026 - [Step 66] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:32,063 - [Step 66] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:33,017 - [Step 67] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:33,048 - [Step 67] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:33,087 - [Step 67] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:33,125 - [Step 67] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:34,101 - [Step 68] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:34,140 - [Step 68] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:34,179 - [Step 68] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:34,216 - [Step 68] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:35,166 - [Step 69] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:35,204 - [Step 69] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:35,247 - [Step 69] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:35,284 - [Step 69] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:36,257 - [Step 70] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:36,298 - [Step 70] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:36,334 - [Step 70] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:36,372 - [Step 70] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:37,336 - [Step 71] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:37,374 - [Step 71] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:37,413 - [Step 71] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:37,452 - [Step 71] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:38,370 - Avg. fwd time: 36.4839 / Avg. bwd time: 70.4072 / Avg. batch time: 885.4928 (ms) / GPU bubble ratio: 3.43%
2025-10-22 15:27:38,395 - [Step 72] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:38,403 - Avg. fwd time: 17.4415 / Avg. bwd time: 28.6747 / Avg. batch time: 933.0180 (ms) / GPU bubble ratio: 60.46%
2025-10-22 15:27:38,429 - [Step 72] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:38,445 - Avg. fwd time: 26.1332 / Avg. bwd time: 37.7352 / Avg. batch time: 994.1091 (ms) / GPU bubble ratio: 48.60%
2025-10-22 15:27:38,469 - [Step 72] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:38,483 - Avg. fwd time: 23.5298 / Avg. bwd time: 34.2670 / Avg. batch time: 1048.3700 (ms) / GPU bubble ratio: 55.90%
2025-10-22 15:27:38,510 - [Step 72] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:39,476 - [Step 73] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:39,516 - [Step 73] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:39,554 - [Step 73] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:39,598 - [Step 73] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:40,575 - [Step 74] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:40,614 - [Step 74] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:40,655 - [Step 74] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:40,690 - [Step 74] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:41,657 - [Step 75] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:41,697 - [Step 75] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:41,735 - [Step 75] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:41,771 - [Step 75] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:42,724 - [Step 76] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:42,762 - [Step 76] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:42,801 - [Step 76] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:42,839 - [Step 76] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:43,793 - [Step 77] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:43,831 - [Step 77] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:43,871 - [Step 77] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:43,910 - [Step 77] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:44,883 - [Step 78] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:44,921 - [Step 78] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:44,960 - [Step 78] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:44,999 - [Step 78] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:45,954 - [Step 79] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:45,994 - [Step 79] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:46,035 - [Step 79] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:46,070 - [Step 79] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:47,034 - [Step 80] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:47,073 - [Step 80] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:47,112 - [Step 80] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:47,150 - [Step 80] 🚦  Starting local step (in the unit of a batch computation)
2025-10-22 15:27:47,756 - [Step 6] 🚦  Starting global accumulation step (in the unit of optimizer)
2025-10-22 15:28:06,758 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:28:06,928 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:28:06,931 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:28:06,935 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:28:07,561 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:28:07,663 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:28:07,777 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:28:08,238 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:28:08,242 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:28:08,246 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:28:08,250 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:28:08,260 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:28:08,263 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:28:08,561 - Loading tokenizer from tokenizer.json
2025-10-22 15:28:08,951 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:28:11,912 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:28:12,340 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:28:12,372 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:28:12,381 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:12,395 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:28:12,414 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:12,440 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:28:12,441 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:28:12,476 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:28:12,476 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:28:12,615 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:12,615 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:28:12,617 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:28:12,630 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:28:12,650 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:12,651 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:28:12,652 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:28:12,671 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:12,730 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:28:12,731 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:28:13,575 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:13,575 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:28:13,577 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:28:16,461 - WandB logging enabled
2025-10-22 15:28:16,648 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:28:16,686 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:16,751 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:28:16,752 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:28:17,279 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:28:17,282 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:28:17,285 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:28:17,312 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:28:17,313 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:28:17,313 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:28:17,313 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:28:17,313 - Mixed precision training is disabled
2025-10-22 15:28:17,313 - Mixed precision training is disabled
2025-10-22 15:28:17,314 - Mixed precision training is disabled
2025-10-22 15:28:17,314 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:28:17,314 - Mixed precision training is disabled
2025-10-22 15:28:17,319 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:28:17,320 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:28:17,321 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:28:17,322 - Training starts at step 1
2025-10-22 15:28:17,323 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:28:17,327 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:28:29,983 - Avg. fwd time: 34.8861 / Avg. bwd time: 70.3819 / Avg. batch time: 865.4558 (ms) / GPU bubble ratio: 2.69%
2025-10-22 15:28:30,017 - Avg. fwd time: 17.4820 / Avg. bwd time: 28.6180 / Avg. batch time: 913.0177 (ms) / GPU bubble ratio: 59.61%
2025-10-22 15:28:30,057 - Avg. fwd time: 26.6981 / Avg. bwd time: 37.9640 / Avg. batch time: 974.6342 (ms) / GPU bubble ratio: 46.92%
2025-10-22 15:28:30,095 - Avg. fwd time: 23.1969 / Avg. bwd time: 34.3278 / Avg. batch time: 1028.6756 (ms) / GPU bubble ratio: 55.26%
2025-10-22 15:28:35,553 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1647 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,432 [36m tflops: 11.20 [35m mfu: 3.59%[39m
2025-10-22 15:28:35,553 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:28:35,558 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1647 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,416 [36m tflops: 11.07 [35m mfu: 3.55%[39m
2025-10-22 15:28:35,559 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:28:35,564 - [31m step:  1 [32m loss: 12.2478 [38;2;180;60;0m grad_norm:  0.1647 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,736 [36m tflops: 13.57 [35m mfu: 4.35%[39m
2025-10-22 15:28:35,566 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:28:35,588 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1647 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,412 [36m tflops: 11.04 [35m mfu: 3.54%[39m
2025-10-22 15:28:35,589 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:28:35,590 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:28:35,590 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:28:35,590 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:28:35,590 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:28:41,123 - Avg. fwd time: 36.0765 / Avg. bwd time: 70.6698 / Avg. batch time: 876.8777 (ms) / GPU bubble ratio: 2.61%
2025-10-22 15:28:41,157 - Avg. fwd time: 17.4435 / Avg. bwd time: 28.6404 / Avg. batch time: 937.1713 (ms) / GPU bubble ratio: 60.66%
2025-10-22 15:28:41,198 - Avg. fwd time: 25.6270 / Avg. bwd time: 37.7693 / Avg. batch time: 998.5847 (ms) / GPU bubble ratio: 49.21%
2025-10-22 15:28:41,235 - Avg. fwd time: 23.1993 / Avg. bwd time: 34.2835 / Avg. batch time: 1052.7906 (ms) / GPU bubble ratio: 56.32%
2025-10-22 15:28:51,849 - Avg. fwd time: 36.1253 / Avg. bwd time: 70.8393 / Avg. batch time: 878.6776 (ms) / GPU bubble ratio: 2.61%
2025-10-22 15:28:51,882 - Avg. fwd time: 17.5371 / Avg. bwd time: 28.6865 / Avg. batch time: 934.8892 (ms) / GPU bubble ratio: 60.45%
2025-10-22 15:28:51,924 - Avg. fwd time: 25.7759 / Avg. bwd time: 37.7672 / Avg. batch time: 996.2835 (ms) / GPU bubble ratio: 48.98%
2025-10-22 15:28:51,962 - Avg. fwd time: 23.0875 / Avg. bwd time: 34.3468 / Avg. batch time: 1050.7259 (ms) / GPU bubble ratio: 56.27%
2025-10-22 15:28:52,584 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:28:52,975 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:28:53,015 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:28:53,052 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:29:02,635 - Avg. fwd time: 36.3230 / Avg. bwd time: 70.9110 / Avg. batch time: 880.7579 (ms) / GPU bubble ratio: 2.60%
2025-10-22 15:29:02,668 - Avg. fwd time: 17.5403 / Avg. bwd time: 28.7176 / Avg. batch time: 934.8511 (ms) / GPU bubble ratio: 60.41%
2025-10-22 15:29:02,709 - Avg. fwd time: 25.9256 / Avg. bwd time: 37.7793 / Avg. batch time: 996.1824 (ms) / GPU bubble ratio: 48.84%
2025-10-22 15:29:02,746 - Avg. fwd time: 23.0974 / Avg. bwd time: 34.3404 / Avg. batch time: 1050.6086 (ms) / GPU bubble ratio: 56.26%
2025-10-22 15:29:09,801 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:29:10,197 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:29:10,236 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:29:10,272 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:29:13,378 - Avg. fwd time: 36.3664 / Avg. bwd time: 70.9248 / Avg. batch time: 881.1146 (ms) / GPU bubble ratio: 2.59%
2025-10-22 15:29:13,411 - Avg. fwd time: 17.5176 / Avg. bwd time: 28.7297 / Avg. batch time: 933.9362 (ms) / GPU bubble ratio: 60.39%
2025-10-22 15:29:13,453 - Avg. fwd time: 25.9827 / Avg. bwd time: 37.7697 / Avg. batch time: 995.2550 (ms) / GPU bubble ratio: 48.75%
2025-10-22 15:29:13,490 - Avg. fwd time: 23.2289 / Avg. bwd time: 34.3538 / Avg. batch time: 1049.6881 (ms) / GPU bubble ratio: 56.11%
2025-10-22 15:29:24,127 - Avg. fwd time: 36.4734 / Avg. bwd time: 70.9013 / Avg. batch time: 881.7173 (ms) / GPU bubble ratio: 2.58%
2025-10-22 15:29:24,161 - Avg. fwd time: 17.5174 / Avg. bwd time: 28.7568 / Avg. batch time: 933.7254 (ms) / GPU bubble ratio: 60.35%
2025-10-22 15:29:24,200 - Avg. fwd time: 26.0036 / Avg. bwd time: 37.7891 / Avg. batch time: 995.0337 (ms) / GPU bubble ratio: 48.71%
2025-10-22 15:29:24,238 - Avg. fwd time: 23.2552 / Avg. bwd time: 34.3800 / Avg. batch time: 1049.5001 (ms) / GPU bubble ratio: 56.07%
2025-10-22 15:29:27,016 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:29:27,404 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:29:27,445 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:29:27,481 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:29:34,905 - Avg. fwd time: 36.5432 / Avg. bwd time: 70.8960 / Avg. batch time: 882.2551 (ms) / GPU bubble ratio: 2.58%
2025-10-22 15:29:34,938 - Avg. fwd time: 17.5324 / Avg. bwd time: 28.7728 / Avg. batch time: 933.6680 (ms) / GPU bubble ratio: 60.32%
2025-10-22 15:29:34,978 - Avg. fwd time: 25.9595 / Avg. bwd time: 37.7824 / Avg. batch time: 994.9825 (ms) / GPU bubble ratio: 48.75%
2025-10-22 15:29:35,016 - Avg. fwd time: 23.2498 / Avg. bwd time: 34.3880 / Avg. batch time: 1049.4617 (ms) / GPU bubble ratio: 56.06%
2025-10-22 15:29:44,270 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:29:44,666 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:29:44,706 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:29:44,742 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:29:45,739 - Avg. fwd time: 36.6203 / Avg. bwd time: 70.9645 / Avg. batch time: 883.4568 (ms) / GPU bubble ratio: 2.58%
2025-10-22 15:29:45,773 - Avg. fwd time: 17.5538 / Avg. bwd time: 28.7800 / Avg. batch time: 934.4280 (ms) / GPU bubble ratio: 60.33%
2025-10-22 15:29:45,813 - Avg. fwd time: 26.0286 / Avg. bwd time: 37.7935 / Avg. batch time: 995.7331 (ms) / GPU bubble ratio: 48.72%
2025-10-22 15:29:45,850 - Avg. fwd time: 23.2988 / Avg. bwd time: 34.3988 / Avg. batch time: 1050.2175 (ms) / GPU bubble ratio: 56.05%
2025-10-22 15:29:56,488 - Avg. fwd time: 36.5769 / Avg. bwd time: 71.0142 / Avg. batch time: 883.4483 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:29:56,521 - Avg. fwd time: 17.5738 / Avg. bwd time: 28.8013 / Avg. batch time: 934.1619 (ms) / GPU bubble ratio: 60.29%
2025-10-22 15:29:56,564 - Avg. fwd time: 26.0334 / Avg. bwd time: 37.8046 / Avg. batch time: 995.5053 (ms) / GPU bubble ratio: 48.70%
2025-10-22 15:29:56,601 - Avg. fwd time: 23.3247 / Avg. bwd time: 34.4224 / Avg. batch time: 1050.0057 (ms) / GPU bubble ratio: 56.00%
2025-10-22 15:30:01,566 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:30:01,963 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:30:02,004 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:30:02,040 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:30:07,369 - Avg. fwd time: 36.6860 / Avg. bwd time: 71.0630 / Avg. batch time: 884.6825 (ms) / GPU bubble ratio: 2.56%
2025-10-22 15:30:07,403 - Avg. fwd time: 17.5667 / Avg. bwd time: 28.8106 / Avg. batch time: 935.1336 (ms) / GPU bubble ratio: 60.32%
2025-10-22 15:30:07,443 - Avg. fwd time: 26.0925 / Avg. bwd time: 37.8045 / Avg. batch time: 996.4665 (ms) / GPU bubble ratio: 48.70%
2025-10-22 15:30:07,481 - Avg. fwd time: 23.3626 / Avg. bwd time: 34.4289 / Avg. batch time: 1050.9808 (ms) / GPU bubble ratio: 56.01%
2025-10-22 15:30:18,202 - Avg. fwd time: 36.7036 / Avg. bwd time: 71.1148 / Avg. batch time: 885.3084 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:30:18,235 - Avg. fwd time: 17.5748 / Avg. bwd time: 28.8210 / Avg. batch time: 935.5291 (ms) / GPU bubble ratio: 60.33%
2025-10-22 15:30:18,278 - Avg. fwd time: 26.1060 / Avg. bwd time: 37.8125 / Avg. batch time: 996.9165 (ms) / GPU bubble ratio: 48.71%
2025-10-22 15:30:18,315 - Avg. fwd time: 23.3685 / Avg. bwd time: 34.4401 / Avg. batch time: 1051.4513 (ms) / GPU bubble ratio: 56.02%
2025-10-22 15:30:18,943 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:30:19,337 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:30:19,378 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:30:19,414 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:30:29,069 - Avg. fwd time: 36.7690 / Avg. bwd time: 71.1383 / Avg. batch time: 886.0350 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:30:29,102 - Avg. fwd time: 17.5747 / Avg. bwd time: 28.8292 / Avg. batch time: 936.0831 (ms) / GPU bubble ratio: 60.34%
2025-10-22 15:30:29,142 - Avg. fwd time: 26.2140 / Avg. bwd time: 37.8275 / Avg. batch time: 997.4681 (ms) / GPU bubble ratio: 48.64%
2025-10-22 15:30:29,180 - Avg. fwd time: 23.3392 / Avg. bwd time: 34.4442 / Avg. batch time: 1052.0104 (ms) / GPU bubble ratio: 56.06%
2025-10-22 15:30:36,299 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:30:36,691 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:30:36,731 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:30:36,768 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:30:39,908 - Avg. fwd time: 36.7934 / Avg. bwd time: 71.1581 / Avg. batch time: 886.4329 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:30:39,941 - Avg. fwd time: 17.5835 / Avg. bwd time: 28.8432 / Avg. batch time: 936.3345 (ms) / GPU bubble ratio: 60.33%
2025-10-22 15:30:39,981 - Avg. fwd time: 26.2482 / Avg. bwd time: 37.8364 / Avg. batch time: 997.7588 (ms) / GPU bubble ratio: 48.62%
2025-10-22 15:30:40,019 - Avg. fwd time: 23.3779 / Avg. bwd time: 34.4464 / Avg. batch time: 1052.3072 (ms) / GPU bubble ratio: 56.04%
2025-10-22 15:30:50,733 - Avg. fwd time: 36.8243 / Avg. bwd time: 71.1770 / Avg. batch time: 886.8040 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:30:50,766 - Avg. fwd time: 17.6011 / Avg. bwd time: 28.8546 / Avg. batch time: 936.5815 (ms) / GPU bubble ratio: 60.32%
2025-10-22 15:30:50,808 - Avg. fwd time: 26.2439 / Avg. bwd time: 37.8535 / Avg. batch time: 998.0428 (ms) / GPU bubble ratio: 48.62%
2025-10-22 15:30:50,846 - Avg. fwd time: 23.3991 / Avg. bwd time: 34.4565 / Avg. batch time: 1052.6080 (ms) / GPU bubble ratio: 56.03%
2025-10-22 15:30:53,634 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:30:54,028 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:30:54,069 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:30:54,106 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:31:01,600 - Avg. fwd time: 36.8587 / Avg. bwd time: 71.1928 / Avg. batch time: 887.2508 (ms) / GPU bubble ratio: 2.57%
2025-10-22 15:31:01,637 - Avg. fwd time: 17.6097 / Avg. bwd time: 28.8608 / Avg. batch time: 936.9245 (ms) / GPU bubble ratio: 60.32%
2025-10-22 15:31:01,678 - Avg. fwd time: 26.2792 / Avg. bwd time: 37.8610 / Avg. batch time: 998.4274 (ms) / GPU bubble ratio: 48.61%
2025-10-22 15:31:01,715 - Avg. fwd time: 23.4058 / Avg. bwd time: 34.4579 / Avg. batch time: 1053.0016 (ms) / GPU bubble ratio: 56.04%
2025-10-22 15:31:11,494 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.6661 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 15:31:11,497 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.6661 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 15:31:11,507 - [31m step: 10 [32m loss:  8.1933 [38;2;180;60;0m grad_norm:  0.6661 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,891 [36m tflops: 14.78 [35m mfu: 4.74%[39m
2025-10-22 15:31:11,508 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.6661 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,891 [36m tflops: 14.79 [35m mfu: 4.74%[39m
2025-10-22 15:31:11,518 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 15:31:11,518 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 15:31:11,518 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 15:31:11,713 - Destroying the purge thread.
2025-10-22 15:34:43,129 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:34:43,317 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:34:43,321 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:34:43,396 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:34:43,399 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:34:43,582 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:34:43,622 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:34:43,626 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:34:43,630 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:34:43,638 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:34:43,643 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:34:43,750 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:34:43,753 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:34:44,032 - Loading tokenizer from tokenizer.json
2025-10-22 15:34:44,439 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:34:47,649 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:34:47,919 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:34:47,959 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,021 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:34:48,022 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:34:48,281 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:34:48,287 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:34:48,323 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,327 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,334 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:34:48,377 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:34:48,377 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:34:48,381 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:34:48,381 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:34:48,951 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,951 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:34:48,953 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:34:48,960 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,961 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:34:48,962 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:34:48,973 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:48,974 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:34:48,976 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:34:52,338 - WandB logging enabled
2025-10-22 15:34:52,341 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:34:52,381 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:52,445 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:34:52,447 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:34:52,621 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:34:52,623 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:34:52,626 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:34:52,653 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:34:52,654 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:34:52,654 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:34:52,655 - Mixed precision training is disabled
2025-10-22 15:34:52,655 - Mixed precision training is disabled
2025-10-22 15:34:52,655 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:34:52,656 - Mixed precision training is disabled
2025-10-22 15:34:52,659 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:34:52,659 - Mixed precision training is disabled
2025-10-22 15:34:52,660 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:34:52,662 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:34:52,663 - Training starts at step 1
2025-10-22 15:35:10,743 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1687 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,438 [36m tflops: 11.24 [35m mfu: 3.60%[39m
2025-10-22 15:35:10,744 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:35:10,746 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1687 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,461 [36m tflops: 11.42 [35m mfu: 3.66%[39m
2025-10-22 15:35:10,746 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:35:10,754 - [31m step:  1 [32m loss: 12.2550 [38;2;180;60;0m grad_norm:  0.1687 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,784 [36m tflops: 13.95 [35m mfu: 4.47%[39m
2025-10-22 15:35:10,755 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:35:10,780 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1687 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,459 [36m tflops: 11.41 [35m mfu: 3.66%[39m
2025-10-22 15:35:10,781 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:35:10,782 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:35:27,560 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:35:44,730 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:36:01,861 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:36:19,041 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:36:36,270 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:36:53,616 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:37:10,918 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:37:28,213 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:37:45,991 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.4001 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,900 [36m tflops: 14.85 [35m mfu: 4.76%[39m
2025-10-22 15:37:45,994 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.4001 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,900 [36m tflops: 14.85 [35m mfu: 4.76%[39m
2025-10-22 15:37:46,004 - [31m step: 10 [32m loss:  8.1373 [38;2;180;60;0m grad_norm:  0.4001 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,900 [36m tflops: 14.85 [35m mfu: 4.76%[39m
2025-10-22 15:37:46,009 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.4001 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,900 [36m tflops: 14.85 [35m mfu: 4.76%[39m
2025-10-22 15:37:46,214 - Destroying the purge thread.
2025-10-22 15:38:45,281 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:38:45,384 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:38:45,490 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:38:45,561 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 15:38:45,885 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:38:45,887 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:38:45,889 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:38:45,890 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:38:45,893 - [GC] Initial GC collection 0.00 seconds
2025-10-22 15:38:45,894 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:38:45,898 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:38:45,981 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 15:38:45,985 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 15:38:46,280 - Loading tokenizer from tokenizer.json
2025-10-22 15:38:46,669 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 15:38:49,670 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 15:38:50,095 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:38:50,129 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:38:50,134 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:50,148 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 15:38:50,170 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:50,187 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:38:50,195 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 15:38:50,196 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:38:50,226 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:50,228 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 15:38:50,229 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:38:50,297 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 15:38:50,298 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:38:50,390 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:50,391 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:38:50,392 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 15:38:50,407 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:50,407 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:38:50,409 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 15:38:51,246 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:51,246 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:38:51,248 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 15:38:53,745 - WandB logging enabled
2025-10-22 15:38:53,747 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 15:38:53,789 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:53,852 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 15:38:53,853 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 15:38:54,030 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 15:38:54,032 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 15:38:54,034 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 15:38:54,062 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:38:54,062 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 15:38:54,063 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:38:54,063 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:38:54,063 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 15:38:54,064 - Mixed precision training is disabled
2025-10-22 15:38:54,064 - Mixed precision training is disabled
2025-10-22 15:38:54,064 - Mixed precision training is disabled
2025-10-22 15:38:54,064 - Mixed precision training is disabled
2025-10-22 15:38:54,069 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 15:38:54,070 - Training starts at step 1
2025-10-22 15:38:54,073 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 15:39:12,363 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1672 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,477 [36m tflops: 11.54 [35m mfu: 3.70%[39m
2025-10-22 15:39:12,364 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:39:12,366 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1672 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,480 [36m tflops: 11.57 [35m mfu: 3.71%[39m
2025-10-22 15:39:12,367 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:39:12,374 - [31m step:  1 [32m loss: 12.2561 [38;2;180;60;0m grad_norm:  0.1672 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,764 [36m tflops: 13.79 [35m mfu: 4.42%[39m
2025-10-22 15:39:12,376 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:39:12,401 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1672 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,472 [36m tflops: 11.51 [35m mfu: 3.69%[39m
2025-10-22 15:39:12,402 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 15:39:12,403 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 15:39:29,252 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 15:39:46,462 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 15:40:03,677 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 15:40:20,831 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 15:40:38,146 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 15:40:55,431 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 15:41:12,791 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 15:41:30,180 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 15:41:47,977 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2770 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,895 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 15:41:47,980 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2770 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,895 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 15:41:47,990 - [31m step: 10 [32m loss:  8.1276 [38;2;180;60;0m grad_norm:  0.2770 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,895 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 15:41:47,995 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2770 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,895 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 15:41:48,532 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1541_real_warmupend10_rank3.svg
> Batch Time: 1022.69 ms, GPU Bubble Ratio: 54.56%, 49.94%, 63.82%, 15.42%
2025-10-22 15:41:48,676 - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/freeze_ratio_history/rank3/251022_1541_stage3_warmupend10.svg
2025-10-22 15:41:48,677 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 15:42:11,885 - 🚦  Starting global accumulation step 12 (in the unit of optimizer)
2025-10-22 15:42:32,839 - 🚦  Starting global accumulation step 13 (in the unit of optimizer)
2025-10-22 15:42:49,972 - 🚦  Starting global accumulation step 14 (in the unit of optimizer)
2025-10-22 15:43:07,166 - 🚦  Starting global accumulation step 15 (in the unit of optimizer)
2025-10-22 15:43:24,308 - 🚦  Starting global accumulation step 16 (in the unit of optimizer)
2025-10-22 15:43:41,467 - 🚦  Starting global accumulation step 17 (in the unit of optimizer)
2025-10-22 15:43:58,684 - 🚦  Starting global accumulation step 18 (in the unit of optimizer)
2025-10-22 15:44:15,913 - 🚦  Starting global accumulation step 19 (in the unit of optimizer)
2025-10-22 15:44:33,119 - 🚦  Starting global accumulation step 20 (in the unit of optimizer)
2025-10-22 15:44:50,416 - [Step 20] 〰️ Monitoring Upperbound
2025-10-22 15:44:50,902 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2145 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,791 [36m tflops: 14.00 [35m mfu: 4.49%[39m
2025-10-22 15:44:50,906 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2145 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,791 [36m tflops: 14.00 [35m mfu: 4.49%[39m
2025-10-22 15:44:50,915 - [31m step: 20 [32m loss:  7.0650 [38;2;180;60;0m grad_norm:  0.2145 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,791 [36m tflops: 14.00 [35m mfu: 4.49%[39m
2025-10-22 15:44:50,923 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2145 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,791 [36m tflops: 14.00 [35m mfu: 4.49%[39m
2025-10-22 15:44:51,447 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1544_real_step20_rank3.svg
> Batch Time: 1018.56 ms, GPU Bubble Ratio: 54.28%, 49.46%, 63.65%, 15.51%
2025-10-22 15:44:51,590 - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/freeze_ratio_history/rank3/251022_1544_stage3_step20.svg
2025-10-22 15:44:51,591 - 🚦  Starting global accumulation step 21 (in the unit of optimizer)
2025-10-22 15:45:08,367 - 🚦  Starting global accumulation step 22 (in the unit of optimizer)
2025-10-22 15:45:25,617 - 🚦  Starting global accumulation step 23 (in the unit of optimizer)
2025-10-22 15:45:42,836 - 🚦  Starting global accumulation step 24 (in the unit of optimizer)
2025-10-22 15:46:00,152 - 🚦  Starting global accumulation step 25 (in the unit of optimizer)
2025-10-22 15:46:17,413 - 🚦  Starting global accumulation step 26 (in the unit of optimizer)
2025-10-22 15:46:34,806 - 🚦  Starting global accumulation step 27 (in the unit of optimizer)
2025-10-22 15:46:52,121 - 🚦  Starting global accumulation step 28 (in the unit of optimizer)
2025-10-22 15:47:09,436 - 🚦  Starting global accumulation step 29 (in the unit of optimizer)
2025-10-22 15:47:26,809 - 🚦  Starting global accumulation step 30 (in the unit of optimizer)
2025-10-22 15:47:44,224 - Destroying the purge thread.
2025-10-22 15:47:44,620 - Destroying the purge thread.
2025-10-22 15:47:44,660 - Destroying the purge thread.
2025-10-22 15:47:44,696 - Destroying the purge thread.
2025-10-22 16:06:42,778 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:06:42,782 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:06:43,010 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:06:43,167 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:06:43,171 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:06:43,175 - [GC] Initial GC collection 0.00 seconds
2025-10-22 16:06:43,178 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:06:43,182 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:06:43,251 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:06:43,266 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:06:43,269 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:06:43,417 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:06:43,421 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:06:43,742 - Loading tokenizer from tokenizer.json
2025-10-22 16:06:44,143 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 16:06:47,262 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 16:06:47,693 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:06:47,735 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:47,751 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 16:06:47,799 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 16:06:47,800 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:06:47,876 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:06:47,917 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:47,980 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 16:06:47,981 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:06:48,642 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:48,643 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:06:48,644 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 16:06:48,653 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:48,653 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:06:48,655 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 16:06:49,759 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:06:49,799 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:49,860 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 16:06:49,860 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:06:50,393 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:50,394 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:06:50,396 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 16:06:51,126 - WandB logging enabled
2025-10-22 16:06:51,129 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:06:51,172 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:51,240 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 16:06:51,241 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:06:51,436 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:06:51,438 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:06:51,440 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 16:06:51,468 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:06:51,468 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 16:06:51,468 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:06:51,468 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:06:51,469 - Mixed precision training is disabled
2025-10-22 16:06:51,469 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:06:51,469 - Mixed precision training is disabled
2025-10-22 16:06:51,469 - Mixed precision training is disabled
2025-10-22 16:06:51,470 - Mixed precision training is disabled
2025-10-22 16:06:51,470 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 16:06:51,470 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 16:06:51,471 - Training starts at step 1
2025-10-22 16:07:09,687 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1644 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,505 [36m tflops: 11.77 [35m mfu: 3.77%[39m
2025-10-22 16:07:09,688 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:07:09,688 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1644 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,648 [36m tflops: 12.88 [35m mfu: 4.13%[39m
2025-10-22 16:07:09,689 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:07:09,696 - [31m step:  1 [32m loss: 12.2108 [38;2;180;60;0m grad_norm:  0.1644 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,769 [36m tflops: 13.83 [35m mfu: 4.43%[39m
2025-10-22 16:07:09,698 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:07:09,722 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1644 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,490 [36m tflops: 11.65 [35m mfu: 3.73%[39m
2025-10-22 16:07:09,723 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:07:09,723 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 16:07:26,364 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 16:07:43,441 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 16:08:00,600 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 16:08:17,786 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 16:08:35,043 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 16:08:52,316 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 16:09:09,551 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 16:09:26,850 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 16:09:44,611 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.3116 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,904 [36m tflops: 14.88 [35m mfu: 4.77%[39m
2025-10-22 16:09:44,615 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.3116 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,904 [36m tflops: 14.88 [35m mfu: 4.77%[39m
2025-10-22 16:09:44,625 - [31m step: 10 [32m loss:  8.4921 [38;2;180;60;0m grad_norm:  1.3116 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,904 [36m tflops: 14.88 [35m mfu: 4.77%[39m
2025-10-22 16:09:44,626 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 16:09:44,629 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.3116 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,904 [36m tflops: 14.88 [35m mfu: 4.77%[39m
2025-10-22 16:10:01,370 - 🚦  Starting global accumulation step 12 (in the unit of optimizer)
2025-10-22 16:10:18,715 - 🚦  Starting global accumulation step 13 (in the unit of optimizer)
2025-10-22 16:10:35,918 - 🚦  Starting global accumulation step 14 (in the unit of optimizer)
2025-10-22 16:10:53,027 - 🚦  Starting global accumulation step 15 (in the unit of optimizer)
2025-10-22 16:11:10,090 - 🚦  Starting global accumulation step 16 (in the unit of optimizer)
2025-10-22 16:11:27,218 - 🚦  Starting global accumulation step 17 (in the unit of optimizer)
2025-10-22 16:11:44,389 - 🚦  Starting global accumulation step 18 (in the unit of optimizer)
2025-10-22 16:12:01,566 - 🚦  Starting global accumulation step 19 (in the unit of optimizer)
2025-10-22 16:12:18,719 - 🚦  Starting global accumulation step 20 (in the unit of optimizer)
2025-10-22 16:12:35,958 - [Step 20] 〰️ Monitoring Upperbound
2025-10-22 16:12:36,444 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.5002 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,907 [36m tflops: 14.91 [35m mfu: 4.78%[39m
2025-10-22 16:12:36,448 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.5002 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,907 [36m tflops: 14.91 [35m mfu: 4.78%[39m
2025-10-22 16:12:36,458 - [31m step: 20 [32m loss:  7.8174 [38;2;180;60;0m grad_norm:  1.5002 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,907 [36m tflops: 14.91 [35m mfu: 4.78%[39m
2025-10-22 16:12:36,460 - 🚦  Starting global accumulation step 21 (in the unit of optimizer)
2025-10-22 16:12:36,463 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  1.5002 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,907 [36m tflops: 14.91 [35m mfu: 4.78%[39m
2025-10-22 16:12:53,180 - 🚦  Starting global accumulation step 22 (in the unit of optimizer)
2025-10-22 16:13:10,435 - 🚦  Starting global accumulation step 23 (in the unit of optimizer)
2025-10-22 16:13:27,593 - 🚦  Starting global accumulation step 24 (in the unit of optimizer)
2025-10-22 16:13:44,861 - 🚦  Starting global accumulation step 25 (in the unit of optimizer)
2025-10-22 16:14:02,185 - 🚦  Starting global accumulation step 26 (in the unit of optimizer)
2025-10-22 16:14:19,497 - 🚦  Starting global accumulation step 27 (in the unit of optimizer)
2025-10-22 16:14:36,816 - 🚦  Starting global accumulation step 28 (in the unit of optimizer)
2025-10-22 16:14:54,146 - 🚦  Starting global accumulation step 29 (in the unit of optimizer)
2025-10-22 16:15:11,499 - 🚦  Starting global accumulation step 30 (in the unit of optimizer)
2025-10-22 16:15:28,911 - [Step 30] ✔️  Setting Upperbound
2025-10-22 16:15:29,441 - [Step 30] 〰️ Monitoring Lowerbound
2025-10-22 16:15:29,448 - Destroying the purge thread.
2025-10-22 16:15:29,451 - Destroying the purge thread.
2025-10-22 16:15:29,452 - Destroying the purge thread.
2025-10-22 16:15:29,466 - Destroying the purge thread.
2025-10-22 16:20:26,798 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:20:26,977 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:20:26,981 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:20:26,984 - [GC] Initial GC collection 0.00 seconds
2025-10-22 16:20:27,635 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:20:27,689 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:20:27,737 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:20:28,173 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:20:28,177 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:20:28,178 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:20:28,182 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:20:28,189 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:20:28,193 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:20:28,478 - Loading tokenizer from tokenizer.json
2025-10-22 16:20:28,870 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 16:20:32,466 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 16:20:32,874 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:20:32,913 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:20:33,326 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:20:33,368 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:20:33,383 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 16:20:33,405 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:20:33,448 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:20:36,562 - WandB logging enabled
2025-10-22 16:20:36,751 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:20:36,792 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:18,248 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:24:18,375 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:24:18,441 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:24:18,441 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:24:18,445 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:24:18,543 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:24:18,547 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:24:18,648 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:24:18,651 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:24:18,656 - [GC] Initial GC collection 0.00 seconds
2025-10-22 16:24:18,846 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:24:18,997 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:24:19,001 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:24:20,051 - Loading tokenizer from tokenizer.json
2025-10-22 16:24:20,440 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 16:24:23,721 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 16:24:24,132 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:24:24,171 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:24,185 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 16:24:24,203 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:24:24,230 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 16:24:24,231 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:24:24,240 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:24,303 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 16:24:24,303 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:24:24,412 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:24,412 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:24:24,414 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 16:24:24,482 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:24,483 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:24:24,484 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 16:24:25,570 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:24:25,613 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:25,675 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 16:24:25,675 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:24:25,846 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:25,847 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:24:25,848 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 16:24:28,046 - WandB logging enabled
2025-10-22 16:24:28,050 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:24:28,091 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:28,156 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 16:24:28,158 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:24:28,361 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:24:28,363 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:24:28,365 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 16:24:28,392 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:24:28,393 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:24:28,393 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 16:24:28,393 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:24:28,394 - Mixed precision training is disabled
2025-10-22 16:24:28,394 - Mixed precision training is disabled
2025-10-22 16:24:28,394 - Mixed precision training is disabled
2025-10-22 16:24:28,394 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:24:28,395 - Mixed precision training is disabled
2025-10-22 16:24:28,395 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 16:24:28,395 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 16:24:28,396 - Training starts at step 1
2025-10-22 16:24:46,471 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1676 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,571 [36m tflops: 12.28 [35m mfu: 3.94%[39m
2025-10-22 16:24:46,472 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:24:46,474 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1676 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,474 [36m tflops: 11.52 [35m mfu: 3.69%[39m
2025-10-22 16:24:46,476 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:24:46,483 - [31m step:  1 [32m loss: 12.2494 [38;2;180;60;0m grad_norm:  0.1676 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,782 [36m tflops: 13.93 [35m mfu: 4.47%[39m
2025-10-22 16:24:46,485 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:24:46,508 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1676 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,467 [36m tflops: 11.47 [35m mfu: 3.68%[39m
2025-10-22 16:24:46,509 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:24:46,510 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 16:25:03,076 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 16:25:20,193 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 16:25:37,303 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 16:25:54,536 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 16:26:11,802 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 16:26:29,096 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 16:26:46,448 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 16:27:03,735 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 16:27:21,529 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3291 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,902 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:27:21,533 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3291 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,902 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:27:21,542 - [31m step: 10 [32m loss:  8.7409 [38;2;180;60;0m grad_norm:  0.3291 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,902 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:27:21,544 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 16:27:21,547 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3291 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,902 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:27:38,354 - 🚦  Starting global accumulation step 12 (in the unit of optimizer)
2025-10-22 16:27:55,548 - 🚦  Starting global accumulation step 13 (in the unit of optimizer)
2025-10-22 16:28:12,796 - 🚦  Starting global accumulation step 14 (in the unit of optimizer)
2025-10-22 16:28:30,020 - 🚦  Starting global accumulation step 15 (in the unit of optimizer)
2025-10-22 16:28:47,234 - 🚦  Starting global accumulation step 16 (in the unit of optimizer)
2025-10-22 16:29:04,514 - 🚦  Starting global accumulation step 17 (in the unit of optimizer)
2025-10-22 16:29:21,808 - 🚦  Starting global accumulation step 18 (in the unit of optimizer)
2025-10-22 16:29:39,113 - 🚦  Starting global accumulation step 19 (in the unit of optimizer)
2025-10-22 16:29:56,373 - 🚦  Starting global accumulation step 20 (in the unit of optimizer)
2025-10-22 16:30:13,643 - [Step 20] 〰️ Monitoring Upperbound
2025-10-22 16:30:14,129 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1827 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,899 [36m tflops: 14.84 [35m mfu: 4.76%[39m
2025-10-22 16:30:14,132 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1827 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,899 [36m tflops: 14.84 [35m mfu: 4.76%[39m
2025-10-22 16:30:14,142 - [31m step: 20 [32m loss:  7.1564 [38;2;180;60;0m grad_norm:  0.1827 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,899 [36m tflops: 14.84 [35m mfu: 4.76%[39m
2025-10-22 16:30:14,143 - 🚦  Starting global accumulation step 21 (in the unit of optimizer)
2025-10-22 16:30:14,147 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1827 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,899 [36m tflops: 14.84 [35m mfu: 4.76%[39m
2025-10-22 16:30:30,943 - 🚦  Starting global accumulation step 22 (in the unit of optimizer)
2025-10-22 16:30:48,205 - 🚦  Starting global accumulation step 23 (in the unit of optimizer)
2025-10-22 16:31:05,420 - 🚦  Starting global accumulation step 24 (in the unit of optimizer)
2025-10-22 16:31:22,695 - 🚦  Starting global accumulation step 25 (in the unit of optimizer)
2025-10-22 16:31:40,009 - 🚦  Starting global accumulation step 26 (in the unit of optimizer)
2025-10-22 16:31:57,190 - 🚦  Starting global accumulation step 27 (in the unit of optimizer)
2025-10-22 16:32:14,533 - 🚦  Starting global accumulation step 28 (in the unit of optimizer)
2025-10-22 16:32:31,871 - 🚦  Starting global accumulation step 29 (in the unit of optimizer)
2025-10-22 16:32:49,202 - 🚦  Starting global accumulation step 30 (in the unit of optimizer)
2025-10-22 16:33:06,564 - [Step 30] ✔️  Setting Upperbound
2025-10-22 16:33:07,090 - [Step 30] 〰️ Monitoring Lowerbound
2025-10-22 16:33:07,097 - [31m step: 30 [32m loss:  6.9113 [38;2;180;60;0m grad_norm:  0.1431 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,895 [36m tflops: 14.81 [35m mfu: 4.75%[39m
2025-10-22 16:33:07,097 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1431 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,894 [36m tflops: 14.81 [35m mfu: 4.75%[39m
2025-10-22 16:33:07,098 - 🚦  Starting global accumulation step 31 (in the unit of optimizer)
2025-10-22 16:33:07,099 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1431 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,895 [36m tflops: 14.81 [35m mfu: 4.75%[39m
2025-10-22 16:33:07,103 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1431 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,894 [36m tflops: 14.81 [35m mfu: 4.75%[39m
2025-10-22 16:33:19,151 - 🚦  Starting global accumulation step 32 (in the unit of optimizer)
2025-10-22 16:33:30,975 - 🚦  Starting global accumulation step 33 (in the unit of optimizer)
2025-10-22 16:33:42,759 - 🚦  Starting global accumulation step 34 (in the unit of optimizer)
2025-10-22 16:33:54,565 - 🚦  Starting global accumulation step 35 (in the unit of optimizer)
2025-10-22 16:34:06,308 - 🚦  Starting global accumulation step 36 (in the unit of optimizer)
2025-10-22 16:34:18,137 - 🚦  Starting global accumulation step 37 (in the unit of optimizer)
2025-10-22 16:34:29,959 - 🚦  Starting global accumulation step 38 (in the unit of optimizer)
2025-10-22 16:34:41,762 - 🚦  Starting global accumulation step 39 (in the unit of optimizer)
2025-10-22 16:34:53,605 - 🚦  Starting global accumulation step 40 (in the unit of optimizer)
2025-10-22 16:35:05,449 - [Step 40] ✔️  Setting Lowerbound
2025-10-22 16:35:05,510 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 16:35:05,512 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:35:05,512 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.03/0.03, [MB2] 0.00/0.00, [MB3] 0.32/0.32, [MB4] 0.14/0.14, [MB5] 0.16/0.16, [MB6] 0.93/0.93, [MB7] 1.00/1.00
2025-10-22 16:35:05,513 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:35:05,519 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.88/0.88, [MB7] 1.00/1.00
2025-10-22 16:35:05,524 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:35:06,067 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1635_max_batch_time.svg
> Batch Time: 1027.67 ms, GPU Bubble Ratio: 54.32%, 49.04%, 63.65%, 15.50%
2025-10-22 16:35:06,585 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1635_min_batch_time.svg
> Batch Time: 686.50 ms, GPU Bubble Ratio: 67.56%, 45.23%, 61.88%, 13.95%
2025-10-22 16:35:07,094 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1635_frozen_pipeline_schedule.svg
> Batch Time: 686.50 ms, GPU Bubble Ratio: 40.09%, 30.64%, 47.63%, 13.95%
2025-10-22 16:35:07,095 - 	> Batch Time: 686.50 ms (Average Freeze Ratio: 0.42, Time Reduction Rate: 0.33)
2025-10-22 16:35:07,098 - Current/Expected Freeze Ratio per Block: [MB0] 1.00/1.00, [MB1] 1.00/1.00, [MB2] 1.00/1.00, [MB3] 1.00/1.00, [MB4] 1.00/1.00, [MB5] 1.00/1.00, [MB6] 1.00/1.00, [MB7] 1.00/1.00
2025-10-22 16:35:07,102 - [31m step: 40 [32m loss:  6.9069 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,731 [36m tflops: 21.35 [35m mfu: 6.84%[39m
2025-10-22 16:35:07,104 - 🚦  Starting global accumulation step 41 (in the unit of optimizer)
2025-10-22 16:35:18,709 - 🚦  Starting global accumulation step 42 (in the unit of optimizer)
2025-10-22 16:35:30,419 - 🚦  Starting global accumulation step 43 (in the unit of optimizer)
2025-10-22 16:35:42,041 - 🚦  Starting global accumulation step 44 (in the unit of optimizer)
2025-10-22 16:35:53,747 - 🚦  Starting global accumulation step 45 (in the unit of optimizer)
2025-10-22 16:36:05,452 - 🚦  Starting global accumulation step 46 (in the unit of optimizer)
2025-10-22 16:36:17,131 - 🚦  Starting global accumulation step 47 (in the unit of optimizer)
2025-10-22 16:36:28,834 - 🚦  Starting global accumulation step 48 (in the unit of optimizer)
2025-10-22 16:36:40,569 - 🚦  Starting global accumulation step 49 (in the unit of optimizer)
2025-10-22 16:36:52,302 - 🚦  Starting global accumulation step 50 (in the unit of optimizer)
2025-10-22 16:36:52,346 - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 16:37:04,050 - Current/Expected Freeze Ratio per Block: [MB0] 1.00/1.00, [MB1] 1.00/1.00, [MB2] 1.00/1.00, [MB3] 1.00/1.00, [MB4] 1.00/1.00, [MB5] 1.00/1.00, [MB6] 1.00/1.00, [MB7] 1.00/1.00
2025-10-22 16:37:04,057 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 16:37:04,075 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,764 [36m tflops: 21.61 [35m mfu: 6.92%[39m
2025-10-22 16:37:04,077 - [31m step: 50 [32m loss:  6.8914 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,801 [36m tflops: 21.90 [35m mfu: 7.02%[39m
2025-10-22 16:37:04,078 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.88/0.88, [MB7] 1.00/1.00
2025-10-22 16:37:04,078 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.03/0.03, [MB2] 0.00/0.00, [MB3] 0.32/0.32, [MB4] 0.14/0.14, [MB5] 0.16/0.16, [MB6] 0.93/0.93, [MB7] 1.00/1.00
2025-10-22 16:37:04,079 - 🚦  Starting global accumulation step 51 (in the unit of optimizer)
2025-10-22 16:37:04,079 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,764 [36m tflops: 21.61 [35m mfu: 6.93%[39m
2025-10-22 16:37:04,079 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,764 [36m tflops: 21.61 [35m mfu: 6.92%[39m
2025-10-22 16:37:15,792 - 🚦  Starting global accumulation step 52 (in the unit of optimizer)
2025-10-22 16:37:27,553 - 🚦  Starting global accumulation step 53 (in the unit of optimizer)
2025-10-22 16:37:39,284 - 🚦  Starting global accumulation step 54 (in the unit of optimizer)
2025-10-22 16:37:51,002 - 🚦  Starting global accumulation step 55 (in the unit of optimizer)
2025-10-22 16:38:02,755 - 🚦  Starting global accumulation step 56 (in the unit of optimizer)
2025-10-22 16:38:14,494 - 🚦  Starting global accumulation step 57 (in the unit of optimizer)
2025-10-22 16:38:26,277 - 🚦  Starting global accumulation step 58 (in the unit of optimizer)
2025-10-22 16:38:38,013 - 🚦  Starting global accumulation step 59 (in the unit of optimizer)
2025-10-22 16:38:49,677 - 🚦  Starting global accumulation step 60 (in the unit of optimizer)
2025-10-22 16:39:01,833 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1639_rank3_trend_line.svg
2025-10-22 16:39:01,857 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1639_rank2_trend_line.svg
2025-10-22 16:39:01,880 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1639_rank1_trend_line.svg
2025-10-22 16:39:01,882 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1639_rank0_trend_line.svg
2025-10-22 16:39:01,892 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,892 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,892 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,894 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,895 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,895 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:01,895 - [31m step: 60 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,781 [36m tflops: 21.74 [35m mfu: 6.97%[39m
2025-10-22 16:39:01,896 - [31m step: 60 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,781 [36m tflops: 21.74 [35m mfu: 6.97%[39m
2025-10-22 16:39:01,896 - [31m step: 60 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,781 [36m tflops: 21.74 [35m mfu: 6.97%[39m
2025-10-22 16:39:02,366 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1639_adjusted_frozen_pipeline_schedule.svg
> Batch Time: 545.31 ms, GPU Bubble Ratio: 64.68%, 30.96%, 51.95%, 18.72%
2025-10-22 16:39:02,367 - 	> Batch Time: 545.31 ms (Average Freeze Ratio: 0.00)
2025-10-22 16:39:02,369 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:02,372 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 0.00/0.00
2025-10-22 16:39:02,376 - [31m step: 60 [32m loss:  6.8762 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,770 [36m tflops: 21.65 [35m mfu: 6.94%[39m
2025-10-22 16:39:02,377 - 🚦  Starting global accumulation step 61 (in the unit of optimizer)
2025-10-22 16:39:18,876 - 🚦  Starting global accumulation step 62 (in the unit of optimizer)
2025-10-22 16:39:36,230 - 🚦  Starting global accumulation step 63 (in the unit of optimizer)
2025-10-22 16:39:53,592 - 🚦  Starting global accumulation step 64 (in the unit of optimizer)
2025-10-22 16:40:10,985 - 🚦  Starting global accumulation step 65 (in the unit of optimizer)
2025-10-22 16:40:28,346 - 🚦  Starting global accumulation step 66 (in the unit of optimizer)
2025-10-22 16:40:45,757 - 🚦  Starting global accumulation step 67 (in the unit of optimizer)
2025-10-22 16:41:03,193 - 🚦  Starting global accumulation step 68 (in the unit of optimizer)
2025-10-22 16:41:20,604 - 🚦  Starting global accumulation step 69 (in the unit of optimizer)
2025-10-22 16:41:37,924 - 🚦  Starting global accumulation step 70 (in the unit of optimizer)
2025-10-22 16:41:55,762 - [31m step: 70 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0965 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,885 [36m tflops: 14.73 [35m mfu: 4.72%[39m
2025-10-22 16:41:55,766 - [31m step: 70 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0965 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,885 [36m tflops: 14.73 [35m mfu: 4.72%[39m
2025-10-22 16:41:55,776 - [31m step: 70 [32m loss:  6.6693 [38;2;180;60;0m grad_norm:  0.0965 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,890 [36m tflops: 14.77 [35m mfu: 4.74%[39m
2025-10-22 16:41:55,778 - 🚦  Starting global accumulation step 71 (in the unit of optimizer)
2025-10-22 16:41:55,780 - [31m step: 70 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0965 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,884 [36m tflops: 14.73 [35m mfu: 4.72%[39m
2025-10-22 16:42:12,748 - 🚦  Starting global accumulation step 72 (in the unit of optimizer)
2025-10-22 16:42:30,147 - 🚦  Starting global accumulation step 73 (in the unit of optimizer)
2025-10-22 16:42:47,494 - 🚦  Starting global accumulation step 74 (in the unit of optimizer)
2025-10-22 16:43:04,846 - 🚦  Starting global accumulation step 75 (in the unit of optimizer)
2025-10-22 16:43:22,251 - 🚦  Starting global accumulation step 76 (in the unit of optimizer)
2025-10-22 16:43:39,626 - 🚦  Starting global accumulation step 77 (in the unit of optimizer)
2025-10-22 16:43:56,979 - 🚦  Starting global accumulation step 78 (in the unit of optimizer)
2025-10-22 16:44:14,374 - 🚦  Starting global accumulation step 79 (in the unit of optimizer)
2025-10-22 16:44:31,732 - 🚦  Starting global accumulation step 80 (in the unit of optimizer)
2025-10-22 16:44:50,032 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1644_rank3_trend_line.svg
2025-10-22 16:44:50,039 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1644_rank2_trend_line.svg
2025-10-22 16:44:50,074 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1644_rank1_trend_line.svg
2025-10-22 16:44:50,110 - Observed values (freeze ratio vs time) with Trend Line is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule_adjustment/251022_1644_rank0_trend_line.svg
2025-10-22 16:44:50,121 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 16:44:50,121 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 16:44:50,121 - Adjusted Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 16:44:50,124 - [31m step: 80 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0916 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,880 [36m tflops: 14.69 [35m mfu: 4.71%[39m
2025-10-22 16:44:50,124 - [31m step: 80 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0916 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,879 [36m tflops: 14.69 [35m mfu: 4.71%[39m
2025-10-22 16:44:50,125 - [31m step: 80 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0916 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,879 [36m tflops: 14.69 [35m mfu: 4.71%[39m
2025-10-22 16:44:50,635 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1644_adjusted_frozen_pipeline_schedule.svg
> Batch Time: 784.28 ms, GPU Bubble Ratio: 53.41%, 39.53%, 58.03%, 12.88%
2025-10-22 16:44:50,636 - 	> Batch Time: 784.28 ms (Average Freeze Ratio: 0.34)
2025-10-22 16:44:50,637 - Adjusted Freeze Ratio per Block: [MB0] 1.00/1.00, [MB1] 1.00/1.00, [MB2] 1.00/1.00, [MB3] 1.00/1.00, [MB4] 1.00/1.00, [MB5] 1.00/1.00, [MB6] 1.00/1.00, [MB7] 1.00/1.00
2025-10-22 16:44:50,644 - [31m step: 80 [32m loss:  6.4153 [38;2;180;60;0m grad_norm:  0.0916 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,874 [36m tflops: 14.65 [35m mfu: 4.70%[39m
2025-10-22 16:44:50,645 - 🚦  Starting global accumulation step 81 (in the unit of optimizer)
2025-10-22 16:45:02,643 - 🚦  Starting global accumulation step 82 (in the unit of optimizer)
2025-10-22 16:45:14,394 - 🚦  Starting global accumulation step 83 (in the unit of optimizer)
2025-10-22 16:45:26,128 - 🚦  Starting global accumulation step 84 (in the unit of optimizer)
2025-10-22 16:45:37,880 - 🚦  Starting global accumulation step 85 (in the unit of optimizer)
2025-10-22 16:45:49,638 - 🚦  Starting global accumulation step 86 (in the unit of optimizer)
2025-10-22 16:46:01,455 - 🚦  Starting global accumulation step 87 (in the unit of optimizer)
2025-10-22 16:46:13,235 - 🚦  Starting global accumulation step 88 (in the unit of optimizer)
2025-10-22 16:46:25,019 - 🚦  Starting global accumulation step 89 (in the unit of optimizer)
2025-10-22 16:46:36,804 - 🚦  Starting global accumulation step 90 (in the unit of optimizer)
2025-10-22 16:46:48,590 - [31m step: 90 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 16:46:48,592 - [31m step: 90 [32m loss:  6.4932 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,778 [36m tflops: 21.72 [35m mfu: 6.96%[39m
2025-10-22 16:46:48,593 - [31m step: 90 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 16:46:48,593 - [31m step: 90 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 16:46:48,594 - 🚦  Starting global accumulation step 91 (in the unit of optimizer)
2025-10-22 16:47:00,394 - 🚦  Starting global accumulation step 92 (in the unit of optimizer)
2025-10-22 16:47:12,172 - 🚦  Starting global accumulation step 93 (in the unit of optimizer)
2025-10-22 16:47:24,007 - 🚦  Starting global accumulation step 94 (in the unit of optimizer)
2025-10-22 16:47:35,918 - 🚦  Starting global accumulation step 95 (in the unit of optimizer)
2025-10-22 16:47:47,788 - 🚦  Starting global accumulation step 96 (in the unit of optimizer)
2025-10-22 16:47:59,629 - 🚦  Starting global accumulation step 97 (in the unit of optimizer)
2025-10-22 16:48:11,484 - 🚦  Starting global accumulation step 98 (in the unit of optimizer)
2025-10-22 16:48:23,276 - 🚦  Starting global accumulation step 99 (in the unit of optimizer)
2025-10-22 16:48:35,153 - 🚦  Starting global accumulation step 100 (in the unit of optimizer)
2025-10-22 16:48:35,193 - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 16:48:47,026 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:48:47,028 - [31m step: 100 [32m loss:  6.4230 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:48:47,029 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:48:47,029 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,767 [36m tflops: 21.63 [35m mfu: 6.93%[39m
2025-10-22 16:48:47,030 - 🚦  Starting global accumulation step 101 (in the unit of optimizer)
2025-10-22 16:48:47,748 - Avg. fwd time: 36.8809 / Avg. bwd time: 54.0864 / Avg. batch time: 751.2742 (ms) / GPU bubble ratio: 3.13%
2025-10-22 16:48:47,769 - Avg. fwd time: 17.7000 / Avg. bwd time: 22.0312 / Avg. batch time: 793.5648 (ms) / GPU bubble ratio: 59.95%
2025-10-22 16:48:47,793 - Avg. fwd time: 27.0520 / Avg. bwd time: 29.2477 / Avg. batch time: 846.9266 (ms) / GPU bubble ratio: 46.82%
2025-10-22 16:48:47,798 - Avg. fwd time: 23.2094 / Avg. bwd time: 17.6527 / Avg. batch time: 884.0820 (ms) / GPU bubble ratio: 63.02%
2025-10-22 16:48:58,812 - 🚦  Starting global accumulation step 102 (in the unit of optimizer)
2025-10-22 16:49:10,569 - 🚦  Starting global accumulation step 103 (in the unit of optimizer)
2025-10-22 16:49:22,427 - 🚦  Starting global accumulation step 104 (in the unit of optimizer)
2025-10-22 16:49:34,219 - 🚦  Starting global accumulation step 105 (in the unit of optimizer)
2025-10-22 16:49:54,281 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:49:54,335 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:49:54,376 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:49:54,431 - Starting job: Llama 3.2 1B training with APF Freezer
2025-10-22 16:49:54,677 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:49:54,680 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:49:54,681 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:49:54,684 - [GC] Initial GC collection 0.00 seconds
2025-10-22 16:49:54,685 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:49:54,705 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:49:54,708 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:49:54,784 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 16:49:54,789 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 16:49:55,065 - Loading tokenizer from tokenizer.json
2025-10-22 16:49:55,459 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 16:49:58,629 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 16:49:59,077 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:49:59,118 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:49:59,132 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 16:49:59,180 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 16:49:59,180 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:49:59,696 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:49:59,735 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:49:59,796 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 16:49:59,797 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:49:59,899 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:49:59,937 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:49:59,999 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 16:50:00,000 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:50:00,059 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:50:00,059 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:50:00,060 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 16:50:00,084 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:50:00,085 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:50:00,085 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 16:50:00,173 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:50:00,174 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:50:00,175 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 16:50:02,570 - WandB logging enabled
2025-10-22 16:50:02,572 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 16:50:02,612 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:50:02,677 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 16:50:02,678 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-10-22 16:50:02,859 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 16:50:02,861 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 16:50:02,863 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 16:50:02,893 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:50:02,893 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:50:02,894 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/llama3.2_1b_timelyapf_debug
2025-10-22 16:50:02,894 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:50:02,894 - Mixed precision training is disabled
2025-10-22 16:50:02,894 - Mixed precision training is disabled
2025-10-22 16:50:02,895 - Mixed precision training is disabled
2025-10-22 16:50:02,895 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 16:50:02,895 - Mixed precision training is disabled
2025-10-22 16:50:02,895 - 🚦  Starting global accumulation step 1 (in the unit of optimizer)
2025-10-22 16:50:02,896 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 10000 (warmup 10)
2025-10-22 16:50:02,896 - Training starts at step 1
2025-10-22 16:50:21,409 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1638 [38;2;54;234;195m memory:  8.13GiB(17.12%) [34m tps: 1,526 [36m tflops: 11.93 [35m mfu: 3.82%[39m
2025-10-22 16:50:21,410 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:50:21,412 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1638 [38;2;54;234;195m memory: 10.09GiB(21.23%) [34m tps: 1,512 [36m tflops: 11.82 [35m mfu: 3.79%[39m
2025-10-22 16:50:21,413 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:50:21,421 - [31m step:  1 [32m loss: 12.2443 [38;2;180;60;0m grad_norm:  0.1638 [38;2;54;234;195m memory: 20.03GiB(42.15%) [34m tps: 1,743 [36m tflops: 13.62 [35m mfu: 4.37%[39m
2025-10-22 16:50:21,423 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:50:21,449 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1638 [38;2;54;234;195m memory: 10.96GiB(23.08%) [34m tps: 1,467 [36m tflops: 11.47 [35m mfu: 3.68%[39m
2025-10-22 16:50:21,450 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 16:50:21,450 - 🚦  Starting global accumulation step 2 (in the unit of optimizer)
2025-10-22 16:50:38,219 - 🚦  Starting global accumulation step 3 (in the unit of optimizer)
2025-10-22 16:50:55,468 - 🚦  Starting global accumulation step 4 (in the unit of optimizer)
2025-10-22 16:51:12,717 - 🚦  Starting global accumulation step 5 (in the unit of optimizer)
2025-10-22 16:51:29,982 - 🚦  Starting global accumulation step 6 (in the unit of optimizer)
2025-10-22 16:51:47,305 - 🚦  Starting global accumulation step 7 (in the unit of optimizer)
2025-10-22 16:52:04,587 - 🚦  Starting global accumulation step 8 (in the unit of optimizer)
2025-10-22 16:52:21,926 - 🚦  Starting global accumulation step 9 (in the unit of optimizer)
2025-10-22 16:52:39,216 - 🚦  Starting global accumulation step 10 (in the unit of optimizer)
2025-10-22 16:52:56,977 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2854 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:52:56,981 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2854 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:52:56,991 - [31m step: 10 [32m loss:  8.1167 [38;2;180;60;0m grad_norm:  0.2854 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:52:56,992 - 🚦  Starting global accumulation step 11 (in the unit of optimizer)
2025-10-22 16:52:56,996 - [31m step: 10 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2854 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:53:13,834 - 🚦  Starting global accumulation step 12 (in the unit of optimizer)
2025-10-22 16:53:31,032 - 🚦  Starting global accumulation step 13 (in the unit of optimizer)
2025-10-22 16:53:48,226 - 🚦  Starting global accumulation step 14 (in the unit of optimizer)
2025-10-22 16:54:05,404 - 🚦  Starting global accumulation step 15 (in the unit of optimizer)
2025-10-22 16:54:22,528 - 🚦  Starting global accumulation step 16 (in the unit of optimizer)
2025-10-22 16:54:39,726 - 🚦  Starting global accumulation step 17 (in the unit of optimizer)
2025-10-22 16:54:56,967 - 🚦  Starting global accumulation step 18 (in the unit of optimizer)
2025-10-22 16:55:14,264 - 🚦  Starting global accumulation step 19 (in the unit of optimizer)
2025-10-22 16:55:31,497 - 🚦  Starting global accumulation step 20 (in the unit of optimizer)
2025-10-22 16:55:48,721 - [Step 20] 〰️ Monitoring Upperbound
2025-10-22 16:55:49,204 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1756 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,903 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:55:49,208 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1756 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,903 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:55:49,217 - [31m step: 20 [32m loss:  7.6339 [38;2;180;60;0m grad_norm:  0.1756 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,903 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:55:49,219 - 🚦  Starting global accumulation step 21 (in the unit of optimizer)
2025-10-22 16:55:49,222 - [31m step: 20 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1756 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,903 [36m tflops: 14.87 [35m mfu: 4.77%[39m
2025-10-22 16:56:05,929 - 🚦  Starting global accumulation step 22 (in the unit of optimizer)
2025-10-22 16:56:23,193 - 🚦  Starting global accumulation step 23 (in the unit of optimizer)
2025-10-22 16:56:40,516 - 🚦  Starting global accumulation step 24 (in the unit of optimizer)
2025-10-22 16:56:57,831 - 🚦  Starting global accumulation step 25 (in the unit of optimizer)
2025-10-22 16:57:15,077 - 🚦  Starting global accumulation step 26 (in the unit of optimizer)
2025-10-22 16:57:32,361 - 🚦  Starting global accumulation step 27 (in the unit of optimizer)
2025-10-22 16:57:49,640 - 🚦  Starting global accumulation step 28 (in the unit of optimizer)
2025-10-22 16:58:06,890 - 🚦  Starting global accumulation step 29 (in the unit of optimizer)
2025-10-22 16:58:24,188 - 🚦  Starting global accumulation step 30 (in the unit of optimizer)
2025-10-22 16:58:41,540 - [Step 30] ✔️  Setting Upperbound
2025-10-22 16:58:42,065 - [Step 30] 〰️ Monitoring Lowerbound
2025-10-22 16:58:42,072 - [31m step: 30 [32m loss:  6.9887 [38;2;180;60;0m grad_norm:  0.1697 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:58:42,072 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1697 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:58:42,073 - 🚦  Starting global accumulation step 31 (in the unit of optimizer)
2025-10-22 16:58:42,073 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1697 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:58:42,077 - [31m step: 30 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1697 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 1,896 [36m tflops: 14.82 [35m mfu: 4.75%[39m
2025-10-22 16:58:54,189 - 🚦  Starting global accumulation step 32 (in the unit of optimizer)
2025-10-22 16:59:06,010 - 🚦  Starting global accumulation step 33 (in the unit of optimizer)
2025-10-22 16:59:17,756 - 🚦  Starting global accumulation step 34 (in the unit of optimizer)
2025-10-22 16:59:29,504 - 🚦  Starting global accumulation step 35 (in the unit of optimizer)
2025-10-22 16:59:41,405 - 🚦  Starting global accumulation step 36 (in the unit of optimizer)
2025-10-22 16:59:53,167 - 🚦  Starting global accumulation step 37 (in the unit of optimizer)
2025-10-22 17:00:04,971 - 🚦  Starting global accumulation step 38 (in the unit of optimizer)
2025-10-22 17:00:16,742 - 🚦  Starting global accumulation step 39 (in the unit of optimizer)
2025-10-22 17:00:28,505 - 🚦  Starting global accumulation step 40 (in the unit of optimizer)
2025-10-22 17:00:40,317 - [Step 40] ✔️  Setting Lowerbound
2025-10-22 17:00:40,381 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.32/0.32, [MB4] 0.06/0.06, [MB5] 0.22/0.22, [MB6] 0.92/0.92, [MB7] 1.00/1.00
2025-10-22 17:00:40,381 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.93/0.93, [MB7] 1.00/1.00
2025-10-22 17:00:40,384 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0005 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,770 [36m tflops: 21.65 [35m mfu: 6.94%[39m
2025-10-22 17:00:40,384 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,770 [36m tflops: 21.65 [35m mfu: 6.94%[39m
2025-10-22 17:00:40,392 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 17:00:40,394 - [31m step: 40 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,769 [36m tflops: 21.65 [35m mfu: 6.94%[39m
2025-10-22 17:00:40,937 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1700_max_batch_time.svg
> Batch Time: 1030.52 ms, GPU Bubble Ratio: 54.64%, 49.03%, 63.86%, 15.39%
2025-10-22 17:00:41,439 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1700_min_batch_time.svg
> Batch Time: 691.30 ms, GPU Bubble Ratio: 66.19%, 45.17%, 62.30%, 14.06%
2025-10-22 17:00:41,945 - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/llama3.2_1b_timelyapf_debug/pipeline_schedule/251022_1700_frozen_pipeline_schedule.svg
> Batch Time: 691.30 ms, GPU Bubble Ratio: 40.52%, 30.68%, 48.14%, 14.06%
2025-10-22 17:00:41,947 - 	> Batch Time: 691.30 ms (Average Freeze Ratio: 0.42, Time Reduction Rate: 0.33)
2025-10-22 17:00:41,950 - Current/Expected Freeze Ratio per Block: [MB0] 1.00/1.00, [MB1] 1.00/1.00, [MB2] 1.00/1.00, [MB3] 1.00/1.00, [MB4] 1.00/1.00, [MB5] 1.00/1.00, [MB6] 1.00/1.00, [MB7] 1.00/1.00
2025-10-22 17:00:41,954 - [31m step: 40 [32m loss:  6.9779 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,733 [36m tflops: 21.37 [35m mfu: 6.85%[39m
2025-10-22 17:00:41,955 - 🚦  Starting global accumulation step 41 (in the unit of optimizer)
2025-10-22 17:00:53,537 - 🚦  Starting global accumulation step 42 (in the unit of optimizer)
2025-10-22 17:01:05,186 - 🚦  Starting global accumulation step 43 (in the unit of optimizer)
2025-10-22 17:01:16,867 - 🚦  Starting global accumulation step 44 (in the unit of optimizer)
2025-10-22 17:01:28,604 - 🚦  Starting global accumulation step 45 (in the unit of optimizer)
2025-10-22 17:01:40,223 - 🚦  Starting global accumulation step 46 (in the unit of optimizer)
2025-10-22 17:01:51,928 - 🚦  Starting global accumulation step 47 (in the unit of optimizer)
2025-10-22 17:02:03,669 - 🚦  Starting global accumulation step 48 (in the unit of optimizer)
2025-10-22 17:02:15,385 - 🚦  Starting global accumulation step 49 (in the unit of optimizer)
2025-10-22 17:02:27,107 - 🚦  Starting global accumulation step 50 (in the unit of optimizer)
2025-10-22 17:02:27,149 - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 17:02:38,832 - Current/Expected Freeze Ratio per Block: [MB0] 1.00/1.00, [MB1] 1.00/1.00, [MB2] 1.00/1.00, [MB3] 1.00/1.00, [MB4] 1.00/1.00, [MB5] 1.00/1.00, [MB6] 1.00/1.00, [MB7] 1.00/1.00
2025-10-22 17:02:38,839 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.00/0.00, [MB7] 1.00/1.00
2025-10-22 17:02:38,858 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory:  9.95GiB(20.94%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 17:02:38,860 - [31m step: 50 [32m loss:  6.9702 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 23.38GiB(49.22%) [34m tps: 2,803 [36m tflops: 21.91 [35m mfu: 7.02%[39m
2025-10-22 17:02:38,860 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.00/0.00, [MB4] 0.00/0.00, [MB5] 0.00/0.00, [MB6] 0.93/0.93, [MB7] 1.00/1.00
2025-10-22 17:02:38,861 - Current/Expected Freeze Ratio per Block: [MB0] 0.00/0.00, [MB1] 0.00/0.00, [MB2] 0.00/0.00, [MB3] 0.32/0.32, [MB4] 0.06/0.06, [MB5] 0.22/0.22, [MB6] 0.92/0.92, [MB7] 1.00/1.00
2025-10-22 17:02:38,861 - 🚦  Starting global accumulation step 51 (in the unit of optimizer)
2025-10-22 17:02:38,861 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 14.75GiB(31.05%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 17:02:38,862 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0000 [38;2;54;234;195m memory: 12.35GiB(26.00%) [34m tps: 2,766 [36m tflops: 21.62 [35m mfu: 6.93%[39m
2025-10-22 17:02:50,625 - 🚦  Starting global accumulation step 52 (in the unit of optimizer)
2025-10-22 17:03:02,381 - 🚦  Starting global accumulation step 53 (in the unit of optimizer)
2025-10-22 17:03:14,088 - 🚦  Starting global accumulation step 54 (in the unit of optimizer)
2025-10-22 17:03:25,847 - 🚦  Starting global accumulation step 55 (in the unit of optimizer)
2025-10-22 17:03:37,602 - 🚦  Starting global accumulation step 56 (in the unit of optimizer)
