
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 10. 22. (Ïàò) 09:19:37 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1022_llama1binstruct/run.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1022_llama1binstruct/1022_1f1b_nofreeze.log
‚úîÔ∏èMain Table Experiment
‚úîÔ∏èRunning with nofreeze x 1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1022_llama1binstruct/config.toml --job.description="Main Table Experiment" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --metrics.log_freq=50 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
2025-10-22 09:19:44,657 - Starting job: "Main Table Experiment"
2025-10-22 09:19:44,696 - Starting job: "Main Table Experiment"
2025-10-22 09:19:44,843 - Starting job: "Main Table Experiment"
[rank3]:[titan] 2025-10-22 09:19:44,843 - root - INFO - Starting job: "Main Table Experiment"
2025-10-22 09:19:44,981 - Starting job: "Main Table Experiment"
[rank0]:[titan] 2025-10-22 09:19:44,981 - root - INFO - Starting job: "Main Table Experiment"
2025-10-22 09:19:45,423 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 09:19:45,426 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 09:19:45,443 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 09:19:45,447 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 09:19:45,447 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 09:19:45,450 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 09:19:45,462 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-22 09:19:45,466 - Building 1-D device mesh with ['pp'], [4]
2025-10-22 09:19:45,472 - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-10-22 09:19:45,443 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-10-22 09:19:45,447 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-10-22 09:19:45,462 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-10-22 09:19:45,466 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-10-22 09:19:45,472 - root - INFO - [GC] Initial GC collection 0.00 seconds
2025-10-22 09:19:45,967 - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-10-22 09:19:45,967 - root - INFO - Loading tokenizer from tokenizer.json
2025-10-22 09:19:46,415 - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-10-22 09:19:46,415 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-22 09:19:49,369 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-10-22 09:19:49,369 - root - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-22 09:19:49,490 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 09:19:49,531 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,543 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 09:19:49,560 - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
2025-10-22 09:19:49,561 - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
2025-10-22 09:19:49,566 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 09:19:49,582 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,605 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,606 - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
2025-10-22 09:19:49,610 - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
2025-10-22 09:19:49,611 - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
2025-10-22 09:19:49,632 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
2025-10-22 09:19:49,633 - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-10-22 09:19:49,566 - root - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:[titan] 2025-10-22 09:19:49,605 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:[titan] 2025-10-22 09:19:49,606 - root - INFO - [34mModel llama3 1B [31msize: 1,498,482,688 total parameters[39m
[rank0]:[titan] 2025-10-22 09:19:49,632 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:[titan] 2025-10-22 09:19:49,633 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
2025-10-22 09:19:49,734 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,734 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 09:19:49,735 - CUDA memory usage for model: 1.13GiB(2.39%)
2025-10-22 09:19:49,736 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-10-22 09:19:49,775 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,775 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 09:19:49,776 - CUDA memory usage for model: 0.92GiB(1.94%)
2025-10-22 09:19:49,776 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-10-22 09:19:49,815 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:49,816 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 09:19:49,816 - CUDA memory usage for model: 1.90GiB(3.99%)
2025-10-22 09:19:49,818 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:[titan] 2025-10-22 09:19:49,815 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:[titan] 2025-10-22 09:19:49,816 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-10-22 09:19:49,816 - root - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank0]:[titan] 2025-10-22 09:19:49,818 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run e98fyn9o
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1022_1f1b_nofreeze_dm1/20251022-0919/wandb/run-20251022_091950-e98fyn9o
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1022_1f1b_nofreeze_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/e98fyn9o
2025-10-22 09:19:51,561 - WandB logging enabled
2025-10-22 09:19:51,561 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-22 09:19:51,602 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:51,631 - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
2025-10-22 09:19:51,631 - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-10-22 09:19:51,561 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-10-22 09:19:51,561 - root - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:[titan] 2025-10-22 09:19:51,602 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:[titan] 2025-10-22 09:19:51,631 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:[titan] 2025-10-22 09:19:51,631 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
2025-10-22 09:19:51,821 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-22 09:19:51,822 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-22 09:19:51,823 - CUDA memory usage for model: 1.66GiB(3.50%)
2025-10-22 09:19:51,825 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-10-22 09:19:51,839 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 09:19:51,839 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 09:19:51,839 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 09:19:51,839 - Mixed precision training is disabled
2025-10-22 09:19:51,840 - Mixed precision training is disabled
2025-10-22 09:19:51,840 - Mixed precision training is disabled
2025-10-22 09:19:51,839 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1022_1f1b_nofreeze_dm1
2025-10-22 09:19:51,840 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-22 09:19:51,840 - Mixed precision training is disabled
2025-10-22 09:19:51,844 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-10-22 09:19:51,845 - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B-Instruct/original_dcp.
[rank0]:[titan] 2025-10-22 09:19:51,839 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1022_1f1b_nofreeze_dm1
[rank0]:[titan] 2025-10-22 09:19:51,840 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-10-22 09:19:51,840 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-10-22 09:19:51,844 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-10-22 09:19:51,845 - root - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B-Instruct/original_dcp.
[rank3]:[titan] 2025-10-22 09:19:51,821 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:[titan] 2025-10-22 09:19:51,822 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-10-22 09:19:51,823 - root - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:[titan] 2025-10-22 09:19:51,825 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-10-22 09:19:51,839 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-10-22 09:19:51,840 - root - INFO - Mixed precision training is disabled
2025-10-22 09:19:54,815 - [GC] GC collection for checkpoint loading. 0.00 seconds
2025-10-22 09:19:54,815 - Finished loading the checkpoint in 2.97 seconds.
2025-10-22 09:19:54,815 - Training starts at step 1
[rank0]:[titan] 2025-10-22 09:19:54,815 - root - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:[titan] 2025-10-22 09:19:54,815 - root - INFO - Finished loading the checkpoint in 2.97 seconds.
[rank0]:[titan] 2025-10-22 09:19:54,815 - root - INFO - Training starts at step 1
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-10-22 09:20:12,303 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory:  5.43GiB(11.44%) [34m tps: 1,439 [36m tflops: 11.25 [35m mfu: 3.61%[39m
2025-10-22 09:20:12,304 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 09:20:12,311 - [31m step:  1 [32m loss:  2.4984 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory: 12.15GiB(25.58%) [34m tps: 1,583 [36m tflops: 12.37 [35m mfu: 3.97%[39m
2025-10-22 09:20:12,311 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 09:20:12,338 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory:  7.95GiB(16.74%) [34m tps: 1,441 [36m tflops: 11.27 [35m mfu: 3.61%[39m
2025-10-22 09:20:12,338 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-10-22 09:20:12,311 - root - INFO - [31m step:  1 [32m loss:  2.4984 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory: 12.15GiB(25.58%) [34m tps: 1,583 [36m tflops: 12.37 [35m mfu: 3.97%[39m
[rank3]:[titan] 2025-10-22 09:20:12,311 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-10-22 09:20:12,338 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory:  7.95GiB(16.74%) [34m tps: 1,441 [36m tflops: 11.27 [35m mfu: 3.61%[39m
[rank0]:[titan] 2025-10-22 09:20:12,338 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 09:20:12,825 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.7671 [38;2;54;234;195m memory:  3.83GiB(8.06%) [34m tps: 1,410 [36m tflops: 11.02 [35m mfu: 3.53%[39m
2025-10-22 09:20:12,826 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-22 09:34:13,575 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 09:34:13,575 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 09:34:31,051 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,871 [36m tflops: 14.63 [35m mfu: 4.69%[39m
2025-10-22 09:34:31,054 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,870 [36m tflops: 14.62 [35m mfu: 4.68%[39m
2025-10-22 09:34:31,064 - [31m step: 50 [32m loss:  2.0180 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,870 [36m tflops: 14.62 [35m mfu: 4.68%[39m
2025-10-22 09:34:31,068 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,870 [36m tflops: 14.62 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 09:34:31,064 - root - INFO - [31m step: 50 [32m loss:  2.0180 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,870 [36m tflops: 14.62 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 09:34:31,068 - root - INFO - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  2.5840 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,870 [36m tflops: 14.62 [35m mfu: 4.68%[39m
2025-10-22 09:48:50,054 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 09:48:50,054 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 09:49:07,516 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 09:49:07,520 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 09:49:07,528 - [31m step: 100 [32m loss:  2.2477 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 09:49:07,533 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 09:49:07,528 - root - INFO - [31m step: 100 [32m loss:  2.2477 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 09:49:07,533 - root - INFO - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2506 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:03:26,721 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 10:03:26,721 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 10:03:44,414 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:03:44,418 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:03:44,426 - [31m step: 150 [32m loss:  2.5142 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:03:44,432 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 10:03:44,432 - root - INFO - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 10:03:44,426 - root - INFO - [31m step: 150 [32m loss:  2.5142 [38;2;180;60;0m grad_norm:  0.2405 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,868 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:18:04,317 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 10:18:04,317 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 10:18:21,902 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 10:18:21,905 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 10:18:21,914 - [31m step: 200 [32m loss:  2.7270 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 10:18:21,918 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 10:18:21,918 - root - INFO - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 10:18:21,914 - root - INFO - [31m step: 200 [32m loss:  2.7270 [38;2;180;60;0m grad_norm:  0.2925 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,867 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 10:32:41,122 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 10:32:41,122 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 10:32:58,607 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:32:58,611 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:32:58,620 - [31m step: 250 [32m loss:  2.8772 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:32:58,624 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 10:32:58,624 - root - INFO - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 10:32:58,620 - root - INFO - [31m step: 250 [32m loss:  2.8772 [38;2;180;60;0m grad_norm:  0.3011 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:47:17,671 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 10:47:17,671 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 10:47:35,230 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:47:35,234 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:47:35,243 - [31m step: 300 [32m loss:  2.7784 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 10:47:35,246 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 10:47:35,246 - root - INFO - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 10:47:35,243 - root - INFO - [31m step: 300 [32m loss:  2.7784 [38;2;180;60;0m grad_norm:  0.1921 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 11:01:54,888 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 11:01:54,888 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 11:02:12,453 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 11:02:12,456 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 11:02:12,465 - [31m step: 350 [32m loss:  2.6826 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 11:02:12,471 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 11:02:12,465 - root - INFO - [31m step: 350 [32m loss:  2.6826 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 11:02:12,471 - root - INFO - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1916 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,868 [36m tflops: 14.60 [35m mfu: 4.68%[39m
2025-10-22 11:16:31,591 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 11:16:31,591 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 11:16:49,196 - [31m step: 400 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 11:16:49,201 - [31m step: 400 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 11:16:49,209 - [31m step: 400 [32m loss:  2.5144 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 11:16:49,210 - [31m step: 400 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 11:16:49,210 - root - INFO - [31m step: 400 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 11:16:49,209 - root - INFO - [31m step: 400 [32m loss:  2.5144 [38;2;180;60;0m grad_norm:  0.1812 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,869 [36m tflops: 14.61 [35m mfu: 4.68%[39m
2025-10-22 11:31:10,340 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 11:31:10,340 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 11:31:27,966 - [31m step: 450 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
2025-10-22 11:31:27,970 - [31m step: 450 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
2025-10-22 11:31:27,979 - [31m step: 450 [32m loss:  2.4127 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
2025-10-22 11:31:27,981 - [31m step: 450 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
[rank0]:[titan] 2025-10-22 11:31:27,981 - root - INFO - [31m step: 450 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
[rank3]:[titan] 2025-10-22 11:31:27,979 - root - INFO - [31m step: 450 [32m loss:  2.4127 [38;2;180;60;0m grad_norm:  0.1437 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,864 [36m tflops: 14.58 [35m mfu: 4.67%[39m
2025-10-22 11:45:48,430 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-22 11:45:48,430 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-22 11:46:05,951 - [31m step: 500 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory:  5.49GiB(11.56%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
2025-10-22 11:46:05,951 - Saving the checkpoint (or staging if async is enabled).
2025-10-22 11:46:05,952 - Saving a model only checkpoint in torch.float16 at last step, step 500.
2025-10-22 11:46:05,955 - [31m step: 500 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory:  7.70GiB(16.21%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
2025-10-22 11:46:05,955 - Saving the checkpoint (or staging if async is enabled).
2025-10-22 11:46:05,956 - Saving a model only checkpoint in torch.float16 at last step, step 500.
2025-10-22 11:46:05,963 - [31m step: 500 [32m loss:  2.3231 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
2025-10-22 11:46:05,964 - Saving the checkpoint (or staging if async is enabled).
2025-10-22 11:46:05,965 - Saving a model only checkpoint in torch.float16 at last step, step 500.
2025-10-22 11:46:05,968 - [31m step: 500 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
2025-10-22 11:46:05,968 - Saving the checkpoint (or staging if async is enabled).
2025-10-22 11:46:05,969 - Saving a model only checkpoint in torch.float16 at last step, step 500.
[rank0]:[titan] 2025-10-22 11:46:05,968 - root - INFO - [31m step: 500 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory: 11.74GiB(24.72%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
[rank0]:[titan] 2025-10-22 11:46:05,968 - root - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:[titan] 2025-10-22 11:46:05,969 - root - INFO - Saving a model only checkpoint in torch.float16 at last step, step 500.
[rank3]:[titan] 2025-10-22 11:46:05,963 - root - INFO - [31m step: 500 [32m loss:  2.3231 [38;2;180;60;0m grad_norm:  0.1959 [38;2;54;234;195m memory: 15.59GiB(32.81%) [34m tps: 1,866 [36m tflops: 14.59 [35m mfu: 4.68%[39m
[rank3]:[titan] 2025-10-22 11:46:05,964 - root - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:[titan] 2025-10-22 11:46:05,965 - root - INFO - Saving a model only checkpoint in torch.float16 at last step, step 500.
2025-10-22 11:46:07,880 - [GC] GC collection invoked by checkpointer. 0.00 seconds
2025-10-22 11:46:07,880 - Sleeping 2 seconds for other ranks to complete
2025-10-22 11:46:07,881 - Destroying the purge thread.
2025-10-22 11:46:07,881 - Destroying the purge thread.
2025-10-22 11:46:07,881 - Destroying the purge thread.
[rank0]:[titan] 2025-10-22 11:46:07,880 - root - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:[titan] 2025-10-22 11:46:07,880 - root - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:[titan] 2025-10-22 11:46:07,881 - root - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
2025-10-22 11:46:08,340 - Process group destroyed
2025-10-22 11:46:08,364 - Process group destroyed
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ
[rank3]:wandb:                           lr ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 0.19586
[rank3]:wandb: loss_metrics/global_avg_loss 2.32306
[rank3]:wandb: loss_metrics/global_max_loss 2.32306
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 30.78624
[rank3]:wandb:       memory/max_active(GiB) 14.62561
[rank3]:wandb:       memory/max_reserved(%) 32.81181
[rank3]:wandb:     memory/max_reserved(GiB) 15.58789
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1022_1f1b_nofreeze_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/e98fyn9o
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1022_1f1b_nofreeze_dm1/20251022-0919/wandb/run-20251022_091950-e98fyn9o/logs
2025-10-22 11:46:09,667 - Process group destroyed
[rank3]:[titan] 2025-10-22 11:46:09,667 - root - INFO - Process group destroyed
2025-10-22 11:46:09,881 - Training completed
2025-10-22 11:46:09,881 - Destroying the purge thread.
[rank0]:[titan] 2025-10-22 11:46:09,881 - root - INFO - Training completed
[rank0]:[titan] 2025-10-22 11:46:09,881 - root - INFO - Destroying the purge thread.
2025-10-22 11:46:10,069 - Process group destroyed
[rank0]:[titan] 2025-10-22 11:46:10,069 - root - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.0', 'layers.2', 'layers.3', 'tok_embeddings'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.15', 'layers.14', 'layers.13', 'output'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1022_llama1binstruct/config.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1022_1f1b_nofreeze_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: profile_trace/1022_1f1b_nofreeze_dm1/1022_1f1b_nofreeze_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: memory_snapshot/1022_1f1b_nofreeze_dm1/1022_1f1b_nofreeze_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: False
[rank3]:		- save_tb_folder: tb/1022_1f1b_nofreeze_dm1/1022_1f1b_nofreeze_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- log_file: /home/shcho/torchtitan/logs/dmserver1/1022_llama1binstruct/1022_1f1b_nofreeze.log
[rank3]:		- wandb_name: 1022_1f1b_nofreeze_dm1
[rank3]:		- draw_graph: False
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 0.0003
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 200
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: slimorca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 8
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 500
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1f1b
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 1
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 1
[rank3]:		- pp_scheduler: None
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- folder: checkpoint/1022_1f1b_nofreeze_dm1/1022_1f1b_nofreeze_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: comm_traces/1022_1f1b_nofreeze_dm1/1022_1f1b_nofreeze_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 8
[rank3]:		- seq_len: 2048
[rank3]:		- freq: 10
[rank3]:		- steps: -1
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0.1
[rank3]:
