
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 07. (ê¸ˆ) 15:56:21 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:2025-11-07 15:56:27,316 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-07 15:56:27,387 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank3]:2025-11-07 15:56:27,315 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-07 15:56:27,458 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-07 15:56:27,636 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-07 15:56:27,638 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-07 15:56:27,646 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-07 15:56:27,648 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 15:56:27,753 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 15:56:27,755 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 15:56:27,758 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-11-07 15:56:27,767 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-07 15:56:27,770 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 15:56:28,276 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-07 15:56:28,701 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-07 15:56:31,470 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-07 15:56:31,576 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 15:56:31,625 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 15:56:31,665 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 15:56:31,667 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-07 15:56:31,693 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-07 15:56:31,693 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-07 15:56:31,617 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 15:56:31,646 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-07 15:56:31,646 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 15:56:31,872 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 15:56:31,872 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-07 15:56:31,872 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-07 15:56:31,836 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 15:56:31,836 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-07 15:56:31,837 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-07 15:56:31,945 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-07 15:56:31,982 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 15:56:32,009 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-07 15:56:32,009 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-07 15:56:32,187 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 15:56:32,187 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-07 15:56:32,187 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 301sqjz8
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_dm1/20251107-1556/wandb/run-20251107_155632-301sqjz8
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/301sqjz8
[rank3]:2025-11-07 15:56:33,663 - INFO - WandB logging enabled
[rank3]:2025-11-07 15:56:33,664 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-07 15:56:33,703 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 15:56:33,732 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-07 15:56:33,733 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 15:56:33,954 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_dm1
[rank2]:2025-11-07 15:56:33,954 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 15:56:33,954 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 15:56:33,954 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-07 15:56:33,954 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-07 15:56:33,955 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-07 15:56:33,938 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 15:56:33,939 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-07 15:56:33,940 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-07 15:56:33,954 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 15:56:36,715 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-07 15:56:36,715 - INFO - Finished loading the checkpoint in 2.76 seconds.
[rank0]:2025-11-07 15:56:36,715 - INFO - Training starts at step 1
[rank2]:2025-11-07 15:56:40,008 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8881  memory:  9.99GiB(21.03%)  tps: 2,041  tflops: 15.55  mfu: 4.98%
[rank2]:2025-11-07 15:56:40,008 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-07 15:56:40,018 - INFO -  step:  1  loss:  8.4951  grad_norm: 132.8881  memory: 24.19GiB(50.91%)  tps: 2,595  tflops: 19.77  mfu: 6.34%
[rank1]:2025-11-07 15:56:40,010 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8881  memory: 12.38GiB(26.05%)  tps: 1,952  tflops: 14.87  mfu: 4.77%
[rank1]:2025-11-07 15:56:40,010 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-07 15:56:40,019 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 15:56:40,058 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8881  memory: 12.80GiB(26.95%)  tps: 1,952  tflops: 14.87  mfu: 4.77%
[rank0]:2025-11-07 15:56:40,059 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 15:58:44,438 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 15:58:46,737 - INFO - Avg. fwd time: 11.5382 / Avg. bwd time: 45.1863 / Avg. batch time: 513.8675 (ms) / GPU bubble ratio: 11.69%
[rank0]:2025-11-07 15:58:46,858 - INFO - Avg. fwd time: 7.9588 / Avg. bwd time: 23.4677 / Avg. batch time: 623.0802 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-07 15:58:46,811 - INFO - Avg. fwd time: 7.1846 / Avg. bwd time: 18.8301 / Avg. batch time: 546.0765 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-07 15:58:46,851 - INFO - Avg. fwd time: 9.1457 / Avg. bwd time: 23.9632 / Avg. batch time: 585.8334 (ms) / GPU bubble ratio: 54.79%
[rank3]:2025-11-07 15:58:47,048 - INFO -  step: 50  loss:  7.2330  grad_norm: 12.1375  memory: 26.98GiB(56.79%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-07 15:58:47,050 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.1375  memory: 16.57GiB(34.88%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank2]:2025-11-07 15:58:47,035 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.1375  memory: 11.81GiB(24.85%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-07 15:58:47,039 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.1375  memory: 14.64GiB(30.82%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-07 16:00:54,894 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:00:57,241 - INFO - Avg. fwd time: 11.5681 / Avg. bwd time: 45.5358 / Avg. batch time: 516.7467 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-07 16:00:57,316 - INFO - Avg. fwd time: 7.1786 / Avg. bwd time: 18.8931 / Avg. batch time: 548.4446 (ms) / GPU bubble ratio: 61.97%
[rank0]:2025-11-07 16:00:57,364 - INFO - Avg. fwd time: 7.9347 / Avg. bwd time: 23.4994 / Avg. batch time: 624.4219 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-07 16:00:57,356 - INFO - Avg. fwd time: 9.1441 / Avg. bwd time: 24.0410 / Avg. batch time: 587.6301 (ms) / GPU bubble ratio: 54.82%
[rank3]:2025-11-07 16:00:57,555 - INFO -  step: 100  loss:  2.6764  grad_norm: 17.1009  memory: 26.98GiB(56.79%)  tps: 6,277  tflops: 47.81  mfu: 15.32%
[rank0]:2025-11-07 16:00:57,557 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.1009  memory: 16.57GiB(34.88%)  tps: 6,277  tflops: 47.81  mfu: 15.32%
[rank1]:2025-11-07 16:00:57,546 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.1009  memory: 14.64GiB(30.82%)  tps: 6,277  tflops: 47.81  mfu: 15.32%
[rank2]:2025-11-07 16:00:57,542 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.1009  memory: 11.81GiB(24.85%)  tps: 6,277  tflops: 47.81  mfu: 15.32%
[rank3]:2025-11-07 16:00:57,762 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1600_real_step100_rank3.svg
[rank3]:> Batch Time: 625.61 ms, GPU Bubble Ratio: 59.51%, 57.37%, 66.50%, 26.33%
[rank0]:2025-11-07 16:03:05,279 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:03:07,784 - INFO - Avg. fwd time: 11.5579 / Avg. bwd time: 45.5946 / Avg. batch time: 517.0777 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-07 16:03:07,871 - INFO - Avg. fwd time: 7.9231 / Avg. bwd time: 23.5184 / Avg. batch time: 624.5821 (ms) / GPU bubble ratio: 59.73%
[rank2]:2025-11-07 16:03:07,811 - INFO - Avg. fwd time: 7.1716 / Avg. bwd time: 18.9220 / Avg. batch time: 548.8915 (ms) / GPU bubble ratio: 61.97%
[rank2]:2025-11-07 16:03:07,892 - INFO -  step: 150  loss: -4.0000  grad_norm:  2.4735  memory: 11.81GiB(24.85%)  tps: 6,285  tflops: 47.87  mfu: 15.34%
[rank1]:2025-11-07 16:03:07,841 - INFO - Avg. fwd time: 9.1329 / Avg. bwd time: 24.0746 / Avg. batch time: 587.9109 (ms) / GPU bubble ratio: 54.81%
[rank1]:2025-11-07 16:03:07,896 - INFO -  step: 150  loss: -4.0000  grad_norm:  2.4735  memory: 14.64GiB(30.82%)  tps: 6,285  tflops: 47.87  mfu: 15.34%
[rank0]:2025-11-07 16:03:07,907 - INFO -  step: 150  loss: -4.0000  grad_norm:  2.4735  memory: 16.57GiB(34.88%)  tps: 6,285  tflops: 47.87  mfu: 15.34%
[rank3]:2025-11-07 16:03:07,905 - INFO -  step: 150  loss:  0.5614  grad_norm:  2.4735  memory: 26.98GiB(56.79%)  tps: 6,285  tflops: 47.87  mfu: 15.34%
[rank0]:2025-11-07 16:05:15,151 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:05:17,477 - INFO - Avg. fwd time: 11.5380 / Avg. bwd time: 45.5901 / Avg. batch time: 516.8535 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-07 16:05:17,550 - INFO - Avg. fwd time: 7.1658 / Avg. bwd time: 18.9333 / Avg. batch time: 548.5291 (ms) / GPU bubble ratio: 61.94%
[rank0]:2025-11-07 16:05:17,597 - INFO - Avg. fwd time: 7.9143 / Avg. bwd time: 23.5214 / Avg. batch time: 624.0547 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-07 16:05:17,589 - INFO - Avg. fwd time: 9.1237 / Avg. bwd time: 24.0861 / Avg. batch time: 587.4597 (ms) / GPU bubble ratio: 54.78%
[rank1]:2025-11-07 16:05:17,777 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3655  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-07 16:05:17,788 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3655  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank2]:2025-11-07 16:05:17,773 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3655  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-07 16:05:17,786 - INFO -  step: 200  loss:  0.5099  grad_norm:  0.3655  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-07 16:05:17,939 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1605_real_step200_rank3.svg
[rank3]:> Batch Time: 621.61 ms, GPU Bubble Ratio: 59.29%, 57.15%, 66.29%, 26.59%
[rank0]:2025-11-07 16:07:25,361 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:07:27,680 - INFO - Avg. fwd time: 11.5313 / Avg. bwd time: 45.5942 / Avg. batch time: 516.8159 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-07 16:07:27,801 - INFO - Avg. fwd time: 7.9118 / Avg. bwd time: 23.5243 / Avg. batch time: 623.9419 (ms) / GPU bubble ratio: 59.69%
[rank2]:2025-11-07 16:07:27,754 - INFO - Avg. fwd time: 7.1635 / Avg. bwd time: 18.9412 / Avg. batch time: 548.5195 (ms) / GPU bubble ratio: 61.93%
[rank1]:2025-11-07 16:07:27,793 - INFO - Avg. fwd time: 9.1215 / Avg. bwd time: 24.0954 / Avg. batch time: 587.3947 (ms) / GPU bubble ratio: 54.76%
[rank0]:2025-11-07 16:07:27,989 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4500  memory: 16.57GiB(34.88%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank2]:2025-11-07 16:07:27,975 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4500  memory: 11.81GiB(24.85%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank3]:2025-11-07 16:07:27,987 - INFO -  step: 250  loss:  0.5229  grad_norm:  0.4500  memory: 26.98GiB(56.79%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank1]:2025-11-07 16:07:27,978 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4500  memory: 14.64GiB(30.82%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank0]:2025-11-07 16:09:35,556 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:09:38,101 - INFO - Avg. fwd time: 11.5309 / Avg. bwd time: 45.6109 / Avg. batch time: 516.9372 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:09:38,129 - INFO - Avg. fwd time: 7.1630 / Avg. bwd time: 18.9485 / Avg. batch time: 548.5676 (ms) / GPU bubble ratio: 61.92%
[rank1]:2025-11-07 16:09:38,159 - INFO - Avg. fwd time: 9.1229 / Avg. bwd time: 24.1038 / Avg. batch time: 587.4133 (ms) / GPU bubble ratio: 54.75%
[rank0]:2025-11-07 16:09:38,190 - INFO - Avg. fwd time: 7.9099 / Avg. bwd time: 23.5292 / Avg. batch time: 623.9372 (ms) / GPU bubble ratio: 59.69%
[rank0]:2025-11-07 16:09:38,226 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3715  memory: 16.57GiB(34.88%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank2]:2025-11-07 16:09:38,212 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3715  memory: 11.81GiB(24.85%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank1]:2025-11-07 16:09:38,215 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3715  memory: 14.64GiB(30.82%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank3]:2025-11-07 16:09:38,224 - INFO -  step: 300  loss:  0.5069  grad_norm:  0.3715  memory: 26.98GiB(56.79%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank3]:2025-11-07 16:09:38,373 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1609_real_step300_rank3.svg
[rank3]:> Batch Time: 623.62 ms, GPU Bubble Ratio: 59.37%, 57.16%, 66.34%, 26.57%
[rank0]:2025-11-07 16:11:45,707 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:11:48,000 - INFO - Avg. fwd time: 11.5281 / Avg. bwd time: 45.6038 / Avg. batch time: 516.8510 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-07 16:11:48,120 - INFO - Avg. fwd time: 7.9111 / Avg. bwd time: 23.5301 / Avg. batch time: 623.8447 (ms) / GPU bubble ratio: 59.68%
[rank2]:2025-11-07 16:11:48,072 - INFO - Avg. fwd time: 7.1624 / Avg. bwd time: 18.9512 / Avg. batch time: 548.5119 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-07 16:11:48,112 - INFO - Avg. fwd time: 9.1224 / Avg. bwd time: 24.1052 / Avg. batch time: 587.3365 (ms) / GPU bubble ratio: 54.74%
[rank0]:2025-11-07 16:11:48,307 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3821  memory: 16.57GiB(34.88%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-07 16:11:48,293 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3821  memory: 11.81GiB(24.85%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-07 16:11:48,306 - INFO -  step: 350  loss:  0.4826  grad_norm:  0.3821  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-07 16:11:48,297 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3821  memory: 14.64GiB(30.82%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-07 16:13:55,342 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:13:57,661 - INFO - Avg. fwd time: 11.5209 / Avg. bwd time: 45.5790 / Avg. batch time: 516.5872 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:13:57,733 - INFO - Avg. fwd time: 7.1606 / Avg. bwd time: 18.9502 / Avg. batch time: 548.1866 (ms) / GPU bubble ratio: 61.89%
[rank0]:2025-11-07 16:13:57,780 - INFO - Avg. fwd time: 7.9084 / Avg. bwd time: 23.5277 / Avg. batch time: 623.4732 (ms) / GPU bubble ratio: 59.66%
[rank1]:2025-11-07 16:13:57,772 - INFO - Avg. fwd time: 9.1194 / Avg. bwd time: 24.1033 / Avg. batch time: 586.9885 (ms) / GPU bubble ratio: 54.72%
[rank2]:2025-11-07 16:13:57,954 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3644  memory: 11.81GiB(24.85%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank3]:2025-11-07 16:13:57,967 - INFO -  step: 400  loss:  0.4714  grad_norm:  0.3644  memory: 26.98GiB(56.79%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank0]:2025-11-07 16:13:57,969 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3644  memory: 16.57GiB(34.88%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank1]:2025-11-07 16:13:57,958 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3644  memory: 14.64GiB(30.82%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank3]:2025-11-07 16:13:58,116 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1613_real_step400_rank3.svg
[rank3]:> Batch Time: 621.08 ms, GPU Bubble Ratio: 59.31%, 57.16%, 66.29%, 26.51%
[rank3]:2025-11-07 16:14:06,853 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-07 16:14:07,070 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-07 16:14:07,122 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-07 16:14:07,096 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-07 16:16:04,921 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:16:07,447 - INFO - Avg. fwd time: 11.5115 / Avg. bwd time: 45.5642 / Avg. batch time: 516.3833 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-07 16:16:07,475 - INFO - Avg. fwd time: 7.1591 / Avg. bwd time: 18.9505 / Avg. batch time: 547.9981 (ms) / GPU bubble ratio: 61.88%
[rank1]:2025-11-07 16:16:07,506 - INFO - Avg. fwd time: 9.1156 / Avg. bwd time: 24.1030 / Avg. batch time: 586.7817 (ms) / GPU bubble ratio: 54.71%
[rank3]:2025-11-07 16:16:07,572 - INFO -  step: 450  loss:  0.5214  grad_norm:  0.3720  memory: 26.98GiB(56.79%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank2]:2025-11-07 16:16:07,559 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3720  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank0]:2025-11-07 16:16:07,538 - INFO - Avg. fwd time: 7.9060 / Avg. bwd time: 23.5261 / Avg. batch time: 623.2468 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-07 16:16:07,574 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3720  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank1]:2025-11-07 16:16:07,563 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3720  memory: 14.64GiB(30.82%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank0]:2025-11-07 16:18:15,205 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:18:17,534 - INFO - Avg. fwd time: 11.5118 / Avg. bwd time: 45.5743 / Avg. batch time: 516.4645 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:18:17,609 - INFO - Avg. fwd time: 7.1589 / Avg. bwd time: 18.9519 / Avg. batch time: 548.0471 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-07 16:18:17,650 - INFO - Avg. fwd time: 9.1152 / Avg. bwd time: 24.1046 / Avg. batch time: 586.8272 (ms) / GPU bubble ratio: 54.71%
[rank0]:2025-11-07 16:18:17,657 - INFO - Avg. fwd time: 7.9052 / Avg. bwd time: 23.5269 / Avg. batch time: 623.2878 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-07 16:18:17,829 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2993  memory: 11.81GiB(24.85%)  tps: 6,289  tflops: 47.89  mfu: 15.35%
[rank3]:2025-11-07 16:18:17,842 - INFO -  step: 500  loss:  0.3924  grad_norm:  0.2993  memory: 26.98GiB(56.79%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank1]:2025-11-07 16:18:17,833 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2993  memory: 14.64GiB(30.82%)  tps: 6,289  tflops: 47.89  mfu: 15.35%
[rank0]:2025-11-07 16:18:17,843 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2993  memory: 16.57GiB(34.88%)  tps: 6,289  tflops: 47.89  mfu: 15.35%
[rank3]:2025-11-07 16:18:17,993 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1618_real_step500_rank3.svg
[rank3]:> Batch Time: 623.61 ms, GPU Bubble Ratio: 59.37%, 57.24%, 66.37%, 26.39%
[rank0]:2025-11-07 16:20:25,836 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 16:20:28,228 - INFO - Avg. fwd time: 7.1595 / Avg. bwd time: 18.9539 / Avg. batch time: 548.2874 (ms) / GPU bubble ratio: 61.90%
[rank3]:2025-11-07 16:20:28,151 - INFO - Avg. fwd time: 11.5171 / Avg. bwd time: 45.5955 / Avg. batch time: 516.6769 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-07 16:20:28,266 - INFO - Avg. fwd time: 9.1167 / Avg. bwd time: 24.1079 / Avg. batch time: 587.0747 (ms) / GPU bubble ratio: 54.73%
[rank0]:2025-11-07 16:20:28,275 - INFO - Avg. fwd time: 7.9052 / Avg. bwd time: 23.5285 / Avg. batch time: 623.5310 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-07 16:20:28,459 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3755  memory: 16.57GiB(34.88%)  tps: 6,272  tflops: 47.77  mfu: 15.31%
[rank1]:2025-11-07 16:20:28,448 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3755  memory: 14.64GiB(30.82%)  tps: 6,272  tflops: 47.77  mfu: 15.31%
[rank2]:2025-11-07 16:20:28,444 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3755  memory: 11.81GiB(24.85%)  tps: 6,272  tflops: 47.77  mfu: 15.31%
[rank3]:2025-11-07 16:20:28,457 - INFO -  step: 550  loss:  0.4293  grad_norm:  0.3755  memory: 26.98GiB(56.79%)  tps: 6,272  tflops: 47.77  mfu: 15.31%
[rank0]:2025-11-07 16:22:35,882 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:22:38,414 - INFO - Avg. fwd time: 11.5175 / Avg. bwd time: 45.6004 / Avg. batch time: 516.7172 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:22:38,441 - INFO - Avg. fwd time: 7.1596 / Avg. bwd time: 18.9550 / Avg. batch time: 548.2980 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-07 16:22:38,474 - INFO - Avg. fwd time: 9.1172 / Avg. bwd time: 24.1090 / Avg. batch time: 587.0897 (ms) / GPU bubble ratio: 54.72%
[rank1]:2025-11-07 16:22:38,530 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3524  memory: 14.64GiB(30.82%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-07 16:22:38,505 - INFO - Avg. fwd time: 7.9031 / Avg. bwd time: 23.5291 / Avg. batch time: 623.5362 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-07 16:22:38,541 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3524  memory: 16.57GiB(34.88%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-07 16:22:38,540 - INFO -  step: 600  loss:  0.4250  grad_norm:  0.3524  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-07 16:22:38,527 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3524  memory: 11.81GiB(24.85%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-07 16:22:38,690 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1622_real_step600_rank3.svg
[rank3]:> Batch Time: 622.09 ms, GPU Bubble Ratio: 59.34%, 57.18%, 66.31%, 26.68%
[rank0]:2025-11-07 16:24:45,555 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:24:47,847 - INFO - Avg. fwd time: 11.5136 / Avg. bwd time: 45.5840 / Avg. batch time: 516.5496 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:24:47,921 - INFO - Avg. fwd time: 7.1595 / Avg. bwd time: 18.9546 / Avg. batch time: 548.1493 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-07 16:24:47,961 - INFO - Avg. fwd time: 9.1170 / Avg. bwd time: 24.1091 / Avg. batch time: 586.9424 (ms) / GPU bubble ratio: 54.71%
[rank0]:2025-11-07 16:24:47,968 - INFO - Avg. fwd time: 7.9026 / Avg. bwd time: 23.5286 / Avg. batch time: 623.3838 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-07 16:24:48,137 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3527  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank3]:2025-11-07 16:24:48,150 - INFO -  step: 650  loss:  0.3693  grad_norm:  0.3527  memory: 26.98GiB(56.79%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank1]:2025-11-07 16:24:48,141 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3527  memory: 14.64GiB(30.82%)  tps: 6,320  tflops: 48.14  mfu: 15.43%
[rank0]:2025-11-07 16:24:48,152 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3527  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank0]:2025-11-07 16:26:54,983 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:26:57,296 - INFO - Avg. fwd time: 11.5068 / Avg. bwd time: 45.5689 / Avg. batch time: 516.3680 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:26:57,373 - INFO - Avg. fwd time: 7.1591 / Avg. bwd time: 18.9542 / Avg. batch time: 547.9375 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-07 16:26:57,413 - INFO - Avg. fwd time: 9.1164 / Avg. bwd time: 24.1099 / Avg. batch time: 586.7331 (ms) / GPU bubble ratio: 54.70%
[rank0]:2025-11-07 16:26:57,420 - INFO - Avg. fwd time: 7.9013 / Avg. bwd time: 23.5284 / Avg. batch time: 623.1643 (ms) / GPU bubble ratio: 59.65%
[rank3]:2025-11-07 16:26:57,604 - INFO -  step: 700  loss:  0.4343  grad_norm:  0.4058  memory: 26.98GiB(56.79%)  tps: 6,328  tflops: 48.20  mfu: 15.45%
[rank1]:2025-11-07 16:26:57,595 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4058  memory: 14.64GiB(30.82%)  tps: 6,328  tflops: 48.20  mfu: 15.45%
[rank0]:2025-11-07 16:26:57,606 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4058  memory: 16.57GiB(34.88%)  tps: 6,328  tflops: 48.20  mfu: 15.45%
[rank2]:2025-11-07 16:26:57,591 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4058  memory: 11.81GiB(24.85%)  tps: 6,328  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-07 16:26:57,754 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1626_real_step700_rank3.svg
[rank3]:> Batch Time: 622.14 ms, GPU Bubble Ratio: 59.31%, 57.13%, 66.31%, 26.81%
[rank0]:2025-11-07 16:29:05,042 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:29:07,563 - INFO - Avg. fwd time: 11.5060 / Avg. bwd time: 45.5676 / Avg. batch time: 516.3503 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:29:07,592 - INFO - Avg. fwd time: 7.1595 / Avg. bwd time: 18.9554 / Avg. batch time: 547.9340 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-07 16:29:07,657 - INFO - Avg. fwd time: 7.9010 / Avg. bwd time: 23.5298 / Avg. batch time: 623.1733 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-07 16:29:07,693 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4355  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-07 16:29:07,625 - INFO - Avg. fwd time: 9.1167 / Avg. bwd time: 24.1144 / Avg. batch time: 586.7448 (ms) / GPU bubble ratio: 54.69%
[rank1]:2025-11-07 16:29:07,682 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4355  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-07 16:29:07,679 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4355  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-07 16:29:07,692 - INFO -  step: 750  loss:  0.4135  grad_norm:  0.4355  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-07 16:31:14,971 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 16:31:17,291 - INFO - Avg. fwd time: 11.5068 / Avg. bwd time: 45.5657 / Avg. batch time: 516.3401 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-07 16:31:17,363 - INFO - Avg. fwd time: 7.1601 / Avg. bwd time: 18.9569 / Avg. batch time: 547.9044 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-07 16:31:17,410 - INFO - Avg. fwd time: 7.9002 / Avg. bwd time: 23.5314 / Avg. batch time: 623.1533 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-07 16:31:17,404 - INFO - Avg. fwd time: 9.1178 / Avg. bwd time: 24.1197 / Avg. batch time: 586.7271 (ms) / GPU bubble ratio: 54.68%
[rank1]:2025-11-07 16:31:17,585 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3792  memory: 14.64GiB(30.82%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank1]:2025-11-07 16:31:17,585 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3792  tps: 6,676  tflops: 50.85  mfu: 14.76%
[rank1]:2025-11-07 16:31:17,585 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-07 16:31:17,586 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-07 16:31:17,581 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3792  memory: 11.81GiB(24.85%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank2]:2025-11-07 16:31:17,581 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3792  tps: 6,677  tflops: 50.86  mfu: 14.77%
[rank2]:2025-11-07 16:31:17,581 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-07 16:31:17,582 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-07 16:31:17,594 - INFO -  step: 800  loss:  0.4187  grad_norm:  0.3792  memory: 26.98GiB(56.79%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank3]:2025-11-07 16:31:17,595 - INFO -  final step: 800  loss:  0.4187  grad_norm:  0.3792  tps: 6,683  tflops: 50.90  mfu: 14.85%
[rank3]:2025-11-07 16:31:17,595 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-07 16:31:17,596 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-07 16:31:17,596 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3792  memory: 16.57GiB(34.88%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank0]:2025-11-07 16:31:17,596 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3792  tps: 6,676  tflops: 50.85  mfu: 14.76%
[rank0]:2025-11-07 16:31:17,596 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-07 16:31:17,597 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-07 16:31:19,649 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-07 16:31:19,662 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-07 16:31:19,662 - INFO - Destroying the purge thread.
[rank2]:2025-11-07 16:31:19,662 - INFO - Destroying the purge thread.
[rank3]:2025-11-07 16:31:19,804 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1631_real_final800_rank3.svg
[rank3]:> Batch Time: 622.61 ms, GPU Bubble Ratio: 59.34%, 57.17%, 66.33%, 26.61%
[rank3]:2025-11-07 16:31:19,942 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1/pipeline_schedule/251107_1631_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.52 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-07 16:31:19,943 - INFO - Destroying the purge thread.
[rank1]:2025-11-07 16:31:19,981 - INFO - Process group destroyed
[rank2]:2025-11-07 16:31:19,967 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 16-16, summary, console lines 231-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.45159
[rank3]:wandb:               final/avg_loss 0.41874
[rank3]:wandb:             final/avg_mfu(%) 14.84964
[rank3]:wandb:             final/avg_tflops 50.89928
[rank3]:wandb:    final/avg_throughput(tps) 6683.01639
[rank3]:wandb:              final/grad_norm 0.37917
[rank3]:wandb:               final/max_loss 0.41874
[rank3]:wandb:                    grad_norm 0.37917
[rank3]:wandb: loss_metrics/global_avg_loss 0.41874
[rank3]:wandb: loss_metrics/global_max_loss 0.41874
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/301sqjz8
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_dm1/20251107-1556/wandb/run-20251107_155632-301sqjz8/logs
[rank3]:2025-11-07 16:31:20,993 - INFO - Process group destroyed
[rank0]:2025-11-07 16:31:21,662 - INFO - Training completed
[rank0]:2025-11-07 16:31:21,663 - INFO - Destroying the purge thread.
[rank0]:2025-11-07 16:31:21,967 - INFO - Process group destroyed
[rank1]:Stage 1: Modules to keep: {'layers.6', 'layers.8', 'layers.5', 'layers.4', 'layers.7'}
[rank0]:Stage 0: Modules to keep: {'layers.0', 'layers.2', 'layers.3', 'tok_embeddings', 'layers.1'}
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.10', 'layers.11', 'layers.12'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.15', 'output', 'layers.13', 'layers.14'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 1e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
