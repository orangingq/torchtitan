
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 15:21:44 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed11.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank1]:2025-11-09 15:21:50,848 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-09 15:21:50,771 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-09 15:21:50,772 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-09 15:21:50,997 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 15:21:51,010 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 15:21:50,996 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 15:21:51,010 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 15:21:51,014 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 15:21:51,016 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-09 15:21:51,010 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-09 15:21:51,075 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 15:21:51,078 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 15:21:51,189 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 15:21:51,192 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 15:21:51,399 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 15:21:54,295 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 15:21:54,446 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 15:21:54,466 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 15:21:54,514 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 15:21:54,543 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 15:21:54,543 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 15:21:54,474 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 15:21:54,516 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 15:21:54,483 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 15:21:54,484 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-09 15:21:54,543 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 15:21:54,543 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 15:21:54,510 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 15:21:54,510 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 15:21:54,740 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 15:21:54,740 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 15:21:54,741 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-09 15:21:54,732 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 15:21:54,732 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 15:21:54,733 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-09 15:21:54,694 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 15:21:54,694 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 15:21:54,695 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1/20251109-1521/wandb/run-20251109_152155-3a0ddpjf
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed11_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/3a0ddpjf
[rank3]:2025-11-09 15:21:56,492 - INFO - WandB logging enabled
[rank3]:2025-11-09 15:21:56,492 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 15:21:56,532 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 15:21:56,561 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 15:21:56,561 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 15:21:56,766 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1
[rank2]:2025-11-09 15:21:56,766 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 15:21:56,748 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 15:21:56,749 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 15:21:56,766 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 15:21:56,767 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 15:21:56,767 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-09 15:21:56,750 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 15:21:56,766 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 15:21:56,766 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 15:21:59,176 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 15:21:59,176 - INFO - Finished loading the checkpoint in 2.41 seconds.
[rank0]:2025-11-09 15:21:59,176 - INFO - Training starts at step 1
[rank1]:2025-11-09 15:22:02,291 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory: 12.38GiB(26.05%)  tps: 2,107  tflops: 16.04  mfu: 5.14%
[rank1]:2025-11-09 15:22:02,292 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 15:22:02,288 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory:  9.99GiB(21.03%)  tps: 2,108  tflops: 16.06  mfu: 5.15%
[rank0]:2025-11-09 15:22:02,327 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory: 12.80GiB(26.95%)  tps: 2,089  tflops: 15.91  mfu: 5.10%
[rank3]:2025-11-09 15:22:02,299 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5955  memory: 24.19GiB(50.91%)  tps: 2,842  tflops: 21.65  mfu: 6.94%
[rank3]:2025-11-09 15:22:02,300 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 15:22:02,327 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 15:22:02,289 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 15:24:06,141 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:24:08,434 - INFO - Avg. fwd time: 11.4800 / Avg. bwd time: 45.1235 / Avg. batch time: 512.6920 (ms) / GPU bubble ratio: 11.68%
[rank2]:2025-11-09 15:24:08,507 - INFO - Avg. fwd time: 7.1749 / Avg. bwd time: 18.8002 / Avg. batch time: 544.7035 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 15:24:08,547 - INFO - Avg. fwd time: 9.1312 / Avg. bwd time: 23.9930 / Avg. batch time: 584.2509 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 15:24:08,554 - INFO - Avg. fwd time: 7.9062 / Avg. bwd time: 23.4194 / Avg. batch time: 621.4159 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 15:24:08,732 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5682  memory: 11.81GiB(24.85%)  tps: 6,349  tflops: 48.36  mfu: 15.50%
[rank1]:2025-11-09 15:24:08,735 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5682  memory: 14.64GiB(30.82%)  tps: 6,349  tflops: 48.36  mfu: 15.50%
[rank0]:2025-11-09 15:24:08,746 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5682  memory: 16.57GiB(34.88%)  tps: 6,350  tflops: 48.37  mfu: 15.50%
[rank3]:2025-11-09 15:24:08,744 - INFO -  step: 50  loss:  8.2866  grad_norm: 17.5682  memory: 26.98GiB(56.79%)  tps: 6,349  tflops: 48.36  mfu: 15.50%
[rank0]:2025-11-09 15:26:15,679 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:26:18,009 - INFO - Avg. fwd time: 11.4934 / Avg. bwd time: 45.3663 / Avg. batch time: 514.6407 (ms) / GPU bubble ratio: 11.61%
[rank2]:2025-11-09 15:26:18,083 - INFO - Avg. fwd time: 7.1696 / Avg. bwd time: 18.8622 / Avg. batch time: 546.1995 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 15:26:18,131 - INFO - Avg. fwd time: 7.9036 / Avg. bwd time: 23.4615 / Avg. batch time: 622.0844 (ms) / GPU bubble ratio: 59.66%
[rank1]:2025-11-09 15:26:18,123 - INFO - Avg. fwd time: 9.1314 / Avg. bwd time: 24.0598 / Avg. batch time: 585.3499 (ms) / GPU bubble ratio: 54.64%
[rank1]:2025-11-09 15:26:18,312 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1156  memory: 14.64GiB(30.82%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 15:26:18,321 - INFO -  step: 100  loss:  4.2780  grad_norm: 19.1156  memory: 26.98GiB(56.79%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank2]:2025-11-09 15:26:18,309 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1156  memory: 11.81GiB(24.85%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 15:26:18,323 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1156  memory: 16.57GiB(34.88%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 15:26:18,501 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1526_real_step100_rank3.svg
[rank3]:> Batch Time: 621.14 ms, GPU Bubble Ratio: 59.29%, 57.05%, 66.29%, 26.57%
[rank0]:2025-11-09 15:28:25,354 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:28:27,870 - INFO - Avg. fwd time: 7.1637 / Avg. bwd time: 18.8833 / Avg. batch time: 546.8775 (ms) / GPU bubble ratio: 61.90%
[rank3]:2025-11-09 15:28:27,844 - INFO - Avg. fwd time: 11.4892 / Avg. bwd time: 45.4478 / Avg. batch time: 515.2222 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 15:28:27,951 - INFO -  step: 150  loss: -4.0000  grad_norm: 29.1472  memory: 11.81GiB(24.85%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-09 15:28:27,900 - INFO - Avg. fwd time: 9.1243 / Avg. bwd time: 24.0858 / Avg. batch time: 585.8864 (ms) / GPU bubble ratio: 54.65%
[rank1]:2025-11-09 15:28:27,955 - INFO -  step: 150  loss: -4.0000  grad_norm: 29.1472  memory: 14.64GiB(30.82%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 15:28:27,930 - INFO - Avg. fwd time: 7.8940 / Avg. bwd time: 23.4736 / Avg. batch time: 622.4921 (ms) / GPU bubble ratio: 59.69%
[rank0]:2025-11-09 15:28:27,966 - INFO -  step: 150  loss: -4.0000  grad_norm: 29.1472  memory: 16.57GiB(34.88%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 15:28:27,963 - INFO -  step: 150  loss:  3.4901  grad_norm: 29.1472  memory: 26.98GiB(56.79%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 15:30:35,355 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:30:37,699 - INFO - Avg. fwd time: 11.4968 / Avg. bwd time: 45.5271 / Avg. batch time: 515.9020 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 15:30:37,773 - INFO - Avg. fwd time: 7.1623 / Avg. bwd time: 18.9058 / Avg. batch time: 547.4695 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-09 15:30:37,813 - INFO - Avg. fwd time: 9.1221 / Avg. bwd time: 24.1138 / Avg. batch time: 586.4166 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 15:30:37,821 - INFO - Avg. fwd time: 7.8905 / Avg. bwd time: 23.4896 / Avg. batch time: 622.9740 (ms) / GPU bubble ratio: 59.70%
[rank2]:2025-11-09 15:30:37,997 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.2922  memory: 11.81GiB(24.85%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank1]:2025-11-09 15:30:38,000 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.2922  memory: 14.64GiB(30.82%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-09 15:30:38,011 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.2922  memory: 16.57GiB(34.88%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-09 15:30:38,009 - INFO -  step: 200  loss:  1.2124  grad_norm: 14.2922  memory: 26.98GiB(56.79%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-09 15:30:38,160 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1530_real_step200_rank3.svg
[rank3]:> Batch Time: 623.17 ms, GPU Bubble Ratio: 59.39%, 57.10%, 66.32%, 26.59%
[rank0]:2025-11-09 15:32:45,823 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:32:48,138 - INFO - Avg. fwd time: 11.5063 / Avg. bwd time: 45.6028 / Avg. batch time: 516.5807 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 15:32:48,212 - INFO - Avg. fwd time: 7.1630 / Avg. bwd time: 18.9355 / Avg. batch time: 548.2093 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-09 15:32:48,251 - INFO - Avg. fwd time: 9.1242 / Avg. bwd time: 24.1471 / Avg. batch time: 587.1349 (ms) / GPU bubble ratio: 54.67%
[rank0]:2025-11-09 15:32:48,259 - INFO - Avg. fwd time: 7.8926 / Avg. bwd time: 23.5079 / Avg. batch time: 623.6685 (ms) / GPU bubble ratio: 59.72%
[rank2]:2025-11-09 15:32:48,435 - INFO -  step: 250  loss: -4.0000  grad_norm: 14.3939  memory: 11.81GiB(24.85%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank1]:2025-11-09 15:32:48,439 - INFO -  step: 250  loss: -4.0000  grad_norm: 14.3939  memory: 14.64GiB(30.82%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank3]:2025-11-09 15:32:48,448 - INFO -  step: 250  loss:  0.6582  grad_norm: 14.3939  memory: 26.98GiB(56.79%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank0]:2025-11-09 15:32:48,450 - INFO -  step: 250  loss: -4.0000  grad_norm: 14.3939  memory: 16.57GiB(34.88%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank0]:2025-11-09 15:34:56,083 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:34:58,654 - INFO - Avg. fwd time: 7.1622 / Avg. bwd time: 18.9517 / Avg. batch time: 548.6117 (ms) / GPU bubble ratio: 61.92%
[rank3]:2025-11-09 15:34:58,627 - INFO - Avg. fwd time: 11.5107 / Avg. bwd time: 45.6558 / Avg. batch time: 517.0355 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 15:34:58,736 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.3805  memory: 11.81GiB(24.85%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank1]:2025-11-09 15:34:58,684 - INFO - Avg. fwd time: 9.1239 / Avg. bwd time: 24.1612 / Avg. batch time: 587.5088 (ms) / GPU bubble ratio: 54.68%
[rank1]:2025-11-09 15:34:58,739 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.3805  memory: 14.64GiB(30.82%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-09 15:34:58,714 - INFO - Avg. fwd time: 7.8933 / Avg. bwd time: 23.5165 / Avg. batch time: 624.0185 (ms) / GPU bubble ratio: 59.73%
[rank0]:2025-11-09 15:34:58,750 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.3805  memory: 16.57GiB(34.88%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-09 15:34:58,748 - INFO -  step: 300  loss:  0.5581  grad_norm:  1.3805  memory: 26.98GiB(56.79%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-09 15:34:58,898 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1534_real_step300_rank3.svg
[rank3]:> Batch Time: 624.12 ms, GPU Bubble Ratio: 59.45%, 57.21%, 66.35%, 26.25%
[rank0]:2025-11-09 15:37:05,958 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:37:08,258 - INFO - Avg. fwd time: 11.5055 / Avg. bwd time: 45.6537 / Avg. batch time: 516.9726 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 15:37:08,336 - INFO - Avg. fwd time: 7.1603 / Avg. bwd time: 18.9579 / Avg. batch time: 548.5933 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-09 15:37:08,371 - INFO - Avg. fwd time: 9.1197 / Avg. bwd time: 24.1621 / Avg. batch time: 587.4625 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 15:37:08,379 - INFO - Avg. fwd time: 7.8916 / Avg. bwd time: 23.5178 / Avg. batch time: 623.9533 (ms) / GPU bubble ratio: 59.73%
[rank3]:2025-11-09 15:37:08,565 - INFO -  step: 350  loss:  0.4966  grad_norm:  3.9747  memory: 26.98GiB(56.79%)  tps: 6,311  tflops: 48.06  mfu: 15.40%
[rank1]:2025-11-09 15:37:08,556 - INFO -  step: 350  loss: -4.0000  grad_norm:  3.9747  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank2]:2025-11-09 15:37:08,553 - INFO -  step: 350  loss: -4.0000  grad_norm:  3.9747  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 15:37:08,567 - INFO -  step: 350  loss: -4.0000  grad_norm:  3.9747  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 15:39:15,501 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:39:17,821 - INFO - Avg. fwd time: 11.4973 / Avg. bwd time: 45.6424 / Avg. batch time: 516.8098 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 15:39:17,896 - INFO - Avg. fwd time: 7.1583 / Avg. bwd time: 18.9625 / Avg. batch time: 548.3811 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 15:39:17,935 - INFO - Avg. fwd time: 9.1151 / Avg. bwd time: 24.1603 / Avg. batch time: 587.2165 (ms) / GPU bubble ratio: 54.67%
[rank0]:2025-11-09 15:39:17,943 - INFO - Avg. fwd time: 7.8870 / Avg. bwd time: 23.5182 / Avg. batch time: 623.6865 (ms) / GPU bubble ratio: 59.72%
[rank3]:2025-11-09 15:39:18,132 - INFO -  step: 400  loss:  0.4814  grad_norm:  0.4840  memory: 26.98GiB(56.79%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank1]:2025-11-09 15:39:18,124 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4840  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank2]:2025-11-09 15:39:18,120 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4840  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 15:39:18,135 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4840  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 15:39:18,283 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1539_real_step400_rank3.svg
[rank3]:> Batch Time: 621.61 ms, GPU Bubble Ratio: 59.32%, 57.12%, 66.26%, 26.61%
[rank3]:2025-11-09 15:39:27,027 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 15:39:27,275 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 15:39:27,249 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 15:39:27,300 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 15:41:25,200 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:41:27,758 - INFO - Avg. fwd time: 7.1565 / Avg. bwd time: 18.9659 / Avg. batch time: 548.4227 (ms) / GPU bubble ratio: 61.89%
[rank3]:2025-11-09 15:41:27,731 - INFO - Avg. fwd time: 11.4920 / Avg. bwd time: 45.6494 / Avg. batch time: 516.8176 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 15:41:27,789 - INFO - Avg. fwd time: 9.1122 / Avg. bwd time: 24.1591 / Avg. batch time: 587.2310 (ms) / GPU bubble ratio: 54.67%
[rank1]:2025-11-09 15:41:27,844 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.0506  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 15:41:27,820 - INFO - Avg. fwd time: 7.8841 / Avg. bwd time: 23.5198 / Avg. batch time: 623.6868 (ms) / GPU bubble ratio: 59.72%
[rank0]:2025-11-09 15:41:27,855 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.0506  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-09 15:41:27,841 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.0506  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 15:41:27,854 - INFO -  step: 450  loss:  0.5367  grad_norm:  1.0506  memory: 26.98GiB(56.79%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 15:43:35,079 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:43:37,399 - INFO - Avg. fwd time: 11.4914 / Avg. bwd time: 45.6607 / Avg. batch time: 516.9003 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 15:43:37,512 - INFO - Avg. fwd time: 9.1108 / Avg. bwd time: 24.1577 / Avg. batch time: 587.2714 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 15:43:37,519 - INFO - Avg. fwd time: 7.8822 / Avg. bwd time: 23.5206 / Avg. batch time: 623.7130 (ms) / GPU bubble ratio: 59.72%
[rank2]:2025-11-09 15:43:37,472 - INFO - Avg. fwd time: 7.1558 / Avg. bwd time: 18.9687 / Avg. batch time: 548.4816 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-09 15:43:37,694 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4907  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 15:43:37,705 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4907  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 15:43:37,691 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4907  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 15:43:37,703 - INFO -  step: 500  loss:  0.4098  grad_norm:  0.4907  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 15:43:37,852 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1543_real_step500_rank3.svg
[rank3]:> Batch Time: 623.58 ms, GPU Bubble Ratio: 59.45%, 57.23%, 66.35%, 26.59%
[rank0]:2025-11-09 15:45:44,750 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:45:47,040 - INFO - Avg. fwd time: 11.4897 / Avg. bwd time: 45.6543 / Avg. batch time: 516.8368 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 15:45:47,112 - INFO - Avg. fwd time: 7.1553 / Avg. bwd time: 18.9695 / Avg. batch time: 548.4378 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 15:45:47,152 - INFO - Avg. fwd time: 9.1095 / Avg. bwd time: 24.1523 / Avg. batch time: 587.2084 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 15:45:47,159 - INFO - Avg. fwd time: 7.8798 / Avg. bwd time: 23.5192 / Avg. batch time: 623.6371 (ms) / GPU bubble ratio: 59.72%
[rank2]:2025-11-09 15:45:47,328 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.8285  memory: 11.81GiB(24.85%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 15:45:47,340 - INFO -  step: 550  loss:  0.4494  grad_norm:  0.8285  memory: 26.98GiB(56.79%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-09 15:45:47,331 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.8285  memory: 14.64GiB(30.82%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 15:45:47,342 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.8285  memory: 16.57GiB(34.88%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 15:47:54,253 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:47:56,815 - INFO - Avg. fwd time: 7.1546 / Avg. bwd time: 18.9701 / Avg. batch time: 548.3286 (ms) / GPU bubble ratio: 61.88%
[rank3]:2025-11-09 15:47:56,787 - INFO - Avg. fwd time: 11.4868 / Avg. bwd time: 45.6477 / Avg. batch time: 516.7575 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 15:47:56,847 - INFO - Avg. fwd time: 9.1083 / Avg. bwd time: 24.1485 / Avg. batch time: 587.0851 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 15:47:56,878 - INFO - Avg. fwd time: 7.8780 / Avg. bwd time: 23.5186 / Avg. batch time: 623.5013 (ms) / GPU bubble ratio: 59.72%
[rank2]:2025-11-09 15:47:56,900 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4900  memory: 11.81GiB(24.85%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 15:47:56,912 - INFO -  step: 600  loss:  0.4516  grad_norm:  0.4900  memory: 26.98GiB(56.79%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank1]:2025-11-09 15:47:56,903 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4900  memory: 14.64GiB(30.82%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 15:47:56,914 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4900  memory: 16.57GiB(34.88%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 15:47:57,064 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1547_real_step600_rank3.svg
[rank3]:> Batch Time: 623.56 ms, GPU Bubble Ratio: 59.48%, 57.27%, 66.37%, 26.57%
[rank0]:2025-11-09 15:50:04,435 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:50:06,741 - INFO - Avg. fwd time: 11.4887 / Avg. bwd time: 45.6549 / Avg. batch time: 516.8286 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 15:50:06,854 - INFO - Avg. fwd time: 9.1089 / Avg. bwd time: 24.1469 / Avg. batch time: 587.1836 (ms) / GPU bubble ratio: 54.69%
[rank2]:2025-11-09 15:50:06,813 - INFO - Avg. fwd time: 7.1548 / Avg. bwd time: 18.9716 / Avg. batch time: 548.4304 (ms) / GPU bubble ratio: 61.89%
[rank0]:2025-11-09 15:50:06,861 - INFO - Avg. fwd time: 7.8785 / Avg. bwd time: 23.5191 / Avg. batch time: 623.5961 (ms) / GPU bubble ratio: 59.72%
[rank1]:2025-11-09 15:50:07,033 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4142  memory: 14.64GiB(30.82%)  tps: 6,295  tflops: 47.95  mfu: 15.37%
[rank2]:2025-11-09 15:50:07,029 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4142  memory: 11.81GiB(24.85%)  tps: 6,295  tflops: 47.95  mfu: 15.37%
[rank0]:2025-11-09 15:50:07,044 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4142  memory: 16.57GiB(34.88%)  tps: 6,295  tflops: 47.95  mfu: 15.37%
[rank3]:2025-11-09 15:50:07,042 - INFO -  step: 650  loss:  0.3826  grad_norm:  0.4142  memory: 26.98GiB(56.79%)  tps: 6,295  tflops: 47.95  mfu: 15.37%
[rank0]:2025-11-09 15:52:14,520 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:52:16,846 - INFO - Avg. fwd time: 11.4915 / Avg. bwd time: 45.6672 / Avg. batch time: 516.9515 (ms) / GPU bubble ratio: 11.54%
[rank2]:2025-11-09 15:52:16,923 - INFO - Avg. fwd time: 7.1550 / Avg. bwd time: 18.9734 / Avg. batch time: 548.5323 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 15:52:16,963 - INFO - Avg. fwd time: 9.1097 / Avg. bwd time: 24.1464 / Avg. batch time: 587.2873 (ms) / GPU bubble ratio: 54.70%
[rank0]:2025-11-09 15:52:16,970 - INFO - Avg. fwd time: 7.8789 / Avg. bwd time: 23.5193 / Avg. batch time: 623.6971 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-09 15:52:17,146 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4035  memory: 14.64GiB(30.82%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank2]:2025-11-09 15:52:17,142 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4035  memory: 11.81GiB(24.85%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank3]:2025-11-09 15:52:17,155 - INFO -  step: 700  loss:  0.4465  grad_norm:  0.4035  memory: 26.98GiB(56.79%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank0]:2025-11-09 15:52:17,157 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4035  memory: 16.57GiB(34.88%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank3]:2025-11-09 15:52:17,304 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1552_real_step700_rank3.svg
[rank3]:> Batch Time: 624.59 ms, GPU Bubble Ratio: 59.50%, 57.26%, 66.39%, 26.60%
[rank0]:2025-11-09 15:54:24,596 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:54:27,111 - INFO - Avg. fwd time: 11.4929 / Avg. bwd time: 45.6725 / Avg. batch time: 517.0051 (ms) / GPU bubble ratio: 11.54%
[rank2]:2025-11-09 15:54:27,140 - INFO - Avg. fwd time: 7.1552 / Avg. bwd time: 18.9754 / Avg. batch time: 548.6100 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-09 15:54:27,173 - INFO - Avg. fwd time: 9.1104 / Avg. bwd time: 24.1470 / Avg. batch time: 587.3692 (ms) / GPU bubble ratio: 54.70%
[rank0]:2025-11-09 15:54:27,209 - INFO - Avg. fwd time: 7.8790 / Avg. bwd time: 23.5195 / Avg. batch time: 623.7731 (ms) / GPU bubble ratio: 59.73%
[rank3]:2025-11-09 15:54:27,245 - INFO -  step: 750  loss:  0.4446  grad_norm:  0.6333  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-09 15:54:27,231 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.6333  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-09 15:54:27,235 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.6333  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 15:54:27,246 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.6333  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 15:56:34,120 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:56:36,446 - INFO - Avg. fwd time: 11.4905 / Avg. bwd time: 45.6656 / Avg. batch time: 516.9293 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 15:56:36,522 - INFO - Avg. fwd time: 7.1551 / Avg. bwd time: 18.9757 / Avg. batch time: 548.5155 (ms) / GPU bubble ratio: 61.89%
[rank0]:2025-11-09 15:56:36,569 - INFO - Avg. fwd time: 7.8780 / Avg. bwd time: 23.5182 / Avg. batch time: 623.6686 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-09 15:56:36,562 - INFO - Avg. fwd time: 9.1098 / Avg. bwd time: 24.1450 / Avg. batch time: 587.2733 (ms) / GPU bubble ratio: 54.70%
[rank2]:2025-11-09 15:56:36,741 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3897  memory: 11.81GiB(24.85%)  tps: 6,325  tflops: 48.18  mfu: 15.44%
[rank2]:2025-11-09 15:56:36,741 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3897  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank2]:2025-11-09 15:56:36,741 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 15:56:36,741 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 15:56:36,755 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3897  memory: 16.57GiB(34.88%)  tps: 6,325  tflops: 48.18  mfu: 15.44%
[rank0]:2025-11-09 15:56:36,755 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3897  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank0]:2025-11-09 15:56:36,755 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 15:56:36,756 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 15:56:36,744 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3897  memory: 14.64GiB(30.82%)  tps: 6,325  tflops: 48.18  mfu: 15.44%
[rank1]:2025-11-09 15:56:36,744 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3897  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank1]:2025-11-09 15:56:36,744 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 15:56:36,745 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 15:56:36,753 - INFO -  step: 800  loss:  0.4365  grad_norm:  0.3897  memory: 26.98GiB(56.79%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank3]:2025-11-09 15:56:36,754 - INFO -  final step: 800  loss:  0.4365  grad_norm:  0.3897  tps: 6,695  tflops: 50.99  mfu: 14.91%
[rank3]:2025-11-09 15:56:36,754 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 15:56:36,755 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 15:56:38,848 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 15:56:38,835 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 15:56:38,848 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-09 15:56:38,848 - INFO - Destroying the purge thread.
[rank3]:2025-11-09 15:56:38,990 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1556_real_final800_rank3.svg
[rank3]:> Batch Time: 623.11 ms, GPU Bubble Ratio: 59.42%, 57.18%, 66.33%, 26.55%
[rank3]:2025-11-09 15:56:39,127 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_1556_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.08 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 15:56:39,128 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 15:56:39,285 - INFO - Process group destroyed
[rank1]:2025-11-09 15:56:39,252 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44727
[rank3]:wandb:               final/avg_loss 0.43655
[rank3]:wandb:             final/avg_mfu(%) 14.90693
[rank3]:wandb:             final/avg_tflops 50.98897
[rank3]:wandb:    final/avg_throughput(tps) 6694.79322
[rank3]:wandb:              final/grad_norm 0.38965
[rank3]:wandb:               final/max_loss 0.43655
[rank3]:wandb:                    grad_norm 0.38965
[rank3]:wandb: loss_metrics/global_avg_loss 0.43655
[rank3]:wandb: loss_metrics/global_max_loss 0.43655
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed11_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/3a0ddpjf
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1/20251109-1521/wandb/run-20251109_152155-3a0ddpjf/logs
[rank3]:2025-11-09 15:56:40,276 - INFO - Process group destroyed
[rank0]:2025-11-09 15:56:40,848 - INFO - Training completed
[rank0]:2025-11-09 15:56:40,849 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 15:56:41,298 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.2', 'layers.0', 'tok_embeddings', 'layers.3'}
[rank2]:Stage 2: Modules to keep: {'layers.11', 'layers.12', 'layers.9', 'layers.10'}
[rank1]:Stage 1: Modules to keep: {'layers.6', 'layers.8', 'layers.5', 'layers.4', 'layers.7'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.15', 'layers.14', 'layers.13', 'output'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed11_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed11_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 11
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 20:56:16 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed11.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 20:56:22,483 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank1]:2025-11-09 20:56:22,492 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank2]:2025-11-09 20:56:22,582 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:"
[rank0]:2025-11-09 20:56:22,700 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 20:56:22,702 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:56:22,705 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 20:56:22,706 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-09 20:56:22,682 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 20:56:22,684 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 20:56:22,708 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank2]:2025-11-09 20:56:22,811 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 20:56:22,813 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 20:56:22,895 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 20:56:22,898 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:56:23,088 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 20:56:25,879 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-09 20:56:25,968 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 20:56:26,026 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 20:56:26,065 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:56:26,066 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank1]:2025-11-09 20:56:26,006 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:56:26,033 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 20:56:26,033 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:56:26,090 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 20:56:26,090 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 20:56:26,151 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 20:56:26,277 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:56:26,277 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 20:56:26,277 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 20:56:26,217 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:56:26,217 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 20:56:26,218 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-09 20:56:26,192 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:56:26,220 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 20:56:26,220 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 20:56:26,386 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:56:26,386 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 20:56:26,387 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 4s0mo3l7
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1/20251109-2056/wandb/run-20251109_205627-4s0mo3l7
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed11_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/4s0mo3l7
[rank3]:2025-11-09 20:56:28,204 - INFO - WandB logging enabled
[rank3]:2025-11-09 20:56:28,205 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 20:56:28,242 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:56:28,270 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 20:56:28,270 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:56:28,475 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1
[rank0]:2025-11-09 20:56:28,475 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 20:56:28,475 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 20:56:28,476 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank1]:2025-11-09 20:56:28,476 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1.
[rank0]:2025-11-09 20:56:28,476 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1.
[rank0]:2025-11-09 20:56:28,477 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1/step-800.
[rank2]:2025-11-09 20:56:28,476 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 20:56:28,476 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1.
[rank3]:2025-11-09 20:56:28,464 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:56:28,464 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 20:56:28,465 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 20:56:28,475 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 20:56:28,476 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1.
[rank0]:2025-11-09 20:56:30,805 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 20:56:30,805 - INFO - Finished loading the checkpoint in 2.33 seconds.
[rank0]:2025-11-09 20:56:30,805 - INFO - Training starts at step 1
[rank2]:2025-11-09 20:56:33,924 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.5194  memory:  9.99GiB(21.03%)  tps: 2,119  tflops: 16.14  mfu: 5.17%
[rank2]:2025-11-09 20:56:33,924 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 20:56:33,959 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.5194  memory: 12.80GiB(26.95%)  tps: 2,075  tflops: 15.81  mfu: 5.07%
[rank0]:2025-11-09 20:56:33,959 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 20:56:33,926 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.5194  memory: 12.38GiB(26.05%)  tps: 2,069  tflops: 15.76  mfu: 5.05%
[rank1]:2025-11-09 20:56:33,927 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 20:56:33,934 - INFO -  step:  1  loss:  0.4238  grad_norm:  0.5194  memory: 24.19GiB(50.91%)  tps: 2,880  tflops: 21.93  mfu: 7.03%
[rank3]:2025-11-09 20:56:33,935 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 20:58:37,402 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:58:39,702 - INFO - Avg. fwd time: 11.3671 / Avg. bwd time: 45.0478 / Avg. batch time: 511.2459 (ms) / GPU bubble ratio: 11.72%
[rank1]:2025-11-09 20:58:39,815 - INFO - Avg. fwd time: 9.0683 / Avg. bwd time: 23.9911 / Avg. batch time: 582.7387 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 20:58:39,822 - INFO - Avg. fwd time: 7.8777 / Avg. bwd time: 23.4484 / Avg. batch time: 619.7695 (ms) / GPU bubble ratio: 59.56%
[rank2]:2025-11-09 20:58:39,775 - INFO - Avg. fwd time: 7.1348 / Avg. bwd time: 18.9112 / Avg. batch time: 543.3221 (ms) / GPU bubble ratio: 61.65%
[rank1]:2025-11-09 20:58:40,003 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.0605  memory: 14.64GiB(30.82%)  tps: 6,368  tflops: 48.50  mfu: 15.54%
[rank0]:2025-11-09 20:58:40,014 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.0605  memory: 16.57GiB(34.88%)  tps: 6,369  tflops: 48.51  mfu: 15.55%
[rank2]:2025-11-09 20:58:39,999 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.0605  memory: 11.81GiB(24.85%)  tps: 6,368  tflops: 48.50  mfu: 15.54%
[rank3]:2025-11-09 20:58:40,011 - INFO -  step: 50  loss:  0.4268  grad_norm: 14.0605  memory: 26.98GiB(56.79%)  tps: 6,368  tflops: 48.50  mfu: 15.54%
[rank0]:2025-11-09 21:00:46,976 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:00:49,294 - INFO - Avg. fwd time: 11.4275 / Avg. bwd time: 45.3764 / Avg. batch time: 514.2148 (ms) / GPU bubble ratio: 11.63%
[rank2]:2025-11-09 21:00:49,369 - INFO - Avg. fwd time: 7.1466 / Avg. bwd time: 18.9589 / Avg. batch time: 545.8701 (ms) / GPU bubble ratio: 61.74%
[rank1]:2025-11-09 21:00:49,410 - INFO - Avg. fwd time: 9.0932 / Avg. bwd time: 24.0738 / Avg. batch time: 584.9587 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 21:00:49,417 - INFO - Avg. fwd time: 7.8854 / Avg. bwd time: 23.4916 / Avg. batch time: 621.6004 (ms) / GPU bubble ratio: 59.62%
[rank0]:2025-11-09 21:00:49,608 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.5418  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank2]:2025-11-09 21:00:49,594 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.5418  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank1]:2025-11-09 21:00:49,597 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.5418  memory: 14.64GiB(30.82%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank3]:2025-11-09 21:00:49,606 - INFO -  step: 100  loss:  0.4619  grad_norm:  0.5418  memory: 26.98GiB(56.79%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank3]:2025-11-09 21:00:49,783 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2100_real_step100_rank3.svg
[rank3]:> Batch Time: 622.62 ms, GPU Bubble Ratio: 59.36%, 57.12%, 66.29%, 26.42%
[rank0]:2025-11-09 21:02:56,851 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 21:02:59,368 - INFO - Avg. fwd time: 7.1510 / Avg. bwd time: 18.9752 / Avg. batch time: 547.0201 (ms) / GPU bubble ratio: 61.79%
[rank1]:2025-11-09 21:02:59,399 - INFO - Avg. fwd time: 9.1071 / Avg. bwd time: 24.1081 / Avg. batch time: 586.0567 (ms) / GPU bubble ratio: 54.66%
[rank3]:2025-11-09 21:02:59,343 - INFO - Avg. fwd time: 11.4499 / Avg. bwd time: 45.4916 / Avg. batch time: 515.2635 (ms) / GPU bubble ratio: 11.59%
[rank0]:2025-11-09 21:02:59,428 - INFO - Avg. fwd time: 7.8882 / Avg. bwd time: 23.5083 / Avg. batch time: 622.5869 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 21:02:59,464 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.9377  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 21:02:59,450 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.9377  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 21:02:59,453 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.9377  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 21:02:59,462 - INFO -  step: 150  loss:  0.4216  grad_norm:  0.9377  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:05:06,730 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:05:09,064 - INFO - Avg. fwd time: 11.4615 / Avg. bwd time: 45.5536 / Avg. batch time: 515.8316 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 21:05:09,139 - INFO - Avg. fwd time: 7.1540 / Avg. bwd time: 18.9858 / Avg. batch time: 547.4839 (ms) / GPU bubble ratio: 61.80%
[rank1]:2025-11-09 21:05:09,181 - INFO - Avg. fwd time: 9.1138 / Avg. bwd time: 24.1297 / Avg. batch time: 586.5156 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 21:05:09,188 - INFO - Avg. fwd time: 7.8882 / Avg. bwd time: 23.5169 / Avg. batch time: 622.9931 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 21:05:09,365 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.8895  memory: 11.81GiB(24.85%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank1]:2025-11-09 21:05:09,368 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.8895  memory: 14.64GiB(30.82%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank3]:2025-11-09 21:05:09,377 - INFO -  step: 200  loss:  0.4811  grad_norm:  0.8895  memory: 26.98GiB(56.79%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank0]:2025-11-09 21:05:09,380 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.8895  memory: 16.57GiB(34.88%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank3]:2025-11-09 21:05:09,529 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2105_real_step200_rank3.svg
[rank3]:> Batch Time: 623.11 ms, GPU Bubble Ratio: 59.40%, 57.11%, 66.28%, 26.64%
[rank0]:2025-11-09 21:07:16,624 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:07:18,921 - INFO - Avg. fwd time: 11.4687 / Avg. bwd time: 45.5744 / Avg. batch time: 516.0449 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-09 21:07:19,041 - INFO - Avg. fwd time: 7.8870 / Avg. bwd time: 23.5184 / Avg. batch time: 623.1986 (ms) / GPU bubble ratio: 59.68%
[rank2]:2025-11-09 21:07:18,991 - INFO - Avg. fwd time: 7.1560 / Avg. bwd time: 18.9892 / Avg. batch time: 547.7381 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 21:07:19,033 - INFO - Avg. fwd time: 9.1180 / Avg. bwd time: 24.1414 / Avg. batch time: 586.7620 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 21:07:19,230 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.5194  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 21:07:19,216 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.5194  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 21:07:19,220 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.5194  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 21:07:19,228 - INFO -  step: 250  loss:  0.5090  grad_norm:  0.5194  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:09:25,927 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 21:09:28,483 - INFO - Avg. fwd time: 7.1553 / Avg. bwd time: 18.9901 / Avg. batch time: 547.4509 (ms) / GPU bubble ratio: 61.79%
[rank1]:2025-11-09 21:09:28,513 - INFO - Avg. fwd time: 9.1179 / Avg. bwd time: 24.1476 / Avg. batch time: 586.4731 (ms) / GPU bubble ratio: 54.62%
[rank3]:2025-11-09 21:09:28,457 - INFO - Avg. fwd time: 11.4629 / Avg. bwd time: 45.5548 / Avg. batch time: 515.8301 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-09 21:09:28,543 - INFO - Avg. fwd time: 7.8864 / Avg. bwd time: 23.5189 / Avg. batch time: 622.8800 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 21:09:28,579 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.0160  memory: 16.57GiB(34.88%)  tps: 6,333  tflops: 48.24  mfu: 15.46%
[rank2]:2025-11-09 21:09:28,565 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.0160  memory: 11.81GiB(24.85%)  tps: 6,333  tflops: 48.24  mfu: 15.46%
[rank1]:2025-11-09 21:09:28,569 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.0160  memory: 14.64GiB(30.82%)  tps: 6,333  tflops: 48.24  mfu: 15.46%
[rank3]:2025-11-09 21:09:28,577 - INFO -  step: 300  loss:  0.4827  grad_norm:  1.0160  memory: 26.98GiB(56.79%)  tps: 6,333  tflops: 48.24  mfu: 15.46%
[rank3]:2025-11-09 21:09:28,729 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2109_real_step300_rank3.svg
[rank3]:> Batch Time: 621.12 ms, GPU Bubble Ratio: 59.26%, 56.99%, 66.20%, 26.53%
[rank0]:2025-11-09 21:11:35,849 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:11:38,149 - INFO - Avg. fwd time: 11.4665 / Avg. bwd time: 45.5684 / Avg. batch time: 515.9639 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-09 21:11:38,263 - INFO - Avg. fwd time: 9.1173 / Avg. bwd time: 24.1561 / Avg. batch time: 586.6514 (ms) / GPU bubble ratio: 54.63%
[rank0]:2025-11-09 21:11:38,271 - INFO - Avg. fwd time: 7.8853 / Avg. bwd time: 23.5202 / Avg. batch time: 623.0443 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 21:11:38,223 - INFO - Avg. fwd time: 7.1550 / Avg. bwd time: 18.9921 / Avg. batch time: 547.6256 (ms) / GPU bubble ratio: 61.80%
[rank1]:2025-11-09 21:11:38,449 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6486  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 21:11:38,460 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6486  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank2]:2025-11-09 21:11:38,445 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6486  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-09 21:11:38,458 - INFO -  step: 350  loss:  0.4711  grad_norm:  0.6486  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 21:13:45,654 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:13:47,973 - INFO - Avg. fwd time: 11.4683 / Avg. bwd time: 45.5834 / Avg. batch time: 516.0937 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 21:13:48,050 - INFO - Avg. fwd time: 7.1552 / Avg. bwd time: 18.9948 / Avg. batch time: 547.7036 (ms) / GPU bubble ratio: 61.80%
[rank1]:2025-11-09 21:13:48,090 - INFO - Avg. fwd time: 9.1186 / Avg. bwd time: 24.1657 / Avg. batch time: 586.7380 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 21:13:48,097 - INFO - Avg. fwd time: 7.8835 / Avg. bwd time: 23.5223 / Avg. batch time: 623.1152 (ms) / GPU bubble ratio: 59.68%
[rank2]:2025-11-09 21:13:48,272 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3894  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 21:13:48,285 - INFO -  step: 400  loss:  0.4531  grad_norm:  0.3894  memory: 26.98GiB(56.79%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank1]:2025-11-09 21:13:48,276 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3894  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 21:13:48,287 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3894  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 21:13:48,437 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2113_real_step400_rank3.svg
[rank3]:> Batch Time: 623.61 ms, GPU Bubble Ratio: 59.39%, 57.12%, 66.32%, 26.48%
[rank3]:2025-11-09 21:13:57,198 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 21:13:57,417 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 21:13:57,470 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 21:13:57,444 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 21:15:55,492 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 21:15:58,050 - INFO - Avg. fwd time: 7.1552 / Avg. bwd time: 18.9976 / Avg. batch time: 547.8485 (ms) / GPU bubble ratio: 61.81%
[rank3]:2025-11-09 21:15:58,023 - INFO - Avg. fwd time: 11.4683 / Avg. bwd time: 45.5981 / Avg. batch time: 516.2078 (ms) / GPU bubble ratio: 11.56%
[rank1]:2025-11-09 21:15:58,081 - INFO - Avg. fwd time: 9.1188 / Avg. bwd time: 24.1749 / Avg. batch time: 586.8903 (ms) / GPU bubble ratio: 54.62%
[rank2]:2025-11-09 21:15:58,134 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5201  memory: 11.81GiB(24.85%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:15:58,112 - INFO - Avg. fwd time: 7.8818 / Avg. bwd time: 23.5242 / Avg. batch time: 623.2597 (ms) / GPU bubble ratio: 59.69%
[rank0]:2025-11-09 21:15:58,148 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5201  memory: 16.57GiB(34.88%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 21:15:58,147 - INFO -  step: 450  loss:  0.5083  grad_norm:  0.5201  memory: 26.98GiB(56.79%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 21:15:58,137 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5201  memory: 14.64GiB(30.82%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:18:05,594 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:18:07,921 - INFO - Avg. fwd time: 11.4731 / Avg. bwd time: 45.6231 / Avg. batch time: 516.4427 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 21:18:07,998 - INFO - Avg. fwd time: 7.1556 / Avg. bwd time: 18.9996 / Avg. batch time: 548.0514 (ms) / GPU bubble ratio: 61.82%
[rank0]:2025-11-09 21:18:08,041 - INFO - Avg. fwd time: 7.8812 / Avg. bwd time: 23.5248 / Avg. batch time: 623.4453 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-09 21:18:08,038 - INFO - Avg. fwd time: 9.1190 / Avg. bwd time: 24.1814 / Avg. batch time: 587.0849 (ms) / GPU bubble ratio: 54.62%
[rank2]:2025-11-09 21:18:08,218 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4250  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 21:18:08,233 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4250  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 21:18:08,230 - INFO -  step: 500  loss:  0.3685  grad_norm:  0.4250  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-09 21:18:08,222 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4250  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 21:18:08,383 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2118_real_step500_rank3.svg
[rank3]:> Batch Time: 624.09 ms, GPU Bubble Ratio: 59.45%, 57.14%, 66.35%, 26.23%
[rank0]:2025-11-09 21:20:15,670 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:20:17,974 - INFO - Avg. fwd time: 11.4759 / Avg. bwd time: 45.6385 / Avg. batch time: 516.5852 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 21:20:18,047 - INFO - Avg. fwd time: 7.1563 / Avg. bwd time: 19.0006 / Avg. batch time: 548.2186 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 21:20:18,094 - INFO - Avg. fwd time: 7.8817 / Avg. bwd time: 23.5243 / Avg. batch time: 623.5959 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-09 21:20:18,087 - INFO - Avg. fwd time: 9.1198 / Avg. bwd time: 24.1856 / Avg. batch time: 587.2427 (ms) / GPU bubble ratio: 54.63%
[rank1]:2025-11-09 21:20:18,268 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4533  memory: 14.64GiB(30.82%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank2]:2025-11-09 21:20:18,264 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4533  memory: 11.81GiB(24.85%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-09 21:20:18,279 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4533  memory: 16.57GiB(34.88%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-09 21:20:18,277 - INFO -  step: 550  loss:  0.4204  grad_norm:  0.4533  memory: 26.98GiB(56.79%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-09 21:22:25,085 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 21:22:27,644 - INFO - Avg. fwd time: 7.1559 / Avg. bwd time: 18.9994 / Avg. batch time: 548.0976 (ms) / GPU bubble ratio: 61.82%
[rank3]:2025-11-09 21:22:27,616 - INFO - Avg. fwd time: 11.4736 / Avg. bwd time: 45.6300 / Avg. batch time: 516.4977 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 21:22:27,676 - INFO - Avg. fwd time: 9.1189 / Avg. bwd time: 24.1854 / Avg. batch time: 587.1092 (ms) / GPU bubble ratio: 54.62%
[rank1]:2025-11-09 21:22:27,733 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3907  memory: 14.64GiB(30.82%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank2]:2025-11-09 21:22:27,730 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3907  memory: 11.81GiB(24.85%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank0]:2025-11-09 21:22:27,708 - INFO - Avg. fwd time: 7.8807 / Avg. bwd time: 23.5220 / Avg. batch time: 623.4533 (ms) / GPU bubble ratio: 59.70%
[rank0]:2025-11-09 21:22:27,744 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3907  memory: 16.57GiB(34.88%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 21:22:27,743 - INFO -  step: 600  loss:  0.4236  grad_norm:  0.3907  memory: 26.98GiB(56.79%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 21:22:27,891 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2122_real_step600_rank3.svg
[rank3]:> Batch Time: 621.56 ms, GPU Bubble Ratio: 59.32%, 57.08%, 66.24%, 26.54%
[rank0]:2025-11-09 21:24:35,012 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:24:37,300 - INFO - Avg. fwd time: 11.4748 / Avg. bwd time: 45.6333 / Avg. batch time: 516.5320 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 21:24:37,413 - INFO - Avg. fwd time: 9.1184 / Avg. bwd time: 24.1855 / Avg. batch time: 587.1510 (ms) / GPU bubble ratio: 54.62%
[rank2]:2025-11-09 21:24:37,373 - INFO - Avg. fwd time: 7.1559 / Avg. bwd time: 18.9991 / Avg. batch time: 548.1519 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 21:24:37,421 - INFO - Avg. fwd time: 7.8808 / Avg. bwd time: 23.5214 / Avg. batch time: 623.4898 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-09 21:24:37,594 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3903  memory: 14.64GiB(30.82%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 21:24:37,590 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3903  memory: 11.81GiB(24.85%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:24:37,605 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3903  memory: 16.57GiB(34.88%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 21:24:37,603 - INFO -  step: 650  loss:  0.3771  grad_norm:  0.3903  memory: 26.98GiB(56.79%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 21:26:44,899 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:26:47,237 - INFO - Avg. fwd time: 11.4764 / Avg. bwd time: 45.6417 / Avg. batch time: 516.6115 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 21:26:47,311 - INFO - Avg. fwd time: 7.1561 / Avg. bwd time: 18.9997 / Avg. batch time: 548.2095 (ms) / GPU bubble ratio: 61.83%
[rank1]:2025-11-09 21:26:47,351 - INFO - Avg. fwd time: 9.1178 / Avg. bwd time: 24.1862 / Avg. batch time: 587.1914 (ms) / GPU bubble ratio: 54.63%
[rank0]:2025-11-09 21:26:47,359 - INFO - Avg. fwd time: 7.8811 / Avg. bwd time: 23.5205 / Avg. batch time: 623.5285 (ms) / GPU bubble ratio: 59.71%
[rank3]:2025-11-09 21:26:47,546 - INFO -  step: 700  loss:  0.4268  grad_norm:  0.4165  memory: 26.98GiB(56.79%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank1]:2025-11-09 21:26:47,537 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4165  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank2]:2025-11-09 21:26:47,534 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4165  memory: 11.81GiB(24.85%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-09 21:26:47,549 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4165  memory: 16.57GiB(34.88%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-09 21:26:47,700 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2126_real_step700_rank3.svg
[rank3]:> Batch Time: 624.12 ms, GPU Bubble Ratio: 59.48%, 57.19%, 66.35%, 26.68%
[rank0]:2025-11-09 21:28:55,222 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 21:28:57,765 - INFO - Avg. fwd time: 7.1565 / Avg. bwd time: 19.0009 / Avg. batch time: 548.4013 (ms) / GPU bubble ratio: 61.84%
[rank3]:2025-11-09 21:28:57,736 - INFO - Avg. fwd time: 11.4803 / Avg. bwd time: 45.6590 / Avg. batch time: 516.7807 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-09 21:28:57,798 - INFO - Avg. fwd time: 9.1180 / Avg. bwd time: 24.1867 / Avg. batch time: 587.3701 (ms) / GPU bubble ratio: 54.64%
[rank2]:2025-11-09 21:28:57,851 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.5006  memory: 11.81GiB(24.85%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-09 21:28:57,830 - INFO - Avg. fwd time: 7.8822 / Avg. bwd time: 23.5209 / Avg. batch time: 623.7065 (ms) / GPU bubble ratio: 59.72%
[rank0]:2025-11-09 21:28:57,866 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.5006  memory: 16.57GiB(34.88%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-09 21:28:57,865 - INFO -  step: 750  loss:  0.4178  grad_norm:  0.5006  memory: 26.98GiB(56.79%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank1]:2025-11-09 21:28:57,855 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.5006  memory: 14.64GiB(30.82%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-09 21:31:05,520 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 21:31:07,867 - INFO - Avg. fwd time: 11.4839 / Avg. bwd time: 45.6730 / Avg. batch time: 516.9218 (ms) / GPU bubble ratio: 11.54%
[rank2]:2025-11-09 21:31:07,942 - INFO - Avg. fwd time: 7.1569 / Avg. bwd time: 19.0020 / Avg. batch time: 548.5285 (ms) / GPU bubble ratio: 61.85%
[rank0]:2025-11-09 21:31:07,989 - INFO - Avg. fwd time: 7.8832 / Avg. bwd time: 23.5213 / Avg. batch time: 623.8239 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-09 21:31:07,982 - INFO - Avg. fwd time: 9.1184 / Avg. bwd time: 24.1864 / Avg. batch time: 587.4827 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 21:31:08,163 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4097  memory: 11.81GiB(24.85%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank2]:2025-11-09 21:31:08,163 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4097  tps: 6,689  tflops: 50.95  mfu: 14.80%
[rank2]:2025-11-09 21:31:08,163 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 21:31:08,164 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank2]:  warnings.warn(
[rank0]:2025-11-09 21:31:08,177 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4097  memory: 16.57GiB(34.88%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-09 21:31:08,177 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4097  tps: 6,689  tflops: 50.94  mfu: 14.80%
[rank0]:2025-11-09 21:31:08,177 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 21:31:08,178 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank0]:  warnings.warn(
[rank3]:2025-11-09 21:31:08,175 - INFO -  step: 800  loss:  0.4189  grad_norm:  0.4097  memory: 26.98GiB(56.79%)  tps: 6,287  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-09 21:31:08,176 - INFO -  final step: 800  loss:  0.4189  grad_norm:  0.4097  tps: 6,696  tflops: 51.00  mfu: 14.91%
[rank3]:2025-11-09 21:31:08,176 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 21:31:08,177 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank3]:  warnings.warn(
[rank1]:2025-11-09 21:31:08,167 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4097  memory: 14.64GiB(30.82%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank1]:2025-11-09 21:31:08,167 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4097  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank1]:2025-11-09 21:31:08,167 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 21:31:08,168 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank1]:  warnings.warn(
[rank0]:2025-11-09 21:31:10,277 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 21:31:10,289 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-09 21:31:10,289 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 21:31:10,289 - INFO - Destroying the purge thread.
[rank3]:2025-11-09 21:31:10,432 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2131_real_final800_rank3.svg
[rank3]:> Batch Time: 623.11 ms, GPU Bubble Ratio: 59.39%, 57.14%, 66.30%, 26.54%
[rank2]:2025-11-09 21:31:10,539 - INFO - Process group destroyed
[rank3]:2025-11-09 21:31:10,571 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1/pipeline_schedule/251109_2131_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.22 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 21:31:10,572 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 21:31:10,531 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 228-239
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–„â–„â–†â–„â–‡â–ˆâ–‡â–†â–…â–ˆâ–â–„â–„â–â–„â–ƒâ–„
[rank3]:wandb: loss_metrics/global_max_loss â–„â–„â–†â–„â–‡â–ˆâ–‡â–†â–…â–ˆâ–â–„â–„â–â–„â–ƒâ–„
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44693
[rank3]:wandb:               final/avg_loss 0.4189
[rank3]:wandb:             final/avg_mfu(%) 14.91392
[rank3]:wandb:             final/avg_tflops 50.99606
[rank3]:wandb:    final/avg_throughput(tps) 6695.72431
[rank3]:wandb:              final/grad_norm 0.40968
[rank3]:wandb:               final/max_loss 0.4189
[rank3]:wandb:                    grad_norm 0.40968
[rank3]:wandb: loss_metrics/global_avg_loss 0.4189
[rank3]:wandb: loss_metrics/global_max_loss 0.4189
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed11_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/4s0mo3l7
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1/20251109-2056/wandb/run-20251109_205627-4s0mo3l7/logs
[rank3]:2025-11-09 21:31:12,014 - INFO - Process group destroyed
[rank0]:2025-11-09 21:31:12,289 - INFO - Training completed
[rank0]:2025-11-09 21:31:12,290 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 21:31:12,519 - INFO - Process group destroyed
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.8', 'layers.6', 'layers.5', 'layers.7'}
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.0', 'layers.1', 'layers.2', 'tok_embeddings'}
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.10', 'layers.9', 'layers.11'}
[rank3]:Stage 3: Modules to keep: {'layers.15', 'layers.13', 'layers.14', 'output', 'norm'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed11_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed11_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.1
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 11
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed11_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
