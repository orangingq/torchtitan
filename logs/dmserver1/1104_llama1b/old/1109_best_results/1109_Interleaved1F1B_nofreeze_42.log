
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 11. (í™”) 00:24:02 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1109_Interleaved1F1B_nofreeze_42.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
1109: switch to alpaca_gpt4+alpaca_cleaned dataset

âœ”ï¸Running with nofreeze x Interleaved1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1109.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
1109: switch to alpaca_gpt4+alpaca_cleaned dataset
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank3]:2025-11-11 00:24:08,616 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank3]:"
[rank1]:2025-11-11 00:24:08,618 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank1]:"
[rank0]:2025-11-11 00:24:08,859 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank0]:"
[rank3]:2025-11-11 00:24:08,893 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-11 00:24:08,839 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank2]:"
[rank1]:2025-11-11 00:24:08,845 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-11 00:24:08,848 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-11 00:24:08,895 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-11 00:24:09,071 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-11 00:24:09,073 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-11 00:24:09,076 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-11 00:24:09,078 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-11 00:24:09,050 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-11 00:24:09,052 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-11 00:24:09,480 - INFO - Preparing alpaca_gpt4 dataset from vicgalle/alpaca-gpt4
[rank0]:2025-11-11 00:24:12,156 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-11 00:24:12,155 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-11 00:24:12,179 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-11 00:24:12,192 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
[rank2]:2025-11-11 00:24:12,261 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-11 00:24:12,203 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
[rank1]:2025-11-11 00:24:12,205 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-11 00:24:12,313 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-11 00:24:12,350 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-11 00:24:12,352 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-11 00:24:12,377 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:2025-11-11 00:24:12,390 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:2025-11-11 00:24:12,391 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-11 00:24:12,310 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-11 00:24:12,338 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
[rank2]:2025-11-11 00:24:12,351 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
[rank2]:2025-11-11 00:24:12,352 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-11-11 00:24:12,388 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-11 00:24:12,388 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-11 00:24:12,388 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-11 00:24:12,542 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-11 00:24:12,542 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-11 00:24:12,543 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-11 00:24:12,603 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-11 00:24:12,603 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-11 00:24:12,604 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 5iknne6l
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1109_Interleaved1F1B_nofreeze_42_dm1/20251111-0024/wandb/run-20251111_002415-5iknne6l
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/5iknne6l
[rank3]:2025-11-11 00:24:19,127 - INFO - WandB logging enabled
[rank3]:2025-11-11 00:24:19,128 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-11 00:24:19,167 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-11 00:24:19,198 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:2025-11-11 00:24:19,212 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:2025-11-11 00:24:19,214 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-11 00:24:19,427 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-11 00:24:19,427 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1109_Interleaved1F1B_nofreeze_42_dm1
[rank0]:2025-11-11 00:24:19,428 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-11 00:24:19,428 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 1024, total steps 800 (warmup 100)
[rank0]:2025-11-11 00:24:19,428 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-11 00:24:19,409 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-11 00:24:19,410 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-11 00:24:19,411 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-11 00:24:19,427 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-11 00:24:19,428 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-11 00:24:21,974 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-11 00:24:21,974 - INFO - Finished loading the checkpoint in 2.55 seconds.
[rank0]:2025-11-11 00:24:21,974 - INFO - Training starts at step 1
[rank2]:2025-11-11 00:24:27,542 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0667  memory: 10.24GiB(21.56%)  tps: 2,151  tflops: 16.82  mfu: 5.39%
[rank2]:2025-11-11 00:24:27,542 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-11 00:24:27,552 - INFO -  step:  1  loss: 10.6092  grad_norm: 203.0667  memory: 25.52GiB(53.71%)  tps: 3,909  tflops: 30.56  mfu: 9.79%
[rank3]:2025-11-11 00:24:27,552 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-11 00:24:27,547 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0667  memory: 16.17GiB(34.04%)  tps: 2,132  tflops: 16.67  mfu: 5.34%
[rank1]:2025-11-11 00:24:27,547 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-11 00:24:27,577 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0667  memory: 16.58GiB(34.89%)  tps: 2,152  tflops: 16.82  mfu: 5.39%
[rank0]:2025-11-11 00:24:27,577 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-11 00:28:32,412 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:28:36,892 - INFO - Avg. fwd time: 14.2561 / Avg. bwd time: 40.1132 / Avg. batch time: 1086.9391 (ms) / GPU bubble ratio: 19.97%
[rank2]:2025-11-11 00:28:36,997 - INFO - Avg. fwd time: 7.7684 / Avg. bwd time: 18.4619 / Avg. batch time: 1147.9949 (ms) / GPU bubble ratio: 63.44%
[rank0]:2025-11-11 00:28:37,123 - INFO - Avg. fwd time: 8.2394 / Avg. bwd time: 20.6773 / Avg. batch time: 1245.5177 (ms) / GPU bubble ratio: 62.85%
[rank1]:2025-11-11 00:28:37,189 - INFO - Avg. fwd time: 9.9212 / Avg. bwd time: 23.8609 / Avg. batch time: 1207.0292 (ms) / GPU bubble ratio: 55.22%
[rank0]:2025-11-11 00:28:37,496 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9559  memory: 20.42GiB(42.99%)  tps: 6,425  tflops: 50.22  mfu: 16.10%
[rank3]:2025-11-11 00:28:37,492 - INFO -  step: 50  loss:  9.8813  grad_norm: 23.9559  memory: 29.23GiB(61.52%)  tps: 6,424  tflops: 50.22  mfu: 16.10%
[rank1]:2025-11-11 00:28:37,483 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9559  memory: 18.44GiB(38.81%)  tps: 6,424  tflops: 50.22  mfu: 16.10%
[rank2]:2025-11-11 00:28:37,480 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9559  memory: 12.06GiB(25.39%)  tps: 6,424  tflops: 50.22  mfu: 16.10%
[rank0]:2025-11-11 00:32:47,632 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:32:52,110 - INFO - Avg. fwd time: 14.3413 / Avg. bwd time: 40.2120 / Avg. batch time: 1089.7013 (ms) / GPU bubble ratio: 19.90%
[rank2]:2025-11-11 00:32:52,216 - INFO - Avg. fwd time: 7.7625 / Avg. bwd time: 18.5009 / Avg. batch time: 1150.6078 (ms) / GPU bubble ratio: 63.48%
[rank0]:2025-11-11 00:32:52,342 - INFO - Avg. fwd time: 8.2278 / Avg. bwd time: 20.7120 / Avg. batch time: 1247.9926 (ms) / GPU bubble ratio: 62.90%
[rank1]:2025-11-11 00:32:52,411 - INFO - Avg. fwd time: 9.9063 / Avg. bwd time: 23.8594 / Avg. batch time: 1209.5105 (ms) / GPU bubble ratio: 55.33%
[rank2]:2025-11-11 00:32:52,702 - INFO -  step: 100  loss: -4.0000  grad_norm: 16.9713  memory: 12.06GiB(25.39%)  tps: 6,420  tflops: 50.18  mfu: 16.08%
[rank1]:2025-11-11 00:32:52,706 - INFO -  step: 100  loss: -4.0000  grad_norm: 16.9713  memory: 18.44GiB(38.81%)  tps: 6,420  tflops: 50.18  mfu: 16.08%
[rank0]:2025-11-11 00:32:52,718 - INFO -  step: 100  loss: -4.0000  grad_norm: 16.9713  memory: 20.42GiB(42.99%)  tps: 6,420  tflops: 50.18  mfu: 16.08%
[rank3]:2025-11-11 00:32:52,714 - INFO -  step: 100  loss:  8.2263  grad_norm: 16.9713  memory: 29.23GiB(61.52%)  tps: 6,420  tflops: 50.19  mfu: 16.08%
[rank3]:2025-11-11 00:32:53,017 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0032_real_step100_rank3.svg
[rank3]:> Batch Time: 1245.05 ms, GPU Bubble Ratio: 62.69%, 56.57%, 66.15%, 29.39%
[rank0]:2025-11-11 00:37:03,532 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:37:08,541 - INFO - Avg. fwd time: 14.3269 / Avg. bwd time: 40.2843 / Avg. batch time: 1090.5598 (ms) / GPU bubble ratio: 19.88%
[rank1]:2025-11-11 00:37:08,637 - INFO - Avg. fwd time: 9.9120 / Avg. bwd time: 23.9138 / Avg. batch time: 1210.8576 (ms) / GPU bubble ratio: 55.30%
[rank2]:2025-11-11 00:37:08,591 - INFO - Avg. fwd time: 7.7576 / Avg. bwd time: 18.5297 / Avg. batch time: 1151.8559 (ms) / GPU bubble ratio: 63.49%
[rank0]:2025-11-11 00:37:08,671 - INFO - Avg. fwd time: 8.2243 / Avg. bwd time: 20.7301 / Avg. batch time: 1249.3493 (ms) / GPU bubble ratio: 62.92%
[rank1]:2025-11-11 00:37:08,697 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.0258  memory: 18.44GiB(38.81%)  tps: 6,400  tflops: 50.03  mfu: 16.04%
[rank2]:2025-11-11 00:37:08,693 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.0258  memory: 12.06GiB(25.39%)  tps: 6,400  tflops: 50.03  mfu: 16.04%
[rank0]:2025-11-11 00:37:08,708 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.0258  memory: 20.42GiB(42.99%)  tps: 6,400  tflops: 50.03  mfu: 16.04%
[rank3]:2025-11-11 00:37:08,705 - INFO -  step: 150  loss:  3.3342  grad_norm: 24.0258  memory: 29.23GiB(61.52%)  tps: 6,400  tflops: 50.03  mfu: 16.04%
[rank0]:2025-11-11 00:41:19,013 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:41:23,577 - INFO - Avg. fwd time: 14.3447 / Avg. bwd time: 40.2729 / Avg. batch time: 1090.6008 (ms) / GPU bubble ratio: 19.87%
[rank2]:2025-11-11 00:41:23,684 - INFO - Avg. fwd time: 7.7532 / Avg. bwd time: 18.5504 / Avg. batch time: 1151.8510 (ms) / GPU bubble ratio: 63.46%
[rank0]:2025-11-11 00:41:23,812 - INFO - Avg. fwd time: 8.2213 / Avg. bwd time: 20.7497 / Avg. batch time: 1249.4903 (ms) / GPU bubble ratio: 62.90%
[rank1]:2025-11-11 00:41:23,882 - INFO - Avg. fwd time: 9.9062 / Avg. bwd time: 23.9421 / Avg. batch time: 1210.9858 (ms) / GPU bubble ratio: 55.28%
[rank2]:2025-11-11 00:41:24,170 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.6325  memory: 12.06GiB(25.39%)  tps: 6,413  tflops: 50.13  mfu: 16.07%
[rank0]:2025-11-11 00:41:24,184 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.6325  memory: 20.42GiB(42.99%)  tps: 6,413  tflops: 50.13  mfu: 16.07%
[rank3]:2025-11-11 00:41:24,182 - INFO -  step: 200  loss:  1.0374  grad_norm: 15.6325  memory: 29.23GiB(61.52%)  tps: 6,413  tflops: 50.14  mfu: 16.07%
[rank1]:2025-11-11 00:41:24,173 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.6325  memory: 18.44GiB(38.81%)  tps: 6,413  tflops: 50.13  mfu: 16.07%
[rank3]:2025-11-11 00:41:24,441 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0041_real_step200_rank3.svg
[rank3]:> Batch Time: 1249.12 ms, GPU Bubble Ratio: 62.72%, 56.39%, 66.19%, 30.06%
[rank0]:2025-11-11 00:45:35,011 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:45:39,526 - INFO - Avg. fwd time: 14.3626 / Avg. bwd time: 40.2729 / Avg. batch time: 1090.8448 (ms) / GPU bubble ratio: 19.86%
[rank2]:2025-11-11 00:45:39,632 - INFO - Avg. fwd time: 7.7497 / Avg. bwd time: 18.5732 / Avg. batch time: 1152.2862 (ms) / GPU bubble ratio: 63.45%
[rank0]:2025-11-11 00:45:39,759 - INFO - Avg. fwd time: 8.2184 / Avg. bwd time: 20.7667 / Avg. batch time: 1249.9647 (ms) / GPU bubble ratio: 62.90%
[rank1]:2025-11-11 00:45:39,829 - INFO - Avg. fwd time: 9.9008 / Avg. bwd time: 23.9605 / Avg. batch time: 1211.4490 (ms) / GPU bubble ratio: 55.28%
[rank1]:2025-11-11 00:45:40,122 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8006  memory: 18.44GiB(38.81%)  tps: 6,401  tflops: 50.04  mfu: 16.04%
[rank2]:2025-11-11 00:45:40,119 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8006  memory: 12.06GiB(25.39%)  tps: 6,401  tflops: 50.04  mfu: 16.04%
[rank3]:2025-11-11 00:45:40,131 - INFO -  step: 250  loss:  0.3681  grad_norm:  3.8006  memory: 29.23GiB(61.52%)  tps: 6,401  tflops: 50.04  mfu: 16.04%
[rank0]:2025-11-11 00:45:40,135 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8006  memory: 20.42GiB(42.99%)  tps: 6,401  tflops: 50.04  mfu: 16.04%
[rank0]:2025-11-11 00:49:50,952 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:49:55,892 - INFO - Avg. fwd time: 14.3636 / Avg. bwd time: 40.2972 / Avg. batch time: 1091.2268 (ms) / GPU bubble ratio: 19.85%
[rank2]:2025-11-11 00:49:55,942 - INFO - Avg. fwd time: 7.7484 / Avg. bwd time: 18.5921 / Avg. batch time: 1152.5886 (ms) / GPU bubble ratio: 63.43%
[rank0]:2025-11-11 00:49:56,022 - INFO - Avg. fwd time: 8.2196 / Avg. bwd time: 20.7846 / Avg. batch time: 1250.3603 (ms) / GPU bubble ratio: 62.89%
[rank1]:2025-11-11 00:49:55,986 - INFO - Avg. fwd time: 9.9021 / Avg. bwd time: 23.9939 / Avg. batch time: 1211.8336 (ms) / GPU bubble ratio: 55.25%
[rank2]:2025-11-11 00:49:56,044 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.9812  memory: 12.06GiB(25.39%)  tps: 6,402  tflops: 50.05  mfu: 16.04%
[rank3]:2025-11-11 00:49:56,056 - INFO -  step: 300  loss:  0.2725  grad_norm:  0.9812  memory: 29.23GiB(61.52%)  tps: 6,402  tflops: 50.05  mfu: 16.04%
[rank0]:2025-11-11 00:49:56,058 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.9812  memory: 20.42GiB(42.99%)  tps: 6,402  tflops: 50.05  mfu: 16.04%
[rank1]:2025-11-11 00:49:56,047 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.9812  memory: 18.44GiB(38.81%)  tps: 6,402  tflops: 50.05  mfu: 16.04%
[rank3]:2025-11-11 00:49:56,320 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0049_real_step300_rank3.svg
[rank3]:> Batch Time: 1246.12 ms, GPU Bubble Ratio: 62.64%, 56.23%, 66.12%, 29.95%
[rank0]:2025-11-11 00:54:06,031 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:54:10,563 - INFO - Avg. fwd time: 14.3490 / Avg. bwd time: 40.2883 / Avg. batch time: 1090.8404 (ms) / GPU bubble ratio: 19.86%
[rank2]:2025-11-11 00:54:10,671 - INFO - Avg. fwd time: 7.7472 / Avg. bwd time: 18.5989 / Avg. batch time: 1152.3563 (ms) / GPU bubble ratio: 63.42%
[rank0]:2025-11-11 00:54:10,798 - INFO - Avg. fwd time: 8.2188 / Avg. bwd time: 20.7913 / Avg. batch time: 1250.1019 (ms) / GPU bubble ratio: 62.87%
[rank1]:2025-11-11 00:54:10,869 - INFO - Avg. fwd time: 9.9015 / Avg. bwd time: 24.0001 / Avg. batch time: 1211.5761 (ms) / GPU bubble ratio: 55.23%
[rank2]:2025-11-11 00:54:11,156 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2728  memory: 12.06GiB(25.39%)  tps: 6,422  tflops: 50.21  mfu: 16.09%
[rank3]:2025-11-11 00:54:11,168 - INFO -  step: 350  loss:  0.2244  grad_norm:  0.2728  memory: 29.23GiB(61.52%)  tps: 6,422  tflops: 50.21  mfu: 16.09%
[rank0]:2025-11-11 00:54:11,172 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2728  memory: 20.42GiB(42.99%)  tps: 6,422  tflops: 50.21  mfu: 16.09%
[rank1]:2025-11-11 00:54:11,159 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2728  memory: 18.44GiB(38.81%)  tps: 6,422  tflops: 50.21  mfu: 16.09%
[rank0]:2025-11-11 00:58:21,774 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 00:58:26,335 - INFO - Avg. fwd time: 14.3414 / Avg. bwd time: 40.2994 / Avg. batch time: 1090.8909 (ms) / GPU bubble ratio: 19.86%
[rank2]:2025-11-11 00:58:26,443 - INFO - Avg. fwd time: 7.7471 / Avg. bwd time: 18.6070 / Avg. batch time: 1152.3483 (ms) / GPU bubble ratio: 63.41%
[rank0]:2025-11-11 00:58:26,566 - INFO - Avg. fwd time: 8.2190 / Avg. bwd time: 20.8014 / Avg. batch time: 1250.1061 (ms) / GPU bubble ratio: 62.86%
[rank1]:2025-11-11 00:58:26,637 - INFO - Avg. fwd time: 9.9025 / Avg. bwd time: 24.0143 / Avg. batch time: 1211.5735 (ms) / GPU bubble ratio: 55.21%
[rank0]:2025-11-11 00:58:26,934 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2758  memory: 20.42GiB(42.99%)  tps: 6,406  tflops: 50.08  mfu: 16.05%
[rank3]:2025-11-11 00:58:26,932 - INFO -  step: 400  loss:  0.2137  grad_norm:  0.2758  memory: 29.23GiB(61.52%)  tps: 6,406  tflops: 50.08  mfu: 16.05%
[rank1]:2025-11-11 00:58:26,924 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2758  memory: 18.44GiB(38.81%)  tps: 6,406  tflops: 50.08  mfu: 16.05%
[rank2]:2025-11-11 00:58:26,919 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2758  memory: 12.06GiB(25.39%)  tps: 6,406  tflops: 50.08  mfu: 16.05%
[rank3]:2025-11-11 00:58:27,189 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0058_real_step400_rank3.svg
[rank3]:> Batch Time: 1248.16 ms, GPU Bubble Ratio: 62.63%, 56.25%, 66.12%, 29.68%
[rank3]:2025-11-11 00:58:55,946 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank2]:2025-11-11 00:58:56,389 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank0]:2025-11-11 00:58:56,446 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank1]:2025-11-11 00:58:56,421 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank0]:2025-11-11 01:02:36,226 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:02:41,166 - INFO - Avg. fwd time: 14.3171 / Avg. bwd time: 40.2834 / Avg. batch time: 1090.2386 (ms) / GPU bubble ratio: 19.87%
[rank2]:2025-11-11 01:02:41,215 - INFO - Avg. fwd time: 7.7461 / Avg. bwd time: 18.6097 / Avg. batch time: 1151.8090 (ms) / GPU bubble ratio: 63.39%
[rank1]:2025-11-11 01:02:41,260 - INFO - Avg. fwd time: 9.9042 / Avg. bwd time: 24.0237 / Avg. batch time: 1211.0475 (ms) / GPU bubble ratio: 55.18%
[rank1]:2025-11-11 01:02:41,322 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.8454  memory: 18.44GiB(38.81%)  tps: 6,440  tflops: 50.35  mfu: 16.14%
[rank3]:2025-11-11 01:02:41,331 - INFO -  step: 450  loss:  0.2255  grad_norm:  0.8454  memory: 29.23GiB(61.52%)  tps: 6,440  tflops: 50.35  mfu: 16.14%
[rank0]:2025-11-11 01:02:41,297 - INFO - Avg. fwd time: 8.2178 / Avg. bwd time: 20.8056 / Avg. batch time: 1249.5836 (ms) / GPU bubble ratio: 62.84%
[rank0]:2025-11-11 01:02:41,333 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.8454  memory: 20.42GiB(42.99%)  tps: 6,440  tflops: 50.35  mfu: 16.14%
[rank2]:2025-11-11 01:02:41,319 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.8454  memory: 12.06GiB(25.39%)  tps: 6,440  tflops: 50.35  mfu: 16.14%
[rank0]:2025-11-11 01:06:50,664 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:06:55,156 - INFO - Avg. fwd time: 14.3066 / Avg. bwd time: 40.2679 / Avg. batch time: 1089.8088 (ms) / GPU bubble ratio: 19.88%
[rank2]:2025-11-11 01:06:55,262 - INFO - Avg. fwd time: 7.7450 / Avg. bwd time: 18.6101 / Avg. batch time: 1151.3517 (ms) / GPU bubble ratio: 63.38%
[rank0]:2025-11-11 01:06:55,389 - INFO - Avg. fwd time: 8.2169 / Avg. bwd time: 20.8076 / Avg. batch time: 1249.1193 (ms) / GPU bubble ratio: 62.82%
[rank1]:2025-11-11 01:06:55,462 - INFO - Avg. fwd time: 9.9030 / Avg. bwd time: 24.0246 / Avg. batch time: 1210.5879 (ms) / GPU bubble ratio: 55.16%
[rank1]:2025-11-11 01:06:55,741 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2038  memory: 18.44GiB(38.81%)  tps: 6,440  tflops: 50.34  mfu: 16.14%
[rank2]:2025-11-11 01:06:55,737 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2038  memory: 12.06GiB(25.39%)  tps: 6,440  tflops: 50.34  mfu: 16.14%
[rank0]:2025-11-11 01:06:55,751 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2038  memory: 20.42GiB(42.99%)  tps: 6,440  tflops: 50.34  mfu: 16.14%
[rank3]:2025-11-11 01:06:55,749 - INFO -  step: 500  loss:  0.2152  grad_norm:  0.2038  memory: 29.23GiB(61.52%)  tps: 6,440  tflops: 50.34  mfu: 16.14%
[rank3]:2025-11-11 01:06:56,016 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0106_real_step500_rank3.svg
[rank3]:> Batch Time: 1242.08 ms, GPU Bubble Ratio: 62.52%, 56.23%, 66.01%, 29.76%
[rank0]:2025-11-11 01:11:06,083 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:11:10,613 - INFO - Avg. fwd time: 14.2986 / Avg. bwd time: 40.2760 / Avg. batch time: 1089.8110 (ms) / GPU bubble ratio: 19.88%
[rank2]:2025-11-11 01:11:10,720 - INFO - Avg. fwd time: 7.7460 / Avg. bwd time: 18.6147 / Avg. batch time: 1151.4118 (ms) / GPU bubble ratio: 63.37%
[rank0]:2025-11-11 01:11:10,847 - INFO - Avg. fwd time: 8.2172 / Avg. bwd time: 20.8131 / Avg. batch time: 1249.1581 (ms) / GPU bubble ratio: 62.82%
[rank1]:2025-11-11 01:11:10,923 - INFO - Avg. fwd time: 9.9053 / Avg. bwd time: 24.0324 / Avg. batch time: 1210.6243 (ms) / GPU bubble ratio: 55.15%
[rank2]:2025-11-11 01:11:11,195 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1765  memory: 12.06GiB(25.39%)  tps: 6,414  tflops: 50.14  mfu: 16.07%
[rank1]:2025-11-11 01:11:11,199 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1765  memory: 18.44GiB(38.81%)  tps: 6,414  tflops: 50.14  mfu: 16.07%
[rank0]:2025-11-11 01:11:11,210 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1765  memory: 20.42GiB(42.99%)  tps: 6,414  tflops: 50.14  mfu: 16.07%
[rank3]:2025-11-11 01:11:11,207 - INFO -  step: 550  loss:  0.2229  grad_norm:  0.1765  memory: 29.23GiB(61.52%)  tps: 6,414  tflops: 50.14  mfu: 16.07%
[rank0]:2025-11-11 01:15:20,328 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-11 01:15:25,394 - INFO - Avg. fwd time: 7.7460 / Avg. bwd time: 18.6148 / Avg. batch time: 1150.9651 (ms) / GPU bubble ratio: 63.35%
[rank3]:2025-11-11 01:15:25,342 - INFO - Avg. fwd time: 14.2855 / Avg. bwd time: 40.2610 / Avg. batch time: 1089.3624 (ms) / GPU bubble ratio: 19.88%
[rank2]:2025-11-11 01:15:25,501 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1797  memory: 12.06GiB(25.39%)  tps: 6,443  tflops: 50.37  mfu: 16.14%
[rank1]:2025-11-11 01:15:25,440 - INFO - Avg. fwd time: 9.9057 / Avg. bwd time: 24.0350 / Avg. batch time: 1210.1803 (ms) / GPU bubble ratio: 55.13%
[rank1]:2025-11-11 01:15:25,504 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1797  memory: 18.44GiB(38.81%)  tps: 6,443  tflops: 50.37  mfu: 16.14%
[rank0]:2025-11-11 01:15:25,479 - INFO - Avg. fwd time: 8.2166 / Avg. bwd time: 20.8149 / Avg. batch time: 1248.7146 (ms) / GPU bubble ratio: 62.80%
[rank0]:2025-11-11 01:15:25,515 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1797  memory: 20.42GiB(42.99%)  tps: 6,443  tflops: 50.37  mfu: 16.14%
[rank3]:2025-11-11 01:15:25,513 - INFO -  step: 600  loss:  0.2155  grad_norm:  0.1797  memory: 29.23GiB(61.52%)  tps: 6,443  tflops: 50.37  mfu: 16.14%
[rank3]:2025-11-11 01:15:25,775 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0115_real_step600_rank3.svg
[rank3]:> Batch Time: 1238.09 ms, GPU Bubble Ratio: 62.38%, 56.09%, 65.91%, 29.77%
[rank0]:2025-11-11 01:19:35,516 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:19:40,025 - INFO - Avg. fwd time: 14.2856 / Avg. bwd time: 40.2514 / Avg. batch time: 1089.2039 (ms) / GPU bubble ratio: 19.89%
[rank2]:2025-11-11 01:19:40,129 - INFO - Avg. fwd time: 7.7458 / Avg. bwd time: 18.6146 / Avg. batch time: 1150.8833 (ms) / GPU bubble ratio: 63.35%
[rank1]:2025-11-11 01:19:40,323 - INFO - Avg. fwd time: 9.9045 / Avg. bwd time: 24.0342 / Avg. batch time: 1210.0864 (ms) / GPU bubble ratio: 55.13%
[rank0]:2025-11-11 01:19:40,256 - INFO - Avg. fwd time: 8.2164 / Avg. bwd time: 20.8176 / Avg. batch time: 1248.6172 (ms) / GPU bubble ratio: 62.80%
[rank2]:2025-11-11 01:19:40,598 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1753  memory: 12.06GiB(25.39%)  tps: 6,423  tflops: 50.21  mfu: 16.09%
[rank1]:2025-11-11 01:19:40,602 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1753  memory: 18.44GiB(38.81%)  tps: 6,423  tflops: 50.21  mfu: 16.09%
[rank3]:2025-11-11 01:19:40,610 - INFO -  step: 650  loss:  0.2252  grad_norm:  0.1753  memory: 29.23GiB(61.52%)  tps: 6,423  tflops: 50.21  mfu: 16.09%
[rank0]:2025-11-11 01:19:40,612 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1753  memory: 20.42GiB(42.99%)  tps: 6,423  tflops: 50.21  mfu: 16.09%
[rank0]:2025-11-11 01:23:50,466 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:23:54,956 - INFO - Avg. fwd time: 14.2890 / Avg. bwd time: 40.2481 / Avg. batch time: 1089.2011 (ms) / GPU bubble ratio: 19.89%
[rank2]:2025-11-11 01:23:55,060 - INFO - Avg. fwd time: 7.7459 / Avg. bwd time: 18.6146 / Avg. batch time: 1150.8570 (ms) / GPU bubble ratio: 63.35%
[rank0]:2025-11-11 01:23:55,189 - INFO - Avg. fwd time: 8.2174 / Avg. bwd time: 20.8210 / Avg. batch time: 1248.5843 (ms) / GPU bubble ratio: 62.79%
[rank1]:2025-11-11 01:23:55,258 - INFO - Avg. fwd time: 9.9039 / Avg. bwd time: 24.0343 / Avg. batch time: 1210.0520 (ms) / GPU bubble ratio: 55.12%
[rank2]:2025-11-11 01:23:55,534 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1590  memory: 12.06GiB(25.39%)  tps: 6,427  tflops: 50.24  mfu: 16.10%
[rank3]:2025-11-11 01:23:55,546 - INFO -  step: 700  loss:  0.1830  grad_norm:  0.1590  memory: 29.23GiB(61.52%)  tps: 6,427  tflops: 50.24  mfu: 16.10%
[rank0]:2025-11-11 01:23:55,548 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1590  memory: 20.42GiB(42.99%)  tps: 6,427  tflops: 50.24  mfu: 16.10%
[rank1]:2025-11-11 01:23:55,538 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1590  memory: 18.44GiB(38.81%)  tps: 6,427  tflops: 50.24  mfu: 16.10%
[rank3]:2025-11-11 01:23:55,820 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0123_real_step700_rank3.svg
[rank3]:> Batch Time: 1251.14 ms, GPU Bubble Ratio: 62.67%, 56.49%, 66.20%, 29.90%
[rank0]:2025-11-11 01:28:06,427 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:28:11,410 - INFO - Avg. fwd time: 14.2884 / Avg. bwd time: 40.2600 / Avg. batch time: 1089.3841 (ms) / GPU bubble ratio: 19.88%
[rank1]:2025-11-11 01:28:11,507 - INFO - Avg. fwd time: 9.9051 / Avg. bwd time: 24.0424 / Avg. batch time: 1210.2759 (ms) / GPU bubble ratio: 55.12%
[rank2]:2025-11-11 01:28:11,458 - INFO - Avg. fwd time: 7.7463 / Avg. bwd time: 18.6167 / Avg. batch time: 1151.0703 (ms) / GPU bubble ratio: 63.36%
[rank1]:2025-11-11 01:28:11,572 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.2144  memory: 18.44GiB(38.81%)  tps: 6,399  tflops: 50.03  mfu: 16.03%
[rank0]:2025-11-11 01:28:11,547 - INFO - Avg. fwd time: 8.2179 / Avg. bwd time: 20.8252 / Avg. batch time: 1248.8133 (ms) / GPU bubble ratio: 62.79%
[rank3]:2025-11-11 01:28:11,581 - INFO -  step: 750  loss:  0.2251  grad_norm:  0.2144  memory: 29.23GiB(61.52%)  tps: 6,399  tflops: 50.03  mfu: 16.03%
[rank0]:2025-11-11 01:28:11,583 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.2144  memory: 20.42GiB(42.99%)  tps: 6,399  tflops: 50.03  mfu: 16.03%
[rank2]:2025-11-11 01:28:11,568 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.2144  memory: 12.06GiB(25.39%)  tps: 6,399  tflops: 50.03  mfu: 16.03%
[rank0]:2025-11-11 01:32:21,131 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-11 01:32:25,713 - INFO - Avg. fwd time: 14.2855 / Avg. bwd time: 40.2557 / Avg. batch time: 1089.2696 (ms) / GPU bubble ratio: 19.89%
[rank2]:2025-11-11 01:32:25,821 - INFO - Avg. fwd time: 7.7457 / Avg. bwd time: 18.6157 / Avg. batch time: 1150.9292 (ms) / GPU bubble ratio: 63.35%
[rank1]:2025-11-11 01:32:26,019 - INFO - Avg. fwd time: 9.9046 / Avg. bwd time: 24.0416 / Avg. batch time: 1210.1186 (ms) / GPU bubble ratio: 55.12%
[rank0]:2025-11-11 01:32:25,949 - INFO - Avg. fwd time: 8.2174 / Avg. bwd time: 20.8255 / Avg. batch time: 1248.6525 (ms) / GPU bubble ratio: 62.78%
[rank2]:2025-11-11 01:32:26,299 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1811  memory: 12.06GiB(25.39%)  tps: 6,432  tflops: 50.28  mfu: 16.12%
[rank2]:2025-11-11 01:32:26,300 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1811  tps: 6,803  tflops: 53.19  mfu: 15.45%
[rank2]:2025-11-11 01:32:26,300 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-11 01:32:26,300 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-11 01:32:26,303 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1811  memory: 18.44GiB(38.81%)  tps: 6,432  tflops: 50.28  mfu: 16.12%
[rank1]:2025-11-11 01:32:26,304 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1811  tps: 6,803  tflops: 53.18  mfu: 15.45%
[rank1]:2025-11-11 01:32:26,304 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-11 01:32:26,304 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-11 01:32:26,311 - INFO -  step: 800  loss:  0.2277  grad_norm:  0.1811  memory: 29.23GiB(61.52%)  tps: 6,432  tflops: 50.28  mfu: 16.12%
[rank3]:2025-11-11 01:32:26,312 - INFO -  final step: 800  loss:  0.2277  grad_norm:  0.1811  tps: 6,815  tflops: 53.27  mfu: 15.71%
[rank3]:2025-11-11 01:32:26,312 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-11 01:32:26,313 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-11 01:32:26,315 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1811  memory: 20.42GiB(42.99%)  tps: 6,432  tflops: 50.28  mfu: 16.12%
[rank0]:2025-11-11 01:32:26,315 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1811  tps: 6,803  tflops: 53.19  mfu: 15.45%
[rank0]:2025-11-11 01:32:26,315 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-11 01:32:26,315 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-11 01:32:28,609 - INFO - Destroying the purge thread.
[rank1]:2025-11-11 01:32:28,609 - INFO - Destroying the purge thread.
[rank0]:2025-11-11 01:32:28,588 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-11 01:32:28,608 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-11 01:32:28,844 - INFO - Process group destroyed
[rank3]:2025-11-11 01:32:28,865 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0132_real_final800_rank3.svg
[rank3]:> Batch Time: 1247.11 ms, GPU Bubble Ratio: 62.65%, 56.39%, 66.15%, 29.80%
[rank1]:2025-11-11 01:32:29,047 - INFO - Process group destroyed
[rank3]:2025-11-11 01:32:29,115 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1/pipeline_schedule/251111_0132_thry_final800_rank3.svg
[rank3]:> Batch Time: 573.69 ms, GPU Bubble Ratio: 15.79%, 15.79%, 15.79%, 15.79%
[rank3]:2025-11-11 01:32:29,115 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 4.80837
[rank3]:wandb:               final/avg_loss 0.22775
[rank3]:wandb:             final/avg_mfu(%) 15.71385
[rank3]:wandb:             final/avg_tflops 53.27486
[rank3]:wandb:    final/avg_throughput(tps) 6814.78635
[rank3]:wandb:              final/grad_norm 0.18112
[rank3]:wandb:               final/max_loss 0.22775
[rank3]:wandb:                    grad_norm 0.18112
[rank3]:wandb: loss_metrics/global_avg_loss 0.22775
[rank3]:wandb: loss_metrics/global_max_loss 0.22775
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1109_Interleaved1F1B_nofreeze_42_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/5iknne6l
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1109_Interleaved1F1B_nofreeze_42_dm1/20251111-0024/wandb/run-20251111_002415-5iknne6l/logs
[rank3]:2025-11-11 01:32:30,475 - INFO - Process group destroyed
[rank0]:2025-11-11 01:32:30,609 - INFO - Training completed
[rank0]:2025-11-11 01:32:30,609 - INFO - Destroying the purge thread.
[rank0]:2025-11-11 01:32:31,034 - INFO - Process group destroyed
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.2', 'layers.3'}
[rank1]:Stage 5: Modules to keep: {'layers.12', 'layers.11'}
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.0', 'layers.1'}
[rank0]:Stage 4: Modules to keep: {'layers.9', 'layers.10'}
[rank2]:Stage 2: Modules to keep: {'layers.6', 'layers.5'}
[rank2]:Stage 6: Modules to keep: {'layers.13', 'layers.14'}
[rank3]:Stage 3: Modules to keep: {'layers.7', 'layers.8'}
[rank3]:Stage 7: Modules to keep: {'norm', 'output', 'layers.15'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1109.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.01
[rank3]:	- training:
[rank3]:		- dataset: alpaca_gpt4
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1109_Interleaved1F1B_nofreeze_42_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.8
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 80
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
