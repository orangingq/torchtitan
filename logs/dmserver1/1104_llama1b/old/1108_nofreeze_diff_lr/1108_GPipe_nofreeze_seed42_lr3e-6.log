
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 19:02:21 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42_lr3e-6.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=3e-6 --lr_scheduler.lr_min=3e-7  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:â•­â”€ Unrecognized options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank2]:â”‚ Unrecognized options: --lr-scheduler.lr-min=3e-7 â”‚
[rank2]:â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
[rank2]:â”‚ For full helptext, run train.py --help           â”‚
[rank2]:â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank3]:â•­â”€ Unrecognized options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank3]:â”‚ Unrecognized options: --lr-scheduler.lr-min=3e-7 â”‚
[rank3]:â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
[rank3]:â”‚ For full helptext, run train.py --help           â”‚
[rank3]:â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank0]:â•­â”€ Unrecognized options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank0]:â”‚ Unrecognized options: --lr-scheduler.lr-min=3e-7 â”‚
[rank0]:â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
[rank0]:â”‚ For full helptext, run train.py --help           â”‚
[rank0]:â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[rank1]:â•­â”€ Unrecognized options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[rank1]:â”‚ Unrecognized options: --lr-scheduler.lr-min=3e-7 â”‚
[rank1]:â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
[rank1]:â”‚ For full helptext, run train.py --help           â”‚
[rank1]:â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
W1109 19:02:29.213000 2613823 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2613863 closing signal SIGTERM
W1109 19:02:29.213000 2613823 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2613864 closing signal SIGTERM
E1109 19:02:29.400000 2613823 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 2) local_rank: 2 (pid: 2613865) of binary: /data2/shcho/miniforge3/envs/llm_eval/bin/python3.11
Traceback (most recent call last):
  File "/data2/shcho/miniforge3/envs/llm_eval/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
  File "/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
  File "/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-09_19:02:29
  host      : elga.kaist.ac.kr
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 2613866)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-11-09_19:02:29
  host      : elga.kaist.ac.kr
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2613863)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2613863
[3]:
  time      : 2025-11-09_19:02:29
  host      : elga.kaist.ac.kr
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2613864)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2613864
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-09_19:02:29
  host      : elga.kaist.ac.kr
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 2613865)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 19:04:26 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42_lr3e-6.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=3e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 19:04:32,741 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-09 19:04:32,757 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-09 19:04:32,699 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-09 19:04:32,793 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-09 19:04:32,941 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 19:04:32,944 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 19:04:33,045 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 19:04:33,048 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 19:04:33,052 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 19:04:33,053 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-09 19:04:33,109 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 19:04:33,112 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 19:04:33,093 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 19:04:33,095 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 19:04:33,444 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 19:04:36,223 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 19:04:36,375 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 19:04:36,411 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 19:04:36,412 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 19:04:36,438 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 19:04:36,438 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 19:04:36,401 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 19:04:36,439 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 19:04:36,475 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 19:04:36,522 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 19:04:36,549 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 19:04:36,549 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 19:04:36,464 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 19:04:36,465 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 19:04:36,632 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 19:04:36,633 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 19:04:36,633 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-09 19:04:36,652 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 19:04:36,652 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 19:04:36,653 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-09 19:04:36,734 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 19:04:36,734 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 19:04:36,734 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run fqstz3ao
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/20251109-1904/wandb/run-20251109_190437-fqstz3ao
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/fqstz3ao
[rank3]:2025-11-09 19:04:38,542 - INFO - WandB logging enabled
[rank3]:2025-11-09 19:04:38,543 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 19:04:38,581 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 19:04:38,608 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 19:04:38,609 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 19:04:38,810 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank0]:2025-11-09 19:04:38,810 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 19:04:38,812 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 19:04:38,813 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank2]:2025-11-09 19:04:38,810 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 19:04:38,792 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 19:04:38,793 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 19:04:38,793 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 19:04:38,810 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 19:04:38,810 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 19:04:41,177 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 19:04:41,177 - INFO - Finished loading the checkpoint in 2.36 seconds.
[rank0]:2025-11-09 19:04:41,177 - INFO - Training starts at step 1
[rank3]:2025-11-09 19:04:44,224 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5953  memory: 24.19GiB(50.91%)  tps: 2,905  tflops: 22.12  mfu: 7.09%
[rank3]:2025-11-09 19:04:44,225 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 19:04:44,214 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5953  memory:  9.99GiB(21.03%)  tps: 2,107  tflops: 16.05  mfu: 5.14%
[rank2]:2025-11-09 19:04:44,214 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 19:04:44,251 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5953  memory: 12.80GiB(26.95%)  tps: 2,090  tflops: 15.92  mfu: 5.10%
[rank0]:2025-11-09 19:04:44,251 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 19:04:44,219 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5953  memory: 12.38GiB(26.05%)  tps: 2,129  tflops: 16.21  mfu: 5.20%
[rank1]:2025-11-09 19:04:44,219 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 19:06:46,111 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:06:48,386 - INFO - Avg. fwd time: 11.2971 / Avg. bwd time: 44.0391 / Avg. batch time: 502.5334 (ms) / GPU bubble ratio: 11.91%
[rank1]:2025-11-09 19:06:48,499 - INFO - Avg. fwd time: 9.0618 / Avg. bwd time: 23.7790 / Avg. batch time: 573.5072 (ms) / GPU bubble ratio: 54.19%
[rank2]:2025-11-09 19:06:48,459 - INFO - Avg. fwd time: 7.1543 / Avg. bwd time: 18.7431 / Avg. batch time: 534.3803 (ms) / GPU bubble ratio: 61.23%
[rank0]:2025-11-09 19:06:48,507 - INFO - Avg. fwd time: 7.8548 / Avg. bwd time: 23.3485 / Avg. batch time: 610.5014 (ms) / GPU bubble ratio: 59.11%
[rank1]:2025-11-09 19:06:48,685 - INFO -  step: 50  loss: -4.0000  grad_norm: 27.7415  memory: 14.64GiB(30.82%)  tps: 6,450  tflops: 49.13  mfu: 15.75%
[rank3]:2025-11-09 19:06:48,693 - INFO -  step: 50  loss:  8.8516  grad_norm: 27.7415  memory: 26.98GiB(56.79%)  tps: 6,450  tflops: 49.12  mfu: 15.75%
[rank2]:2025-11-09 19:06:48,681 - INFO -  step: 50  loss: -4.0000  grad_norm: 27.7415  memory: 11.81GiB(24.85%)  tps: 6,450  tflops: 49.12  mfu: 15.75%
[rank0]:2025-11-09 19:06:48,696 - INFO -  step: 50  loss: -4.0000  grad_norm: 27.7415  memory: 16.57GiB(34.88%)  tps: 6,451  tflops: 49.13  mfu: 15.75%
[rank0]:2025-11-09 19:08:55,638 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:08:57,973 - INFO - Avg. fwd time: 11.4016 / Avg. bwd time: 44.8554 / Avg. batch time: 509.8310 (ms) / GPU bubble ratio: 11.72%
[rank1]:2025-11-09 19:08:58,087 - INFO - Avg. fwd time: 9.1030 / Avg. bwd time: 23.9289 / Avg. batch time: 580.1941 (ms) / GPU bubble ratio: 54.45%
[rank2]:2025-11-09 19:08:58,047 - INFO - Avg. fwd time: 7.1683 / Avg. bwd time: 18.8377 / Avg. batch time: 541.3317 (ms) / GPU bubble ratio: 61.57%
[rank0]:2025-11-09 19:08:58,095 - INFO - Avg. fwd time: 7.8710 / Avg. bwd time: 23.4310 / Avg. batch time: 616.8172 (ms) / GPU bubble ratio: 59.40%
[rank1]:2025-11-09 19:08:58,277 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.7986  memory: 14.64GiB(30.82%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank2]:2025-11-09 19:08:58,273 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.7986  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 19:08:58,286 - INFO -  step: 100  loss:  4.8634  grad_norm: 18.7986  memory: 26.98GiB(56.79%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 19:08:58,288 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.7986  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 19:08:58,460 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1908_real_step100_rank3.svg
[rank3]:> Batch Time: 624.12 ms, GPU Bubble Ratio: 59.42%, 57.25%, 66.39%, 26.37%
[rank0]:2025-11-09 19:11:05,689 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:11:08,181 - INFO - Avg. fwd time: 11.4254 / Avg. bwd time: 45.2031 / Avg. batch time: 512.7851 (ms) / GPU bubble ratio: 11.65%
[rank1]:2025-11-09 19:11:08,237 - INFO - Avg. fwd time: 9.1161 / Avg. bwd time: 23.9948 / Avg. batch time: 583.2383 (ms) / GPU bubble ratio: 54.58%
[rank2]:2025-11-09 19:11:08,207 - INFO - Avg. fwd time: 7.1682 / Avg. bwd time: 18.8769 / Avg. batch time: 544.4214 (ms) / GPU bubble ratio: 61.73%
[rank3]:2025-11-09 19:11:08,300 - INFO -  step: 150  loss:  6.1551  grad_norm: 12.7171  memory: 26.98GiB(56.79%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank0]:2025-11-09 19:11:08,266 - INFO - Avg. fwd time: 7.8795 / Avg. bwd time: 23.4618 / Avg. batch time: 619.7453 (ms) / GPU bubble ratio: 59.54%
[rank0]:2025-11-09 19:11:08,302 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.7171  memory: 16.57GiB(34.88%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank1]:2025-11-09 19:11:08,291 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.7171  memory: 14.64GiB(30.82%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank2]:2025-11-09 19:11:08,288 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.7171  memory: 11.81GiB(24.85%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank0]:2025-11-09 19:13:15,766 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:13:18,100 - INFO - Avg. fwd time: 11.4340 / Avg. bwd time: 45.3748 / Avg. batch time: 514.2167 (ms) / GPU bubble ratio: 11.62%
[rank2]:2025-11-09 19:13:18,178 - INFO - Avg. fwd time: 7.1681 / Avg. bwd time: 18.8951 / Avg. batch time: 545.7683 (ms) / GPU bubble ratio: 61.80%
[rank0]:2025-11-09 19:13:18,225 - INFO - Avg. fwd time: 7.8888 / Avg. bwd time: 23.4742 / Avg. batch time: 621.0205 (ms) / GPU bubble ratio: 59.60%
[rank1]:2025-11-09 19:13:18,218 - INFO - Avg. fwd time: 9.1204 / Avg. bwd time: 24.0240 / Avg. batch time: 584.5647 (ms) / GPU bubble ratio: 54.64%
[rank1]:2025-11-09 19:13:18,405 - INFO -  step: 200  loss: -4.0000  grad_norm: 118.6684  memory: 14.64GiB(30.82%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank2]:2025-11-09 19:13:18,401 - INFO -  step: 200  loss: -4.0000  grad_norm: 118.6684  memory: 11.81GiB(24.85%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank3]:2025-11-09 19:13:18,413 - INFO -  step: 200  loss:  2.9032  grad_norm: 118.6684  memory: 26.98GiB(56.79%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank0]:2025-11-09 19:13:18,415 - INFO -  step: 200  loss: -4.0000  grad_norm: 118.6684  memory: 16.57GiB(34.88%)  tps: 6,296  tflops: 47.95  mfu: 15.37%
[rank3]:2025-11-09 19:13:18,565 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1913_real_step200_rank3.svg
[rank3]:> Batch Time: 622.59 ms, GPU Bubble Ratio: 59.34%, 57.13%, 66.34%, 26.13%
[rank0]:2025-11-09 19:15:26,135 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:15:28,455 - INFO - Avg. fwd time: 11.4484 / Avg. bwd time: 45.4863 / Avg. batch time: 515.2204 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 19:15:28,529 - INFO - Avg. fwd time: 7.1675 / Avg. bwd time: 18.9128 / Avg. batch time: 546.8354 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 19:15:28,569 - INFO - Avg. fwd time: 9.1242 / Avg. bwd time: 24.0552 / Avg. batch time: 585.6501 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 19:15:28,577 - INFO - Avg. fwd time: 7.8940 / Avg. bwd time: 23.4875 / Avg. batch time: 622.0862 (ms) / GPU bubble ratio: 59.64%
[rank3]:2025-11-09 19:15:28,765 - INFO -  step: 250  loss:  1.7904  grad_norm: 18.1110  memory: 26.98GiB(56.79%)  tps: 6,285  tflops: 47.87  mfu: 15.34%
[rank1]:2025-11-09 19:15:28,757 - INFO -  step: 250  loss: -4.0000  grad_norm: 18.1110  memory: 14.64GiB(30.82%)  tps: 6,285  tflops: 47.86  mfu: 15.34%
[rank0]:2025-11-09 19:15:28,768 - INFO -  step: 250  loss: -4.0000  grad_norm: 18.1110  memory: 16.57GiB(34.88%)  tps: 6,285  tflops: 47.86  mfu: 15.34%
[rank2]:2025-11-09 19:15:28,753 - INFO -  step: 250  loss: -4.0000  grad_norm: 18.1110  memory: 11.81GiB(24.85%)  tps: 6,285  tflops: 47.86  mfu: 15.34%
[rank0]:2025-11-09 19:17:36,240 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 19:17:38,812 - INFO - Avg. fwd time: 7.1664 / Avg. bwd time: 18.9323 / Avg. batch time: 547.3450 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 19:17:38,843 - INFO - Avg. fwd time: 9.1262 / Avg. bwd time: 24.0850 / Avg. batch time: 586.1864 (ms) / GPU bubble ratio: 54.67%
[rank3]:2025-11-09 19:17:38,786 - INFO - Avg. fwd time: 11.4492 / Avg. bwd time: 45.5565 / Avg. batch time: 515.7802 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 19:17:38,895 - INFO -  step: 300  loss: -4.0000  grad_norm: 22.0422  memory: 11.81GiB(24.85%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank1]:2025-11-09 19:17:38,898 - INFO -  step: 300  loss: -4.0000  grad_norm: 22.0422  memory: 14.64GiB(30.82%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank3]:2025-11-09 19:17:38,907 - INFO -  step: 300  loss:  1.1632  grad_norm: 22.0422  memory: 26.98GiB(56.79%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank0]:2025-11-09 19:17:38,873 - INFO - Avg. fwd time: 7.8971 / Avg. bwd time: 23.4998 / Avg. batch time: 622.6197 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 19:17:38,909 - INFO -  step: 300  loss: -4.0000  grad_norm: 22.0422  memory: 16.57GiB(34.88%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank3]:2025-11-09 19:17:39,055 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1917_real_step300_rank3.svg
[rank3]:> Batch Time: 622.67 ms, GPU Bubble Ratio: 59.32%, 56.97%, 66.23%, 26.42%
[rank0]:2025-11-09 19:19:46,438 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 19:19:48,809 - INFO - Avg. fwd time: 7.1656 / Avg. bwd time: 18.9465 / Avg. batch time: 547.7000 (ms) / GPU bubble ratio: 61.86%
[rank3]:2025-11-09 19:19:48,736 - INFO - Avg. fwd time: 11.4493 / Avg. bwd time: 45.5953 / Avg. batch time: 516.0876 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-09 19:19:48,849 - INFO - Avg. fwd time: 9.1262 / Avg. bwd time: 24.1069 / Avg. batch time: 586.5611 (ms) / GPU bubble ratio: 54.67%
[rank0]:2025-11-09 19:19:48,857 - INFO - Avg. fwd time: 7.8964 / Avg. bwd time: 23.5073 / Avg. batch time: 622.9861 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 19:19:49,046 - INFO -  step: 350  loss: -4.0000  grad_norm: 13.5437  memory: 16.57GiB(34.88%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank2]:2025-11-09 19:19:49,031 - INFO -  step: 350  loss: -4.0000  grad_norm: 13.5437  memory: 11.81GiB(24.85%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank3]:2025-11-09 19:19:49,044 - INFO -  step: 350  loss:  0.7870  grad_norm: 13.5437  memory: 26.98GiB(56.79%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank1]:2025-11-09 19:19:49,036 - INFO -  step: 350  loss: -4.0000  grad_norm: 13.5437  memory: 14.64GiB(30.82%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank0]:2025-11-09 19:21:56,036 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:21:58,357 - INFO - Avg. fwd time: 11.4423 / Avg. bwd time: 45.6021 / Avg. batch time: 516.0853 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 19:21:58,431 - INFO - Avg. fwd time: 7.1639 / Avg. bwd time: 18.9565 / Avg. batch time: 547.6493 (ms) / GPU bubble ratio: 61.84%
[rank0]:2025-11-09 19:21:58,479 - INFO - Avg. fwd time: 7.8959 / Avg. bwd time: 23.5105 / Avg. batch time: 622.9366 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 19:21:58,472 - INFO - Avg. fwd time: 9.1248 / Avg. bwd time: 24.1209 / Avg. batch time: 586.5247 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 19:21:58,656 - INFO -  step: 400  loss: -4.0000  grad_norm:  6.9551  memory: 11.81GiB(24.85%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 19:21:58,668 - INFO -  step: 400  loss:  0.6181  grad_norm:  6.9551  memory: 26.98GiB(56.79%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 19:21:58,670 - INFO -  step: 400  loss: -4.0000  grad_norm:  6.9551  memory: 16.57GiB(34.88%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-09 19:21:58,660 - INFO -  step: 400  loss: -4.0000  grad_norm:  6.9551  memory: 14.64GiB(30.82%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 19:21:58,817 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1921_real_step400_rank3.svg
[rank3]:> Batch Time: 621.12 ms, GPU Bubble Ratio: 59.29%, 57.01%, 66.20%, 26.60%
[rank3]:2025-11-09 19:22:07,543 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 19:22:07,762 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 19:22:07,815 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 19:22:07,789 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 19:24:05,394 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 19:24:07,935 - INFO - Avg. fwd time: 7.1611 / Avg. bwd time: 18.9624 / Avg. batch time: 547.5285 (ms) / GPU bubble ratio: 61.83%
[rank3]:2025-11-09 19:24:07,908 - INFO - Avg. fwd time: 11.4324 / Avg. bwd time: 45.5942 / Avg. batch time: 515.9393 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 19:24:07,967 - INFO - Avg. fwd time: 9.1214 / Avg. bwd time: 24.1304 / Avg. batch time: 586.4159 (ms) / GPU bubble ratio: 54.64%
[rank2]:2025-11-09 19:24:08,019 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.7376  memory: 11.81GiB(24.85%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 19:24:07,997 - INFO - Avg. fwd time: 7.8942 / Avg. bwd time: 23.5126 / Avg. batch time: 622.8226 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 19:24:08,033 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.7376  memory: 16.57GiB(34.88%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank3]:2025-11-09 19:24:08,032 - INFO -  step: 450  loss:  0.6022  grad_norm:  1.7376  memory: 26.98GiB(56.79%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank1]:2025-11-09 19:24:08,023 - INFO -  step: 450  loss: -4.0000  grad_norm:  1.7376  memory: 14.64GiB(30.82%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 19:26:15,078 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:26:17,409 - INFO - Avg. fwd time: 11.4274 / Avg. bwd time: 45.6022 / Avg. batch time: 515.9588 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 19:26:17,480 - INFO - Avg. fwd time: 7.1601 / Avg. bwd time: 18.9677 / Avg. batch time: 547.5171 (ms) / GPU bubble ratio: 61.82%
[rank0]:2025-11-09 19:26:17,530 - INFO - Avg. fwd time: 7.8946 / Avg. bwd time: 23.5155 / Avg. batch time: 622.8287 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 19:26:17,523 - INFO - Avg. fwd time: 9.1214 / Avg. bwd time: 24.1404 / Avg. batch time: 586.4257 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 19:26:17,718 - INFO -  step: 500  loss: -4.0000  grad_norm:  6.9426  memory: 16.57GiB(34.88%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 19:26:17,716 - INFO -  step: 500  loss:  0.4671  grad_norm:  6.9426  memory: 26.98GiB(56.79%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank1]:2025-11-09 19:26:17,707 - INFO -  step: 500  loss: -4.0000  grad_norm:  6.9426  memory: 14.64GiB(30.82%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank2]:2025-11-09 19:26:17,703 - INFO -  step: 500  loss: -4.0000  grad_norm:  6.9426  memory: 11.81GiB(24.85%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 19:26:17,864 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1926_real_step500_rank3.svg
[rank3]:> Batch Time: 622.66 ms, GPU Bubble Ratio: 59.32%, 57.00%, 66.26%, 26.52%
[rank0]:2025-11-09 19:28:25,195 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:28:27,500 - INFO - Avg. fwd time: 11.4269 / Avg. bwd time: 45.6232 / Avg. batch time: 516.1205 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-09 19:28:27,616 - INFO - Avg. fwd time: 9.1222 / Avg. bwd time: 24.1525 / Avg. batch time: 586.6311 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 19:28:27,623 - INFO - Avg. fwd time: 7.8963 / Avg. bwd time: 23.5185 / Avg. batch time: 623.0345 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-09 19:28:27,576 - INFO - Avg. fwd time: 7.1599 / Avg. bwd time: 18.9724 / Avg. batch time: 547.6999 (ms) / GPU bubble ratio: 61.83%
[rank1]:2025-11-09 19:28:27,798 - INFO -  step: 550  loss: -4.0000  grad_norm: 16.6328  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 19:28:27,809 - INFO -  step: 550  loss: -4.0000  grad_norm: 16.6328  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 19:28:27,807 - INFO -  step: 550  loss:  0.4845  grad_norm: 16.6328  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-09 19:28:27,794 - INFO -  step: 550  loss: -4.0000  grad_norm: 16.6328  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 19:30:35,320 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:30:37,864 - INFO - Avg. fwd time: 11.4291 / Avg. bwd time: 45.6472 / Avg. batch time: 516.3299 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 19:30:37,892 - INFO - Avg. fwd time: 7.1599 / Avg. bwd time: 18.9770 / Avg. batch time: 547.8837 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 19:30:37,925 - INFO - Avg. fwd time: 9.1225 / Avg. bwd time: 24.1636 / Avg. batch time: 586.8276 (ms) / GPU bubble ratio: 54.62%
[rank1]:2025-11-09 19:30:37,982 - INFO -  step: 600  loss: -4.0000  grad_norm:  7.5632  memory: 14.64GiB(30.82%)  tps: 6,293  tflops: 47.93  mfu: 15.36%
[rank0]:2025-11-09 19:30:37,957 - INFO - Avg. fwd time: 7.8976 / Avg. bwd time: 23.5219 / Avg. batch time: 623.2314 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 19:30:37,993 - INFO -  step: 600  loss: -4.0000  grad_norm:  7.5632  memory: 16.57GiB(34.88%)  tps: 6,293  tflops: 47.93  mfu: 15.36%
[rank3]:2025-11-09 19:30:37,991 - INFO -  step: 600  loss:  0.4921  grad_norm:  7.5632  memory: 26.98GiB(56.79%)  tps: 6,293  tflops: 47.93  mfu: 15.36%
[rank2]:2025-11-09 19:30:37,978 - INFO -  step: 600  loss: -4.0000  grad_norm:  7.5632  memory: 11.81GiB(24.85%)  tps: 6,293  tflops: 47.93  mfu: 15.36%
[rank3]:2025-11-09 19:30:38,138 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1930_real_step600_rank3.svg
[rank3]:> Batch Time: 624.19 ms, GPU Bubble Ratio: 59.36%, 57.09%, 66.33%, 26.38%
[rank0]:2025-11-09 19:32:45,634 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 19:32:48,014 - INFO - Avg. fwd time: 7.1602 / Avg. bwd time: 18.9806 / Avg. batch time: 548.0610 (ms) / GPU bubble ratio: 61.84%
[rank3]:2025-11-09 19:32:47,940 - INFO - Avg. fwd time: 11.4304 / Avg. bwd time: 45.6652 / Avg. batch time: 516.4819 (ms) / GPU bubble ratio: 11.56%
[rank1]:2025-11-09 19:32:48,054 - INFO - Avg. fwd time: 9.1234 / Avg. bwd time: 24.1725 / Avg. batch time: 587.0143 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 19:32:48,062 - INFO - Avg. fwd time: 7.8979 / Avg. bwd time: 23.5244 / Avg. batch time: 623.4185 (ms) / GPU bubble ratio: 59.68%
[rank1]:2025-11-09 19:32:48,236 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.3426  memory: 14.64GiB(30.82%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-09 19:32:48,247 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.3426  memory: 16.57GiB(34.88%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank2]:2025-11-09 19:32:48,232 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.3426  memory: 11.81GiB(24.85%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank3]:2025-11-09 19:32:48,245 - INFO -  step: 650  loss:  0.4145  grad_norm:  1.3426  memory: 26.98GiB(56.79%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-09 19:34:55,475 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:34:57,793 - INFO - Avg. fwd time: 11.4288 / Avg. bwd time: 45.6715 / Avg. batch time: 516.5163 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 19:34:57,867 - INFO - Avg. fwd time: 7.1599 / Avg. bwd time: 18.9830 / Avg. batch time: 548.0716 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 19:34:57,908 - INFO - Avg. fwd time: 9.1237 / Avg. bwd time: 24.1794 / Avg. batch time: 587.0341 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 19:34:57,915 - INFO - Avg. fwd time: 7.8993 / Avg. bwd time: 23.5257 / Avg. batch time: 623.4432 (ms) / GPU bubble ratio: 59.68%
[rank2]:2025-11-09 19:34:58,088 - INFO -  step: 700  loss: -4.0000  grad_norm:  5.1454  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 19:34:58,101 - INFO -  step: 700  loss:  0.4930  grad_norm:  5.1454  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 19:34:58,092 - INFO -  step: 700  loss: -4.0000  grad_norm:  5.1454  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 19:34:58,103 - INFO -  step: 700  loss: -4.0000  grad_norm:  5.1454  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 19:34:58,248 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1934_real_step700_rank3.svg
[rank3]:> Batch Time: 620.69 ms, GPU Bubble Ratio: 59.24%, 56.91%, 66.20%, 26.74%
[rank0]:2025-11-09 19:37:05,195 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 19:37:07,735 - INFO - Avg. fwd time: 7.1593 / Avg. bwd time: 18.9843 / Avg. batch time: 548.0259 (ms) / GPU bubble ratio: 61.84%
[rank3]:2025-11-09 19:37:07,706 - INFO - Avg. fwd time: 11.4249 / Avg. bwd time: 45.6679 / Avg. batch time: 516.4538 (ms) / GPU bubble ratio: 11.56%
[rank1]:2025-11-09 19:37:07,768 - INFO - Avg. fwd time: 9.1227 / Avg. bwd time: 24.1834 / Avg. batch time: 586.9938 (ms) / GPU bubble ratio: 54.61%
[rank1]:2025-11-09 19:37:07,825 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.8691  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:37:07,800 - INFO - Avg. fwd time: 7.8997 / Avg. bwd time: 23.5265 / Avg. batch time: 623.4041 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 19:37:07,836 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.8691  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank2]:2025-11-09 19:37:07,822 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.8691  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 19:37:07,835 - INFO -  step: 750  loss:  0.4741  grad_norm:  0.8691  memory: 26.98GiB(56.79%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:39:14,971 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:39:17,305 - INFO - Avg. fwd time: 11.4229 / Avg. bwd time: 45.6699 / Avg. batch time: 516.4509 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 19:39:17,381 - INFO - Avg. fwd time: 7.1591 / Avg. bwd time: 18.9861 / Avg. batch time: 548.0052 (ms) / GPU bubble ratio: 61.83%
[rank1]:2025-11-09 19:39:17,421 - INFO - Avg. fwd time: 9.1218 / Avg. bwd time: 24.1875 / Avg. batch time: 586.9704 (ms) / GPU bubble ratio: 54.60%
[rank0]:2025-11-09 19:39:17,428 - INFO - Avg. fwd time: 7.9001 / Avg. bwd time: 23.5276 / Avg. batch time: 623.3861 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 19:39:17,600 - INFO -  step: 800  loss: -4.0000  grad_norm:  4.4748  memory: 11.81GiB(24.85%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank2]:2025-11-09 19:39:17,601 - INFO -  final step: 800  loss: -4.0000  grad_norm:  4.4748  tps: 6,692  tflops: 50.97  mfu: 14.81%
[rank2]:2025-11-09 19:39:17,601 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 19:39:17,602 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 19:39:17,604 - INFO -  step: 800  loss: -4.0000  grad_norm:  4.4748  memory: 14.64GiB(30.82%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank1]:2025-11-09 19:39:17,604 - INFO -  final step: 800  loss: -4.0000  grad_norm:  4.4748  tps: 6,692  tflops: 50.97  mfu: 14.81%
[rank1]:2025-11-09 19:39:17,604 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 19:39:17,605 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 19:39:17,615 - INFO -  step: 800  loss: -4.0000  grad_norm:  4.4748  memory: 16.57GiB(34.88%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-09 19:39:17,615 - INFO -  final step: 800  loss: -4.0000  grad_norm:  4.4748  tps: 6,692  tflops: 50.96  mfu: 14.81%
[rank0]:2025-11-09 19:39:17,615 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 19:39:17,616 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 19:39:17,613 - INFO -  step: 800  loss:  0.4681  grad_norm:  4.4748  memory: 26.98GiB(56.79%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 19:39:17,614 - INFO -  final step: 800  loss:  0.4681  grad_norm:  4.4748  tps: 6,699  tflops: 51.02  mfu: 14.92%
[rank3]:2025-11-09 19:39:17,614 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 19:39:17,615 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 19:39:19,691 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 19:39:19,691 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 19:39:19,677 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 19:39:19,691 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-09 19:39:19,814 - INFO - Process group destroyed
[rank1]:2025-11-09 19:39:19,847 - INFO - Process group destroyed
[rank3]:2025-11-09 19:39:19,831 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1939_real_final800_rank3.svg
[rank3]:> Batch Time: 623.14 ms, GPU Bubble Ratio: 59.37%, 57.11%, 66.30%, 26.58%
[rank3]:2025-11-09 19:39:19,971 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/pipeline_schedule/251109_1939_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.89 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 19:39:19,972 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–â–†â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–„â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–„â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44588
[rank3]:wandb:               final/avg_loss 0.46808
[rank3]:wandb:             final/avg_mfu(%) 14.92401
[rank3]:wandb:             final/avg_tflops 51.01815
[rank3]:wandb:    final/avg_throughput(tps) 6698.62436
[rank3]:wandb:              final/grad_norm 4.47484
[rank3]:wandb:               final/max_loss 0.46808
[rank3]:wandb:                    grad_norm 4.47484
[rank3]:wandb: loss_metrics/global_avg_loss 0.46808
[rank3]:wandb: loss_metrics/global_max_loss 0.46808
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed42_lr3e-6_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/fqstz3ao
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_lr3e-6_dm1/20251109-1904/wandb/run-20251109_190437-fqstz3ao/logs
[rank3]:2025-11-09 19:39:21,360 - INFO - Process group destroyed
[rank0]:2025-11-09 19:39:21,691 - INFO - Training completed
[rank0]:2025-11-09 19:39:21,692 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 19:39:21,850 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.3', 'tok_embeddings', 'layers.0', 'layers.2'}
[rank2]:Stage 2: Modules to keep: {'layers.11', 'layers.10', 'layers.9', 'layers.12'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.5', 'layers.8', 'layers.6', 'layers.7'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.14', 'norm', 'output', 'layers.15'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 3e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.001
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed42_lr3e-6_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
