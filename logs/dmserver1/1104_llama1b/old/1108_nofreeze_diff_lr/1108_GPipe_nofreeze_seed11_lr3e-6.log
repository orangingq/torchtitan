
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 19:39:23 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed11_lr3e-6.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=3e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 19:39:29,883 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank3]:2025-11-09 19:39:29,883 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank2]:2025-11-09 19:39:29,967 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-09 19:39:30,099 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 19:39:30,101 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 19:39:30,121 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank0]:2025-11-09 19:39:30,103 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 19:39:30,105 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 19:39:30,107 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 19:39:30,108 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-09 19:39:30,202 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 19:39:30,204 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 19:39:30,311 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 19:39:30,313 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 19:39:30,497 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 19:39:33,290 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-09 19:39:33,474 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 19:39:33,440 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 19:39:33,477 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 19:39:33,478 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-09 19:39:33,515 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 19:39:33,542 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 19:39:33,542 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 19:39:33,504 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 19:39:33,504 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 19:39:33,728 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 19:39:33,729 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 19:39:33,729 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-09 19:39:33,696 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 19:39:33,696 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 19:39:33,696 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 19:39:33,926 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 19:39:33,962 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 19:39:33,989 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 19:39:33,989 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 19:39:34,174 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 19:39:34,174 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 19:39:34,175 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run ga8t6kh5
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/20251109-1939/wandb/run-20251109_193934-ga8t6kh5
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/ga8t6kh5
[rank3]:2025-11-09 19:39:35,961 - INFO - WandB logging enabled
[rank3]:2025-11-09 19:39:35,962 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 19:39:36,001 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 19:39:36,031 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 19:39:36,031 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-09 19:39:36,215 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 19:39:36,215 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 19:39:36,216 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 19:39:36,233 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 19:39:36,233 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 19:39:36,233 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 19:39:36,233 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank0]:2025-11-09 19:39:36,233 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 19:39:36,233 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 19:39:36,234 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank0]:2025-11-09 19:39:38,815 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 19:39:38,815 - INFO - Finished loading the checkpoint in 2.58 seconds.
[rank0]:2025-11-09 19:39:38,815 - INFO - Training starts at step 1
[rank3]:2025-11-09 19:39:41,912 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5942  memory: 24.19GiB(50.91%)  tps: 2,773  tflops: 21.12  mfu: 6.77%
[rank3]:2025-11-09 19:39:41,912 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 19:39:41,903 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5942  memory:  9.99GiB(21.03%)  tps: 1,953  tflops: 14.88  mfu: 4.77%
[rank2]:2025-11-09 19:39:41,903 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 19:39:41,909 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5942  memory: 12.38GiB(26.05%)  tps: 2,062  tflops: 15.70  mfu: 5.03%
[rank1]:2025-11-09 19:39:41,909 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 19:39:41,939 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5942  memory: 12.80GiB(26.95%)  tps: 1,936  tflops: 14.75  mfu: 4.73%
[rank0]:2025-11-09 19:39:41,939 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 19:41:45,567 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:41:47,864 - INFO - Avg. fwd time: 11.4539 / Avg. bwd time: 45.0531 / Avg. batch time: 511.9192 (ms) / GPU bubble ratio: 11.69%
[rank2]:2025-11-09 19:41:47,937 - INFO - Avg. fwd time: 7.1809 / Avg. bwd time: 18.8098 / Avg. batch time: 543.8128 (ms) / GPU bubble ratio: 61.77%
[rank1]:2025-11-09 19:41:47,976 - INFO - Avg. fwd time: 9.1366 / Avg. bwd time: 23.9796 / Avg. batch time: 583.3933 (ms) / GPU bubble ratio: 54.59%
[rank0]:2025-11-09 19:41:47,984 - INFO - Avg. fwd time: 7.8971 / Avg. bwd time: 23.4182 / Avg. batch time: 620.5366 (ms) / GPU bubble ratio: 59.63%
[rank2]:2025-11-09 19:41:48,160 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.7565  memory: 11.81GiB(24.85%)  tps: 6,359  tflops: 48.43  mfu: 15.52%
[rank1]:2025-11-09 19:41:48,164 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.7565  memory: 14.64GiB(30.82%)  tps: 6,359  tflops: 48.43  mfu: 15.52%
[rank0]:2025-11-09 19:41:48,175 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.7565  memory: 16.57GiB(34.88%)  tps: 6,360  tflops: 48.44  mfu: 15.52%
[rank3]:2025-11-09 19:41:48,172 - INFO -  step: 50  loss:  8.8541  grad_norm: 26.7565  memory: 26.98GiB(56.79%)  tps: 6,359  tflops: 48.43  mfu: 15.52%
[rank0]:2025-11-09 19:43:55,536 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:43:57,869 - INFO - Avg. fwd time: 11.4862 / Avg. bwd time: 45.4810 / Avg. batch time: 515.5269 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 19:43:57,946 - INFO - Avg. fwd time: 7.1832 / Avg. bwd time: 18.8773 / Avg. batch time: 547.0569 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 19:43:57,986 - INFO - Avg. fwd time: 9.1470 / Avg. bwd time: 24.0619 / Avg. batch time: 586.2028 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 19:43:57,994 - INFO - Avg. fwd time: 7.9092 / Avg. bwd time: 23.4730 / Avg. batch time: 622.9153 (ms) / GPU bubble ratio: 59.70%
[rank2]:2025-11-09 19:43:58,172 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.8121  memory: 11.81GiB(24.85%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank3]:2025-11-09 19:43:58,185 - INFO -  step: 100  loss:  4.8646  grad_norm: 18.8121  memory: 26.98GiB(56.79%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank1]:2025-11-09 19:43:58,176 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.8121  memory: 14.64GiB(30.82%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank0]:2025-11-09 19:43:58,187 - INFO -  step: 100  loss: -4.0000  grad_norm: 18.8121  memory: 16.57GiB(34.88%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank3]:2025-11-09 19:43:58,360 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_1943_real_step100_rank3.svg
[rank3]:> Batch Time: 624.66 ms, GPU Bubble Ratio: 59.41%, 57.25%, 66.41%, 26.28%
[rank0]:2025-11-09 19:46:05,643 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:46:08,131 - INFO - Avg. fwd time: 11.4771 / Avg. bwd time: 45.6163 / Avg. batch time: 516.4967 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 19:46:08,158 - INFO - Avg. fwd time: 7.1785 / Avg. bwd time: 18.9049 / Avg. batch time: 548.1528 (ms) / GPU bubble ratio: 61.93%
[rank1]:2025-11-09 19:46:08,188 - INFO - Avg. fwd time: 9.1424 / Avg. bwd time: 24.0850 / Avg. batch time: 587.1535 (ms) / GPU bubble ratio: 54.73%
[rank0]:2025-11-09 19:46:08,218 - INFO - Avg. fwd time: 7.9118 / Avg. bwd time: 23.4917 / Avg. batch time: 623.7483 (ms) / GPU bubble ratio: 59.72%
[rank3]:2025-11-09 19:46:08,252 - INFO -  step: 150  loss:  6.1305  grad_norm: 13.5859  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank2]:2025-11-09 19:46:08,240 - INFO -  step: 150  loss: -4.0000  grad_norm: 13.5859  memory: 11.81GiB(24.85%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank1]:2025-11-09 19:46:08,243 - INFO -  step: 150  loss: -4.0000  grad_norm: 13.5859  memory: 14.64GiB(30.82%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank0]:2025-11-09 19:46:08,254 - INFO -  step: 150  loss: -4.0000  grad_norm: 13.5859  memory: 16.57GiB(34.88%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank0]:2025-11-09 19:48:14,938 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:48:17,261 - INFO - Avg. fwd time: 11.4681 / Avg. bwd time: 45.5874 / Avg. batch time: 516.1712 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 19:48:17,335 - INFO - Avg. fwd time: 7.1733 / Avg. bwd time: 18.9137 / Avg. batch time: 547.7206 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-09 19:48:17,375 - INFO - Avg. fwd time: 9.1362 / Avg. bwd time: 24.0908 / Avg. batch time: 586.6525 (ms) / GPU bubble ratio: 54.69%
[rank0]:2025-11-09 19:48:17,383 - INFO - Avg. fwd time: 7.9067 / Avg. bwd time: 23.4950 / Avg. batch time: 623.1859 (ms) / GPU bubble ratio: 59.69%
[rank2]:2025-11-09 19:48:17,559 - INFO -  step: 200  loss: -4.0000  grad_norm: 74.7023  memory: 11.81GiB(24.85%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank1]:2025-11-09 19:48:17,562 - INFO -  step: 200  loss: -4.0000  grad_norm: 74.7023  memory: 14.64GiB(30.82%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank0]:2025-11-09 19:48:17,573 - INFO -  step: 200  loss: -4.0000  grad_norm: 74.7023  memory: 16.57GiB(34.88%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank3]:2025-11-09 19:48:17,571 - INFO -  step: 200  loss:  2.7605  grad_norm: 74.7023  memory: 26.98GiB(56.79%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank3]:2025-11-09 19:48:17,721 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_1948_real_step200_rank3.svg
[rank3]:> Batch Time: 620.62 ms, GPU Bubble Ratio: 59.25%, 57.09%, 66.27%, 26.36%
[rank0]:2025-11-09 19:50:24,714 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:50:27,009 - INFO - Avg. fwd time: 11.4585 / Avg. bwd time: 45.6000 / Avg. batch time: 516.1878 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-09 19:50:27,130 - INFO - Avg. fwd time: 7.9056 / Avg. bwd time: 23.5016 / Avg. batch time: 623.1494 (ms) / GPU bubble ratio: 59.68%
[rank1]:2025-11-09 19:50:27,122 - INFO - Avg. fwd time: 9.1302 / Avg. bwd time: 24.1032 / Avg. batch time: 586.6580 (ms) / GPU bubble ratio: 54.68%
[rank2]:2025-11-09 19:50:27,083 - INFO - Avg. fwd time: 7.1699 / Avg. bwd time: 18.9253 / Avg. batch time: 547.7878 (ms) / GPU bubble ratio: 61.89%
[rank3]:2025-11-09 19:50:27,316 - INFO -  step: 250  loss:  1.8032  grad_norm: 16.8037  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:50:27,318 - INFO -  step: 250  loss: -4.0000  grad_norm: 16.8037  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-09 19:50:27,307 - INFO -  step: 250  loss: -4.0000  grad_norm: 16.8037  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank2]:2025-11-09 19:50:27,303 - INFO -  step: 250  loss: -4.0000  grad_norm: 16.8037  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:52:34,393 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:52:36,934 - INFO - Avg. fwd time: 11.4514 / Avg. bwd time: 45.6229 / Avg. batch time: 516.3064 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-09 19:52:37,021 - INFO - Avg. fwd time: 7.9071 / Avg. bwd time: 23.5116 / Avg. batch time: 623.1635 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 19:52:36,960 - INFO - Avg. fwd time: 7.1676 / Avg. bwd time: 18.9429 / Avg. batch time: 547.8575 (ms) / GPU bubble ratio: 61.87%
[rank2]:2025-11-09 19:52:37,043 - INFO -  step: 300  loss: -4.0000  grad_norm: 17.2487  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-09 19:52:36,991 - INFO - Avg. fwd time: 9.1270 / Avg. bwd time: 24.1232 / Avg. batch time: 586.6964 (ms) / GPU bubble ratio: 54.66%
[rank1]:2025-11-09 19:52:37,047 - INFO -  step: 300  loss: -4.0000  grad_norm: 17.2487  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 19:52:37,056 - INFO -  step: 300  loss:  1.1460  grad_norm: 17.2487  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:52:37,057 - INFO -  step: 300  loss: -4.0000  grad_norm: 17.2487  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 19:52:37,206 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_1952_real_step300_rank3.svg
[rank3]:> Batch Time: 622.16 ms, GPU Bubble Ratio: 59.26%, 57.02%, 66.20%, 26.42%
[rank0]:2025-11-09 19:54:44,411 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:54:46,709 - INFO - Avg. fwd time: 11.4483 / Avg. bwd time: 45.6462 / Avg. batch time: 516.4640 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 19:54:46,784 - INFO - Avg. fwd time: 7.1662 / Avg. bwd time: 18.9564 / Avg. batch time: 548.0689 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-09 19:54:46,824 - INFO - Avg. fwd time: 9.1246 / Avg. bwd time: 24.1348 / Avg. batch time: 586.8905 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 19:54:46,831 - INFO - Avg. fwd time: 7.9061 / Avg. bwd time: 23.5179 / Avg. batch time: 623.3436 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 19:54:47,008 - INFO -  step: 350  loss: -4.0000  grad_norm:  6.1356  memory: 11.81GiB(24.85%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank1]:2025-11-09 19:54:47,012 - INFO -  step: 350  loss: -4.0000  grad_norm:  6.1356  memory: 14.64GiB(30.82%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-09 19:54:47,022 - INFO -  step: 350  loss: -4.0000  grad_norm:  6.1356  memory: 16.57GiB(34.88%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-09 19:54:47,021 - INFO -  step: 350  loss:  0.7567  grad_norm:  6.1356  memory: 26.98GiB(56.79%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-09 19:56:54,350 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:56:56,686 - INFO - Avg. fwd time: 11.4437 / Avg. bwd time: 45.6716 / Avg. batch time: 516.6261 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 19:56:56,760 - INFO - Avg. fwd time: 7.1645 / Avg. bwd time: 18.9684 / Avg. batch time: 548.1988 (ms) / GPU bubble ratio: 61.86%
[rank1]:2025-11-09 19:56:56,800 - INFO - Avg. fwd time: 9.1225 / Avg. bwd time: 24.1460 / Avg. batch time: 587.0062 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 19:56:56,807 - INFO - Avg. fwd time: 7.9061 / Avg. bwd time: 23.5239 / Avg. batch time: 623.4449 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 19:56:56,983 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.7858  memory: 11.81GiB(24.85%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank1]:2025-11-09 19:56:56,987 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.7858  memory: 14.64GiB(30.82%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank0]:2025-11-09 19:56:56,998 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.7858  memory: 16.57GiB(34.88%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank3]:2025-11-09 19:56:56,996 - INFO -  step: 400  loss:  0.6018  grad_norm:  4.7858  memory: 26.98GiB(56.79%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank3]:2025-11-09 19:56:57,147 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_1956_real_step400_rank3.svg
[rank3]:> Batch Time: 623.64 ms, GPU Bubble Ratio: 59.35%, 57.14%, 66.29%, 26.48%
[rank3]:2025-11-09 19:57:05,918 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 19:57:06,141 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 19:57:06,167 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 19:57:06,194 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 19:59:04,117 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 19:59:06,626 - INFO - Avg. fwd time: 11.4352 / Avg. bwd time: 45.6752 / Avg. batch time: 516.5911 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 19:59:06,653 - INFO - Avg. fwd time: 7.1624 / Avg. bwd time: 18.9761 / Avg. batch time: 548.1998 (ms) / GPU bubble ratio: 61.86%
[rank0]:2025-11-09 19:59:06,715 - INFO - Avg. fwd time: 7.9092 / Avg. bwd time: 23.5268 / Avg. batch time: 623.4388 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-09 19:59:06,737 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.5976  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-09 19:59:06,684 - INFO - Avg. fwd time: 9.1193 / Avg. bwd time: 24.1503 / Avg. batch time: 586.9962 (ms) / GPU bubble ratio: 54.66%
[rank1]:2025-11-09 19:59:06,741 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.5976  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 19:59:06,754 - INFO -  step: 450  loss:  0.5969  grad_norm:  2.5976  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 19:59:06,752 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.5976  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 20:01:13,407 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:01:15,729 - INFO - Avg. fwd time: 11.4351 / Avg. bwd time: 45.6531 / Avg. batch time: 516.4111 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 20:01:15,803 - INFO - Avg. fwd time: 7.1605 / Avg. bwd time: 18.9779 / Avg. batch time: 547.9880 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 20:01:15,842 - INFO - Avg. fwd time: 9.1167 / Avg. bwd time: 24.1489 / Avg. batch time: 586.7700 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 20:01:15,850 - INFO - Avg. fwd time: 7.9073 / Avg. bwd time: 23.5256 / Avg. batch time: 623.2005 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 20:01:16,022 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.1560  memory: 11.81GiB(24.85%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank1]:2025-11-09 20:01:16,026 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.1560  memory: 14.64GiB(30.82%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 20:01:16,034 - INFO -  step: 500  loss:  0.4575  grad_norm:  1.1560  memory: 26.98GiB(56.79%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank0]:2025-11-09 20:01:16,036 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.1560  memory: 16.57GiB(34.88%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 20:01:16,188 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_2001_real_step500_rank3.svg
[rank3]:> Batch Time: 620.09 ms, GPU Bubble Ratio: 59.22%, 57.02%, 66.17%, 26.59%
[rank0]:2025-11-09 20:03:23,122 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:03:25,429 - INFO - Avg. fwd time: 11.4363 / Avg. bwd time: 45.6469 / Avg. batch time: 516.3667 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 20:03:25,502 - INFO - Avg. fwd time: 7.1595 / Avg. bwd time: 18.9813 / Avg. batch time: 547.9709 (ms) / GPU bubble ratio: 61.84%
[rank0]:2025-11-09 20:03:25,550 - INFO - Avg. fwd time: 7.9060 / Avg. bwd time: 23.5258 / Avg. batch time: 623.1673 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 20:03:25,542 - INFO - Avg. fwd time: 9.1157 / Avg. bwd time: 24.1508 / Avg. batch time: 586.7460 (ms) / GPU bubble ratio: 54.64%
[rank2]:2025-11-09 20:03:25,719 - INFO -  step: 550  loss: -4.0000  grad_norm:  1.8779  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank1]:2025-11-09 20:03:25,723 - INFO -  step: 550  loss: -4.0000  grad_norm:  1.8779  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank0]:2025-11-09 20:03:25,733 - INFO -  step: 550  loss: -4.0000  grad_norm:  1.8779  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 20:03:25,732 - INFO -  step: 550  loss:  0.4741  grad_norm:  1.8779  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank0]:2025-11-09 20:05:33,092 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:05:35,630 - INFO - Avg. fwd time: 11.4370 / Avg. bwd time: 45.6609 / Avg. batch time: 516.4836 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 20:05:35,659 - INFO - Avg. fwd time: 7.1595 / Avg. bwd time: 18.9858 / Avg. batch time: 548.0679 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 20:05:35,691 - INFO - Avg. fwd time: 9.1167 / Avg. bwd time: 24.1540 / Avg. batch time: 586.8486 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 20:05:35,722 - INFO - Avg. fwd time: 7.9062 / Avg. bwd time: 23.5279 / Avg. batch time: 623.2692 (ms) / GPU bubble ratio: 59.65%
[rank3]:2025-11-09 20:05:35,757 - INFO -  step: 600  loss:  0.4852  grad_norm:  2.6687  memory: 26.98GiB(56.79%)  tps: 6,300  tflops: 47.99  mfu: 15.38%
[rank2]:2025-11-09 20:05:35,744 - INFO -  step: 600  loss: -4.0000  grad_norm:  2.6687  memory: 11.81GiB(24.85%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank1]:2025-11-09 20:05:35,747 - INFO -  step: 600  loss: -4.0000  grad_norm:  2.6687  memory: 14.64GiB(30.82%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-09 20:05:35,758 - INFO -  step: 600  loss: -4.0000  grad_norm:  2.6687  memory: 16.57GiB(34.88%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-09 20:05:35,912 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_2005_real_step600_rank3.svg
[rank3]:> Batch Time: 624.62 ms, GPU Bubble Ratio: 59.42%, 57.20%, 66.33%, 26.61%
[rank0]:2025-11-09 20:07:43,546 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:07:45,852 - INFO - Avg. fwd time: 11.4400 / Avg. bwd time: 45.6784 / Avg. batch time: 516.6463 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 20:07:45,928 - INFO - Avg. fwd time: 7.1600 / Avg. bwd time: 18.9904 / Avg. batch time: 548.2651 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 20:07:45,968 - INFO - Avg. fwd time: 9.1187 / Avg. bwd time: 24.1598 / Avg. batch time: 587.0584 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 20:07:45,975 - INFO - Avg. fwd time: 7.9077 / Avg. bwd time: 23.5305 / Avg. batch time: 623.4753 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-09 20:07:46,146 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.6121  memory: 11.81GiB(24.85%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank1]:2025-11-09 20:07:46,150 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.6121  memory: 14.64GiB(30.82%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank0]:2025-11-09 20:07:46,161 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.6121  memory: 16.57GiB(34.88%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank3]:2025-11-09 20:07:46,159 - INFO -  step: 650  loss:  0.4167  grad_norm:  0.6121  memory: 26.98GiB(56.79%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank0]:2025-11-09 20:09:53,668 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:09:55,995 - INFO - Avg. fwd time: 11.4422 / Avg. bwd time: 45.6938 / Avg. batch time: 516.7871 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 20:09:56,071 - INFO - Avg. fwd time: 7.1603 / Avg. bwd time: 18.9944 / Avg. batch time: 548.3899 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 20:09:56,111 - INFO - Avg. fwd time: 9.1201 / Avg. bwd time: 24.1655 / Avg. batch time: 587.1955 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 20:09:56,119 - INFO - Avg. fwd time: 7.9078 / Avg. bwd time: 23.5320 / Avg. batch time: 623.6091 (ms) / GPU bubble ratio: 59.67%
[rank3]:2025-11-09 20:09:56,303 - INFO -  step: 700  loss:  0.4868  grad_norm:  3.5826  memory: 26.98GiB(56.79%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank2]:2025-11-09 20:09:56,291 - INFO -  step: 700  loss: -4.0000  grad_norm:  3.5826  memory: 11.81GiB(24.85%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank1]:2025-11-09 20:09:56,294 - INFO -  step: 700  loss: -4.0000  grad_norm:  3.5826  memory: 14.64GiB(30.82%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank0]:2025-11-09 20:09:56,306 - INFO -  step: 700  loss: -4.0000  grad_norm:  3.5826  memory: 16.57GiB(34.88%)  tps: 6,295  tflops: 47.94  mfu: 15.37%
[rank3]:2025-11-09 20:09:56,461 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_2009_real_step700_rank3.svg
[rank3]:> Batch Time: 624.61 ms, GPU Bubble Ratio: 59.51%, 57.22%, 66.36%, 26.62%
[rank0]:2025-11-09 20:12:03,604 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:12:06,107 - INFO - Avg. fwd time: 11.4451 / Avg. bwd time: 45.6894 / Avg. batch time: 516.7752 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 20:12:06,136 - INFO - Avg. fwd time: 7.1599 / Avg. bwd time: 18.9963 / Avg. batch time: 548.3994 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 20:12:06,169 - INFO - Avg. fwd time: 9.1199 / Avg. bwd time: 24.1682 / Avg. batch time: 587.2142 (ms) / GPU bubble ratio: 54.65%
[rank1]:2025-11-09 20:12:06,227 - INFO -  step: 750  loss: -4.0000  grad_norm:  2.3574  memory: 14.64GiB(30.82%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 20:12:06,202 - INFO - Avg. fwd time: 7.9065 / Avg. bwd time: 23.5322 / Avg. batch time: 623.6227 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 20:12:06,238 - INFO -  step: 750  loss: -4.0000  grad_norm:  2.3574  memory: 16.57GiB(34.88%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-09 20:12:06,237 - INFO -  step: 750  loss:  0.4629  grad_norm:  2.3574  memory: 26.98GiB(56.79%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-09 20:12:06,224 - INFO -  step: 750  loss: -4.0000  grad_norm:  2.3574  memory: 11.81GiB(24.85%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 20:14:13,376 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:14:15,716 - INFO - Avg. fwd time: 11.4463 / Avg. bwd time: 45.6873 / Avg. batch time: 516.7655 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 20:14:15,792 - INFO - Avg. fwd time: 7.1596 / Avg. bwd time: 18.9977 / Avg. batch time: 548.3722 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 20:14:15,832 - INFO - Avg. fwd time: 9.1203 / Avg. bwd time: 24.1707 / Avg. batch time: 587.1965 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 20:14:15,839 - INFO - Avg. fwd time: 7.9059 / Avg. bwd time: 23.5323 / Avg. batch time: 623.6000 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 20:14:16,024 - INFO -  step: 800  loss: -4.0000  grad_norm:  3.8651  memory: 16.57GiB(34.88%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank0]:2025-11-09 20:14:16,024 - INFO -  final step: 800  loss: -4.0000  grad_norm:  3.8651  tps: 6,687  tflops: 50.93  mfu: 14.78%
[rank0]:2025-11-09 20:14:16,024 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 20:14:16,025 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 20:14:16,022 - INFO -  step: 800  loss:  0.4665  grad_norm:  3.8651  memory: 26.98GiB(56.79%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank3]:2025-11-09 20:14:16,023 - INFO -  final step: 800  loss:  0.4665  grad_norm:  3.8651  tps: 6,695  tflops: 50.99  mfu: 14.90%
[rank3]:2025-11-09 20:14:16,023 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 20:14:16,026 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 20:14:16,010 - INFO -  step: 800  loss: -4.0000  grad_norm:  3.8651  memory: 11.81GiB(24.85%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank2]:2025-11-09 20:14:16,010 - INFO -  final step: 800  loss: -4.0000  grad_norm:  3.8651  tps: 6,687  tflops: 50.93  mfu: 14.78%
[rank2]:2025-11-09 20:14:16,010 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 20:14:16,010 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 20:14:16,013 - INFO -  step: 800  loss: -4.0000  grad_norm:  3.8651  memory: 14.64GiB(30.82%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank1]:2025-11-09 20:14:16,013 - INFO -  final step: 800  loss: -4.0000  grad_norm:  3.8651  tps: 6,689  tflops: 50.94  mfu: 14.80%
[rank1]:2025-11-09 20:14:16,013 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 20:14:16,014 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 20:14:18,222 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank2]:2025-11-09 20:14:18,236 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 20:14:18,265 - INFO - Process group destroyed
[rank1]:2025-11-09 20:14:18,236 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 20:14:18,294 - INFO - Process group destroyed
[rank0]:2025-11-09 20:14:18,236 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-09 20:14:18,378 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_2014_real_final800_rank3.svg
[rank3]:> Batch Time: 623.12 ms, GPU Bubble Ratio: 59.36%, 57.13%, 66.30%, 26.53%
[rank3]:2025-11-09 20:14:18,517 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/pipeline_schedule/251109_2014_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.09 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 20:14:18,518 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–„â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–„â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44703
[rank3]:wandb:               final/avg_loss 0.46646
[rank3]:wandb:             final/avg_mfu(%) 14.89947
[rank3]:wandb:             final/avg_tflops 50.99404
[rank3]:wandb:    final/avg_throughput(tps) 6695.45811
[rank3]:wandb:              final/grad_norm 3.86509
[rank3]:wandb:               final/max_loss 0.46646
[rank3]:wandb:                    grad_norm 3.86509
[rank3]:wandb: loss_metrics/global_avg_loss 0.46646
[rank3]:wandb: loss_metrics/global_max_loss 0.46646
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed11_lr3e-6_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/ga8t6kh5
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_lr3e-6_dm1/20251109-1939/wandb/run-20251109_193934-ga8t6kh5/logs
[rank3]:2025-11-09 20:14:19,960 - INFO - Process group destroyed
[rank0]:2025-11-09 20:14:20,236 - INFO - Training completed
[rank0]:2025-11-09 20:14:20,236 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 20:14:20,295 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.2', 'layers.1', 'layers.0', 'tok_embeddings'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.12', 'layers.9', 'layers.11'}
[rank1]:Stage 1: Modules to keep: {'layers.5', 'layers.6', 'layers.7', 'layers.8', 'layers.4'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'output', 'layers.15', 'layers.14', 'norm'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 3e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.001
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 11
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed11_lr3e-6_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
