
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 11. 09. (Ïùº) 20:14:22 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed55_lr3e-6.log
‚úîÔ∏èMain Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

‚úîÔ∏èRunning with nofreeze x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=3e-6  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
[rank1]:2025-11-09 20:14:28,376 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-09 20:14:28,469 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-09 20:14:28,504 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-09 20:14:28,551 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 20:14:28,553 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 20:14:28,544 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-09 20:14:28,677 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 20:14:28,679 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:14:28,765 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 20:14:28,767 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:14:28,773 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 20:14:28,774 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-09 20:14:28,793 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 20:14:28,795 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:14:29,166 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank1]:2025-11-09 20:14:32,040 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 20:14:32,078 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:14:32,104 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 20:14:32,104 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:14:32,342 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-09 20:14:32,301 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:14:32,301 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 20:14:32,302 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-09 20:14:32,491 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 20:14:32,407 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 20:14:32,444 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:14:32,471 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 20:14:32,471 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:14:32,531 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:14:32,533 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 20:14:32,560 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 20:14:32,560 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 20:14:32,649 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:14:32,649 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 20:14:32,649 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-09 20:14:32,771 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:14:32,772 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 20:14:32,773 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run vkyv89ll
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed55_lr3e-6_dm1/20251109-2014/wandb/run-20251109_201433-vkyv89ll
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed55_lr3e-6_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/vkyv89ll
[rank3]:2025-11-09 20:14:34,150 - INFO - WandB logging enabled
[rank3]:2025-11-09 20:14:34,150 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 20:14:34,189 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:14:34,220 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 20:14:34,220 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:14:34,428 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_lr3e-6_dm1
[rank0]:2025-11-09 20:14:34,428 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 20:14:34,428 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 20:14:34,429 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 20:14:34,427 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 20:14:34,427 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 20:14:34,410 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:14:34,411 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 20:14:34,411 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 20:14:34,427 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 20:14:36,915 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-11-09 20:14:36,915 - INFO - Finished loading the checkpoint in 2.49 seconds.
[rank0]:2025-11-09 20:14:36,915 - INFO - Training starts at step 1
[rank0]:2025-11-09 20:14:40,050 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5966  memory: 12.80GiB(26.95%)  tps: 2,179  tflops: 16.60  mfu: 5.32%
[rank0]:2025-11-09 20:14:40,051 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 20:14:40,016 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5966  memory: 12.38GiB(26.05%)  tps: 2,064  tflops: 15.72  mfu: 5.04%
[rank1]:2025-11-09 20:14:40,022 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 20:14:40,023 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5966  memory: 24.19GiB(50.91%)  tps: 2,810  tflops: 21.40  mfu: 6.86%
[rank3]:2025-11-09 20:14:40,023 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 20:14:40,013 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5966  memory:  9.99GiB(21.03%)  tps: 2,165  tflops: 16.49  mfu: 5.28%
[rank2]:2025-11-09 20:14:40,014 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 20:16:43,684 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:16:45,975 - INFO - Avg. fwd time: 11.4928 / Avg. bwd time: 45.0455 / Avg. batch time: 512.2156 (ms) / GPU bubble ratio: 11.70%
[rank2]:2025-11-09 20:16:46,047 - INFO - Avg. fwd time: 7.1784 / Avg. bwd time: 18.8103 / Avg. batch time: 544.1622 (ms) / GPU bubble ratio: 61.79%
[rank1]:2025-11-09 20:16:46,087 - INFO - Avg. fwd time: 9.1254 / Avg. bwd time: 23.9709 / Avg. batch time: 583.7061 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 20:16:46,095 - INFO - Avg. fwd time: 7.9117 / Avg. bwd time: 23.3899 / Avg. batch time: 620.6522 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 20:16:46,272 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.0538  memory: 11.81GiB(24.85%)  tps: 6,359  tflops: 48.43  mfu: 15.52%
[rank1]:2025-11-09 20:16:46,276 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.0538  memory: 14.64GiB(30.82%)  tps: 6,359  tflops: 48.43  mfu: 15.52%
[rank3]:2025-11-09 20:16:46,284 - INFO -  step: 50  loss:  8.8339  grad_norm: 26.0538  memory: 26.98GiB(56.79%)  tps: 6,358  tflops: 48.43  mfu: 15.52%
[rank0]:2025-11-09 20:16:46,286 - INFO -  step: 50  loss: -4.0000  grad_norm: 26.0538  memory: 16.57GiB(34.88%)  tps: 6,360  tflops: 48.44  mfu: 15.52%
