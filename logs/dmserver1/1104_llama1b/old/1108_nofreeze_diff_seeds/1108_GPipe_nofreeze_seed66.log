
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 18:16:23 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed66.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 18:16:29,449 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank3]:2025-11-09 18:16:29,430 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-09 18:16:29,495 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-09 18:16:29,498 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-09 18:16:29,651 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 18:16:29,653 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 18:16:29,656 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 18:16:29,658 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-09 18:16:29,659 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 18:16:29,661 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 18:16:29,796 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 18:16:29,799 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 18:16:29,800 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 18:16:29,802 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 18:16:30,051 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank1]:2025-11-09 18:16:33,053 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 18:16:33,060 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-09 18:16:33,092 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 18:16:33,119 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 18:16:33,119 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 18:16:33,211 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 18:16:33,250 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 18:16:33,252 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 18:16:33,279 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 18:16:33,279 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 18:16:33,303 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 18:16:33,303 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 18:16:33,304 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-09 18:16:33,465 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 18:16:33,465 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 18:16:33,465 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-09 18:16:34,036 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 18:16:34,074 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 18:16:34,100 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 18:16:34,101 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank2]:2025-11-09 18:16:34,280 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 18:16:34,280 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 18:16:34,281 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: setting up run 7keo0a50
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed66_dm1/20251109-1816/wandb/run-20251109_181634-7keo0a50
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed66_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/7keo0a50
[rank3]:2025-11-09 18:16:35,134 - INFO - WandB logging enabled
[rank3]:2025-11-09 18:16:35,135 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 18:16:35,173 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 18:16:35,201 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 18:16:35,202 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 18:16:35,399 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed66_dm1
[rank0]:2025-11-09 18:16:35,399 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 18:16:35,400 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 18:16:35,400 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 18:16:35,399 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 18:16:35,399 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 18:16:35,384 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 18:16:35,384 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 18:16:35,385 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 18:16:35,399 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 18:16:38,270 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 18:16:38,270 - INFO - Finished loading the checkpoint in 2.87 seconds.
[rank0]:2025-11-09 18:16:38,270 - INFO - Training starts at step 1
[rank1]:2025-11-09 18:16:41,465 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory: 12.38GiB(26.05%)  tps: 1,957  tflops: 14.90  mfu: 4.78%
[rank1]:2025-11-09 18:16:41,466 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 18:16:41,461 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory:  9.99GiB(21.03%)  tps: 2,218  tflops: 16.89  mfu: 5.41%
[rank2]:2025-11-09 18:16:41,461 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 18:16:41,505 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory: 12.80GiB(26.95%)  tps: 1,985  tflops: 15.12  mfu: 4.85%
[rank0]:2025-11-09 18:16:41,505 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 18:16:41,474 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5958  memory: 24.19GiB(50.91%)  tps: 2,601  tflops: 19.81  mfu: 6.35%
[rank3]:2025-11-09 18:16:41,474 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 18:18:45,404 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:18:47,690 - INFO - Avg. fwd time: 11.4749 / Avg. bwd time: 45.0249 / Avg. batch time: 512.0032 (ms) / GPU bubble ratio: 11.72%
[rank0]:2025-11-09 18:18:47,809 - INFO - Avg. fwd time: 7.9150 / Avg. bwd time: 23.4212 / Avg. batch time: 620.7398 (ms) / GPU bubble ratio: 59.61%
[rank1]:2025-11-09 18:18:47,802 - INFO - Avg. fwd time: 9.1151 / Avg. bwd time: 23.9175 / Avg. batch time: 583.5307 (ms) / GPU bubble ratio: 54.71%
[rank2]:2025-11-09 18:18:47,762 - INFO - Avg. fwd time: 7.1786 / Avg. bwd time: 18.8149 / Avg. batch time: 544.2129 (ms) / GPU bubble ratio: 61.79%
[rank3]:2025-11-09 18:18:47,998 - INFO -  step: 50  loss:  8.2840  grad_norm: 17.3632  memory: 26.98GiB(56.79%)  tps: 6,345  tflops: 48.33  mfu: 15.49%
[rank0]:2025-11-09 18:18:48,000 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3632  memory: 16.57GiB(34.88%)  tps: 6,347  tflops: 48.34  mfu: 15.49%
[rank1]:2025-11-09 18:18:47,989 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3632  memory: 14.64GiB(30.82%)  tps: 6,345  tflops: 48.33  mfu: 15.49%
[rank2]:2025-11-09 18:18:47,985 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3632  memory: 11.81GiB(24.85%)  tps: 6,345  tflops: 48.33  mfu: 15.49%
[rank0]:2025-11-09 18:20:55,560 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:20:57,894 - INFO - Avg. fwd time: 11.5178 / Avg. bwd time: 45.4231 / Avg. batch time: 515.3794 (ms) / GPU bubble ratio: 11.61%
[rank2]:2025-11-09 18:20:57,970 - INFO - Avg. fwd time: 7.1772 / Avg. bwd time: 18.8880 / Avg. batch time: 547.0605 (ms) / GPU bubble ratio: 61.88%
[rank1]:2025-11-09 18:20:58,010 - INFO - Avg. fwd time: 9.1306 / Avg. bwd time: 24.0168 / Avg. batch time: 586.0399 (ms) / GPU bubble ratio: 54.75%
[rank0]:2025-11-09 18:20:58,017 - INFO - Avg. fwd time: 7.9116 / Avg. bwd time: 23.4738 / Avg. batch time: 622.8339 (ms) / GPU bubble ratio: 59.69%
[rank2]:2025-11-09 18:20:58,197 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1207  memory: 11.81GiB(24.85%)  tps: 6,291  tflops: 47.92  mfu: 15.36%
[rank1]:2025-11-09 18:20:58,200 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1207  memory: 14.64GiB(30.82%)  tps: 6,291  tflops: 47.92  mfu: 15.36%
[rank3]:2025-11-09 18:20:58,209 - INFO -  step: 100  loss:  4.2811  grad_norm: 19.1207  memory: 26.98GiB(56.79%)  tps: 6,291  tflops: 47.92  mfu: 15.36%
[rank0]:2025-11-09 18:20:58,211 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1207  memory: 16.57GiB(34.88%)  tps: 6,291  tflops: 47.92  mfu: 15.36%
[rank3]:2025-11-09 18:20:58,395 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1820_real_step100_rank3.svg
[rank3]:> Batch Time: 623.67 ms, GPU Bubble Ratio: 59.40%, 57.22%, 66.36%, 26.41%
[rank0]:2025-11-09 18:23:05,749 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 18:23:08,265 - INFO - Avg. fwd time: 7.1719 / Avg. bwd time: 18.9088 / Avg. batch time: 547.9912 (ms) / GPU bubble ratio: 61.93%
[rank3]:2025-11-09 18:23:08,240 - INFO - Avg. fwd time: 11.5160 / Avg. bwd time: 45.5314 / Avg. batch time: 516.2159 (ms) / GPU bubble ratio: 11.59%
[rank1]:2025-11-09 18:23:08,296 - INFO - Avg. fwd time: 9.1315 / Avg. bwd time: 24.0445 / Avg. batch time: 586.8951 (ms) / GPU bubble ratio: 54.78%
[rank1]:2025-11-09 18:23:08,351 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.2565  memory: 14.64GiB(30.82%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank2]:2025-11-09 18:23:08,347 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.2565  memory: 11.81GiB(24.85%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank3]:2025-11-09 18:23:08,360 - INFO -  step: 150  loss:  3.5608  grad_norm: 24.2565  memory: 26.98GiB(56.79%)  tps: 6,294  tflops: 47.94  mfu: 15.37%
[rank0]:2025-11-09 18:23:08,326 - INFO - Avg. fwd time: 7.9253 / Avg. bwd time: 23.4865 / Avg. batch time: 623.5570 (ms) / GPU bubble ratio: 59.70%
[rank0]:2025-11-09 18:23:08,362 - INFO -  step: 150  loss: -4.0000  grad_norm: 24.2565  memory: 16.57GiB(34.88%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank0]:2025-11-09 18:25:15,543 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 18:25:17,943 - INFO - Avg. fwd time: 7.1681 / Avg. bwd time: 18.9199 / Avg. batch time: 548.0153 (ms) / GPU bubble ratio: 61.92%
[rank3]:2025-11-09 18:25:17,868 - INFO - Avg. fwd time: 11.5080 / Avg. bwd time: 45.5627 / Avg. batch time: 516.3751 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 18:25:17,982 - INFO - Avg. fwd time: 9.1269 / Avg. bwd time: 24.0638 / Avg. batch time: 586.8795 (ms) / GPU bubble ratio: 54.76%
[rank0]:2025-11-09 18:25:17,990 - INFO - Avg. fwd time: 7.9190 / Avg. bwd time: 23.4942 / Avg. batch time: 623.4789 (ms) / GPU bubble ratio: 59.69%
[rank1]:2025-11-09 18:25:18,170 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.9394  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank2]:2025-11-09 18:25:18,167 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.9394  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 18:25:18,179 - INFO -  step: 200  loss:  1.2606  grad_norm: 14.9394  memory: 26.98GiB(56.79%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 18:25:18,182 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.9394  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 18:25:18,346 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1825_real_step200_rank3.svg
[rank3]:> Batch Time: 620.70 ms, GPU Bubble Ratio: 59.20%, 57.02%, 66.23%, 26.50%
[rank0]:2025-11-09 18:27:25,456 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:27:27,753 - INFO - Avg. fwd time: 11.5016 / Avg. bwd time: 45.5740 / Avg. batch time: 516.3960 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 18:27:27,828 - INFO - Avg. fwd time: 7.1656 / Avg. bwd time: 18.9420 / Avg. batch time: 548.0899 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 18:27:27,867 - INFO - Avg. fwd time: 9.1254 / Avg. bwd time: 24.0920 / Avg. batch time: 586.9657 (ms) / GPU bubble ratio: 54.73%
[rank0]:2025-11-09 18:27:27,875 - INFO - Avg. fwd time: 7.9148 / Avg. bwd time: 23.5045 / Avg. batch time: 623.5177 (ms) / GPU bubble ratio: 59.69%
[rank2]:2025-11-09 18:27:28,049 - INFO -  step: 250  loss: -4.0000  grad_norm: 13.6156  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank1]:2025-11-09 18:27:28,053 - INFO -  step: 250  loss: -4.0000  grad_norm: 13.6156  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-09 18:27:28,061 - INFO -  step: 250  loss:  0.6625  grad_norm: 13.6156  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 18:27:28,064 - INFO -  step: 250  loss: -4.0000  grad_norm: 13.6156  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 18:29:35,103 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 18:29:37,668 - INFO - Avg. fwd time: 7.1637 / Avg. bwd time: 18.9563 / Avg. batch time: 548.0501 (ms) / GPU bubble ratio: 61.87%
[rank3]:2025-11-09 18:29:37,642 - INFO - Avg. fwd time: 11.4941 / Avg. bwd time: 45.5878 / Avg. batch time: 516.4296 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-09 18:29:37,699 - INFO - Avg. fwd time: 9.1248 / Avg. bwd time: 24.1108 / Avg. batch time: 586.9385 (ms) / GPU bubble ratio: 54.70%
[rank1]:2025-11-09 18:29:37,754 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6911  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-09 18:29:37,750 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6911  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:29:37,763 - INFO -  step: 300  loss:  0.5427  grad_norm:  0.6911  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank0]:2025-11-09 18:29:37,729 - INFO - Avg. fwd time: 7.9135 / Avg. bwd time: 23.5125 / Avg. batch time: 623.4568 (ms) / GPU bubble ratio: 59.68%
[rank0]:2025-11-09 18:29:37,765 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6911  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:29:37,916 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1829_real_step300_rank3.svg
[rank3]:> Batch Time: 623.22 ms, GPU Bubble Ratio: 59.32%, 57.06%, 66.27%, 26.67%
[rank0]:2025-11-09 18:31:45,037 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:31:47,334 - INFO - Avg. fwd time: 11.4927 / Avg. bwd time: 45.6009 / Avg. batch time: 516.5163 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:31:47,408 - INFO - Avg. fwd time: 7.1631 / Avg. bwd time: 18.9650 / Avg. batch time: 548.1746 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 18:31:47,456 - INFO - Avg. fwd time: 7.9101 / Avg. bwd time: 23.5189 / Avg. batch time: 623.5767 (ms) / GPU bubble ratio: 59.68%
[rank1]:2025-11-09 18:31:47,449 - INFO - Avg. fwd time: 9.1255 / Avg. bwd time: 24.1258 / Avg. batch time: 587.0773 (ms) / GPU bubble ratio: 54.69%
[rank2]:2025-11-09 18:31:47,631 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9078  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-09 18:31:47,644 - INFO -  step: 350  loss:  0.4891  grad_norm:  0.9078  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 18:31:47,645 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9078  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank1]:2025-11-09 18:31:47,635 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9078  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 18:33:54,712 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:33:57,037 - INFO - Avg. fwd time: 11.4858 / Avg. bwd time: 45.6037 / Avg. batch time: 516.4728 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:33:57,112 - INFO - Avg. fwd time: 7.1624 / Avg. bwd time: 18.9726 / Avg. batch time: 548.0745 (ms) / GPU bubble ratio: 61.85%
[rank0]:2025-11-09 18:33:57,160 - INFO - Avg. fwd time: 7.9076 / Avg. bwd time: 23.5236 / Avg. batch time: 623.4855 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 18:33:57,152 - INFO - Avg. fwd time: 9.1270 / Avg. bwd time: 24.1418 / Avg. batch time: 587.0031 (ms) / GPU bubble ratio: 54.66%
[rank2]:2025-11-09 18:33:57,336 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3786  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:33:57,351 - INFO -  step: 400  loss:  0.4754  grad_norm:  0.3786  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 18:33:57,350 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3786  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank1]:2025-11-09 18:33:57,339 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3786  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:33:57,511 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1833_real_step400_rank3.svg
[rank3]:> Batch Time: 623.20 ms, GPU Bubble Ratio: 59.37%, 57.03%, 66.30%, 26.70%
[rank3]:2025-11-09 18:34:06,245 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 18:34:06,466 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 18:34:06,492 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 18:34:06,518 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 18:36:04,258 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:36:06,773 - INFO - Avg. fwd time: 11.4781 / Avg. bwd time: 45.6037 / Avg. batch time: 516.4020 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:36:06,801 - INFO - Avg. fwd time: 7.1611 / Avg. bwd time: 18.9770 / Avg. batch time: 548.0314 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:36:06,832 - INFO - Avg. fwd time: 9.1275 / Avg. bwd time: 24.1535 / Avg. batch time: 586.9885 (ms) / GPU bubble ratio: 54.64%
[rank3]:2025-11-09 18:36:06,898 - INFO -  step: 450  loss:  0.5289  grad_norm:  0.4835  memory: 26.98GiB(56.79%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 18:36:06,863 - INFO - Avg. fwd time: 7.9039 / Avg. bwd time: 23.5259 / Avg. batch time: 623.4551 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 18:36:06,899 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4835  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank2]:2025-11-09 18:36:06,885 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4835  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank1]:2025-11-09 18:36:06,888 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4835  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 18:38:13,996 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:38:16,317 - INFO - Avg. fwd time: 11.4767 / Avg. bwd time: 45.6084 / Avg. batch time: 516.4271 (ms) / GPU bubble ratio: 11.57%
[rank1]:2025-11-09 18:38:16,431 - INFO - Avg. fwd time: 9.1272 / Avg. bwd time: 24.1616 / Avg. batch time: 586.9898 (ms) / GPU bubble ratio: 54.63%
[rank2]:2025-11-09 18:38:16,391 - INFO - Avg. fwd time: 7.1607 / Avg. bwd time: 18.9793 / Avg. batch time: 548.0181 (ms) / GPU bubble ratio: 61.84%
[rank0]:2025-11-09 18:38:16,439 - INFO - Avg. fwd time: 7.9024 / Avg. bwd time: 23.5275 / Avg. batch time: 623.4442 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 18:38:16,615 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3398  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-09 18:38:16,611 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3398  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 18:38:16,626 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3398  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:38:16,623 - INFO -  step: 500  loss:  0.4025  grad_norm:  0.3398  memory: 26.98GiB(56.79%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 18:38:16,773 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1838_real_step500_rank3.svg
[rank3]:> Batch Time: 622.69 ms, GPU Bubble Ratio: 59.35%, 57.02%, 66.28%, 26.61%
[rank0]:2025-11-09 18:40:23,872 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:40:26,177 - INFO - Avg. fwd time: 11.4761 / Avg. bwd time: 45.6134 / Avg. batch time: 516.4581 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:40:26,251 - INFO - Avg. fwd time: 7.1603 / Avg. bwd time: 18.9801 / Avg. batch time: 548.0639 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:40:26,291 - INFO - Avg. fwd time: 9.1271 / Avg. bwd time: 24.1683 / Avg. batch time: 587.0497 (ms) / GPU bubble ratio: 54.63%
[rank0]:2025-11-09 18:40:26,299 - INFO - Avg. fwd time: 7.9015 / Avg. bwd time: 23.5280 / Avg. batch time: 623.4928 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 18:40:26,469 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4039  memory: 11.81GiB(24.85%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 18:40:26,472 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4039  memory: 14.64GiB(30.82%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 18:40:26,481 - INFO -  step: 550  loss:  0.4435  grad_norm:  0.4039  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 18:40:26,483 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4039  memory: 16.57GiB(34.88%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 18:42:33,450 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:42:35,983 - INFO - Avg. fwd time: 11.4742 / Avg. bwd time: 45.6107 / Avg. batch time: 516.4187 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:42:36,011 - INFO - Avg. fwd time: 7.1597 / Avg. bwd time: 18.9806 / Avg. batch time: 547.9894 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:42:36,043 - INFO - Avg. fwd time: 9.1260 / Avg. bwd time: 24.1725 / Avg. batch time: 586.9802 (ms) / GPU bubble ratio: 54.62%
[rank3]:2025-11-09 18:42:36,108 - INFO -  step: 600  loss:  0.4494  grad_norm:  0.3764  memory: 26.98GiB(56.79%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 18:42:36,075 - INFO - Avg. fwd time: 7.8998 / Avg. bwd time: 23.5288 / Avg. batch time: 623.4137 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 18:42:36,110 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3764  memory: 16.57GiB(34.88%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank2]:2025-11-09 18:42:36,096 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3764  memory: 11.81GiB(24.85%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-09 18:42:36,100 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3764  memory: 14.64GiB(30.82%)  tps: 6,320  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 18:42:36,264 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1842_real_step600_rank3.svg
[rank3]:> Batch Time: 622.66 ms, GPU Bubble Ratio: 59.36%, 57.06%, 66.32%, 26.74%
[rank0]:2025-11-09 18:44:43,313 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:44:45,600 - INFO - Avg. fwd time: 11.4735 / Avg. bwd time: 45.6126 / Avg. batch time: 516.4263 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:44:45,673 - INFO - Avg. fwd time: 7.1594 / Avg. bwd time: 18.9806 / Avg. batch time: 548.0188 (ms) / GPU bubble ratio: 61.84%
[rank0]:2025-11-09 18:44:45,721 - INFO - Avg. fwd time: 7.8981 / Avg. bwd time: 23.5296 / Avg. batch time: 623.4336 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 18:44:45,714 - INFO - Avg. fwd time: 9.1255 / Avg. bwd time: 24.1757 / Avg. batch time: 587.0083 (ms) / GPU bubble ratio: 54.62%
[rank2]:2025-11-09 18:44:45,890 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3748  memory: 11.81GiB(24.85%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank0]:2025-11-09 18:44:45,905 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3748  memory: 16.57GiB(34.88%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank1]:2025-11-09 18:44:45,894 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3748  memory: 14.64GiB(30.82%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank3]:2025-11-09 18:44:45,903 - INFO -  step: 650  loss:  0.3805  grad_norm:  0.3748  memory: 26.98GiB(56.79%)  tps: 6,312  tflops: 48.07  mfu: 15.41%
[rank0]:2025-11-09 18:46:52,917 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:46:55,236 - INFO - Avg. fwd time: 11.4720 / Avg. bwd time: 45.6122 / Avg. batch time: 516.4086 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:46:55,310 - INFO - Avg. fwd time: 7.1591 / Avg. bwd time: 18.9806 / Avg. batch time: 547.9724 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:46:55,350 - INFO - Avg. fwd time: 9.1243 / Avg. bwd time: 24.1779 / Avg. batch time: 586.9525 (ms) / GPU bubble ratio: 54.61%
[rank0]:2025-11-09 18:46:55,358 - INFO - Avg. fwd time: 7.8968 / Avg. bwd time: 23.5298 / Avg. batch time: 623.3680 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 18:46:55,533 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4193  memory: 14.64GiB(30.82%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 18:46:55,542 - INFO -  step: 700  loss:  0.4460  grad_norm:  0.4193  memory: 26.98GiB(56.79%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-09 18:46:55,544 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4193  memory: 16.57GiB(34.88%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank2]:2025-11-09 18:46:55,530 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4193  memory: 11.81GiB(24.85%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank3]:2025-11-09 18:46:55,694 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1846_real_step700_rank3.svg
[rank3]:> Batch Time: 622.62 ms, GPU Bubble Ratio: 59.34%, 57.10%, 66.31%, 26.72%
[rank0]:2025-11-09 18:49:02,678 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:49:05,194 - INFO - Avg. fwd time: 11.4710 / Avg. bwd time: 45.6120 / Avg. batch time: 516.3971 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-09 18:49:05,288 - INFO - Avg. fwd time: 7.8955 / Avg. bwd time: 23.5299 / Avg. batch time: 623.3586 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 18:49:05,223 - INFO - Avg. fwd time: 7.1589 / Avg. bwd time: 18.9808 / Avg. batch time: 547.9757 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:49:05,256 - INFO - Avg. fwd time: 9.1234 / Avg. bwd time: 24.1796 / Avg. batch time: 586.9503 (ms) / GPU bubble ratio: 54.61%
[rank1]:2025-11-09 18:49:05,313 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4668  memory: 14.64GiB(30.82%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-09 18:49:05,324 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4668  memory: 16.57GiB(34.88%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank2]:2025-11-09 18:49:05,310 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4668  memory: 11.81GiB(24.85%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 18:49:05,323 - INFO -  step: 750  loss:  0.4262  grad_norm:  0.4668  memory: 26.98GiB(56.79%)  tps: 6,312  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-09 18:51:12,260 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:51:14,583 - INFO - Avg. fwd time: 11.4695 / Avg. bwd time: 45.6107 / Avg. batch time: 516.3728 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 18:51:14,651 - INFO - Avg. fwd time: 7.1587 / Avg. bwd time: 18.9808 / Avg. batch time: 547.9312 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 18:51:14,696 - INFO - Avg. fwd time: 9.1223 / Avg. bwd time: 24.1799 / Avg. batch time: 586.8971 (ms) / GPU bubble ratio: 54.61%
[rank0]:2025-11-09 18:51:14,704 - INFO - Avg. fwd time: 7.8944 / Avg. bwd time: 23.5294 / Avg. batch time: 623.2981 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 18:51:14,874 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4638  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank2]:2025-11-09 18:51:14,874 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4638  tps: 6,693  tflops: 50.97  mfu: 14.82%
[rank2]:2025-11-09 18:51:14,874 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 18:51:14,875 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 18:51:14,887 - INFO -  step: 800  loss:  0.4414  grad_norm:  0.4638  memory: 26.98GiB(56.79%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank3]:2025-11-09 18:51:14,888 - INFO -  final step: 800  loss:  0.4414  grad_norm:  0.4638  tps: 6,696  tflops: 51.00  mfu: 14.88%
[rank3]:2025-11-09 18:51:14,888 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 18:51:14,889 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 18:51:14,878 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4638  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank1]:2025-11-09 18:51:14,878 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4638  tps: 6,690  tflops: 50.95  mfu: 14.79%
[rank1]:2025-11-09 18:51:14,878 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 18:51:14,879 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 18:51:14,889 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4638  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank0]:2025-11-09 18:51:14,889 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4638  tps: 6,690  tflops: 50.95  mfu: 14.79%
[rank0]:2025-11-09 18:51:14,889 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 18:51:14,890 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 18:51:16,804 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 18:51:16,787 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 18:51:16,804 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-09 18:51:16,804 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 18:51:16,957 - INFO - Process group destroyed
[rank3]:2025-11-09 18:51:16,952 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1851_real_final800_rank3.svg
[rank3]:> Batch Time: 622.66 ms, GPU Bubble Ratio: 59.35%, 57.11%, 66.30%, 26.61%
[rank1]:2025-11-09 18:51:16,945 - INFO - Process group destroyed
[rank3]:2025-11-09 18:51:17,098 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1/pipeline_schedule/251109_1851_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.16 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 18:51:17,099 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44667
[rank3]:wandb:               final/avg_loss 0.4414
[rank3]:wandb:             final/avg_mfu(%) 14.87961
[rank3]:wandb:             final/avg_tflops 51.00157
[rank3]:wandb:    final/avg_throughput(tps) 6696.44698
[rank3]:wandb:              final/grad_norm 0.46377
[rank3]:wandb:               final/max_loss 0.4414
[rank3]:wandb:                    grad_norm 0.46377
[rank3]:wandb: loss_metrics/global_avg_loss 0.4414
[rank3]:wandb: loss_metrics/global_max_loss 0.4414
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed66_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/7keo0a50
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed66_dm1/20251109-1816/wandb/run-20251109_181634-7keo0a50/logs
[rank3]:2025-11-09 18:51:18,421 - INFO - Process group destroyed
[rank0]:2025-11-09 18:51:18,804 - INFO - Training completed
[rank0]:2025-11-09 18:51:18,805 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 18:51:18,921 - INFO - Process group destroyed
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.6', 'layers.7', 'layers.5', 'layers.8'}
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.0', 'layers.2', 'layers.3', 'layers.1'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.9', 'layers.12', 'layers.11'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.14', 'output', 'norm', 'layers.15'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed66_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed66_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 66
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed66_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
