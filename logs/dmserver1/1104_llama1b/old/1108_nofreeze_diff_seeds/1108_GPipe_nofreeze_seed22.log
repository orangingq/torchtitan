
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 15:56:43 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed22.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 15:56:49,340 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-09 15:56:49,342 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank0]:2025-11-09 15:56:49,541 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 15:56:49,544 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 15:56:49,544 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 15:56:49,546 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 15:56:49,536 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:2025-11-09 15:56:49,524 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-09 15:56:49,546 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 15:56:49,547 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-09 15:56:49,709 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 15:56:49,727 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 15:56:49,730 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 15:56:49,712 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 15:56:49,935 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 15:56:52,708 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 15:56:52,855 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 15:56:52,897 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 15:56:52,899 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 15:56:52,924 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 15:56:52,924 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 15:56:52,873 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 15:56:52,914 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 15:56:52,941 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 15:56:52,941 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 15:56:53,109 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 15:56:53,109 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 15:56:53,110 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 15:56:53,127 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 15:56:53,127 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 15:56:53,128 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-09 15:56:53,324 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 15:56:53,360 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 15:56:53,387 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 15:56:53,387 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 15:56:53,570 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 15:56:53,570 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 15:56:53,571 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run f95wge11
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed22_dm1/20251109-1556/wandb/run-20251109_155653-f95wge11
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed22_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/f95wge11
[rank3]:2025-11-09 15:56:55,028 - INFO - WandB logging enabled
[rank3]:2025-11-09 15:56:55,029 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 15:56:55,069 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 15:56:55,097 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 15:56:55,097 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 15:56:55,303 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed22_dm1
[rank0]:2025-11-09 15:56:55,303 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 15:56:55,303 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 15:56:55,304 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 15:56:55,303 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 15:56:55,303 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 15:56:55,284 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 15:56:55,285 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 15:56:55,286 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 15:56:55,302 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 15:56:57,751 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 15:56:57,751 - INFO - Finished loading the checkpoint in 2.45 seconds.
[rank0]:2025-11-09 15:56:57,751 - INFO - Training starts at step 1
[rank2]:2025-11-09 15:57:00,860 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5963  memory:  9.99GiB(21.03%)  tps: 2,185  tflops: 16.64  mfu: 5.33%
[rank2]:2025-11-09 15:57:00,861 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 15:57:00,896 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5963  memory: 12.80GiB(26.95%)  tps: 2,048  tflops: 15.60  mfu: 5.00%
[rank0]:2025-11-09 15:57:00,897 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 15:57:00,869 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5963  memory: 24.19GiB(50.91%)  tps: 2,826  tflops: 21.53  mfu: 6.90%
[rank3]:2025-11-09 15:57:00,869 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 15:57:00,862 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5963  memory: 12.38GiB(26.05%)  tps: 2,062  tflops: 15.70  mfu: 5.03%
[rank1]:2025-11-09 15:57:00,869 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 15:59:04,610 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:59:06,891 - INFO - Avg. fwd time: 11.4912 / Avg. bwd time: 45.0644 / Avg. batch time: 512.3875 (ms) / GPU bubble ratio: 11.70%
[rank1]:2025-11-09 15:59:07,006 - INFO - Avg. fwd time: 9.1133 / Avg. bwd time: 23.8887 / Avg. batch time: 583.7517 (ms) / GPU bubble ratio: 54.77%
[rank0]:2025-11-09 15:59:07,014 - INFO - Avg. fwd time: 7.8817 / Avg. bwd time: 23.3880 / Avg. batch time: 620.8093 (ms) / GPU bubble ratio: 59.70%
[rank2]:2025-11-09 15:59:06,967 - INFO - Avg. fwd time: 7.1762 / Avg. bwd time: 18.7982 / Avg. batch time: 544.3204 (ms) / GPU bubble ratio: 61.82%
[rank3]:2025-11-09 15:59:07,203 - INFO -  step: 50  loss:  8.2870  grad_norm: 17.5003  memory: 26.98GiB(56.79%)  tps: 6,355  tflops: 48.40  mfu: 15.51%
[rank1]:2025-11-09 15:59:07,195 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5003  memory: 14.64GiB(30.82%)  tps: 6,355  tflops: 48.40  mfu: 15.51%
[rank0]:2025-11-09 15:59:07,206 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5003  memory: 16.57GiB(34.88%)  tps: 6,356  tflops: 48.41  mfu: 15.52%
[rank2]:2025-11-09 15:59:07,191 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5003  memory: 11.81GiB(24.85%)  tps: 6,355  tflops: 48.40  mfu: 15.51%
[rank0]:2025-11-09 16:01:14,662 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:01:16,991 - INFO - Avg. fwd time: 11.5417 / Avg. bwd time: 45.4869 / Avg. batch time: 516.0517 (ms) / GPU bubble ratio: 11.59%
[rank0]:2025-11-09 16:01:17,114 - INFO - Avg. fwd time: 7.8936 / Avg. bwd time: 23.4482 / Avg. batch time: 623.3775 (ms) / GPU bubble ratio: 59.78%
[rank2]:2025-11-09 16:01:17,066 - INFO - Avg. fwd time: 7.1774 / Avg. bwd time: 18.8781 / Avg. batch time: 547.6039 (ms) / GPU bubble ratio: 61.94%
[rank1]:2025-11-09 16:01:17,106 - INFO - Avg. fwd time: 9.1291 / Avg. bwd time: 24.0152 / Avg. batch time: 586.7475 (ms) / GPU bubble ratio: 54.81%
[rank3]:2025-11-09 16:01:17,304 - INFO -  step: 100  loss:  4.2805  grad_norm: 19.1171  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 16:01:17,306 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1171  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-09 16:01:17,292 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1171  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-09 16:01:17,295 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1171  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 16:01:17,479 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1601_real_step100_rank3.svg
[rank3]:> Batch Time: 625.61 ms, GPU Bubble Ratio: 59.57%, 57.29%, 66.47%, 26.35%
[rank0]:2025-11-09 16:03:24,446 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 16:03:26,962 - INFO - Avg. fwd time: 7.1712 / Avg. bwd time: 18.8989 / Avg. batch time: 547.9864 (ms) / GPU bubble ratio: 61.94%
[rank3]:2025-11-09 16:03:26,933 - INFO - Avg. fwd time: 11.5264 / Avg. bwd time: 45.5439 / Avg. batch time: 516.3372 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 16:03:26,992 - INFO - Avg. fwd time: 9.1285 / Avg. bwd time: 24.0522 / Avg. batch time: 587.0617 (ms) / GPU bubble ratio: 54.78%
[rank1]:2025-11-09 16:03:27,046 - INFO -  step: 150  loss: -4.0000  grad_norm: 97.3536  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank2]:2025-11-09 16:03:27,043 - INFO -  step: 150  loss: -4.0000  grad_norm: 97.3536  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 16:03:27,021 - INFO - Avg. fwd time: 7.8922 / Avg. bwd time: 23.4608 / Avg. batch time: 623.5572 (ms) / GPU bubble ratio: 59.78%
[rank0]:2025-11-09 16:03:27,057 - INFO -  step: 150  loss: -4.0000  grad_norm: 97.3536  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 16:03:27,055 - INFO -  step: 150  loss:  3.4718  grad_norm: 97.3536  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 16:05:33,693 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:05:36,016 - INFO - Avg. fwd time: 11.5065 / Avg. bwd time: 45.5129 / Avg. batch time: 515.9061 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 16:05:36,131 - INFO - Avg. fwd time: 9.1239 / Avg. bwd time: 24.0764 / Avg. batch time: 586.5059 (ms) / GPU bubble ratio: 54.71%
[rank2]:2025-11-09 16:05:36,092 - INFO - Avg. fwd time: 7.1656 / Avg. bwd time: 18.9100 / Avg. batch time: 547.4444 (ms) / GPU bubble ratio: 61.89%
[rank0]:2025-11-09 16:05:36,139 - INFO - Avg. fwd time: 7.8865 / Avg. bwd time: 23.4681 / Avg. batch time: 622.9454 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-09 16:05:36,319 - INFO -  step: 200  loss: -4.0000  grad_norm: 13.6687  memory: 14.64GiB(30.82%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank2]:2025-11-09 16:05:36,315 - INFO -  step: 200  loss: -4.0000  grad_norm: 13.6687  memory: 11.81GiB(24.85%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 16:05:36,327 - INFO -  step: 200  loss:  1.1924  grad_norm: 13.6687  memory: 26.98GiB(56.79%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank0]:2025-11-09 16:05:36,330 - INFO -  step: 200  loss: -4.0000  grad_norm: 13.6687  memory: 16.57GiB(34.88%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 16:05:36,478 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1605_real_step200_rank3.svg
[rank3]:> Batch Time: 620.56 ms, GPU Bubble Ratio: 59.31%, 56.98%, 66.24%, 26.73%
[rank0]:2025-11-09 16:07:43,584 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:07:45,896 - INFO - Avg. fwd time: 11.5029 / Avg. bwd time: 45.5292 / Avg. batch time: 515.9961 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:07:45,973 - INFO - Avg. fwd time: 7.1628 / Avg. bwd time: 18.9322 / Avg. batch time: 547.5879 (ms) / GPU bubble ratio: 61.88%
[rank0]:2025-11-09 16:07:46,020 - INFO - Avg. fwd time: 7.8885 / Avg. bwd time: 23.4788 / Avg. batch time: 623.0669 (ms) / GPU bubble ratio: 59.73%
[rank1]:2025-11-09 16:07:46,013 - INFO - Avg. fwd time: 9.1208 / Avg. bwd time: 24.1112 / Avg. batch time: 586.6545 (ms) / GPU bubble ratio: 54.68%
[rank1]:2025-11-09 16:07:46,199 - INFO -  step: 250  loss: -4.0000  grad_norm:  6.1760  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-09 16:07:46,208 - INFO -  step: 250  loss:  0.6432  grad_norm:  6.1760  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank2]:2025-11-09 16:07:46,196 - INFO -  step: 250  loss: -4.0000  grad_norm:  6.1760  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 16:07:46,210 - INFO -  step: 250  loss: -4.0000  grad_norm:  6.1760  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-09 16:09:53,490 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:09:56,046 - INFO - Avg. fwd time: 11.5051 / Avg. bwd time: 45.5678 / Avg. batch time: 516.3177 (ms) / GPU bubble ratio: 11.57%
[rank2]:2025-11-09 16:09:56,072 - INFO - Avg. fwd time: 7.1618 / Avg. bwd time: 18.9472 / Avg. batch time: 547.8542 (ms) / GPU bubble ratio: 61.87%
[rank3]:2025-11-09 16:09:56,168 - INFO -  step: 300  loss:  0.5650  grad_norm:  7.9315  memory: 26.98GiB(56.79%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank1]:2025-11-09 16:09:56,103 - INFO - Avg. fwd time: 9.1191 / Avg. bwd time: 24.1363 / Avg. batch time: 586.9199 (ms) / GPU bubble ratio: 54.67%
[rank1]:2025-11-09 16:09:56,159 - INFO -  step: 300  loss: -4.0000  grad_norm:  7.9315  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-09 16:09:56,133 - INFO - Avg. fwd time: 7.8913 / Avg. bwd time: 23.4890 / Avg. batch time: 623.3189 (ms) / GPU bubble ratio: 59.72%
[rank0]:2025-11-09 16:09:56,170 - INFO -  step: 300  loss: -4.0000  grad_norm:  7.9315  memory: 16.57GiB(34.88%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank2]:2025-11-09 16:09:56,155 - INFO -  step: 300  loss: -4.0000  grad_norm:  7.9315  memory: 11.81GiB(24.85%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-09 16:09:56,317 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1609_real_step300_rank3.svg
[rank3]:> Batch Time: 624.16 ms, GPU Bubble Ratio: 59.41%, 57.16%, 66.34%, 26.57%
[rank0]:2025-11-09 16:12:03,794 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:12:06,096 - INFO - Avg. fwd time: 11.5055 / Avg. bwd time: 45.6074 / Avg. batch time: 516.6292 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:12:06,172 - INFO - Avg. fwd time: 7.1610 / Avg. bwd time: 18.9563 / Avg. batch time: 548.2131 (ms) / GPU bubble ratio: 61.89%
[rank1]:2025-11-09 16:12:06,211 - INFO - Avg. fwd time: 9.1188 / Avg. bwd time: 24.1496 / Avg. batch time: 587.2687 (ms) / GPU bubble ratio: 54.68%
[rank0]:2025-11-09 16:12:06,219 - INFO - Avg. fwd time: 7.8939 / Avg. bwd time: 23.4959 / Avg. batch time: 623.6632 (ms) / GPU bubble ratio: 59.73%
[rank2]:2025-11-09 16:12:06,395 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9250  memory: 11.81GiB(24.85%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank3]:2025-11-09 16:12:06,408 - INFO -  step: 350  loss:  0.4955  grad_norm:  0.9250  memory: 26.98GiB(56.79%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank1]:2025-11-09 16:12:06,399 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9250  memory: 14.64GiB(30.82%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank0]:2025-11-09 16:12:06,410 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.9250  memory: 16.57GiB(34.88%)  tps: 6,290  tflops: 47.91  mfu: 15.35%
[rank0]:2025-11-09 16:14:13,248 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:14:15,571 - INFO - Avg. fwd time: 11.4964 / Avg. bwd time: 45.5997 / Avg. batch time: 516.4852 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:14:15,644 - INFO - Avg. fwd time: 7.1590 / Avg. bwd time: 18.9580 / Avg. batch time: 548.0177 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 16:14:15,692 - INFO - Avg. fwd time: 7.8938 / Avg. bwd time: 23.4966 / Avg. batch time: 623.4431 (ms) / GPU bubble ratio: 59.72%
[rank1]:2025-11-09 16:14:15,684 - INFO - Avg. fwd time: 9.1169 / Avg. bwd time: 24.1540 / Avg. batch time: 587.0611 (ms) / GPU bubble ratio: 54.66%
[rank2]:2025-11-09 16:14:15,870 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.1293  memory: 11.81GiB(24.85%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank0]:2025-11-09 16:14:15,884 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.1293  memory: 16.57GiB(34.88%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 16:14:15,882 - INFO -  step: 400  loss:  0.4913  grad_norm:  4.1293  memory: 26.98GiB(56.79%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank1]:2025-11-09 16:14:15,873 - INFO -  step: 400  loss: -4.0000  grad_norm:  4.1293  memory: 14.64GiB(30.82%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 16:14:16,033 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1614_real_step400_rank3.svg
[rank3]:> Batch Time: 621.12 ms, GPU Bubble Ratio: 59.28%, 57.05%, 66.27%, 26.88%
[rank3]:2025-11-09 16:14:24,762 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 16:14:24,981 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 16:14:25,008 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 16:14:25,033 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 16:16:22,672 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:16:25,189 - INFO - Avg. fwd time: 11.4879 / Avg. bwd time: 45.5916 / Avg. batch time: 516.3450 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:16:25,216 - INFO - Avg. fwd time: 7.1570 / Avg. bwd time: 18.9595 / Avg. batch time: 547.9029 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 16:16:25,278 - INFO - Avg. fwd time: 7.8929 / Avg. bwd time: 23.4985 / Avg. batch time: 623.3143 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-09 16:16:25,247 - INFO - Avg. fwd time: 9.1152 / Avg. bwd time: 24.1563 / Avg. batch time: 586.9355 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 16:16:25,299 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5644  memory: 11.81GiB(24.85%)  tps: 6,329  tflops: 48.21  mfu: 15.45%
[rank0]:2025-11-09 16:16:25,313 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5644  memory: 16.57GiB(34.88%)  tps: 6,329  tflops: 48.21  mfu: 15.45%
[rank1]:2025-11-09 16:16:25,303 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5644  memory: 14.64GiB(30.82%)  tps: 6,329  tflops: 48.21  mfu: 15.45%
[rank3]:2025-11-09 16:16:25,312 - INFO -  step: 450  loss:  0.5387  grad_norm:  0.5644  memory: 26.98GiB(56.79%)  tps: 6,329  tflops: 48.21  mfu: 15.45%
[rank0]:2025-11-09 16:18:32,544 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:18:34,861 - INFO - Avg. fwd time: 11.4879 / Avg. bwd time: 45.6006 / Avg. batch time: 516.4151 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:18:34,938 - INFO - Avg. fwd time: 7.1563 / Avg. bwd time: 18.9622 / Avg. batch time: 547.9436 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-09 16:18:34,977 - INFO - Avg. fwd time: 9.1145 / Avg. bwd time: 24.1604 / Avg. batch time: 586.9596 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 16:18:34,984 - INFO - Avg. fwd time: 7.8936 / Avg. bwd time: 23.5018 / Avg. batch time: 623.3388 (ms) / GPU bubble ratio: 59.71%
[rank3]:2025-11-09 16:18:35,170 - INFO -  step: 500  loss:  0.4135  grad_norm:  2.2414  memory: 26.98GiB(56.79%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 16:18:35,162 - INFO -  step: 500  loss: -4.0000  grad_norm:  2.2414  memory: 14.64GiB(30.82%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 16:18:35,158 - INFO -  step: 500  loss: -4.0000  grad_norm:  2.2414  memory: 11.81GiB(24.85%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 16:18:35,173 - INFO -  step: 500  loss: -4.0000  grad_norm:  2.2414  memory: 16.57GiB(34.88%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 16:18:35,322 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1618_real_step500_rank3.svg
[rank3]:> Batch Time: 623.17 ms, GPU Bubble Ratio: 59.35%, 57.15%, 66.32%, 26.56%
[rank0]:2025-11-09 16:20:42,654 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:20:44,959 - INFO - Avg. fwd time: 11.4893 / Avg. bwd time: 45.6119 / Avg. batch time: 516.5143 (ms) / GPU bubble ratio: 11.56%
[rank1]:2025-11-09 16:20:45,073 - INFO - Avg. fwd time: 9.1150 / Avg. bwd time: 24.1655 / Avg. batch time: 587.0684 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 16:20:45,033 - INFO - Avg. fwd time: 7.1567 / Avg. bwd time: 18.9654 / Avg. batch time: 548.0726 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 16:20:45,081 - INFO - Avg. fwd time: 7.8967 / Avg. bwd time: 23.5055 / Avg. batch time: 623.4545 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-09 16:20:45,255 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.7253  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-09 16:20:45,251 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.7253  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 16:20:45,266 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.7253  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 16:20:45,263 - INFO -  step: 550  loss:  0.4504  grad_norm:  0.7253  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 16:22:52,285 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-09 16:22:54,872 - INFO - Avg. fwd time: 9.1143 / Avg. bwd time: 24.1651 / Avg. batch time: 586.9845 (ms) / GPU bubble ratio: 54.64%
[rank2]:2025-11-09 16:22:54,840 - INFO - Avg. fwd time: 7.1562 / Avg. bwd time: 18.9665 / Avg. batch time: 548.0077 (ms) / GPU bubble ratio: 61.87%
[rank3]:2025-11-09 16:22:54,812 - INFO - Avg. fwd time: 11.4884 / Avg. bwd time: 45.6087 / Avg. batch time: 516.4786 (ms) / GPU bubble ratio: 11.56%
[rank0]:2025-11-09 16:22:54,903 - INFO - Avg. fwd time: 7.8979 / Avg. bwd time: 23.5074 / Avg. batch time: 623.3739 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-09 16:22:54,929 - INFO -  step: 600  loss: -4.0000  grad_norm:  1.4823  memory: 14.64GiB(30.82%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank2]:2025-11-09 16:22:54,925 - INFO -  step: 600  loss: -4.0000  grad_norm:  1.4823  memory: 11.81GiB(24.85%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 16:22:54,938 - INFO -  step: 600  loss:  0.4575  grad_norm:  1.4823  memory: 26.98GiB(56.79%)  tps: 6,317  tflops: 48.12  mfu: 15.42%
[rank0]:2025-11-09 16:22:54,940 - INFO -  step: 600  loss: -4.0000  grad_norm:  1.4823  memory: 16.57GiB(34.88%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 16:22:55,090 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1622_real_step600_rank3.svg
[rank3]:> Batch Time: 621.67 ms, GPU Bubble Ratio: 59.28%, 57.15%, 66.27%, 26.54%
[rank0]:2025-11-09 16:25:02,152 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:25:04,458 - INFO - Avg. fwd time: 11.4881 / Avg. bwd time: 45.6070 / Avg. batch time: 516.4624 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:25:04,532 - INFO - Avg. fwd time: 7.1563 / Avg. bwd time: 18.9677 / Avg. batch time: 548.0165 (ms) / GPU bubble ratio: 61.86%
[rank0]:2025-11-09 16:25:04,579 - INFO - Avg. fwd time: 7.8982 / Avg. bwd time: 23.5089 / Avg. batch time: 623.3672 (ms) / GPU bubble ratio: 59.69%
[rank1]:2025-11-09 16:25:04,571 - INFO - Avg. fwd time: 9.1137 / Avg. bwd time: 24.1639 / Avg. batch time: 586.9758 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 16:25:04,749 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4534  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 16:25:04,761 - INFO -  step: 650  loss:  0.3832  grad_norm:  0.4534  memory: 26.98GiB(56.79%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 16:25:04,763 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4534  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank1]:2025-11-09 16:25:04,752 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4534  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank0]:2025-11-09 16:27:12,219 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:27:14,558 - INFO - Avg. fwd time: 11.4908 / Avg. bwd time: 45.6174 / Avg. batch time: 516.5638 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-09 16:27:14,633 - INFO - Avg. fwd time: 7.1569 / Avg. bwd time: 18.9706 / Avg. batch time: 548.1008 (ms) / GPU bubble ratio: 61.86%
[rank1]:2025-11-09 16:27:14,672 - INFO - Avg. fwd time: 9.1141 / Avg. bwd time: 24.1642 / Avg. batch time: 587.0434 (ms) / GPU bubble ratio: 54.65%
[rank0]:2025-11-09 16:27:14,681 - INFO - Avg. fwd time: 7.8991 / Avg. bwd time: 23.5121 / Avg. batch time: 623.4388 (ms) / GPU bubble ratio: 59.69%
[rank2]:2025-11-09 16:27:14,852 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.6714  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-09 16:27:14,855 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.6714  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 16:27:14,864 - INFO -  step: 700  loss:  0.4643  grad_norm:  1.6714  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 16:27:14,866 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.6714  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 16:27:15,016 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1627_real_step700_rank3.svg
[rank3]:> Batch Time: 624.69 ms, GPU Bubble Ratio: 59.41%, 57.21%, 66.36%, 26.53%
[rank0]:2025-11-09 16:29:22,757 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:29:25,271 - INFO - Avg. fwd time: 11.4948 / Avg. bwd time: 45.6383 / Avg. batch time: 516.7617 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 16:29:25,300 - INFO - Avg. fwd time: 7.1576 / Avg. bwd time: 18.9737 / Avg. batch time: 548.3261 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-09 16:29:25,334 - INFO - Avg. fwd time: 9.1157 / Avg. bwd time: 24.1666 / Avg. batch time: 587.2554 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 16:29:25,366 - INFO - Avg. fwd time: 7.9011 / Avg. bwd time: 23.5156 / Avg. batch time: 623.6560 (ms) / GPU bubble ratio: 59.70%
[rank0]:2025-11-09 16:29:25,402 - INFO -  step: 750  loss: -4.0000  grad_norm:  1.3258  memory: 16.57GiB(34.88%)  tps: 6,276  tflops: 47.80  mfu: 15.32%
[rank3]:2025-11-09 16:29:25,400 - INFO -  step: 750  loss:  0.4525  grad_norm:  1.3258  memory: 26.98GiB(56.79%)  tps: 6,276  tflops: 47.80  mfu: 15.32%
[rank2]:2025-11-09 16:29:25,387 - INFO -  step: 750  loss: -4.0000  grad_norm:  1.3258  memory: 11.81GiB(24.85%)  tps: 6,276  tflops: 47.80  mfu: 15.32%
[rank1]:2025-11-09 16:29:25,391 - INFO -  step: 750  loss: -4.0000  grad_norm:  1.3258  memory: 14.64GiB(30.82%)  tps: 6,276  tflops: 47.80  mfu: 15.32%
[rank0]:2025-11-09 16:31:33,018 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:31:35,337 - INFO - Avg. fwd time: 11.4980 / Avg. bwd time: 45.6510 / Avg. batch time: 516.8911 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-09 16:31:35,418 - INFO - Avg. fwd time: 7.1582 / Avg. bwd time: 18.9760 / Avg. batch time: 548.4445 (ms) / GPU bubble ratio: 61.88%
[rank0]:2025-11-09 16:31:35,465 - INFO - Avg. fwd time: 7.9015 / Avg. bwd time: 23.5181 / Avg. batch time: 623.7670 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-09 16:31:35,457 - INFO - Avg. fwd time: 9.1177 / Avg. bwd time: 24.1684 / Avg. batch time: 587.3658 (ms) / GPU bubble ratio: 54.66%
[rank1]:2025-11-09 16:31:35,642 - INFO -  step: 800  loss: -4.0000  grad_norm:  1.9066  memory: 14.64GiB(30.82%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank1]:2025-11-09 16:31:35,642 - INFO -  final step: 800  loss: -4.0000  grad_norm:  1.9066  tps: 6,687  tflops: 50.93  mfu: 14.79%
[rank1]:2025-11-09 16:31:35,642 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 16:31:35,643 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 16:31:35,638 - INFO -  step: 800  loss: -4.0000  grad_norm:  1.9066  memory: 11.81GiB(24.85%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank2]:2025-11-09 16:31:35,638 - INFO -  final step: 800  loss: -4.0000  grad_norm:  1.9066  tps: 6,688  tflops: 50.94  mfu: 14.81%
[rank2]:2025-11-09 16:31:35,638 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 16:31:35,639 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 16:31:35,653 - INFO -  step: 800  loss: -4.0000  grad_norm:  1.9066  memory: 16.57GiB(34.88%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-09 16:31:35,653 - INFO -  final step: 800  loss: -4.0000  grad_norm:  1.9066  tps: 6,687  tflops: 50.93  mfu: 14.79%
[rank0]:2025-11-09 16:31:35,653 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 16:31:35,653 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 16:31:35,650 - INFO -  step: 800  loss:  0.4449  grad_norm:  1.9066  memory: 26.98GiB(56.79%)  tps: 6,290  tflops: 47.90  mfu: 15.35%
[rank3]:2025-11-09 16:31:35,651 - INFO -  final step: 800  loss:  0.4449  grad_norm:  1.9066  tps: 6,694  tflops: 50.98  mfu: 14.90%
[rank3]:2025-11-09 16:31:35,651 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 16:31:35,652 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 16:31:37,780 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 16:31:37,794 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-09 16:31:37,794 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 16:31:37,794 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 16:31:37,838 - INFO - Process group destroyed
[rank1]:2025-11-09 16:31:37,877 - INFO - Process group destroyed
[rank3]:2025-11-09 16:31:37,940 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1631_real_final800_rank3.svg
[rank3]:> Batch Time: 623.14 ms, GPU Bubble Ratio: 59.38%, 57.16%, 66.33%, 26.56%
[rank3]:2025-11-09 16:31:38,082 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1/pipeline_schedule/251109_1631_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.45 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 16:31:38,083 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.4477
[rank3]:wandb:               final/avg_loss 0.44489
[rank3]:wandb:             final/avg_mfu(%) 14.90239
[rank3]:wandb:             final/avg_tflops 50.98014
[rank3]:wandb:    final/avg_throughput(tps) 6693.63405
[rank3]:wandb:              final/grad_norm 1.90655
[rank3]:wandb:               final/max_loss 0.44489
[rank3]:wandb:                    grad_norm 1.90655
[rank3]:wandb: loss_metrics/global_avg_loss 0.44489
[rank3]:wandb: loss_metrics/global_max_loss 0.44489
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed22_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/f95wge11
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed22_dm1/20251109-1556/wandb/run-20251109_155653-f95wge11/logs
[rank3]:2025-11-09 16:31:39,378 - INFO - Process group destroyed
[rank0]:2025-11-09 16:31:39,794 - INFO - Training completed
[rank0]:2025-11-09 16:31:39,794 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 16:31:39,882 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.0', 'layers.3', 'tok_embeddings', 'layers.2'}
[rank1]:Stage 1: Modules to keep: {'layers.8', 'layers.5', 'layers.7', 'layers.6', 'layers.4'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.9', 'layers.12', 'layers.11'}
[rank3]:Stage 3: Modules to keep: {'layers.15', 'output', 'norm', 'layers.13', 'layers.14'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed22_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed22_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 22
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed22_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
