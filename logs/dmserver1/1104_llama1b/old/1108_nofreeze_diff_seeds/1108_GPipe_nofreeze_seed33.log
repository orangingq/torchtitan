
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 16:31:41 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed33.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank3]:2025-11-09 16:31:48,031 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-09 16:31:48,164 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-09 16:31:48,090 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-09 16:31:48,085 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-09 16:31:48,177 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 16:31:48,180 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 16:31:48,310 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 16:31:48,313 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 16:31:48,301 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 16:31:48,303 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 16:31:48,384 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 16:31:48,387 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 16:31:48,390 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 16:31:48,391 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-09 16:31:48,809 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 16:31:51,554 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 16:31:51,707 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 16:31:51,743 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 16:31:51,744 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 16:31:51,770 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 16:31:51,771 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 16:31:51,745 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 16:31:51,783 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 16:31:51,810 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 16:31:51,811 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 16:31:51,954 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 16:31:51,954 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 16:31:51,955 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 16:31:51,996 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 16:31:51,996 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 16:31:51,996 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-09 16:31:52,091 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 16:31:52,127 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 16:31:52,155 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 16:31:52,155 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 16:31:52,358 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 16:31:52,359 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 16:31:52,359 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run p86xx3lt
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed33_dm1/20251109-1631/wandb/run-20251109_163152-p86xx3lt
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed33_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/p86xx3lt
[rank3]:2025-11-09 16:31:53,681 - INFO - WandB logging enabled
[rank3]:2025-11-09 16:31:53,682 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 16:31:53,722 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 16:31:53,750 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 16:31:53,751 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 16:31:53,955 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed33_dm1
[rank0]:2025-11-09 16:31:53,956 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 16:31:53,956 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 16:31:53,956 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 16:31:53,954 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 16:31:53,954 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 16:31:53,938 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 16:31:53,938 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 16:31:53,939 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 16:31:53,955 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 16:31:56,421 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 16:31:56,422 - INFO - Finished loading the checkpoint in 2.46 seconds.
[rank0]:2025-11-09 16:31:56,422 - INFO - Training starts at step 1
[rank2]:2025-11-09 16:31:59,577 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5965  memory:  9.99GiB(21.03%)  tps: 2,199  tflops: 16.75  mfu: 5.37%
[rank2]:2025-11-09 16:31:59,578 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 16:31:59,580 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5965  memory: 12.38GiB(26.05%)  tps: 2,101  tflops: 16.01  mfu: 5.13%
[rank1]:2025-11-09 16:31:59,580 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 16:31:59,588 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5965  memory: 24.19GiB(50.91%)  tps: 2,794  tflops: 21.28  mfu: 6.82%
[rank3]:2025-11-09 16:31:59,588 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 16:31:59,613 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5965  memory: 12.80GiB(26.95%)  tps: 2,082  tflops: 15.86  mfu: 5.08%
[rank0]:2025-11-09 16:31:59,613 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 16:34:02,946 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:34:05,221 - INFO - Avg. fwd time: 11.4583 / Avg. bwd time: 44.8981 / Avg. batch time: 510.8799 (ms) / GPU bubble ratio: 11.75%
[rank0]:2025-11-09 16:34:05,342 - INFO - Avg. fwd time: 7.9052 / Avg. bwd time: 23.3974 / Avg. batch time: 619.3197 (ms) / GPU bubble ratio: 59.57%
[rank2]:2025-11-09 16:34:05,295 - INFO - Avg. fwd time: 7.1675 / Avg. bwd time: 18.7849 / Avg. batch time: 542.8614 (ms) / GPU bubble ratio: 61.75%
[rank1]:2025-11-09 16:34:05,338 - INFO - Avg. fwd time: 9.1154 / Avg. bwd time: 23.8790 / Avg. batch time: 582.2562 (ms) / GPU bubble ratio: 54.67%
[rank0]:2025-11-09 16:34:05,532 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3370  memory: 16.57GiB(34.88%)  tps: 6,376  tflops: 48.56  mfu: 15.56%
[rank3]:2025-11-09 16:34:05,529 - INFO -  step: 50  loss:  8.2776  grad_norm: 17.3370  memory: 26.98GiB(56.79%)  tps: 6,375  tflops: 48.55  mfu: 15.56%
[rank2]:2025-11-09 16:34:05,517 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3370  memory: 11.81GiB(24.85%)  tps: 6,375  tflops: 48.55  mfu: 15.56%
[rank1]:2025-11-09 16:34:05,520 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.3370  memory: 14.64GiB(30.82%)  tps: 6,375  tflops: 48.55  mfu: 15.56%
[rank0]:2025-11-09 16:36:12,269 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:36:14,589 - INFO - Avg. fwd time: 11.4435 / Avg. bwd time: 45.2584 / Avg. batch time: 513.4711 (ms) / GPU bubble ratio: 11.66%
[rank1]:2025-11-09 16:36:14,706 - INFO - Avg. fwd time: 9.1221 / Avg. bwd time: 23.9704 / Avg. batch time: 584.0151 (ms) / GPU bubble ratio: 54.67%
[rank2]:2025-11-09 16:36:14,666 - INFO - Avg. fwd time: 7.1658 / Avg. bwd time: 18.8554 / Avg. batch time: 545.0510 (ms) / GPU bubble ratio: 61.81%
[rank0]:2025-11-09 16:36:14,713 - INFO - Avg. fwd time: 7.9008 / Avg. bwd time: 23.4507 / Avg. batch time: 620.6386 (ms) / GPU bubble ratio: 59.59%
[rank3]:2025-11-09 16:36:14,904 - INFO -  step: 100  loss:  4.2772  grad_norm: 19.1253  memory: 26.98GiB(56.79%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank1]:2025-11-09 16:36:14,896 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1253  memory: 14.64GiB(30.82%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank2]:2025-11-09 16:36:14,892 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1253  memory: 11.81GiB(24.85%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 16:36:14,906 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1253  memory: 16.57GiB(34.88%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank3]:2025-11-09 16:36:15,078 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1636_real_step100_rank3.svg
[rank3]:> Batch Time: 622.62 ms, GPU Bubble Ratio: 59.37%, 57.18%, 66.35%, 26.50%
[rank0]:2025-11-09 16:38:22,104 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:38:24,592 - INFO - Avg. fwd time: 11.4371 / Avg. bwd time: 45.4348 / Avg. batch time: 514.7713 (ms) / GPU bubble ratio: 11.62%
[rank0]:2025-11-09 16:38:24,678 - INFO - Avg. fwd time: 7.9026 / Avg. bwd time: 23.4733 / Avg. batch time: 621.8371 (ms) / GPU bubble ratio: 59.63%
[rank2]:2025-11-09 16:38:24,619 - INFO - Avg. fwd time: 7.1630 / Avg. bwd time: 18.8843 / Avg. batch time: 546.4683 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-09 16:38:24,649 - INFO - Avg. fwd time: 9.1234 / Avg. bwd time: 24.0140 / Avg. batch time: 585.3290 (ms) / GPU bubble ratio: 54.71%
[rank1]:2025-11-09 16:38:24,704 - INFO -  step: 150  loss: -4.0000  grad_norm: 26.6361  memory: 14.64GiB(30.82%)  tps: 6,311  tflops: 48.07  mfu: 15.41%
[rank0]:2025-11-09 16:38:24,715 - INFO -  step: 150  loss: -4.0000  grad_norm: 26.6361  memory: 16.57GiB(34.88%)  tps: 6,311  tflops: 48.06  mfu: 15.41%
[rank3]:2025-11-09 16:38:24,712 - INFO -  step: 150  loss:  3.3762  grad_norm: 26.6361  memory: 26.98GiB(56.79%)  tps: 6,311  tflops: 48.07  mfu: 15.41%
[rank2]:2025-11-09 16:38:24,700 - INFO -  step: 150  loss: -4.0000  grad_norm: 26.6361  memory: 11.81GiB(24.85%)  tps: 6,311  tflops: 48.06  mfu: 15.41%
[rank0]:2025-11-09 16:40:31,919 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:40:34,247 - INFO - Avg. fwd time: 11.4361 / Avg. bwd time: 45.5143 / Avg. batch time: 515.3769 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 16:40:34,323 - INFO - Avg. fwd time: 7.1616 / Avg. bwd time: 18.9092 / Avg. batch time: 546.9841 (ms) / GPU bubble ratio: 61.87%
[rank1]:2025-11-09 16:40:34,361 - INFO - Avg. fwd time: 9.1232 / Avg. bwd time: 24.0524 / Avg. batch time: 585.8202 (ms) / GPU bubble ratio: 54.70%
[rank0]:2025-11-09 16:40:34,369 - INFO - Avg. fwd time: 7.9001 / Avg. bwd time: 23.4919 / Avg. batch time: 622.2887 (ms) / GPU bubble ratio: 59.64%
[rank2]:2025-11-09 16:40:34,549 - INFO -  step: 200  loss: -4.0000  grad_norm: 17.6535  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 16:40:34,563 - INFO -  step: 200  loss: -4.0000  grad_norm: 17.6535  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 16:40:34,561 - INFO -  step: 200  loss:  1.1797  grad_norm: 17.6535  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 16:40:34,552 - INFO -  step: 200  loss: -4.0000  grad_norm: 17.6535  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 16:40:34,714 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1640_real_step200_rank3.svg
[rank3]:> Batch Time: 623.17 ms, GPU Bubble Ratio: 59.34%, 57.08%, 66.28%, 26.56%
[rank0]:2025-11-09 16:42:42,004 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:42:44,306 - INFO - Avg. fwd time: 11.4379 / Avg. bwd time: 45.5691 / Avg. batch time: 515.8191 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 16:42:44,379 - INFO - Avg. fwd time: 7.1612 / Avg. bwd time: 18.9363 / Avg. batch time: 547.4943 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-09 16:42:44,426 - INFO - Avg. fwd time: 7.9013 / Avg. bwd time: 23.5070 / Avg. batch time: 622.7850 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 16:42:44,418 - INFO - Avg. fwd time: 9.1251 / Avg. bwd time: 24.0862 / Avg. batch time: 586.3375 (ms) / GPU bubble ratio: 54.69%
[rank3]:2025-11-09 16:42:44,613 - INFO -  step: 250  loss:  0.6534  grad_norm:  8.6363  memory: 26.98GiB(56.79%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank2]:2025-11-09 16:42:44,600 - INFO -  step: 250  loss: -4.0000  grad_norm:  8.6363  memory: 11.81GiB(24.85%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank0]:2025-11-09 16:42:44,615 - INFO -  step: 250  loss: -4.0000  grad_norm:  8.6363  memory: 16.57GiB(34.88%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank1]:2025-11-09 16:42:44,604 - INFO -  step: 250  loss: -4.0000  grad_norm:  8.6363  memory: 14.64GiB(30.82%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank0]:2025-11-09 16:44:51,490 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:44:54,035 - INFO - Avg. fwd time: 11.4281 / Avg. bwd time: 45.5745 / Avg. batch time: 515.7712 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:44:54,062 - INFO - Avg. fwd time: 7.1592 / Avg. bwd time: 18.9502 / Avg. batch time: 547.3863 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 16:44:54,093 - INFO - Avg. fwd time: 9.1231 / Avg. bwd time: 24.1020 / Avg. batch time: 586.2245 (ms) / GPU bubble ratio: 54.66%
[rank3]:2025-11-09 16:44:54,158 - INFO -  step: 300  loss:  0.5533  grad_norm:  1.4657  memory: 26.98GiB(56.79%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank2]:2025-11-09 16:44:54,145 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.4657  memory: 11.81GiB(24.85%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 16:44:54,123 - INFO - Avg. fwd time: 7.8985 / Avg. bwd time: 23.5118 / Avg. batch time: 622.6482 (ms) / GPU bubble ratio: 59.64%
[rank0]:2025-11-09 16:44:54,159 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.4657  memory: 16.57GiB(34.88%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank1]:2025-11-09 16:44:54,148 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.4657  memory: 14.64GiB(30.82%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank3]:2025-11-09 16:44:54,310 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1644_real_step300_rank3.svg
[rank3]:> Batch Time: 621.09 ms, GPU Bubble Ratio: 59.26%, 57.05%, 66.22%, 26.55%
[rank0]:2025-11-09 16:47:01,488 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:47:03,789 - INFO - Avg. fwd time: 11.4280 / Avg. bwd time: 45.5950 / Avg. batch time: 515.9274 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:47:03,863 - INFO - Avg. fwd time: 7.1580 / Avg. bwd time: 18.9581 / Avg. batch time: 547.5894 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 16:47:03,903 - INFO - Avg. fwd time: 9.1223 / Avg. bwd time: 24.1144 / Avg. batch time: 586.4299 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 16:47:03,910 - INFO - Avg. fwd time: 7.8982 / Avg. bwd time: 23.5146 / Avg. batch time: 622.8400 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 16:47:04,088 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.2590  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 16:47:04,098 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.2590  memory: 16.57GiB(34.88%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-09 16:47:04,096 - INFO -  step: 350  loss:  0.4925  grad_norm:  1.2590  memory: 26.98GiB(56.79%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-09 16:47:04,084 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.2590  memory: 11.81GiB(24.85%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 16:49:11,176 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:49:13,488 - INFO - Avg. fwd time: 11.4224 / Avg. bwd time: 45.5976 / Avg. batch time: 515.8969 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:49:13,562 - INFO - Avg. fwd time: 7.1567 / Avg. bwd time: 18.9638 / Avg. batch time: 547.5121 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 16:49:13,609 - INFO - Avg. fwd time: 7.8943 / Avg. bwd time: 23.5158 / Avg. batch time: 622.7563 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 16:49:13,601 - INFO - Avg. fwd time: 9.1211 / Avg. bwd time: 24.1247 / Avg. batch time: 586.3611 (ms) / GPU bubble ratio: 54.64%
[rank1]:2025-11-09 16:49:13,789 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3917  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 16:49:13,800 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3917  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 16:49:13,799 - INFO -  step: 400  loss:  0.4752  grad_norm:  0.3917  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.11  mfu: 15.42%
[rank2]:2025-11-09 16:49:13,785 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3917  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 16:49:13,951 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1649_real_step400_rank3.svg
[rank3]:> Batch Time: 621.58 ms, GPU Bubble Ratio: 59.32%, 57.03%, 66.26%, 26.61%
[rank3]:2025-11-09 16:49:22,688 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 16:49:22,960 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 16:49:22,909 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 16:49:22,934 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 16:51:20,688 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:51:23,223 - INFO - Avg. fwd time: 11.4141 / Avg. bwd time: 45.5937 / Avg. batch time: 515.7927 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:51:23,251 - INFO - Avg. fwd time: 7.1549 / Avg. bwd time: 18.9664 / Avg. batch time: 547.4361 (ms) / GPU bubble ratio: 61.83%
[rank2]:2025-11-09 16:51:23,335 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3721  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank1]:2025-11-09 16:51:23,282 - INFO - Avg. fwd time: 9.1192 / Avg. bwd time: 24.1317 / Avg. batch time: 586.2958 (ms) / GPU bubble ratio: 54.63%
[rank1]:2025-11-09 16:51:23,339 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3721  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 16:51:23,313 - INFO - Avg. fwd time: 7.8937 / Avg. bwd time: 23.5158 / Avg. batch time: 622.6824 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-09 16:51:23,350 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3721  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank3]:2025-11-09 16:51:23,348 - INFO -  step: 450  loss:  0.5277  grad_norm:  0.3721  memory: 26.98GiB(56.79%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 16:53:30,491 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:53:32,811 - INFO - Avg. fwd time: 11.4108 / Avg. bwd time: 45.5964 / Avg. batch time: 515.7828 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 16:53:32,883 - INFO - Avg. fwd time: 7.1542 / Avg. bwd time: 18.9681 / Avg. batch time: 547.3931 (ms) / GPU bubble ratio: 61.82%
[rank1]:2025-11-09 16:53:32,923 - INFO - Avg. fwd time: 9.1180 / Avg. bwd time: 24.1368 / Avg. batch time: 586.2631 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 16:53:32,932 - INFO - Avg. fwd time: 7.8928 / Avg. bwd time: 23.5154 / Avg. batch time: 622.6455 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 16:53:33,104 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3281  memory: 11.81GiB(24.85%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 16:53:33,116 - INFO -  step: 500  loss:  0.3992  grad_norm:  0.3281  memory: 26.98GiB(56.79%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank1]:2025-11-09 16:53:33,108 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3281  memory: 14.64GiB(30.82%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-09 16:53:33,118 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3281  memory: 16.57GiB(34.88%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 16:53:33,281 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1653_real_step500_rank3.svg
[rank3]:> Batch Time: 621.55 ms, GPU Bubble Ratio: 59.33%, 57.01%, 66.25%, 26.64%
[rank0]:2025-11-09 16:55:40,264 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 16:55:42,553 - INFO - Avg. fwd time: 11.4090 / Avg. bwd time: 45.6024 / Avg. batch time: 515.8136 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 16:55:42,676 - INFO - Avg. fwd time: 7.8929 / Avg. bwd time: 23.5148 / Avg. batch time: 622.7112 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 16:55:42,626 - INFO - Avg. fwd time: 7.1541 / Avg. bwd time: 18.9697 / Avg. batch time: 547.4506 (ms) / GPU bubble ratio: 61.82%
[rank1]:2025-11-09 16:55:42,666 - INFO - Avg. fwd time: 9.1182 / Avg. bwd time: 24.1434 / Avg. batch time: 586.3362 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-09 16:55:42,859 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3856  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank2]:2025-11-09 16:55:42,844 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3856  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 16:55:42,857 - INFO -  step: 550  loss:  0.4434  grad_norm:  0.3856  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-09 16:55:42,848 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3856  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 16:57:49,767 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 16:57:52,328 - INFO - Avg. fwd time: 7.1539 / Avg. bwd time: 18.9710 / Avg. batch time: 547.4073 (ms) / GPU bubble ratio: 61.82%
[rank3]:2025-11-09 16:57:52,300 - INFO - Avg. fwd time: 11.4065 / Avg. bwd time: 45.6037 / Avg. batch time: 515.7995 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 16:57:52,360 - INFO - Avg. fwd time: 9.1186 / Avg. bwd time: 24.1496 / Avg. batch time: 586.3115 (ms) / GPU bubble ratio: 54.61%
[rank1]:2025-11-09 16:57:52,417 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3837  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 16:57:52,391 - INFO - Avg. fwd time: 7.8931 / Avg. bwd time: 23.5148 / Avg. batch time: 622.6805 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-09 16:57:52,427 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3837  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank2]:2025-11-09 16:57:52,413 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3837  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 16:57:52,426 - INFO -  step: 600  loss:  0.4444  grad_norm:  0.3837  memory: 26.98GiB(56.79%)  tps: 6,323  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 16:57:52,574 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1657_real_step600_rank3.svg
[rank3]:> Batch Time: 622.09 ms, GPU Bubble Ratio: 59.31%, 57.03%, 66.27%, 26.58%
[rank0]:2025-11-09 16:59:59,526 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:00:01,814 - INFO - Avg. fwd time: 11.4064 / Avg. bwd time: 45.6052 / Avg. batch time: 515.8098 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 17:00:01,934 - INFO - Avg. fwd time: 7.8925 / Avg. bwd time: 23.5145 / Avg. batch time: 622.7190 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 17:00:01,887 - INFO - Avg. fwd time: 7.1540 / Avg. bwd time: 18.9722 / Avg. batch time: 547.4389 (ms) / GPU bubble ratio: 61.82%
[rank1]:2025-11-09 17:00:01,927 - INFO - Avg. fwd time: 9.1186 / Avg. bwd time: 24.1547 / Avg. batch time: 586.3564 (ms) / GPU bubble ratio: 54.60%
[rank1]:2025-11-09 17:00:02,107 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3963  memory: 14.64GiB(30.82%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank0]:2025-11-09 17:00:02,118 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3963  memory: 16.57GiB(34.88%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank2]:2025-11-09 17:00:02,103 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3963  memory: 11.81GiB(24.85%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank3]:2025-11-09 17:00:02,116 - INFO -  step: 650  loss:  0.3801  grad_norm:  0.3963  memory: 26.98GiB(56.79%)  tps: 6,317  tflops: 48.11  mfu: 15.42%
[rank0]:2025-11-09 17:02:08,786 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:02:11,098 - INFO - Avg. fwd time: 11.4017 / Avg. bwd time: 45.5979 / Avg. batch time: 515.7095 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 17:02:11,221 - INFO - Avg. fwd time: 7.8918 / Avg. bwd time: 23.5133 / Avg. batch time: 622.5977 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 17:02:11,174 - INFO - Avg. fwd time: 7.1535 / Avg. bwd time: 18.9722 / Avg. batch time: 547.3134 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 17:02:11,213 - INFO - Avg. fwd time: 9.1181 / Avg. bwd time: 24.1575 / Avg. batch time: 586.2418 (ms) / GPU bubble ratio: 54.59%
[rank1]:2025-11-09 17:02:11,398 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4275  memory: 14.64GiB(30.82%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank0]:2025-11-09 17:02:11,409 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4275  memory: 16.57GiB(34.88%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank2]:2025-11-09 17:02:11,394 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4275  memory: 11.81GiB(24.85%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 17:02:11,407 - INFO -  step: 700  loss:  0.4541  grad_norm:  0.4275  memory: 26.98GiB(56.79%)  tps: 6,336  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 17:02:11,556 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1702_real_step700_rank3.svg
[rank3]:> Batch Time: 620.05 ms, GPU Bubble Ratio: 59.25%, 56.92%, 66.23%, 26.80%
[rank0]:2025-11-09 17:04:18,048 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:04:20,540 - INFO - Avg. fwd time: 11.3966 / Avg. bwd time: 45.5845 / Avg. batch time: 515.5576 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 17:04:20,569 - INFO - Avg. fwd time: 7.1526 / Avg. bwd time: 18.9713 / Avg. batch time: 547.1763 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 17:04:20,602 - INFO - Avg. fwd time: 9.1167 / Avg. bwd time: 24.1590 / Avg. batch time: 586.1113 (ms) / GPU bubble ratio: 54.58%
[rank1]:2025-11-09 17:04:20,659 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4964  memory: 14.64GiB(30.82%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank0]:2025-11-09 17:04:20,634 - INFO - Avg. fwd time: 7.8904 / Avg. bwd time: 23.5118 / Avg. batch time: 622.4622 (ms) / GPU bubble ratio: 59.64%
[rank0]:2025-11-09 17:04:20,670 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4964  memory: 16.57GiB(34.88%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank3]:2025-11-09 17:04:20,669 - INFO -  step: 750  loss:  0.4353  grad_norm:  0.4964  memory: 26.98GiB(56.79%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank2]:2025-11-09 17:04:20,656 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4964  memory: 11.81GiB(24.85%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank0]:2025-11-09 17:06:27,351 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:06:29,669 - INFO - Avg. fwd time: 11.3929 / Avg. bwd time: 45.5807 / Avg. batch time: 515.4944 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 17:06:29,744 - INFO - Avg. fwd time: 7.1521 / Avg. bwd time: 18.9711 / Avg. batch time: 547.0940 (ms) / GPU bubble ratio: 61.80%
[rank1]:2025-11-09 17:06:29,784 - INFO - Avg. fwd time: 9.1163 / Avg. bwd time: 24.1595 / Avg. batch time: 586.0298 (ms) / GPU bubble ratio: 54.57%
[rank0]:2025-11-09 17:06:29,791 - INFO - Avg. fwd time: 7.8903 / Avg. bwd time: 23.5112 / Avg. batch time: 622.3781 (ms) / GPU bubble ratio: 59.64%
[rank0]:2025-11-09 17:06:29,975 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4037  memory: 16.57GiB(34.88%)  tps: 6,335  tflops: 48.25  mfu: 15.47%
[rank0]:2025-11-09 17:06:29,975 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4037  tps: 6,701  tflops: 51.04  mfu: 14.83%
[rank0]:2025-11-09 17:06:29,975 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 17:06:29,973 - INFO -  step: 800  loss:  0.4297  grad_norm:  0.4037  memory: 26.98GiB(56.79%)  tps: 6,336  tflops: 48.25  mfu: 15.47%
[rank3]:2025-11-09 17:06:29,974 - INFO -  final step: 800  loss:  0.4297  grad_norm:  0.4037  tps: 6,708  tflops: 51.09  mfu: 14.93%
[rank3]:2025-11-09 17:06:29,974 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 17:06:29,975 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 17:06:29,961 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4037  memory: 11.81GiB(24.85%)  tps: 6,335  tflops: 48.25  mfu: 15.47%
[rank2]:2025-11-09 17:06:29,961 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4037  tps: 6,702  tflops: 51.05  mfu: 14.84%
[rank2]:2025-11-09 17:06:29,961 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 17:06:29,962 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 17:06:29,965 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4037  memory: 14.64GiB(30.82%)  tps: 6,335  tflops: 48.25  mfu: 15.47%
[rank1]:2025-11-09 17:06:29,965 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4037  tps: 6,701  tflops: 51.04  mfu: 14.83%
[rank1]:2025-11-09 17:06:29,965 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 17:06:29,966 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 17:06:29,976 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 17:06:32,037 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 17:06:32,049 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-09 17:06:32,049 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 17:06:32,049 - INFO - Destroying the purge thread.
[rank3]:2025-11-09 17:06:32,192 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1706_real_final800_rank3.svg
[rank3]:> Batch Time: 621.59 ms, GPU Bubble Ratio: 59.31%, 57.06%, 66.26%, 26.59%
[rank3]:2025-11-09 17:06:32,331 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1/pipeline_schedule/251109_1706_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.42 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 17:06:32,332 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank2]:2025-11-09 17:06:32,479 - INFO - Process group destroyed
[rank1]:2025-11-09 17:06:32,521 - INFO - Process group destroyed
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.4426
[rank3]:wandb:               final/avg_loss 0.42971
[rank3]:wandb:             final/avg_mfu(%) 14.92861
[rank3]:wandb:             final/avg_tflops 51.08652
[rank3]:wandb:    final/avg_throughput(tps) 6707.60126
[rank3]:wandb:              final/grad_norm 0.40372
[rank3]:wandb:               final/max_loss 0.42971
[rank3]:wandb:                    grad_norm 0.40372
[rank3]:wandb: loss_metrics/global_avg_loss 0.42971
[rank3]:wandb: loss_metrics/global_max_loss 0.42971
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed33_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/p86xx3lt
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed33_dm1/20251109-1631/wandb/run-20251109_163152-p86xx3lt/logs
[rank3]:2025-11-09 17:06:33,485 - INFO - Process group destroyed
[rank0]:2025-11-09 17:06:34,049 - INFO - Training completed
[rank0]:2025-11-09 17:06:34,050 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 17:06:34,519 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.3', 'tok_embeddings', 'layers.2', 'layers.0'}
[rank1]:Stage 1: Modules to keep: {'layers.5', 'layers.4', 'layers.7', 'layers.8', 'layers.6'}
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.11', 'layers.10', 'layers.9'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.14', 'layers.15', 'output', 'layers.13'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed33_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed33_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 33
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed33_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
