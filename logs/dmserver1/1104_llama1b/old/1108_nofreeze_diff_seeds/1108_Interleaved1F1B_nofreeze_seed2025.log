
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 07. (ê¸ˆ) 17:51:12 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_Interleaved1F1B_nofreeze_seed2025.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x Interleaved1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-07 17:51:18,550 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-07 17:51:18,611 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-07 17:51:18,771 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 17:51:18,773 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 17:51:18,776 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-07 17:51:18,777 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-07 17:51:18,852 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank2]:2025-11-07 17:51:18,814 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-07 17:51:18,816 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-07 17:51:19,010 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-07 17:51:19,013 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-07 17:51:19,007 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank0]:2025-11-07 17:51:19,168 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank1]:2025-11-07 17:51:19,159 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-07 17:51:19,162 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 17:51:21,996 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-07 17:51:22,146 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 17:51:22,182 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 17:51:22,183 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-07 17:51:22,114 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-07 17:51:22,154 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 17:51:22,182 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
[rank2]:2025-11-07 17:51:22,195 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
[rank2]:2025-11-07 17:51:22,196 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-07 17:51:22,206 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:2025-11-07 17:51:22,219 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:2025-11-07 17:51:22,220 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-07 17:51:22,392 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 17:51:22,393 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-07 17:51:22,393 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-07 17:51:22,428 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 17:51:22,428 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-07 17:51:22,428 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-07 17:51:22,782 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-07 17:51:22,819 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 17:51:22,840 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
[rank1]:2025-11-07 17:51:22,860 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
[rank1]:2025-11-07 17:51:22,862 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-11-07 17:51:23,053 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 17:51:23,053 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-07 17:51:23,054 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run eg9d7op9
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_dm1/20251107-1751/wandb/run-20251107_175123-eg9d7op9
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/eg9d7op9
[rank3]:2025-11-07 17:51:24,680 - INFO - WandB logging enabled
[rank3]:2025-11-07 17:51:24,681 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-07 17:51:24,720 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 17:51:24,750 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:2025-11-07 17:51:24,763 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:2025-11-07 17:51:24,764 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:2025-11-07 17:51:24,950 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 17:51:24,951 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-07 17:51:24,952 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-07 17:51:24,970 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 17:51:24,972 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank0]:2025-11-07 17:51:24,973 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 17:51:24,973 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-07 17:51:24,973 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank2]:2025-11-07 17:51:24,972 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-07 17:51:24,971 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 17:51:27,509 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-07 17:51:27,509 - INFO - Finished loading the checkpoint in 2.54 seconds.
[rank0]:2025-11-07 17:51:27,509 - INFO - Training starts at step 1
[rank3]:2025-11-07 17:51:30,903 - INFO -  step:  1  loss:  8.4951  grad_norm: 132.8870  memory: 15.04GiB(31.66%)  tps: 2,651  tflops: 20.19  mfu: 6.47%
[rank3]:2025-11-07 17:51:30,904 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-07 17:51:30,905 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8870  memory:  9.58GiB(20.17%)  tps: 2,026  tflops: 15.43  mfu: 4.95%
[rank1]:2025-11-07 17:51:30,905 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-07 17:51:30,889 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8870  memory:  6.31GiB(13.29%)  tps: 1,876  tflops: 14.29  mfu: 4.58%
[rank2]:2025-11-07 17:51:30,890 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 17:51:30,927 - INFO -  step:  1  loss: -4.0000  grad_norm: 132.8870  memory: 10.96GiB(23.08%)  tps: 1,874  tflops: 14.27  mfu: 4.57%
[rank0]:2025-11-07 17:51:30,927 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 17:53:42,125 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 17:53:44,535 - INFO - Avg. fwd time: 6.8396 / Avg. bwd time: 22.0570 / Avg. batch time: 570.8930 (ms) / GPU bubble ratio: 19.01%
[rank0]:2025-11-07 17:53:44,614 - INFO - Avg. fwd time: 4.1150 / Avg. bwd time: 11.9954 / Avg. batch time: 655.6062 (ms) / GPU bubble ratio: 60.68%
[rank2]:2025-11-07 17:53:44,594 - INFO - Avg. fwd time: 3.9710 / Avg. bwd time: 10.0445 / Avg. batch time: 603.3305 (ms) / GPU bubble ratio: 62.83%
[rank1]:2025-11-07 17:53:44,696 - INFO - Avg. fwd time: 4.8930 / Avg. bwd time: 12.2512 / Avg. batch time: 632.7521 (ms) / GPU bubble ratio: 56.65%
[rank2]:2025-11-07 17:53:44,860 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.2682  memory:  8.13GiB(17.12%)  tps: 5,992  tflops: 45.64  mfu: 14.63%
[rank3]:2025-11-07 17:53:44,873 - INFO -  step: 50  loss:  7.2373  grad_norm: 12.2682  memory: 18.46GiB(38.86%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank1]:2025-11-07 17:53:44,864 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.2682  memory: 11.85GiB(24.93%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank0]:2025-11-07 17:53:44,875 - INFO -  step: 50  loss: -4.0000  grad_norm: 12.2682  memory: 14.71GiB(30.97%)  tps: 5,994  tflops: 45.65  mfu: 14.63%
[rank0]:2025-11-07 17:55:59,782 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 17:56:02,234 - INFO - Avg. fwd time: 6.8893 / Avg. bwd time: 22.2107 / Avg. batch time: 574.1445 (ms) / GPU bubble ratio: 18.91%
[rank0]:2025-11-07 17:56:02,313 - INFO - Avg. fwd time: 4.1128 / Avg. bwd time: 12.0163 / Avg. batch time: 658.6646 (ms) / GPU bubble ratio: 60.82%
[rank2]:2025-11-07 17:56:02,293 - INFO - Avg. fwd time: 3.9723 / Avg. bwd time: 10.0738 / Avg. batch time: 606.2997 (ms) / GPU bubble ratio: 62.93%
[rank1]:2025-11-07 17:56:02,397 - INFO - Avg. fwd time: 4.8837 / Avg. bwd time: 12.2883 / Avg. batch time: 635.7856 (ms) / GPU bubble ratio: 56.79%
[rank1]:2025-11-07 17:56:02,569 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.3340  memory: 11.85GiB(24.93%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank0]:2025-11-07 17:56:02,579 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.3340  memory: 14.71GiB(30.97%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank2]:2025-11-07 17:56:02,565 - INFO -  step: 100  loss: -4.0000  grad_norm: 17.3340  memory:  8.13GiB(17.12%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank3]:2025-11-07 17:56:02,577 - INFO -  step: 100  loss:  2.6906  grad_norm: 17.3340  memory: 18.46GiB(38.86%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank3]:2025-11-07 17:56:02,870 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1756_real_step100_rank3.svg
[rank3]:> Batch Time: 662.35 ms, GPU Bubble Ratio: 60.79%, 58.35%, 65.89%, 29.30%
[rank0]:2025-11-07 17:58:17,889 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 17:58:20,562 - INFO - Avg. fwd time: 6.9044 / Avg. bwd time: 22.2576 / Avg. batch time: 575.1225 (ms) / GPU bubble ratio: 18.87%
[rank2]:2025-11-07 17:58:20,586 - INFO - Avg. fwd time: 3.9719 / Avg. bwd time: 10.0891 / Avg. batch time: 607.6462 (ms) / GPU bubble ratio: 62.98%
[rank1]:2025-11-07 17:58:20,612 - INFO - Avg. fwd time: 4.8815 / Avg. bwd time: 12.3080 / Avg. batch time: 637.1746 (ms) / GPU bubble ratio: 56.84%
[rank1]:2025-11-07 17:58:20,663 - INFO -  step: 150  loss: -4.0000  grad_norm:  3.9376  memory: 11.85GiB(24.93%)  tps: 5,932  tflops: 45.18  mfu: 14.48%
[rank0]:2025-11-07 17:58:20,638 - INFO - Avg. fwd time: 4.1160 / Avg. bwd time: 12.0285 / Avg. batch time: 660.0978 (ms) / GPU bubble ratio: 60.87%
[rank0]:2025-11-07 17:58:20,674 - INFO -  step: 150  loss: -4.0000  grad_norm:  3.9376  memory: 14.71GiB(30.97%)  tps: 5,932  tflops: 45.18  mfu: 14.48%
[rank3]:2025-11-07 17:58:20,672 - INFO -  step: 150  loss:  0.6123  grad_norm:  3.9376  memory: 18.46GiB(38.86%)  tps: 5,932  tflops: 45.18  mfu: 14.48%
[rank2]:2025-11-07 17:58:20,660 - INFO -  step: 150  loss: -4.0000  grad_norm:  3.9376  memory:  8.13GiB(17.12%)  tps: 5,932  tflops: 45.18  mfu: 14.48%
[rank0]:2025-11-07 18:00:35,570 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:00:38,027 - INFO - Avg. fwd time: 6.9090 / Avg. bwd time: 22.2803 / Avg. batch time: 575.5558 (ms) / GPU bubble ratio: 18.86%
[rank2]:2025-11-07 18:00:38,086 - INFO - Avg. fwd time: 3.9720 / Avg. bwd time: 10.0968 / Avg. batch time: 607.9347 (ms) / GPU bubble ratio: 62.97%
[rank0]:2025-11-07 18:00:38,106 - INFO - Avg. fwd time: 4.1139 / Avg. bwd time: 12.0350 / Avg. batch time: 660.3954 (ms) / GPU bubble ratio: 60.87%
[rank1]:2025-11-07 18:00:38,190 - INFO - Avg. fwd time: 4.8806 / Avg. bwd time: 12.3203 / Avg. batch time: 637.4800 (ms) / GPU bubble ratio: 56.83%
[rank3]:2025-11-07 18:00:38,368 - INFO -  step: 200  loss:  0.5482  grad_norm:  2.4134  memory: 18.46GiB(38.86%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank2]:2025-11-07 18:00:38,355 - INFO -  step: 200  loss: -4.0000  grad_norm:  2.4134  memory:  8.13GiB(17.12%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank0]:2025-11-07 18:00:38,370 - INFO -  step: 200  loss: -4.0000  grad_norm:  2.4134  memory: 14.71GiB(30.97%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank1]:2025-11-07 18:00:38,359 - INFO -  step: 200  loss: -4.0000  grad_norm:  2.4134  memory: 11.85GiB(24.93%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank3]:2025-11-07 18:00:38,626 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1800_real_step200_rank3.svg
[rank3]:> Batch Time: 661.35 ms, GPU Bubble Ratio: 60.68%, 58.21%, 65.79%, 29.11%
[rank0]:2025-11-07 18:02:53,810 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:02:56,251 - INFO - Avg. fwd time: 6.9157 / Avg. bwd time: 22.3003 / Avg. batch time: 575.9871 (ms) / GPU bubble ratio: 18.84%
[rank2]:2025-11-07 18:02:56,309 - INFO - Avg. fwd time: 3.9726 / Avg. bwd time: 10.1013 / Avg. batch time: 608.5111 (ms) / GPU bubble ratio: 62.99%
[rank0]:2025-11-07 18:02:56,330 - INFO - Avg. fwd time: 4.1148 / Avg. bwd time: 12.0381 / Avg. batch time: 660.9867 (ms) / GPU bubble ratio: 60.90%
[rank1]:2025-11-07 18:02:56,413 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3270 / Avg. batch time: 638.0679 (ms) / GPU bubble ratio: 56.85%
[rank2]:2025-11-07 18:02:56,577 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.7298  memory:  8.13GiB(17.12%)  tps: 5,927  tflops: 45.14  mfu: 14.47%
[rank3]:2025-11-07 18:02:56,590 - INFO -  step: 250  loss:  0.5408  grad_norm:  0.7298  memory: 18.46GiB(38.86%)  tps: 5,927  tflops: 45.14  mfu: 14.47%
[rank0]:2025-11-07 18:02:56,592 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.7298  memory: 14.71GiB(30.97%)  tps: 5,927  tflops: 45.14  mfu: 14.47%
[rank1]:2025-11-07 18:02:56,581 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.7298  memory: 11.85GiB(24.93%)  tps: 5,927  tflops: 45.14  mfu: 14.47%
[rank0]:2025-11-07 18:05:11,851 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 18:05:14,585 - INFO - Avg. fwd time: 3.9732 / Avg. bwd time: 10.1056 / Avg. batch time: 608.7128 (ms) / GPU bubble ratio: 62.99%
[rank3]:2025-11-07 18:05:14,559 - INFO - Avg. fwd time: 6.9203 / Avg. bwd time: 22.3152 / Avg. batch time: 576.2976 (ms) / GPU bubble ratio: 18.83%
[rank1]:2025-11-07 18:05:14,610 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3339 / Avg. batch time: 638.2845 (ms) / GPU bubble ratio: 56.85%
[rank1]:2025-11-07 18:05:14,660 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3840  memory: 11.85GiB(24.93%)  tps: 5,933  tflops: 45.19  mfu: 14.48%
[rank2]:2025-11-07 18:05:14,656 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3840  memory:  8.13GiB(17.12%)  tps: 5,933  tflops: 45.19  mfu: 14.48%
[rank0]:2025-11-07 18:05:14,635 - INFO - Avg. fwd time: 4.1159 / Avg. bwd time: 12.0419 / Avg. batch time: 661.2121 (ms) / GPU bubble ratio: 60.90%
[rank0]:2025-11-07 18:05:14,671 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3840  memory: 14.71GiB(30.97%)  tps: 5,933  tflops: 45.19  mfu: 14.48%
[rank3]:2025-11-07 18:05:14,669 - INFO -  step: 300  loss:  0.5142  grad_norm:  0.3840  memory: 18.46GiB(38.86%)  tps: 5,933  tflops: 45.19  mfu: 14.48%
[rank3]:2025-11-07 18:05:14,955 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1805_real_step300_rank3.svg
[rank3]:> Batch Time: 660.86 ms, GPU Bubble Ratio: 60.65%, 58.17%, 65.75%, 29.19%
[rank0]:2025-11-07 18:07:29,832 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:07:32,257 - INFO - Avg. fwd time: 6.9212 / Avg. bwd time: 22.3163 / Avg. batch time: 576.3280 (ms) / GPU bubble ratio: 18.83%
[rank2]:2025-11-07 18:07:32,320 - INFO - Avg. fwd time: 3.9729 / Avg. bwd time: 10.1073 / Avg. batch time: 608.8964 (ms) / GPU bubble ratio: 63.00%
[rank0]:2025-11-07 18:07:32,340 - INFO - Avg. fwd time: 4.1154 / Avg. bwd time: 12.0442 / Avg. batch time: 661.3960 (ms) / GPU bubble ratio: 60.91%
[rank1]:2025-11-07 18:07:32,425 - INFO - Avg. fwd time: 4.8802 / Avg. bwd time: 12.3365 / Avg. batch time: 638.4714 (ms) / GPU bubble ratio: 56.86%
[rank1]:2025-11-07 18:07:32,590 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3735  memory: 11.85GiB(24.93%)  tps: 5,939  tflops: 45.23  mfu: 14.50%
[rank2]:2025-11-07 18:07:32,586 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3735  memory:  8.13GiB(17.12%)  tps: 5,939  tflops: 45.23  mfu: 14.50%
[rank0]:2025-11-07 18:07:32,601 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3735  memory: 14.71GiB(30.97%)  tps: 5,939  tflops: 45.23  mfu: 14.50%
[rank3]:2025-11-07 18:07:32,599 - INFO -  step: 350  loss:  0.4854  grad_norm:  0.3735  memory: 18.46GiB(38.86%)  tps: 5,939  tflops: 45.24  mfu: 14.50%
[rank0]:2025-11-07 18:09:47,709 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 18:09:50,236 - INFO - Avg. fwd time: 3.9725 / Avg. bwd time: 10.1089 / Avg. batch time: 608.9363 (ms) / GPU bubble ratio: 63.00%
[rank3]:2025-11-07 18:09:50,178 - INFO - Avg. fwd time: 6.9233 / Avg. bwd time: 22.3208 / Avg. batch time: 576.4312 (ms) / GPU bubble ratio: 18.83%
[rank0]:2025-11-07 18:09:50,251 - INFO - Avg. fwd time: 4.1147 / Avg. bwd time: 12.0450 / Avg. batch time: 661.4354 (ms) / GPU bubble ratio: 60.91%
[rank1]:2025-11-07 18:09:50,340 - INFO - Avg. fwd time: 4.8804 / Avg. bwd time: 12.3404 / Avg. batch time: 638.5146 (ms) / GPU bubble ratio: 56.85%
[rank2]:2025-11-07 18:09:50,500 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3562  memory:  8.13GiB(17.12%)  tps: 5,940  tflops: 45.24  mfu: 14.50%
[rank1]:2025-11-07 18:09:50,503 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3562  memory: 11.85GiB(24.93%)  tps: 5,940  tflops: 45.24  mfu: 14.50%
[rank3]:2025-11-07 18:09:50,512 - INFO -  step: 400  loss:  0.4730  grad_norm:  0.3562  memory: 18.46GiB(38.86%)  tps: 5,940  tflops: 45.24  mfu: 14.50%
[rank0]:2025-11-07 18:09:50,514 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3562  memory: 14.71GiB(30.97%)  tps: 5,940  tflops: 45.24  mfu: 14.50%
[rank3]:2025-11-07 18:09:50,761 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1809_real_step400_rank3.svg
[rank3]:> Batch Time: 660.84 ms, GPU Bubble Ratio: 60.65%, 58.14%, 65.79%, 29.07%
[rank3]:2025-11-07 18:10:00,062 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-07 18:10:00,355 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-07 18:10:00,338 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-07 18:10:00,372 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-07 18:12:05,822 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:12:08,515 - INFO - Avg. fwd time: 6.9260 / Avg. bwd time: 22.3283 / Avg. batch time: 576.5935 (ms) / GPU bubble ratio: 18.82%
[rank2]:2025-11-07 18:12:08,541 - INFO - Avg. fwd time: 3.9723 / Avg. bwd time: 10.1104 / Avg. batch time: 609.1611 (ms) / GPU bubble ratio: 63.01%
[rank2]:2025-11-07 18:12:08,616 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3627  memory:  8.13GiB(17.12%)  tps: 5,931  tflops: 45.17  mfu: 14.48%
[rank1]:2025-11-07 18:12:08,568 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3446 / Avg. batch time: 638.7479 (ms) / GPU bubble ratio: 56.85%
[rank1]:2025-11-07 18:12:08,619 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3627  memory: 11.85GiB(24.93%)  tps: 5,931  tflops: 45.17  mfu: 14.48%
[rank0]:2025-11-07 18:12:08,594 - INFO - Avg. fwd time: 4.1150 / Avg. bwd time: 12.0469 / Avg. batch time: 661.6739 (ms) / GPU bubble ratio: 60.92%
[rank0]:2025-11-07 18:12:08,630 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3627  memory: 14.71GiB(30.97%)  tps: 5,931  tflops: 45.17  mfu: 14.48%
[rank3]:2025-11-07 18:12:08,628 - INFO -  step: 450  loss:  0.5240  grad_norm:  0.3627  memory: 18.46GiB(38.86%)  tps: 5,931  tflops: 45.17  mfu: 14.48%
[rank0]:2025-11-07 18:14:23,569 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank0]:2025-11-07 18:14:26,086 - INFO - Avg. fwd time: 4.1145 / Avg. bwd time: 12.0477 / Avg. batch time: 661.6490 (ms) / GPU bubble ratio: 60.92%
[rank3]:2025-11-07 18:14:26,007 - INFO - Avg. fwd time: 6.9264 / Avg. bwd time: 22.3298 / Avg. batch time: 576.6206 (ms) / GPU bubble ratio: 18.82%
[rank2]:2025-11-07 18:14:26,066 - INFO - Avg. fwd time: 3.9720 / Avg. bwd time: 10.1110 / Avg. batch time: 609.1333 (ms) / GPU bubble ratio: 63.01%
[rank1]:2025-11-07 18:14:26,166 - INFO - Avg. fwd time: 4.8806 / Avg. bwd time: 12.3467 / Avg. batch time: 638.7250 (ms) / GPU bubble ratio: 56.85%
[rank0]:2025-11-07 18:14:26,340 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2890  memory: 14.71GiB(30.97%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank3]:2025-11-07 18:14:26,337 - INFO -  step: 500  loss:  0.3924  grad_norm:  0.2890  memory: 18.46GiB(38.86%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank1]:2025-11-07 18:14:26,329 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2890  memory: 11.85GiB(24.93%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank2]:2025-11-07 18:14:26,325 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2890  memory:  8.13GiB(17.12%)  tps: 5,949  tflops: 45.31  mfu: 14.52%
[rank3]:2025-11-07 18:14:26,597 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1814_real_step500_rank3.svg
[rank3]:> Batch Time: 659.84 ms, GPU Bubble Ratio: 60.61%, 58.11%, 65.78%, 29.33%
[rank0]:2025-11-07 18:16:41,319 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:16:43,767 - INFO - Avg. fwd time: 6.9250 / Avg. bwd time: 22.3270 / Avg. batch time: 576.5522 (ms) / GPU bubble ratio: 18.82%
[rank2]:2025-11-07 18:16:43,825 - INFO - Avg. fwd time: 3.9715 / Avg. bwd time: 10.1109 / Avg. batch time: 609.1283 (ms) / GPU bubble ratio: 63.01%
[rank0]:2025-11-07 18:16:43,847 - INFO - Avg. fwd time: 4.1137 / Avg. bwd time: 12.0478 / Avg. batch time: 661.6408 (ms) / GPU bubble ratio: 60.92%
[rank1]:2025-11-07 18:16:43,921 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3473 / Avg. batch time: 638.7206 (ms) / GPU bubble ratio: 56.84%
[rank2]:2025-11-07 18:16:44,081 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3651  memory:  8.13GiB(17.12%)  tps: 5,947  tflops: 45.29  mfu: 14.52%
[rank0]:2025-11-07 18:16:44,095 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3651  memory: 14.71GiB(30.97%)  tps: 5,947  tflops: 45.29  mfu: 14.52%
[rank3]:2025-11-07 18:16:44,094 - INFO -  step: 550  loss:  0.4322  grad_norm:  0.3651  memory: 18.46GiB(38.86%)  tps: 5,947  tflops: 45.29  mfu: 14.52%
[rank1]:2025-11-07 18:16:44,085 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3651  memory: 11.85GiB(24.93%)  tps: 5,947  tflops: 45.29  mfu: 14.52%
[rank0]:2025-11-07 18:18:59,153 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 18:19:01,896 - INFO - Avg. fwd time: 3.9711 / Avg. bwd time: 10.1113 / Avg. batch time: 609.1753 (ms) / GPU bubble ratio: 63.01%
[rank3]:2025-11-07 18:19:01,872 - INFO - Avg. fwd time: 6.9269 / Avg. bwd time: 22.3317 / Avg. batch time: 576.6568 (ms) / GPU bubble ratio: 18.82%
[rank1]:2025-11-07 18:19:01,924 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3493 / Avg. batch time: 638.7716 (ms) / GPU bubble ratio: 56.84%
[rank0]:2025-11-07 18:19:01,952 - INFO - Avg. fwd time: 4.1133 / Avg. bwd time: 12.0488 / Avg. batch time: 661.6932 (ms) / GPU bubble ratio: 60.92%
[rank0]:2025-11-07 18:19:01,987 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3444  memory: 14.71GiB(30.97%)  tps: 5,941  tflops: 45.25  mfu: 14.50%
[rank2]:2025-11-07 18:19:01,973 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3444  memory:  8.13GiB(17.12%)  tps: 5,941  tflops: 45.25  mfu: 14.50%
[rank3]:2025-11-07 18:19:01,986 - INFO -  step: 600  loss:  0.4280  grad_norm:  0.3444  memory: 18.46GiB(38.86%)  tps: 5,941  tflops: 45.25  mfu: 14.50%
[rank1]:2025-11-07 18:19:01,977 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3444  memory: 11.85GiB(24.93%)  tps: 5,941  tflops: 45.25  mfu: 14.50%
[rank3]:2025-11-07 18:19:02,236 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1819_real_step600_rank3.svg
[rank3]:> Batch Time: 662.36 ms, GPU Bubble Ratio: 60.72%, 58.20%, 65.88%, 29.02%
[rank0]:2025-11-07 18:21:17,627 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:21:20,053 - INFO - Avg. fwd time: 6.9301 / Avg. bwd time: 22.3376 / Avg. batch time: 576.8039 (ms) / GPU bubble ratio: 18.81%
[rank0]:2025-11-07 18:21:20,131 - INFO - Avg. fwd time: 4.1133 / Avg. bwd time: 12.0495 / Avg. batch time: 661.9012 (ms) / GPU bubble ratio: 60.93%
[rank2]:2025-11-07 18:21:20,111 - INFO - Avg. fwd time: 3.9713 / Avg. bwd time: 10.1119 / Avg. batch time: 609.3805 (ms) / GPU bubble ratio: 63.02%
[rank1]:2025-11-07 18:21:20,206 - INFO - Avg. fwd time: 4.8808 / Avg. bwd time: 12.3521 / Avg. batch time: 638.9812 (ms) / GPU bubble ratio: 56.85%
[rank0]:2025-11-07 18:21:20,383 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3472  memory: 14.71GiB(30.97%)  tps: 5,919  tflops: 45.08  mfu: 14.45%
[rank3]:2025-11-07 18:21:20,381 - INFO -  step: 650  loss:  0.3883  grad_norm:  0.3472  memory: 18.46GiB(38.86%)  tps: 5,919  tflops: 45.08  mfu: 14.45%
[rank2]:2025-11-07 18:21:20,369 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3472  memory:  8.13GiB(17.12%)  tps: 5,919  tflops: 45.08  mfu: 14.45%
[rank1]:2025-11-07 18:21:20,373 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3472  memory: 11.85GiB(24.93%)  tps: 5,919  tflops: 45.08  mfu: 14.45%
[rank0]:2025-11-07 18:23:35,407 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:23:37,856 - INFO - Avg. fwd time: 6.9313 / Avg. bwd time: 22.3400 / Avg. batch time: 576.8603 (ms) / GPU bubble ratio: 18.81%
[rank0]:2025-11-07 18:23:37,938 - INFO - Avg. fwd time: 4.1131 / Avg. bwd time: 12.0499 / Avg. batch time: 661.9086 (ms) / GPU bubble ratio: 60.93%
[rank2]:2025-11-07 18:23:37,918 - INFO - Avg. fwd time: 3.9711 / Avg. bwd time: 10.1123 / Avg. batch time: 609.3877 (ms) / GPU bubble ratio: 63.02%
[rank1]:2025-11-07 18:23:38,012 - INFO - Avg. fwd time: 4.8808 / Avg. bwd time: 12.3539 / Avg. batch time: 638.9904 (ms) / GPU bubble ratio: 56.85%
[rank0]:2025-11-07 18:23:38,190 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3932  memory: 14.71GiB(30.97%)  tps: 5,945  tflops: 45.28  mfu: 14.51%
[rank3]:2025-11-07 18:23:38,188 - INFO -  step: 700  loss:  0.4351  grad_norm:  0.3932  memory: 18.46GiB(38.86%)  tps: 5,945  tflops: 45.28  mfu: 14.51%
[rank2]:2025-11-07 18:23:38,176 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3932  memory:  8.13GiB(17.12%)  tps: 5,945  tflops: 45.28  mfu: 14.51%
[rank1]:2025-11-07 18:23:38,179 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3932  memory: 11.85GiB(24.93%)  tps: 5,945  tflops: 45.28  mfu: 14.51%
[rank3]:2025-11-07 18:23:38,435 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1823_real_step700_rank3.svg
[rank3]:> Batch Time: 661.36 ms, GPU Bubble Ratio: 60.67%, 58.15%, 65.85%, 29.16%
[rank0]:2025-11-07 18:25:53,220 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:25:55,896 - INFO - Avg. fwd time: 6.9301 / Avg. bwd time: 22.3385 / Avg. batch time: 576.8173 (ms) / GPU bubble ratio: 18.81%
[rank2]:2025-11-07 18:25:55,922 - INFO - Avg. fwd time: 3.9708 / Avg. bwd time: 10.1123 / Avg. batch time: 609.3816 (ms) / GPU bubble ratio: 63.02%
[rank1]:2025-11-07 18:25:55,952 - INFO - Avg. fwd time: 4.8807 / Avg. bwd time: 12.3548 / Avg. batch time: 638.9875 (ms) / GPU bubble ratio: 56.84%
[rank0]:2025-11-07 18:25:55,981 - INFO - Avg. fwd time: 4.1127 / Avg. bwd time: 12.0499 / Avg. batch time: 661.9051 (ms) / GPU bubble ratio: 60.93%
[rank0]:2025-11-07 18:25:56,017 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4470  memory: 14.71GiB(30.97%)  tps: 5,944  tflops: 45.27  mfu: 14.51%
[rank3]:2025-11-07 18:25:56,015 - INFO -  step: 750  loss:  0.4151  grad_norm:  0.4470  memory: 18.46GiB(38.86%)  tps: 5,944  tflops: 45.27  mfu: 14.51%
[rank2]:2025-11-07 18:25:56,003 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4470  memory:  8.13GiB(17.12%)  tps: 5,944  tflops: 45.27  mfu: 14.51%
[rank1]:2025-11-07 18:25:56,006 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4470  memory: 11.85GiB(24.93%)  tps: 5,944  tflops: 45.27  mfu: 14.51%
[rank0]:2025-11-07 18:28:10,955 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 18:28:13,402 - INFO - Avg. fwd time: 6.9298 / Avg. bwd time: 22.3381 / Avg. batch time: 576.8027 (ms) / GPU bubble ratio: 18.81%
[rank0]:2025-11-07 18:28:13,490 - INFO - Avg. fwd time: 4.1127 / Avg. bwd time: 12.0499 / Avg. batch time: 661.8604 (ms) / GPU bubble ratio: 60.93%
[rank2]:2025-11-07 18:28:13,470 - INFO - Avg. fwd time: 3.9705 / Avg. bwd time: 10.1121 / Avg. batch time: 609.3336 (ms) / GPU bubble ratio: 63.02%
[rank1]:2025-11-07 18:28:13,569 - INFO - Avg. fwd time: 4.8805 / Avg. bwd time: 12.3553 / Avg. batch time: 638.9407 (ms) / GPU bubble ratio: 56.84%
[rank0]:2025-11-07 18:28:13,742 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3711  memory: 14.71GiB(30.97%)  tps: 5,948  tflops: 45.30  mfu: 14.52%
[rank0]:2025-11-07 18:28:13,742 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3711  tps: 6,297  tflops: 47.96  mfu: 13.92%
[rank0]:2025-11-07 18:28:13,742 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-07 18:28:13,743 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-07 18:28:13,728 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3711  memory:  8.13GiB(17.12%)  tps: 5,948  tflops: 45.30  mfu: 14.52%
[rank2]:2025-11-07 18:28:13,728 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3711  tps: 6,297  tflops: 47.96  mfu: 13.92%
[rank2]:2025-11-07 18:28:13,728 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-07 18:28:13,729 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-07 18:28:13,740 - INFO -  step: 800  loss:  0.4203  grad_norm:  0.3711  memory: 18.46GiB(38.86%)  tps: 5,948  tflops: 45.30  mfu: 14.52%
[rank3]:2025-11-07 18:28:13,741 - INFO -  final step: 800  loss:  0.4203  grad_norm:  0.3711  tps: 6,304  tflops: 48.02  mfu: 14.03%
[rank3]:2025-11-07 18:28:13,741 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-07 18:28:13,742 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-07 18:28:13,732 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3711  memory: 11.85GiB(24.93%)  tps: 5,948  tflops: 45.30  mfu: 14.52%
[rank1]:2025-11-07 18:28:13,732 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3711  tps: 6,299  tflops: 47.97  mfu: 13.94%
[rank1]:2025-11-07 18:28:13,732 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-07 18:28:13,733 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-07 18:28:15,681 - INFO - Destroying the purge thread.
[rank0]:2025-11-07 18:28:15,658 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-07 18:28:15,681 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-07 18:28:15,681 - INFO - Destroying the purge thread.
[rank3]:2025-11-07 18:28:15,915 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1828_real_final800_rank3.svg
[rank3]:> Batch Time: 661.35 ms, GPU Bubble Ratio: 60.68%, 58.19%, 65.83%, 29.21%
[rank2]:2025-11-07 18:28:16,078 - INFO - Process group destroyed
[rank1]:2025-11-07 18:28:16,065 - INFO - Process group destroyed
[rank3]:2025-11-07 18:28:16,155 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1/pipeline_schedule/251107_1828_thry_final800_rank3.svg
[rank3]:> Batch Time: 282.51 ms, GPU Bubble Ratio: 15.79%, 15.79%, 15.79%, 15.79%
[rank3]:2025-11-07 18:28:16,156 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 228-237
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.59881
[rank3]:wandb:               final/avg_loss 0.42026
[rank3]:wandb:             final/avg_mfu(%) 14.03448
[rank3]:wandb:             final/avg_tflops 48.01589
[rank3]:wandb:    final/avg_throughput(tps) 6304.43114
[rank3]:wandb:              final/grad_norm 0.37107
[rank3]:wandb:               final/max_loss 0.42026
[rank3]:wandb:                    grad_norm 0.37107
[rank3]:wandb: loss_metrics/global_avg_loss 0.42026
[rank3]:wandb: loss_metrics/global_max_loss 0.42026
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_Interleaved1F1B_nofreeze_seed2025_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/eg9d7op9
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_dm1/20251107-1751/wandb/run-20251107_175123-eg9d7op9/logs
[rank3]:2025-11-07 18:28:17,574 - INFO - Process group destroyed
[rank0]:2025-11-07 18:28:17,681 - INFO - Training completed
[rank0]:2025-11-07 18:28:17,681 - INFO - Destroying the purge thread.
[rank0]:2025-11-07 18:28:18,080 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.6', 'layers.5'}
[rank2]:Stage 6: Modules to keep: {'layers.14', 'layers.13'}
[rank0]:Stage 0: Modules to keep: {'layers.0', 'tok_embeddings', 'layers.1'}
[rank0]:Stage 4: Modules to keep: {'layers.10', 'layers.9'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.2', 'layers.3'}
[rank1]:Stage 5: Modules to keep: {'layers.12', 'layers.11'}
[rank3]:Stage 3: Modules to keep: {'layers.8', 'layers.7'}
[rank3]:Stage 7: Modules to keep: {'layers.15', 'norm', 'output'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 1e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 2025
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_Interleaved1F1B_nofreeze_seed2025_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
