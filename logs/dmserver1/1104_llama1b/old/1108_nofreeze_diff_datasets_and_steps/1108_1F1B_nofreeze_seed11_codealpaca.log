
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 23:41:48 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_1F1B_nofreeze_seed11_codealpaca.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x 1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4 --training.dataset=codealpaca  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank1]:2025-11-09 23:41:55,041 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank2]:2025-11-09 23:41:55,123 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:2025-11-09 23:41:55,086 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:"
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank0]:2025-11-09 23:41:55,284 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank1]:2025-11-09 23:41:55,269 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 23:41:55,271 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 23:41:55,325 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 23:41:55,327 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 23:41:55,372 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 23:41:55,374 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 23:41:55,466 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 23:41:55,469 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 23:41:55,472 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 23:41:55,474 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-09 23:41:55,870 - INFO - Preparing codealpaca dataset from sahil2801/CodeAlpaca-20k
[rank1]:2025-11-09 23:41:58,482 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 23:41:58,517 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 23:41:58,545 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 23:41:58,545 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-09 23:41:58,564 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 23:41:58,711 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 23:41:58,729 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 23:41:58,730 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 23:41:58,730 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-09 23:41:58,749 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 23:41:58,750 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 23:41:58,776 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 23:41:58,776 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-09 23:41:58,965 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 23:41:58,965 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 23:41:58,966 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-09 23:41:59,363 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 23:41:59,402 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 23:41:59,429 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 23:41:59,429 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank2]:2025-11-09 23:41:59,611 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 23:41:59,611 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 23:41:59,612 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: setting up run wo1r0tle
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed11_codealpaca_dm1/20251109-2341/wandb/run-20251109_234159-wo1r0tle
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_1F1B_nofreeze_seed11_codealpaca_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/wo1r0tle
[rank3]:2025-11-09 23:42:00,940 - INFO - WandB logging enabled
[rank3]:2025-11-09 23:42:00,940 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 23:42:00,980 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 23:42:01,010 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 23:42:01,011 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-09 23:42:01,219 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed11_codealpaca_dm1
[rank0]:2025-11-09 23:42:01,219 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 23:42:01,219 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 500 (warmup 100)
[rank0]:2025-11-09 23:42:01,220 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 23:42:01,218 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 23:42:01,219 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 23:42:01,201 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 23:42:01,202 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 23:42:01,203 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 23:42:01,219 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 23:42:03,596 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 23:42:03,596 - INFO - Finished loading the checkpoint in 2.38 seconds.
[rank0]:2025-11-09 23:42:03,596 - INFO - Training starts at step 1
[rank2]:2025-11-09 23:42:06,656 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory:  4.63GiB(9.75%)  tps: 2,259  tflops: 17.20  mfu: 5.51%
[rank2]:2025-11-09 23:42:06,657 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 23:42:06,659 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory:  6.76GiB(14.24%)  tps: 2,012  tflops: 15.33  mfu: 4.91%
[rank1]:2025-11-09 23:42:06,660 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 23:42:06,666 - INFO -  step:  1  loss: 10.1253  grad_norm: 147.6782  memory: 12.97GiB(27.30%)  tps: 2,883  tflops: 21.96  mfu: 7.04%
[rank3]:2025-11-09 23:42:06,666 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 23:42:06,694 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory:  9.19GiB(19.34%)  tps: 2,062  tflops: 15.71  mfu: 5.03%
[rank0]:2025-11-09 23:42:06,694 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 23:44:05,767 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank0]:2025-11-09 23:44:07,998 - INFO - Avg. fwd time: 7.5035 / Avg. bwd time: 23.8109 / Avg. batch time: 601.5116 (ms) / GPU bubble ratio: 58.35%
[rank3]:2025-11-09 23:44:07,929 - INFO - Avg. fwd time: 12.4058 / Avg. bwd time: 40.2800 / Avg. batch time: 493.5555 (ms) / GPU bubble ratio: 14.60%
[rank1]:2025-11-09 23:44:08,002 - INFO - Avg. fwd time: 9.3331 / Avg. bwd time: 23.7772 / Avg. batch time: 563.9018 (ms) / GPU bubble ratio: 53.03%
[rank2]:2025-11-09 23:44:07,967 - INFO - Avg. fwd time: 7.1231 / Avg. bwd time: 18.5670 / Avg. batch time: 525.0786 (ms) / GPU bubble ratio: 60.86%
[rank2]:2025-11-09 23:44:08,220 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9640  memory:  6.43GiB(13.53%)  tps: 6,604  tflops: 50.30  mfu: 16.12%
[rank0]:2025-11-09 23:44:08,234 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9640  memory: 12.97GiB(27.31%)  tps: 6,605  tflops: 50.31  mfu: 16.12%
[rank1]:2025-11-09 23:44:08,224 - INFO -  step: 50  loss: -4.0000  grad_norm: 23.9640  memory:  9.03GiB(19.01%)  tps: 6,604  tflops: 50.30  mfu: 16.12%
[rank3]:2025-11-09 23:44:08,233 - INFO -  step: 50  loss: 10.4980  grad_norm: 23.9640  memory: 16.39GiB(34.50%)  tps: 6,604  tflops: 50.30  mfu: 16.12%
[rank0]:2025-11-09 23:46:10,984 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:46:13,232 - INFO - Avg. fwd time: 12.6159 / Avg. bwd time: 40.5011 / Avg. batch time: 496.9338 (ms) / GPU bubble ratio: 14.49%
[rank2]:2025-11-09 23:46:13,270 - INFO - Avg. fwd time: 7.1174 / Avg. bwd time: 18.6409 / Avg. batch time: 528.0446 (ms) / GPU bubble ratio: 60.98%
[rank0]:2025-11-09 23:46:13,301 - INFO - Avg. fwd time: 7.4884 / Avg. bwd time: 23.8686 / Avg. batch time: 603.7050 (ms) / GPU bubble ratio: 58.45%
[rank1]:2025-11-09 23:46:13,306 - INFO - Avg. fwd time: 9.3133 / Avg. bwd time: 23.8657 / Avg. batch time: 566.4906 (ms) / GPU bubble ratio: 53.14%
[rank3]:2025-11-09 23:46:13,545 - INFO -  step: 100  loss:  4.7157  grad_norm: 24.1334  memory: 16.39GiB(34.50%)  tps: 6,537  tflops: 49.79  mfu: 15.96%
[rank0]:2025-11-09 23:46:13,548 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1334  memory: 12.97GiB(27.31%)  tps: 6,537  tflops: 49.79  mfu: 15.96%
[rank1]:2025-11-09 23:46:13,537 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1334  memory:  9.03GiB(19.01%)  tps: 6,537  tflops: 49.79  mfu: 15.96%
[rank2]:2025-11-09 23:46:13,533 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1334  memory:  6.43GiB(13.53%)  tps: 6,537  tflops: 49.79  mfu: 15.96%
[rank3]:2025-11-09 23:46:13,722 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2346_real_step100_rank3.svg
[rank3]:> Batch Time: 606.09 ms, GPU Bubble Ratio: 58.28%, 56.01%, 65.82%, 29.29%
[rank0]:2025-11-09 23:48:16,736 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-09 23:48:19,208 - INFO - Avg. fwd time: 9.3049 / Avg. bwd time: 23.8951 / Avg. batch time: 568.0987 (ms) / GPU bubble ratio: 53.25%
[rank3]:2025-11-09 23:48:19,153 - INFO - Avg. fwd time: 12.7283 / Avg. bwd time: 40.5979 / Avg. batch time: 498.5641 (ms) / GPU bubble ratio: 14.43%
[rank0]:2025-11-09 23:48:19,239 - INFO - Avg. fwd time: 7.4865 / Avg. bwd time: 23.8880 / Avg. batch time: 605.2052 (ms) / GPU bubble ratio: 58.53%
[rank2]:2025-11-09 23:48:19,178 - INFO - Avg. fwd time: 7.1109 / Avg. bwd time: 18.6602 / Avg. batch time: 529.7666 (ms) / GPU bubble ratio: 61.08%
[rank2]:2025-11-09 23:48:19,260 - INFO -  step: 150  loss: -4.0000  grad_norm: 88.6084  memory:  6.43GiB(13.53%)  tps: 6,516  tflops: 49.63  mfu: 15.91%
[rank1]:2025-11-09 23:48:19,264 - INFO -  step: 150  loss: -4.0000  grad_norm: 88.6084  memory:  9.03GiB(19.01%)  tps: 6,516  tflops: 49.63  mfu: 15.91%
[rank3]:2025-11-09 23:48:19,272 - INFO -  step: 150  loss:  3.7377  grad_norm: 88.6084  memory: 16.39GiB(34.50%)  tps: 6,516  tflops: 49.63  mfu: 15.91%
[rank0]:2025-11-09 23:48:19,275 - INFO -  step: 150  loss: -4.0000  grad_norm: 88.6084  memory: 12.97GiB(27.31%)  tps: 6,516  tflops: 49.63  mfu: 15.91%
[rank3]:2025-11-09 23:48:34,033 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:48:34,247 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:48:34,274 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:48:34,299 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:50:21,617 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:50:23,837 - INFO - Avg. fwd time: 12.7234 / Avg. bwd time: 40.5897 / Avg. batch time: 498.4378 (ms) / GPU bubble ratio: 14.43%
[rank0]:2025-11-09 23:50:23,906 - INFO - Avg. fwd time: 7.4849 / Avg. bwd time: 23.9030 / Avg. batch time: 604.9114 (ms) / GPU bubble ratio: 58.49%
[rank2]:2025-11-09 23:50:23,875 - INFO - Avg. fwd time: 7.1050 / Avg. bwd time: 18.6815 / Avg. batch time: 529.5420 (ms) / GPU bubble ratio: 61.04%
[rank1]:2025-11-09 23:50:23,910 - INFO - Avg. fwd time: 9.3045 / Avg. bwd time: 23.9230 / Avg. batch time: 567.8373 (ms) / GPU bubble ratio: 53.19%
[rank1]:2025-11-09 23:50:24,140 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.9261  memory:  9.03GiB(19.01%)  tps: 6,560  tflops: 49.96  mfu: 16.01%
[rank3]:2025-11-09 23:50:24,149 - INFO -  step: 200  loss:  1.0903  grad_norm: 16.9261  memory: 16.39GiB(34.50%)  tps: 6,560  tflops: 49.96  mfu: 16.01%
[rank0]:2025-11-09 23:50:24,151 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.9261  memory: 12.97GiB(27.31%)  tps: 6,560  tflops: 49.96  mfu: 16.01%
[rank2]:2025-11-09 23:50:24,136 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.9261  memory:  6.43GiB(13.53%)  tps: 6,560  tflops: 49.96  mfu: 16.01%
[rank3]:2025-11-09 23:50:24,304 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2350_real_step200_rank3.svg
[rank3]:> Batch Time: 603.66 ms, GPU Bubble Ratio: 58.11%, 55.83%, 65.63%, 29.84%
[rank0]:2025-11-09 23:52:26,657 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:52:28,867 - INFO - Avg. fwd time: 12.7290 / Avg. bwd time: 40.5778 / Avg. batch time: 498.3716 (ms) / GPU bubble ratio: 14.43%
[rank1]:2025-11-09 23:52:28,940 - INFO - Avg. fwd time: 9.3001 / Avg. bwd time: 23.9450 / Avg. batch time: 567.8038 (ms) / GPU bubble ratio: 53.16%
[rank0]:2025-11-09 23:52:28,936 - INFO - Avg. fwd time: 7.4820 / Avg. bwd time: 23.9130 / Avg. batch time: 604.8444 (ms) / GPU bubble ratio: 58.48%
[rank2]:2025-11-09 23:52:28,905 - INFO - Avg. fwd time: 7.0995 / Avg. bwd time: 18.7061 / Avg. batch time: 529.5340 (ms) / GPU bubble ratio: 61.01%
[rank0]:2025-11-09 23:52:29,176 - INFO -  step: 250  loss: -4.0000  grad_norm:  5.9164  memory: 12.97GiB(27.31%)  tps: 6,552  tflops: 49.90  mfu: 15.99%
[rank2]:2025-11-09 23:52:29,162 - INFO -  step: 250  loss: -4.0000  grad_norm:  5.9164  memory:  6.43GiB(13.53%)  tps: 6,552  tflops: 49.90  mfu: 15.99%
[rank1]:2025-11-09 23:52:29,166 - INFO -  step: 250  loss: -4.0000  grad_norm:  5.9164  memory:  9.03GiB(19.01%)  tps: 6,552  tflops: 49.90  mfu: 15.99%
[rank3]:2025-11-09 23:52:29,174 - INFO -  step: 250  loss:  0.3906  grad_norm:  5.9164  memory: 16.39GiB(34.50%)  tps: 6,552  tflops: 49.90  mfu: 15.99%
[rank0]:2025-11-09 23:54:31,965 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:54:34,408 - INFO - Avg. fwd time: 12.7535 / Avg. bwd time: 40.5894 / Avg. batch time: 498.6524 (ms) / GPU bubble ratio: 14.42%
[rank2]:2025-11-09 23:54:34,434 - INFO - Avg. fwd time: 7.0983 / Avg. bwd time: 18.7260 / Avg. batch time: 529.7592 (ms) / GPU bubble ratio: 61.00%
[rank1]:2025-11-09 23:54:34,465 - INFO - Avg. fwd time: 9.2988 / Avg. bwd time: 23.9634 / Avg. batch time: 568.0193 (ms) / GPU bubble ratio: 53.15%
[rank1]:2025-11-09 23:54:34,521 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.9636  memory:  9.03GiB(19.01%)  tps: 6,535  tflops: 49.77  mfu: 15.95%
[rank3]:2025-11-09 23:54:34,529 - INFO -  step: 300  loss:  0.2418  grad_norm:  2.9636  memory: 16.39GiB(34.50%)  tps: 6,535  tflops: 49.77  mfu: 15.95%
[rank0]:2025-11-09 23:54:34,496 - INFO - Avg. fwd time: 7.4816 / Avg. bwd time: 23.9224 / Avg. batch time: 605.0431 (ms) / GPU bubble ratio: 58.48%
[rank0]:2025-11-09 23:54:34,532 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.9636  memory: 12.97GiB(27.31%)  tps: 6,535  tflops: 49.77  mfu: 15.95%
[rank2]:2025-11-09 23:54:34,517 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.9636  memory:  6.43GiB(13.53%)  tps: 6,535  tflops: 49.77  mfu: 15.95%
[rank3]:2025-11-09 23:54:34,681 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2354_real_step300_rank3.svg
[rank3]:> Batch Time: 606.14 ms, GPU Bubble Ratio: 58.20%, 55.88%, 65.68%, 29.23%
[rank3]:2025-11-09 23:55:04,461 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:55:04,675 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:55:04,702 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:55:04,727 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:56:37,433 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:56:39,642 - INFO - Avg. fwd time: 12.7695 / Avg. bwd time: 40.6003 / Avg. batch time: 498.8633 (ms) / GPU bubble ratio: 14.41%
[rank0]:2025-11-09 23:56:39,711 - INFO - Avg. fwd time: 7.4811 / Avg. bwd time: 23.9285 / Avg. batch time: 605.2835 (ms) / GPU bubble ratio: 58.49%
[rank2]:2025-11-09 23:56:39,680 - INFO - Avg. fwd time: 7.0974 / Avg. bwd time: 18.7379 / Avg. batch time: 530.0181 (ms) / GPU bubble ratio: 61.00%
[rank1]:2025-11-09 23:56:39,715 - INFO - Avg. fwd time: 9.2990 / Avg. bwd time: 23.9752 / Avg. batch time: 568.2713 (ms) / GPU bubble ratio: 53.16%
[rank1]:2025-11-09 23:56:39,940 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7022  memory:  9.03GiB(19.01%)  tps: 6,532  tflops: 49.75  mfu: 15.94%
[rank3]:2025-11-09 23:56:39,948 - INFO -  step: 350  loss:  0.1939  grad_norm:  0.7022  memory: 16.39GiB(34.50%)  tps: 6,532  tflops: 49.75  mfu: 15.94%
[rank0]:2025-11-09 23:56:39,951 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7022  memory: 12.97GiB(27.31%)  tps: 6,532  tflops: 49.75  mfu: 15.94%
[rank2]:2025-11-09 23:56:39,936 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7022  memory:  6.43GiB(13.53%)  tps: 6,532  tflops: 49.75  mfu: 15.94%
[rank0]:2025-11-09 23:58:42,140 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:58:44,365 - INFO - Avg. fwd time: 12.7549 / Avg. bwd time: 40.5925 / Avg. batch time: 498.6779 (ms) / GPU bubble ratio: 14.42%
[rank0]:2025-11-09 23:58:44,434 - INFO - Avg. fwd time: 7.4791 / Avg. bwd time: 23.9309 / Avg. batch time: 605.0260 (ms) / GPU bubble ratio: 58.47%
[rank2]:2025-11-09 23:58:44,403 - INFO - Avg. fwd time: 7.0950 / Avg. bwd time: 18.7438 / Avg. batch time: 529.7852 (ms) / GPU bubble ratio: 60.98%
[rank1]:2025-11-09 23:58:44,438 - INFO - Avg. fwd time: 9.2948 / Avg. bwd time: 23.9801 / Avg. batch time: 568.0254 (ms) / GPU bubble ratio: 53.14%
[rank2]:2025-11-09 23:58:44,670 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.0132  memory:  6.43GiB(13.53%)  tps: 6,568  tflops: 50.02  mfu: 16.03%
[rank1]:2025-11-09 23:58:44,673 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.0132  memory:  9.03GiB(19.01%)  tps: 6,568  tflops: 50.02  mfu: 16.03%
[rank3]:2025-11-09 23:58:44,682 - INFO -  step: 400  loss:  0.1654  grad_norm:  1.0132  memory: 16.39GiB(34.50%)  tps: 6,568  tflops: 50.02  mfu: 16.03%
[rank0]:2025-11-09 23:58:44,684 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.0132  memory: 12.97GiB(27.31%)  tps: 6,568  tflops: 50.02  mfu: 16.03%
[rank3]:2025-11-09 23:58:44,839 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2358_real_step400_rank3.svg
[rank3]:> Batch Time: 601.12 ms, GPU Bubble Ratio: 57.99%, 55.72%, 65.50%, 29.78%
