
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 02:34:37 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x Interleaved1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:2025-11-08 02:34:44,352 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank1]:2025-11-08 02:34:44,352 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-08 02:34:44,353 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-08 02:34:44,556 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-08 02:34:44,622 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 02:34:44,624 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-08 02:34:44,635 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 02:34:44,637 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 02:34:44,629 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 02:34:44,631 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 02:34:44,739 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 02:34:44,742 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 02:34:44,746 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 02:34:44,747 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-08 02:34:45,134 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 02:34:47,926 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-11-08 02:34:47,920 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 02:34:47,959 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 02:34:47,986 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
[rank1]:2025-11-08 02:34:47,998 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
[rank1]:2025-11-08 02:34:48,000 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-08 02:34:48,075 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 02:34:48,113 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 02:34:48,114 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-08 02:34:48,140 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:2025-11-08 02:34:48,153 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:2025-11-08 02:34:48,154 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-11-08 02:34:48,192 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 02:34:48,192 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 02:34:48,192 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-08 02:34:48,340 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-08 02:34:48,380 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 02:34:48,407 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
[rank2]:2025-11-08 02:34:48,420 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
[rank2]:2025-11-08 02:34:48,421 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-08 02:34:48,351 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 02:34:48,351 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 02:34:48,352 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-08 02:34:48,605 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 02:34:48,605 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 02:34:48,606 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 6u4ucsuy
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0234/wandb/run-20251108_023448-6u4ucsuy
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/6u4ucsuy
[rank3]:2025-11-08 02:34:50,034 - INFO - WandB logging enabled
[rank3]:2025-11-08 02:34:50,035 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 02:34:50,074 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 02:34:50,100 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:2025-11-08 02:34:50,113 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:2025-11-08 02:34:50,115 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-08 02:34:50,350 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 02:34:50,350 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 02:34:50,350 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 02:34:50,350 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 02:34:50,350 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-08 02:34:50,331 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 02:34:50,332 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 02:34:50,333 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 02:34:50,350 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-08 02:34:50,350 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 02:34:52,943 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 02:34:52,943 - INFO - Finished loading the checkpoint in 2.59 seconds.
[rank0]:2025-11-08 02:34:52,943 - INFO - Training starts at step 1
[rank2]:2025-11-08 02:34:56,239 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5964  memory:  6.31GiB(13.29%)  tps: 2,085  tflops: 15.88  mfu: 5.09%
[rank2]:2025-11-08 02:34:56,240 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 02:34:56,256 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5964  memory:  9.58GiB(20.17%)  tps: 1,975  tflops: 15.04  mfu: 4.82%
[rank1]:2025-11-08 02:34:56,256 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 02:34:56,252 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5964  memory: 15.04GiB(31.66%)  tps: 2,653  tflops: 20.20  mfu: 6.48%
[rank3]:2025-11-08 02:34:56,252 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 02:34:56,277 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5964  memory: 10.96GiB(23.08%)  tps: 2,007  tflops: 15.28  mfu: 4.90%
[rank0]:2025-11-08 02:34:56,278 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 02:37:06,742 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:37:09,128 - INFO - Avg. fwd time: 6.8073 / Avg. bwd time: 22.0073 / Avg. batch time: 569.4977 (ms) / GPU bubble ratio: 19.05%
[rank2]:2025-11-08 02:37:09,188 - INFO - Avg. fwd time: 3.9852 / Avg. bwd time: 10.0648 / Avg. batch time: 601.6679 (ms) / GPU bubble ratio: 62.64%
[rank0]:2025-11-08 02:37:09,209 - INFO - Avg. fwd time: 4.1133 / Avg. bwd time: 12.0034 / Avg. batch time: 653.8647 (ms) / GPU bubble ratio: 60.56%
[rank1]:2025-11-08 02:37:09,287 - INFO - Avg. fwd time: 4.8771 / Avg. bwd time: 12.2522 / Avg. batch time: 631.1102 (ms) / GPU bubble ratio: 56.57%
[rank0]:2025-11-08 02:37:09,465 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.6211  memory: 14.71GiB(30.97%)  tps: 6,028  tflops: 45.91  mfu: 14.71%
[rank1]:2025-11-08 02:37:09,455 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.6211  memory: 11.85GiB(24.93%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank3]:2025-11-08 02:37:09,463 - INFO -  step: 50  loss:  4.4079  grad_norm: 13.6211  memory: 18.46GiB(38.86%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank2]:2025-11-08 02:37:09,451 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.6211  memory:  8.13GiB(17.12%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank0]:2025-11-08 02:39:23,285 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:39:25,719 - INFO - Avg. fwd time: 6.8362 / Avg. bwd time: 22.1099 / Avg. batch time: 571.5582 (ms) / GPU bubble ratio: 18.97%
[rank2]:2025-11-08 02:39:25,778 - INFO - Avg. fwd time: 3.9821 / Avg. bwd time: 10.0964 / Avg. batch time: 603.6211 (ms) / GPU bubble ratio: 62.68%
[rank0]:2025-11-08 02:39:25,798 - INFO - Avg. fwd time: 4.1094 / Avg. bwd time: 12.0307 / Avg. batch time: 655.9125 (ms) / GPU bubble ratio: 60.63%
[rank1]:2025-11-08 02:39:25,882 - INFO - Avg. fwd time: 4.8735 / Avg. bwd time: 12.3006 / Avg. batch time: 633.1468 (ms) / GPU bubble ratio: 56.60%
[rank0]:2025-11-08 02:39:26,062 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4942  memory: 14.71GiB(30.97%)  tps: 5,997  tflops: 45.68  mfu: 14.64%
[rank1]:2025-11-08 02:39:26,052 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4942  memory: 11.85GiB(24.93%)  tps: 5,997  tflops: 45.68  mfu: 14.64%
[rank3]:2025-11-08 02:39:26,060 - INFO -  step: 100  loss:  0.5621  grad_norm:  0.4942  memory: 18.46GiB(38.86%)  tps: 5,997  tflops: 45.68  mfu: 14.64%
[rank2]:2025-11-08 02:39:26,048 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4942  memory:  8.13GiB(17.12%)  tps: 5,997  tflops: 45.68  mfu: 14.64%
[rank3]:2025-11-08 02:39:26,336 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0239_real_step100_rank3.svg
[rank3]:> Batch Time: 657.35 ms, GPU Bubble Ratio: 60.39%, 57.95%, 65.55%, 29.40%
[rank0]:2025-11-08 02:41:40,223 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:41:42,901 - INFO - Avg. fwd time: 3.9820 / Avg. bwd time: 10.1085 / Avg. batch time: 604.7943 (ms) / GPU bubble ratio: 62.72%
[rank1]:2025-11-08 02:41:42,926 - INFO - Avg. fwd time: 4.8726 / Avg. bwd time: 12.3205 / Avg. batch time: 634.3634 (ms) / GPU bubble ratio: 56.64%
[rank3]:2025-11-08 02:41:42,877 - INFO - Avg. fwd time: 6.8482 / Avg. bwd time: 22.1490 / Avg. batch time: 572.3511 (ms) / GPU bubble ratio: 18.94%
[rank0]:2025-11-08 02:41:42,949 - INFO - Avg. fwd time: 4.1082 / Avg. bwd time: 12.0429 / Avg. batch time: 657.1402 (ms) / GPU bubble ratio: 60.68%
[rank0]:2025-11-08 02:41:42,985 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3741  memory: 14.71GiB(30.97%)  tps: 5,983  tflops: 45.57  mfu: 14.60%
[rank2]:2025-11-08 02:41:42,971 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3741  memory:  8.13GiB(17.12%)  tps: 5,983  tflops: 45.57  mfu: 14.60%
[rank1]:2025-11-08 02:41:42,974 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3741  memory: 11.85GiB(24.93%)  tps: 5,983  tflops: 45.57  mfu: 14.61%
[rank3]:2025-11-08 02:41:42,983 - INFO -  step: 150  loss:  0.5051  grad_norm:  0.3741  memory: 18.46GiB(38.86%)  tps: 5,983  tflops: 45.57  mfu: 14.61%
[rank0]:2025-11-08 02:43:56,922 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:43:59,426 - INFO - Avg. fwd time: 3.9819 / Avg. bwd time: 10.1138 / Avg. batch time: 605.0199 (ms) / GPU bubble ratio: 62.72%
[rank3]:2025-11-08 02:43:59,366 - INFO - Avg. fwd time: 6.8542 / Avg. bwd time: 22.1655 / Avg. batch time: 572.7040 (ms) / GPU bubble ratio: 18.93%
[rank0]:2025-11-08 02:43:59,446 - INFO - Avg. fwd time: 4.1082 / Avg. bwd time: 12.0470 / Avg. batch time: 657.3847 (ms) / GPU bubble ratio: 60.68%
[rank1]:2025-11-08 02:43:59,529 - INFO - Avg. fwd time: 4.8722 / Avg. bwd time: 12.3286 / Avg. batch time: 634.6058 (ms) / GPU bubble ratio: 56.63%
[rank0]:2025-11-08 02:43:59,707 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2996  memory: 14.71GiB(30.97%)  tps: 5,992  tflops: 45.63  mfu: 14.63%
[rank2]:2025-11-08 02:43:59,692 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2996  memory:  8.13GiB(17.12%)  tps: 5,992  tflops: 45.63  mfu: 14.63%
[rank1]:2025-11-08 02:43:59,696 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2996  memory: 11.85GiB(24.93%)  tps: 5,992  tflops: 45.63  mfu: 14.63%
[rank3]:2025-11-08 02:43:59,704 - INFO -  step: 200  loss:  0.5854  grad_norm:  0.2996  memory: 18.46GiB(38.86%)  tps: 5,992  tflops: 45.63  mfu: 14.63%
[rank3]:2025-11-08 02:43:59,952 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0243_real_step200_rank3.svg
[rank3]:> Batch Time: 658.84 ms, GPU Bubble Ratio: 60.51%, 58.06%, 65.63%, 29.08%
[rank0]:2025-11-08 02:46:14,208 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:46:16,629 - INFO - Avg. fwd time: 6.8636 / Avg. bwd time: 22.1864 / Avg. batch time: 573.1907 (ms) / GPU bubble ratio: 18.91%
[rank2]:2025-11-08 02:46:16,687 - INFO - Avg. fwd time: 3.9823 / Avg. bwd time: 10.1169 / Avg. batch time: 605.6659 (ms) / GPU bubble ratio: 62.75%
[rank0]:2025-11-08 02:46:16,708 - INFO - Avg. fwd time: 4.1090 / Avg. bwd time: 12.0493 / Avg. batch time: 658.0538 (ms) / GPU bubble ratio: 60.71%
[rank1]:2025-11-08 02:46:16,791 - INFO - Avg. fwd time: 4.8722 / Avg. bwd time: 12.3374 / Avg. batch time: 635.2657 (ms) / GPU bubble ratio: 56.66%
[rank2]:2025-11-08 02:46:16,953 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3359  memory:  8.13GiB(17.12%)  tps: 5,968  tflops: 45.46  mfu: 14.57%
[rank0]:2025-11-08 02:46:16,968 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3359  memory: 14.71GiB(30.97%)  tps: 5,968  tflops: 45.46  mfu: 14.57%
[rank1]:2025-11-08 02:46:16,957 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3359  memory: 11.85GiB(24.93%)  tps: 5,968  tflops: 45.46  mfu: 14.57%
[rank3]:2025-11-08 02:46:16,966 - INFO -  step: 250  loss:  0.5950  grad_norm:  0.3359  memory: 18.46GiB(38.86%)  tps: 5,968  tflops: 45.46  mfu: 14.57%
[rank0]:2025-11-08 02:48:31,414 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:48:34,146 - INFO - Avg. fwd time: 3.9830 / Avg. bwd time: 10.1203 / Avg. batch time: 606.0508 (ms) / GPU bubble ratio: 62.77%
[rank0]:2025-11-08 02:48:34,197 - INFO - Avg. fwd time: 4.1095 / Avg. bwd time: 12.0519 / Avg. batch time: 658.4684 (ms) / GPU bubble ratio: 60.73%
[rank1]:2025-11-08 02:48:34,171 - INFO - Avg. fwd time: 4.8726 / Avg. bwd time: 12.3457 / Avg. batch time: 635.6714 (ms) / GPU bubble ratio: 56.66%
[rank3]:2025-11-08 02:48:34,121 - INFO - Avg. fwd time: 6.8730 / Avg. bwd time: 22.2087 / Avg. batch time: 573.6968 (ms) / GPU bubble ratio: 18.89%
[rank2]:2025-11-08 02:48:34,218 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3059  memory:  8.13GiB(17.12%)  tps: 5,968  tflops: 45.45  mfu: 14.57%
[rank0]:2025-11-08 02:48:34,232 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3059  memory: 14.71GiB(30.97%)  tps: 5,968  tflops: 45.45  mfu: 14.57%
[rank1]:2025-11-08 02:48:34,221 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3059  memory: 11.85GiB(24.93%)  tps: 5,968  tflops: 45.45  mfu: 14.57%
[rank3]:2025-11-08 02:48:34,230 - INFO -  step: 300  loss:  0.5855  grad_norm:  0.3059  memory: 18.46GiB(38.86%)  tps: 5,968  tflops: 45.45  mfu: 14.57%
[rank3]:2025-11-08 02:48:34,479 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0248_real_step300_rank3.svg
[rank3]:> Batch Time: 659.84 ms, GPU Bubble Ratio: 60.56%, 58.06%, 65.65%, 29.14%
[rank0]:2025-11-08 02:50:48,857 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:50:51,284 - INFO - Avg. fwd time: 6.8789 / Avg. bwd time: 22.2218 / Avg. batch time: 574.0014 (ms) / GPU bubble ratio: 18.88%
[rank2]:2025-11-08 02:50:51,343 - INFO - Avg. fwd time: 3.9834 / Avg. bwd time: 10.1224 / Avg. batch time: 606.4550 (ms) / GPU bubble ratio: 62.78%
[rank0]:2025-11-08 02:50:51,364 - INFO - Avg. fwd time: 4.1095 / Avg. bwd time: 12.0532 / Avg. batch time: 658.8874 (ms) / GPU bubble ratio: 60.75%
[rank1]:2025-11-08 02:50:51,447 - INFO - Avg. fwd time: 4.8728 / Avg. bwd time: 12.3514 / Avg. batch time: 636.0864 (ms) / GPU bubble ratio: 56.67%
[rank2]:2025-11-08 02:50:51,607 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3076  memory:  8.13GiB(17.12%)  tps: 5,963  tflops: 45.41  mfu: 14.56%
[rank0]:2025-11-08 02:50:51,622 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3076  memory: 14.71GiB(30.97%)  tps: 5,963  tflops: 45.41  mfu: 14.56%
[rank1]:2025-11-08 02:50:51,611 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3076  memory: 11.85GiB(24.93%)  tps: 5,963  tflops: 45.41  mfu: 14.56%
[rank3]:2025-11-08 02:50:51,619 - INFO -  step: 350  loss:  0.5401  grad_norm:  0.3076  memory: 18.46GiB(38.86%)  tps: 5,963  tflops: 45.41  mfu: 14.56%

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 04:02:04 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x Interleaved1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=42 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=2e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-08 04:02:11,058 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-08 04:02:11,058 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-08 04:02:11,148 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-08 04:02:11,304 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 04:02:11,306 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 04:02:11,309 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 04:02:11,310 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-08 04:02:11,330 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-08 04:02:11,292 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 04:02:11,294 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-08 04:02:11,397 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 04:02:11,400 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 04:02:11,498 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 04:02:11,501 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 04:02:11,699 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 04:02:14,435 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-08 04:02:14,584 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 04:02:14,623 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 04:02:14,625 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank1]:2025-11-08 04:02:14,633 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 04:02:14,659 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:2025-11-08 04:02:14,672 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:2025-11-08 04:02:14,673 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-11-08 04:02:14,673 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 04:02:14,701 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
[rank1]:2025-11-08 04:02:14,714 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
[rank1]:2025-11-08 04:02:14,715 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-08 04:02:14,815 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 04:02:14,868 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 04:02:14,868 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 04:02:14,869 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-08 04:02:14,896 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 04:02:14,896 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 04:02:14,896 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-08 04:02:14,859 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 04:02:14,888 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
[rank2]:2025-11-08 04:02:14,901 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
[rank2]:2025-11-08 04:02:14,902 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-08 04:02:15,081 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 04:02:15,082 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 04:02:15,082 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run ike02ouy
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0402/wandb/run-20251108_040215-ike02ouy
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/ike02ouy
[rank3]:2025-11-08 04:02:16,907 - INFO - WandB logging enabled
[rank3]:2025-11-08 04:02:16,908 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 04:02:16,948 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 04:02:16,971 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:2025-11-08 04:02:16,984 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:2025-11-08 04:02:16,985 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-08 04:02:17,189 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 04:02:17,189 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 04:02:17,189 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 04:02:17,190 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-08 04:02:17,171 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 04:02:17,172 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 04:02:17,173 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 04:02:17,187 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-08 04:02:17,187 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-08 04:02:17,187 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 04:02:19,595 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 04:02:19,595 - INFO - Finished loading the checkpoint in 2.41 seconds.
[rank0]:2025-11-08 04:02:19,596 - INFO - Training starts at step 1
[rank2]:2025-11-08 04:02:22,906 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5940  memory:  6.31GiB(13.29%)  tps: 2,036  tflops: 15.51  mfu: 4.97%
[rank2]:2025-11-08 04:02:22,907 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 04:02:22,945 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5940  memory: 10.96GiB(23.08%)  tps: 1,969  tflops: 15.00  mfu: 4.81%
[rank0]:2025-11-08 04:02:22,945 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 04:02:22,918 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5940  memory:  9.58GiB(20.17%)  tps: 1,987  tflops: 15.13  mfu: 4.85%
[rank1]:2025-11-08 04:02:22,919 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 04:02:22,919 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5940  memory: 15.04GiB(31.66%)  tps: 2,745  tflops: 20.91  mfu: 6.70%
[rank3]:2025-11-08 04:02:22,919 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 04:04:33,688 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:04:36,083 - INFO - Avg. fwd time: 6.8372 / Avg. bwd time: 22.0434 / Avg. batch time: 570.5638 (ms) / GPU bubble ratio: 19.01%
[rank0]:2025-11-08 04:04:36,162 - INFO - Avg. fwd time: 4.1299 / Avg. bwd time: 12.0027 / Avg. batch time: 655.1075 (ms) / GPU bubble ratio: 60.60%
[rank2]:2025-11-08 04:04:36,142 - INFO - Avg. fwd time: 3.9875 / Avg. bwd time: 10.0662 / Avg. batch time: 602.8035 (ms) / GPU bubble ratio: 62.70%
[rank1]:2025-11-08 04:04:36,246 - INFO - Avg. fwd time: 4.8757 / Avg. bwd time: 12.2390 / Avg. batch time: 632.2384 (ms) / GPU bubble ratio: 56.69%
[rank3]:2025-11-08 04:04:36,425 - INFO -  step: 50  loss:  6.9557  grad_norm: 13.4927  memory: 18.46GiB(38.86%)  tps: 6,013  tflops: 45.80  mfu: 14.68%
[rank2]:2025-11-08 04:04:36,413 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.4927  memory:  8.11GiB(17.07%)  tps: 6,013  tflops: 45.80  mfu: 14.68%
[rank1]:2025-11-08 04:04:36,417 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.4927  memory: 11.85GiB(24.93%)  tps: 6,014  tflops: 45.80  mfu: 14.68%
[rank0]:2025-11-08 04:04:36,428 - INFO -  step: 50  loss: -4.0000  grad_norm: 13.4927  memory: 14.71GiB(30.97%)  tps: 6,014  tflops: 45.81  mfu: 14.68%
[rank0]:2025-11-08 04:06:50,507 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 04:06:53,005 - INFO - Avg. fwd time: 3.9864 / Avg. bwd time: 10.0938 / Avg. batch time: 604.9778 (ms) / GPU bubble ratio: 62.76%
[rank3]:2025-11-08 04:06:52,946 - INFO - Avg. fwd time: 6.8707 / Avg. bwd time: 22.1557 / Avg. batch time: 572.8558 (ms) / GPU bubble ratio: 18.93%
[rank0]:2025-11-08 04:06:53,026 - INFO - Avg. fwd time: 4.1191 / Avg. bwd time: 12.0228 / Avg. batch time: 657.3136 (ms) / GPU bubble ratio: 60.71%
[rank1]:2025-11-08 04:06:53,108 - INFO - Avg. fwd time: 4.8736 / Avg. bwd time: 12.2809 / Avg. batch time: 634.4782 (ms) / GPU bubble ratio: 56.74%
[rank1]:2025-11-08 04:06:53,279 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.2320  memory: 11.85GiB(24.93%)  tps: 5,986  tflops: 45.59  mfu: 14.61%
[rank0]:2025-11-08 04:06:53,290 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.2320  memory: 14.71GiB(30.97%)  tps: 5,986  tflops: 45.59  mfu: 14.61%
[rank2]:2025-11-08 04:06:53,276 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.2320  memory:  8.11GiB(17.07%)  tps: 5,986  tflops: 45.59  mfu: 14.61%
[rank3]:2025-11-08 04:06:53,288 - INFO -  step: 100  loss:  0.8496  grad_norm:  8.2320  memory: 18.46GiB(38.86%)  tps: 5,986  tflops: 45.59  mfu: 14.61%
[rank3]:2025-11-08 04:06:53,567 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0406_real_step100_rank3.svg
[rank3]:> Batch Time: 659.35 ms, GPU Bubble Ratio: 60.57%, 58.14%, 65.67%, 29.39%
[rank0]:2025-11-08 04:09:07,494 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:09:10,149 - INFO - Avg. fwd time: 6.8737 / Avg. bwd time: 22.1769 / Avg. batch time: 573.2152 (ms) / GPU bubble ratio: 18.91%
[rank2]:2025-11-08 04:09:10,175 - INFO - Avg. fwd time: 3.9838 / Avg. bwd time: 10.1048 / Avg. batch time: 605.7309 (ms) / GPU bubble ratio: 62.79%
[rank1]:2025-11-08 04:09:10,201 - INFO - Avg. fwd time: 4.8721 / Avg. bwd time: 12.2995 / Avg. batch time: 635.2634 (ms) / GPU bubble ratio: 56.75%
[rank1]:2025-11-08 04:09:10,249 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.6096  memory: 11.85GiB(24.93%)  tps: 5,981  tflops: 45.55  mfu: 14.60%
[rank3]:2025-11-08 04:09:10,258 - INFO -  step: 150  loss:  0.5000  grad_norm:  0.6096  memory: 18.46GiB(38.86%)  tps: 5,981  tflops: 45.55  mfu: 14.60%
[rank2]:2025-11-08 04:09:10,246 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.6096  memory:  8.11GiB(17.07%)  tps: 5,981  tflops: 45.55  mfu: 14.60%
[rank0]:2025-11-08 04:09:10,224 - INFO - Avg. fwd time: 4.1161 / Avg. bwd time: 12.0330 / Avg. batch time: 658.0955 (ms) / GPU bubble ratio: 60.74%
[rank0]:2025-11-08 04:09:10,260 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.6096  memory: 14.71GiB(30.97%)  tps: 5,981  tflops: 45.55  mfu: 14.60%
[rank0]:2025-11-08 04:11:24,359 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:11:26,792 - INFO - Avg. fwd time: 6.8744 / Avg. bwd time: 22.1865 / Avg. batch time: 573.3848 (ms) / GPU bubble ratio: 18.91%
[rank2]:2025-11-08 04:11:26,852 - INFO - Avg. fwd time: 3.9842 / Avg. bwd time: 10.1100 / Avg. batch time: 605.7847 (ms) / GPU bubble ratio: 62.77%
[rank0]:2025-11-08 04:11:26,872 - INFO - Avg. fwd time: 4.1185 / Avg. bwd time: 12.0383 / Avg. batch time: 658.2029 (ms) / GPU bubble ratio: 60.73%
[rank1]:2025-11-08 04:11:26,954 - INFO - Avg. fwd time: 4.8729 / Avg. bwd time: 12.3093 / Avg. batch time: 635.3368 (ms) / GPU bubble ratio: 56.73%
[rank1]:2025-11-08 04:11:27,121 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4208  memory: 11.85GiB(24.93%)  tps: 5,985  tflops: 45.58  mfu: 14.61%
[rank3]:2025-11-08 04:11:27,130 - INFO -  step: 200  loss:  0.5408  grad_norm:  0.4208  memory: 18.46GiB(38.86%)  tps: 5,985  tflops: 45.58  mfu: 14.61%
[rank2]:2025-11-08 04:11:27,117 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4208  memory:  8.11GiB(17.07%)  tps: 5,985  tflops: 45.58  mfu: 14.61%
[rank0]:2025-11-08 04:11:27,132 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4208  memory: 14.71GiB(30.97%)  tps: 5,985  tflops: 45.58  mfu: 14.61%
[rank3]:2025-11-08 04:11:27,377 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0411_real_step200_rank3.svg
[rank3]:> Batch Time: 657.84 ms, GPU Bubble Ratio: 60.39%, 58.02%, 65.58%, 29.66%
[rank0]:2025-11-08 04:13:41,444 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:13:43,872 - INFO - Avg. fwd time: 6.8766 / Avg. bwd time: 22.1946 / Avg. batch time: 573.5542 (ms) / GPU bubble ratio: 18.90%
[rank2]:2025-11-08 04:13:43,930 - INFO - Avg. fwd time: 3.9847 / Avg. bwd time: 10.1136 / Avg. batch time: 606.1227 (ms) / GPU bubble ratio: 62.78%
[rank0]:2025-11-08 04:13:43,951 - INFO - Avg. fwd time: 4.1172 / Avg. bwd time: 12.0423 / Avg. batch time: 658.5496 (ms) / GPU bubble ratio: 60.74%
[rank1]:2025-11-08 04:13:44,034 - INFO - Avg. fwd time: 4.8737 / Avg. bwd time: 12.3163 / Avg. batch time: 635.6889 (ms) / GPU bubble ratio: 56.73%
[rank1]:2025-11-08 04:13:44,199 - INFO -  step: 250  loss: -4.0000  grad_norm:  1.7358  memory: 11.85GiB(24.93%)  tps: 5,976  tflops: 45.52  mfu: 14.59%
[rank3]:2025-11-08 04:13:44,208 - INFO -  step: 250  loss:  0.5418  grad_norm:  1.7358  memory: 18.46GiB(38.86%)  tps: 5,976  tflops: 45.52  mfu: 14.59%
[rank2]:2025-11-08 04:13:44,196 - INFO -  step: 250  loss: -4.0000  grad_norm:  1.7358  memory:  8.11GiB(17.07%)  tps: 5,976  tflops: 45.52  mfu: 14.59%
[rank0]:2025-11-08 04:13:44,210 - INFO -  step: 250  loss: -4.0000  grad_norm:  1.7358  memory: 14.71GiB(30.97%)  tps: 5,976  tflops: 45.52  mfu: 14.59%
[rank0]:2025-11-08 04:15:58,204 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:16:00,903 - INFO - Avg. fwd time: 6.8781 / Avg. bwd time: 22.2008 / Avg. batch time: 573.6690 (ms) / GPU bubble ratio: 18.90%
[rank2]:2025-11-08 04:16:00,927 - INFO - Avg. fwd time: 3.9844 / Avg. bwd time: 10.1164 / Avg. batch time: 606.1374 (ms) / GPU bubble ratio: 62.78%
[rank1]:2025-11-08 04:16:00,952 - INFO - Avg. fwd time: 4.8739 / Avg. bwd time: 12.3222 / Avg. batch time: 635.7151 (ms) / GPU bubble ratio: 56.72%
[rank3]:2025-11-08 04:16:01,011 - INFO -  step: 300  loss:  0.5280  grad_norm:  0.3459  memory: 18.46GiB(38.86%)  tps: 5,988  tflops: 45.61  mfu: 14.62%
[rank2]:2025-11-08 04:16:00,999 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3459  memory:  8.11GiB(17.07%)  tps: 5,988  tflops: 45.61  mfu: 14.62%
[rank0]:2025-11-08 04:16:00,977 - INFO - Avg. fwd time: 4.1164 / Avg. bwd time: 12.0449 / Avg. batch time: 658.5690 (ms) / GPU bubble ratio: 60.74%
[rank0]:2025-11-08 04:16:01,013 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3459  memory: 14.71GiB(30.97%)  tps: 5,988  tflops: 45.61  mfu: 14.62%
[rank1]:2025-11-08 04:16:01,002 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3459  memory: 11.85GiB(24.93%)  tps: 5,988  tflops: 45.61  mfu: 14.62%
[rank3]:2025-11-08 04:16:01,255 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0416_real_step300_rank3.svg
[rank3]:> Batch Time: 659.34 ms, GPU Bubble Ratio: 60.51%, 58.12%, 65.65%, 29.22%
[rank0]:2025-11-08 04:18:15,254 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 04:18:17,728 - INFO - Avg. fwd time: 3.9842 / Avg. bwd time: 10.1180 / Avg. batch time: 606.3136 (ms) / GPU bubble ratio: 62.79%
[rank3]:2025-11-08 04:18:17,669 - INFO - Avg. fwd time: 6.8787 / Avg. bwd time: 22.2056 / Avg. batch time: 573.7530 (ms) / GPU bubble ratio: 18.89%
[rank0]:2025-11-08 04:18:17,749 - INFO - Avg. fwd time: 4.1157 / Avg. bwd time: 12.0463 / Avg. batch time: 658.7446 (ms) / GPU bubble ratio: 60.74%
[rank1]:2025-11-08 04:18:17,831 - INFO - Avg. fwd time: 4.8739 / Avg. bwd time: 12.3259 / Avg. batch time: 635.8960 (ms) / GPU bubble ratio: 56.72%
[rank0]:2025-11-08 04:18:18,005 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3432  memory: 14.71GiB(30.97%)  tps: 5,980  tflops: 45.54  mfu: 14.60%
[rank2]:2025-11-08 04:18:17,991 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3432  memory:  8.11GiB(17.07%)  tps: 5,980  tflops: 45.54  mfu: 14.60%
[rank3]:2025-11-08 04:18:18,003 - INFO -  step: 350  loss:  0.4934  grad_norm:  0.3432  memory: 18.46GiB(38.86%)  tps: 5,980  tflops: 45.54  mfu: 14.60%
[rank1]:2025-11-08 04:18:17,995 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3432  memory: 11.85GiB(24.93%)  tps: 5,980  tflops: 45.54  mfu: 14.60%
[rank0]:2025-11-08 04:20:31,917 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 04:20:34,425 - INFO - Avg. fwd time: 3.9832 / Avg. bwd time: 10.1181 / Avg. batch time: 606.2132 (ms) / GPU bubble ratio: 62.78%
[rank3]:2025-11-08 04:20:34,366 - INFO - Avg. fwd time: 6.8776 / Avg. bwd time: 22.2040 / Avg. batch time: 573.7006 (ms) / GPU bubble ratio: 18.89%
[rank0]:2025-11-08 04:20:34,445 - INFO - Avg. fwd time: 4.1152 / Avg. bwd time: 12.0468 / Avg. batch time: 658.6373 (ms) / GPU bubble ratio: 60.74%
[rank1]:2025-11-08 04:20:34,529 - INFO - Avg. fwd time: 4.8731 / Avg. bwd time: 12.3267 / Avg. batch time: 635.7938 (ms) / GPU bubble ratio: 56.72%
[rank0]:2025-11-08 04:20:34,702 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3409  memory: 14.71GiB(30.97%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank2]:2025-11-08 04:20:34,688 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3409  memory:  8.11GiB(17.07%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank3]:2025-11-08 04:20:34,701 - INFO -  step: 400  loss:  0.4875  grad_norm:  0.3409  memory: 18.46GiB(38.86%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank1]:2025-11-08 04:20:34,692 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3409  memory: 11.85GiB(24.93%)  tps: 5,993  tflops: 45.64  mfu: 14.63%
[rank3]:2025-11-08 04:20:34,946 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0420_real_step400_rank3.svg
[rank3]:> Batch Time: 658.34 ms, GPU Bubble Ratio: 60.47%, 58.07%, 65.66%, 29.44%
[rank3]:2025-11-08 04:20:44,183 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-08 04:20:44,452 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-08 04:20:44,468 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 04:20:44,485 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 04:22:48,922 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:22:51,603 - INFO - Avg. fwd time: 6.8796 / Avg. bwd time: 22.2084 / Avg. batch time: 573.7980 (ms) / GPU bubble ratio: 18.89%
[rank2]:2025-11-08 04:22:51,632 - INFO - Avg. fwd time: 3.9824 / Avg. bwd time: 10.1188 / Avg. batch time: 606.3791 (ms) / GPU bubble ratio: 62.79%
[rank0]:2025-11-08 04:22:51,685 - INFO - Avg. fwd time: 4.1154 / Avg. bwd time: 12.0480 / Avg. batch time: 658.8201 (ms) / GPU bubble ratio: 60.75%
[rank0]:2025-11-08 04:22:51,720 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3291  memory: 14.71GiB(30.97%)  tps: 5,979  tflops: 45.54  mfu: 14.59%
[rank3]:2025-11-08 04:22:51,718 - INFO -  step: 450  loss:  0.5283  grad_norm:  0.3291  memory: 18.46GiB(38.86%)  tps: 5,979  tflops: 45.54  mfu: 14.59%
[rank2]:2025-11-08 04:22:51,706 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3291  memory:  8.11GiB(17.07%)  tps: 5,979  tflops: 45.54  mfu: 14.59%
[rank1]:2025-11-08 04:22:51,658 - INFO - Avg. fwd time: 4.8726 / Avg. bwd time: 12.3286 / Avg. batch time: 635.9678 (ms) / GPU bubble ratio: 56.72%
[rank1]:2025-11-08 04:22:51,709 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3291  memory: 11.85GiB(24.93%)  tps: 5,979  tflops: 45.54  mfu: 14.59%
[rank0]:2025-11-08 04:25:05,882 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:25:08,316 - INFO - Avg. fwd time: 6.8813 / Avg. bwd time: 22.2117 / Avg. batch time: 573.8779 (ms) / GPU bubble ratio: 18.89%
[rank0]:2025-11-08 04:25:08,395 - INFO - Avg. fwd time: 4.1150 / Avg. bwd time: 12.0491 / Avg. batch time: 658.8627 (ms) / GPU bubble ratio: 60.75%
[rank2]:2025-11-08 04:25:08,375 - INFO - Avg. fwd time: 3.9824 / Avg. bwd time: 10.1197 / Avg. batch time: 606.4223 (ms) / GPU bubble ratio: 62.79%
[rank1]:2025-11-08 04:25:08,475 - INFO - Avg. fwd time: 4.8727 / Avg. bwd time: 12.3312 / Avg. batch time: 636.0135 (ms) / GPU bubble ratio: 56.72%
[rank3]:2025-11-08 04:25:08,648 - INFO -  step: 500  loss:  0.4096  grad_norm:  0.2705  memory: 18.46GiB(38.86%)  tps: 5,983  tflops: 45.57  mfu: 14.60%
[rank1]:2025-11-08 04:25:08,640 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2705  memory: 11.85GiB(24.93%)  tps: 5,983  tflops: 45.56  mfu: 14.60%
[rank0]:2025-11-08 04:25:08,650 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2705  memory: 14.71GiB(30.97%)  tps: 5,983  tflops: 45.56  mfu: 14.60%
[rank2]:2025-11-08 04:25:08,636 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2705  memory:  8.11GiB(17.07%)  tps: 5,983  tflops: 45.56  mfu: 14.60%
[rank3]:2025-11-08 04:25:08,892 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0425_real_step500_rank3.svg
[rank3]:> Batch Time: 659.34 ms, GPU Bubble Ratio: 60.51%, 58.06%, 65.67%, 29.19%
[rank0]:2025-11-08 04:27:23,305 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:27:25,724 - INFO - Avg. fwd time: 6.8851 / Avg. bwd time: 22.2213 / Avg. batch time: 574.0963 (ms) / GPU bubble ratio: 18.88%
[rank2]:2025-11-08 04:27:25,783 - INFO - Avg. fwd time: 3.9825 / Avg. bwd time: 10.1207 / Avg. batch time: 606.6718 (ms) / GPU bubble ratio: 62.81%
[rank0]:2025-11-08 04:27:25,803 - INFO - Avg. fwd time: 4.1153 / Avg. bwd time: 12.0502 / Avg. batch time: 659.1234 (ms) / GPU bubble ratio: 60.76%
[rank1]:2025-11-08 04:27:25,885 - INFO - Avg. fwd time: 4.8736 / Avg. bwd time: 12.3344 / Avg. batch time: 636.2705 (ms) / GPU bubble ratio: 56.73%
[rank1]:2025-11-08 04:27:26,044 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3321  memory: 11.85GiB(24.93%)  tps: 5,962  tflops: 45.41  mfu: 14.55%
[rank0]:2025-11-08 04:27:26,055 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3321  memory: 14.71GiB(30.97%)  tps: 5,962  tflops: 45.41  mfu: 14.55%
[rank2]:2025-11-08 04:27:26,041 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3321  memory:  8.11GiB(17.07%)  tps: 5,962  tflops: 45.41  mfu: 14.55%
[rank3]:2025-11-08 04:27:26,053 - INFO -  step: 550  loss:  0.4226  grad_norm:  0.3321  memory: 18.46GiB(38.86%)  tps: 5,962  tflops: 45.41  mfu: 14.55%
[rank0]:2025-11-08 04:29:40,474 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:29:43,176 - INFO - Avg. fwd time: 6.8888 / Avg. bwd time: 22.2295 / Avg. batch time: 574.2879 (ms) / GPU bubble ratio: 18.87%
[rank1]:2025-11-08 04:29:43,229 - INFO - Avg. fwd time: 4.8739 / Avg. bwd time: 12.3378 / Avg. batch time: 636.4105 (ms) / GPU bubble ratio: 56.73%
[rank0]:2025-11-08 04:29:43,257 - INFO - Avg. fwd time: 4.1154 / Avg. bwd time: 12.0512 / Avg. batch time: 659.2646 (ms) / GPU bubble ratio: 60.76%
[rank2]:2025-11-08 04:29:43,201 - INFO - Avg. fwd time: 3.9826 / Avg. bwd time: 10.1217 / Avg. batch time: 606.8051 (ms) / GPU bubble ratio: 62.81%
[rank2]:2025-11-08 04:29:43,278 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3197  memory:  8.11GiB(17.07%)  tps: 5,969  tflops: 45.46  mfu: 14.57%
[rank3]:2025-11-08 04:29:43,290 - INFO -  step: 600  loss:  0.4256  grad_norm:  0.3197  memory: 18.46GiB(38.86%)  tps: 5,969  tflops: 45.46  mfu: 14.57%
[rank1]:2025-11-08 04:29:43,281 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3197  memory: 11.85GiB(24.93%)  tps: 5,969  tflops: 45.46  mfu: 14.57%
[rank0]:2025-11-08 04:29:43,292 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3197  memory: 14.71GiB(30.97%)  tps: 5,969  tflops: 45.46  mfu: 14.57%
[rank3]:2025-11-08 04:29:43,535 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0429_real_step600_rank3.svg
[rank3]:> Batch Time: 659.86 ms, GPU Bubble Ratio: 60.55%, 58.03%, 65.68%, 29.12%
[rank0]:2025-11-08 04:31:58,079 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:32:00,492 - INFO - Avg. fwd time: 6.8925 / Avg. bwd time: 22.2376 / Avg. batch time: 574.4781 (ms) / GPU bubble ratio: 18.87%
[rank2]:2025-11-08 04:32:00,548 - INFO - Avg. fwd time: 3.9830 / Avg. bwd time: 10.1224 / Avg. batch time: 607.0295 (ms) / GPU bubble ratio: 62.82%
[rank0]:2025-11-08 04:32:00,561 - INFO - Avg. fwd time: 4.1153 / Avg. bwd time: 12.0519 / Avg. batch time: 659.4936 (ms) / GPU bubble ratio: 60.78%
[rank1]:2025-11-08 04:32:00,649 - INFO - Avg. fwd time: 4.8743 / Avg. bwd time: 12.3405 / Avg. batch time: 636.6412 (ms) / GPU bubble ratio: 56.74%
[rank1]:2025-11-08 04:32:00,810 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3203  memory: 11.85GiB(24.93%)  tps: 5,957  tflops: 45.37  mfu: 14.54%
[rank0]:2025-11-08 04:32:00,821 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3203  memory: 14.71GiB(30.97%)  tps: 5,957  tflops: 45.37  mfu: 14.54%
[rank2]:2025-11-08 04:32:00,807 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3203  memory:  8.11GiB(17.07%)  tps: 5,957  tflops: 45.37  mfu: 14.54%
[rank3]:2025-11-08 04:32:00,819 - INFO -  step: 650  loss:  0.3624  grad_norm:  0.3203  memory: 18.46GiB(38.86%)  tps: 5,957  tflops: 45.37  mfu: 14.54%
[rank0]:2025-11-08 04:34:15,221 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:34:17,670 - INFO - Avg. fwd time: 6.8950 / Avg. bwd time: 22.2440 / Avg. batch time: 574.6205 (ms) / GPU bubble ratio: 18.86%
[rank2]:2025-11-08 04:34:17,729 - INFO - Avg. fwd time: 3.9831 / Avg. bwd time: 10.1229 / Avg. batch time: 607.1097 (ms) / GPU bubble ratio: 62.82%
[rank0]:2025-11-08 04:34:17,749 - INFO - Avg. fwd time: 4.1152 / Avg. bwd time: 12.0523 / Avg. batch time: 659.5777 (ms) / GPU bubble ratio: 60.78%
[rank1]:2025-11-08 04:34:17,833 - INFO - Avg. fwd time: 4.8747 / Avg. bwd time: 12.3428 / Avg. batch time: 636.7274 (ms) / GPU bubble ratio: 56.74%
[rank1]:2025-11-08 04:34:17,993 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3664  memory: 11.85GiB(24.93%)  tps: 5,972  tflops: 45.48  mfu: 14.58%
[rank0]:2025-11-08 04:34:18,003 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3664  memory: 14.71GiB(30.97%)  tps: 5,972  tflops: 45.48  mfu: 14.58%
[rank2]:2025-11-08 04:34:17,989 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3664  memory:  8.11GiB(17.07%)  tps: 5,972  tflops: 45.48  mfu: 14.58%
[rank3]:2025-11-08 04:34:18,001 - INFO -  step: 700  loss:  0.4289  grad_norm:  0.3664  memory: 18.46GiB(38.86%)  tps: 5,972  tflops: 45.48  mfu: 14.58%
[rank3]:2025-11-08 04:34:18,242 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0434_real_step700_rank3.svg
[rank3]:> Batch Time: 659.85 ms, GPU Bubble Ratio: 60.56%, 58.12%, 65.70%, 29.11%
[rank0]:2025-11-08 04:36:32,538 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 04:36:35,238 - INFO - Avg. fwd time: 3.9830 / Avg. bwd time: 10.1230 / Avg. batch time: 607.2197 (ms) / GPU bubble ratio: 62.83%
[rank3]:2025-11-08 04:36:35,211 - INFO - Avg. fwd time: 6.8965 / Avg. bwd time: 22.2472 / Avg. batch time: 574.6920 (ms) / GPU bubble ratio: 18.86%
[rank1]:2025-11-08 04:36:35,267 - INFO - Avg. fwd time: 4.8744 / Avg. bwd time: 12.3432 / Avg. batch time: 636.8406 (ms) / GPU bubble ratio: 56.74%
[rank0]:2025-11-08 04:36:35,296 - INFO - Avg. fwd time: 4.1148 / Avg. bwd time: 12.0524 / Avg. batch time: 659.6903 (ms) / GPU bubble ratio: 60.79%
[rank0]:2025-11-08 04:36:35,332 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4599  memory: 14.71GiB(30.97%)  tps: 5,965  tflops: 45.43  mfu: 14.56%
[rank2]:2025-11-08 04:36:35,318 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4599  memory:  8.11GiB(17.07%)  tps: 5,965  tflops: 45.43  mfu: 14.56%
[rank3]:2025-11-08 04:36:35,330 - INFO -  step: 750  loss:  0.4016  grad_norm:  0.4599  memory: 18.46GiB(38.86%)  tps: 5,965  tflops: 45.43  mfu: 14.56%
[rank1]:2025-11-08 04:36:35,321 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4599  memory: 11.85GiB(24.93%)  tps: 5,965  tflops: 45.43  mfu: 14.56%
[rank0]:2025-11-08 04:38:49,598 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:38:52,050 - INFO - Avg. fwd time: 6.8971 / Avg. bwd time: 22.2488 / Avg. batch time: 574.7284 (ms) / GPU bubble ratio: 18.86%
[rank2]:2025-11-08 04:38:52,110 - INFO - Avg. fwd time: 3.9831 / Avg. bwd time: 10.1231 / Avg. batch time: 607.2314 (ms) / GPU bubble ratio: 62.83%
[rank0]:2025-11-08 04:38:52,129 - INFO - Avg. fwd time: 4.1148 / Avg. bwd time: 12.0527 / Avg. batch time: 659.7020 (ms) / GPU bubble ratio: 60.79%
[rank1]:2025-11-08 04:38:52,211 - INFO - Avg. fwd time: 4.8744 / Avg. bwd time: 12.3438 / Avg. batch time: 636.8535 (ms) / GPU bubble ratio: 56.74%
[rank2]:2025-11-08 04:38:52,365 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3487  memory:  8.11GiB(17.07%)  tps: 5,978  tflops: 45.53  mfu: 14.59%
[rank2]:2025-11-08 04:38:52,365 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3487  tps: 6,337  tflops: 48.27  mfu: 14.03%
[rank2]:2025-11-08 04:38:52,365 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 04:38:52,366 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-08 04:38:52,369 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3487  memory: 11.85GiB(24.93%)  tps: 5,978  tflops: 45.53  mfu: 14.59%
[rank1]:2025-11-08 04:38:52,369 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3487  tps: 6,337  tflops: 48.26  mfu: 14.02%
[rank1]:2025-11-08 04:38:52,369 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 04:38:52,370 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 04:38:52,379 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3487  memory: 14.71GiB(30.97%)  tps: 5,978  tflops: 45.53  mfu: 14.59%
[rank0]:2025-11-08 04:38:52,379 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3487  tps: 6,337  tflops: 48.26  mfu: 14.02%
[rank0]:2025-11-08 04:38:52,379 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 04:38:52,380 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-08 04:38:52,377 - INFO -  step: 800  loss:  0.4142  grad_norm:  0.3487  memory: 18.46GiB(38.86%)  tps: 5,978  tflops: 45.53  mfu: 14.59%
[rank3]:2025-11-08 04:38:52,378 - INFO -  final step: 800  loss:  0.4142  grad_norm:  0.3487  tps: 6,343  tflops: 48.31  mfu: 14.13%
[rank3]:2025-11-08 04:38:52,378 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 04:38:52,379 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 04:38:54,489 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 04:38:54,508 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-08 04:38:54,508 - INFO - Destroying the purge thread.
[rank1]:2025-11-08 04:38:54,508 - INFO - Destroying the purge thread.
[rank3]:2025-11-08 04:38:54,742 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0438_real_final800_rank3.svg
[rank3]:> Batch Time: 659.34 ms, GPU Bubble Ratio: 60.53%, 58.11%, 65.66%, 29.30%
[rank2]:2025-11-08 04:38:54,846 - INFO - Process group destroyed
[rank1]:2025-11-08 04:38:54,869 - INFO - Process group destroyed
[rank3]:2025-11-08 04:38:54,976 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0438_thry_final800_rank3.svg
[rank3]:> Batch Time: 282.14 ms, GPU Bubble Ratio: 15.79%, 15.79%, 15.79%, 15.79%
[rank3]:2025-11-08 04:38:54,977 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading wandb-summary.json; uploading config.yaml
[rank0]:2025-11-08 04:38:56,508 - INFO - Training completed
[rank0]:2025-11-08 04:38:56,509 - INFO - Destroying the purge thread.
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.58282
[rank3]:wandb:               final/avg_loss 0.41421
[rank3]:wandb:             final/avg_mfu(%) 14.13136
[rank3]:wandb:             final/avg_tflops 48.31308
[rank3]:wandb:    final/avg_throughput(tps) 6343.45203
[rank3]:wandb:              final/grad_norm 0.34873
[rank3]:wandb:               final/max_loss 0.41421
[rank3]:wandb:                    grad_norm 0.34873
[rank3]:wandb: loss_metrics/global_avg_loss 0.41421
[rank3]:wandb: loss_metrics/global_max_loss 0.41421
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/ike02ouy
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0402/wandb/run-20251108_040215-ike02ouy/logs
[rank3]:2025-11-08 04:38:56,862 - INFO - Process group destroyed
[rank0]:2025-11-08 04:38:56,891 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.1', 'layers.0'}
[rank0]:Stage 4: Modules to keep: {'layers.10', 'layers.9'}
[rank1]:Stage 1: Modules to keep: {'layers.3', 'layers.2', 'layers.4'}
[rank1]:Stage 5: Modules to keep: {'layers.11', 'layers.12'}
[rank2]:Stage 2: Modules to keep: {'layers.6', 'layers.5'}
[rank2]:Stage 6: Modules to keep: {'layers.13', 'layers.14'}
[rank3]:Stage 3: Modules to keep: {'layers.8', 'layers.7'}
[rank3]:Stage 7: Modules to keep: {'output', 'layers.15', 'norm'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_Interleaved1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
