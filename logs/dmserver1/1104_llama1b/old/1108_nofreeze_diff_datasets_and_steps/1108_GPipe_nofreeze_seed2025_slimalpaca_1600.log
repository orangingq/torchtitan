
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 07. (ê¸ˆ) 23:08:47 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_slimalpaca_1600.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=slimorca,alpaca --training.steps=1600  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank1]:2025-11-07 23:08:54,306 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank1]:2025-11-07 23:08:54,459 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-07 23:08:54,462 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:08:54,857 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-07 23:08:54,870 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-07 23:08:54,857 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:2025-11-07 23:08:55,176 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 23:08:55,199 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 23:08:55,201 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:08:55,205 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-07 23:08:55,206 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-07 23:08:55,225 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-07 23:08:55,228 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-07 23:08:55,179 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:08:55,742 - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:2025-11-07 23:08:58,502 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank1]:2025-11-07 23:09:00,392 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-07 23:09:00,433 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:09:00,458 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-07 23:09:00,458 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-07 23:09:00,639 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:09:00,639 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-07 23:09:00,640 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-07 23:09:00,816 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-07 23:09:00,857 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:09:00,885 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-07 23:09:00,885 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 23:09:01,017 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-07 23:09:01,086 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:09:01,086 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-07 23:09:01,086 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-07 23:09:01,177 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 23:09:01,214 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:09:01,216 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-07 23:09:01,243 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-07 23:09:01,243 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 23:09:01,436 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:09:01,436 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-07 23:09:01,437 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run dv3fyko5
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/20251107-2309/wandb/run-20251107_230902-dv3fyko5
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/dv3fyko5
[rank3]:2025-11-07 23:09:03,816 - INFO - WandB logging enabled
[rank3]:2025-11-07 23:09:03,817 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-07 23:09:03,856 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:09:03,882 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-07 23:09:03,882 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-07 23:09:04,087 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:09:04,088 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-07 23:09:04,089 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank2]:2025-11-07 23:09:04,108 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-07 23:09:04,108 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:09:04,108 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank0]:2025-11-07 23:09:04,108 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:09:04,109 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 1600 (warmup 100)
[rank3]:2025-11-07 23:09:04,108 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:09:04,109 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank0]:2025-11-07 23:09:06,591 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-07 23:09:06,591 - INFO - Finished loading the checkpoint in 2.48 seconds.
[rank0]:2025-11-07 23:09:06,591 - INFO - Training starts at step 1
[rank2]:2025-11-07 23:09:09,770 - INFO -  step:  1  loss: -4.0000  grad_norm: 34.2867  memory:  9.99GiB(21.03%)  tps: 1,838  tflops: 14.00  mfu: 4.49%
[rank2]:2025-11-07 23:09:09,770 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-07 23:09:09,780 - INFO -  step:  1  loss: 12.7485  grad_norm: 34.2867  memory: 24.19GiB(50.91%)  tps: 2,767  tflops: 21.07  mfu: 6.75%
[rank3]:2025-11-07 23:09:09,780 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-07 23:09:09,776 - INFO -  step:  1  loss: -4.0000  grad_norm: 34.2867  memory: 12.38GiB(26.05%)  tps: 1,754  tflops: 13.36  mfu: 4.28%
[rank1]:2025-11-07 23:09:09,776 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:09:09,806 - INFO -  step:  1  loss: -4.0000  grad_norm: 34.2867  memory: 12.80GiB(26.95%)  tps: 1,907  tflops: 14.52  mfu: 4.66%
[rank0]:2025-11-07 23:09:09,807 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:11:15,631 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:11:17,971 - INFO - Avg. fwd time: 11.5440 / Avg. bwd time: 44.6908 / Avg. batch time: 509.7784 (ms) / GPU bubble ratio: 11.75%
[rank2]:2025-11-07 23:11:18,046 - INFO - Avg. fwd time: 7.2338 / Avg. bwd time: 18.8748 / Avg. batch time: 541.9018 (ms) / GPU bubble ratio: 61.46%
[rank1]:2025-11-07 23:11:18,085 - INFO - Avg. fwd time: 9.1951 / Avg. bwd time: 23.9500 / Avg. batch time: 581.4255 (ms) / GPU bubble ratio: 54.39%
[rank0]:2025-11-07 23:11:18,093 - INFO - Avg. fwd time: 7.9498 / Avg. bwd time: 23.4722 / Avg. batch time: 618.7737 (ms) / GPU bubble ratio: 59.38%
[rank1]:2025-11-07 23:11:18,271 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.8804  memory: 14.64GiB(30.82%)  tps: 6,248  tflops: 47.59  mfu: 15.25%
[rank2]:2025-11-07 23:11:18,268 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.8804  memory: 11.81GiB(24.85%)  tps: 6,248  tflops: 47.58  mfu: 15.25%
[rank3]:2025-11-07 23:11:18,280 - INFO -  step: 50  loss:  8.9924  grad_norm:  9.8804  memory: 26.98GiB(56.79%)  tps: 6,248  tflops: 47.58  mfu: 15.25%
[rank0]:2025-11-07 23:11:18,282 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.8804  memory: 16.57GiB(34.88%)  tps: 6,249  tflops: 47.59  mfu: 15.25%
[rank0]:2025-11-07 23:13:27,340 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:13:29,676 - INFO - Avg. fwd time: 11.4843 / Avg. bwd time: 45.0206 / Avg. batch time: 511.8484 (ms) / GPU bubble ratio: 11.69%
[rank1]:2025-11-07 23:13:29,789 - INFO - Avg. fwd time: 9.2106 / Avg. bwd time: 24.0850 / Avg. batch time: 582.7249 (ms) / GPU bubble ratio: 54.29%
[rank2]:2025-11-07 23:13:29,749 - INFO - Avg. fwd time: 7.2201 / Avg. bwd time: 18.9242 / Avg. batch time: 543.4839 (ms) / GPU bubble ratio: 61.52%
[rank0]:2025-11-07 23:13:29,797 - INFO - Avg. fwd time: 7.9589 / Avg. bwd time: 23.5220 / Avg. batch time: 619.5934 (ms) / GPU bubble ratio: 59.35%
[rank1]:2025-11-07 23:13:29,977 - INFO -  step: 100  loss: -4.0000  grad_norm: 11.3980  memory: 14.64GiB(30.82%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank2]:2025-11-07 23:13:29,973 - INFO -  step: 100  loss: -4.0000  grad_norm: 11.3980  memory: 11.81GiB(24.85%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank3]:2025-11-07 23:13:29,986 - INFO -  step: 100  loss:  6.8345  grad_norm: 11.3980  memory: 26.98GiB(56.79%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank0]:2025-11-07 23:13:29,987 - INFO -  step: 100  loss: -4.0000  grad_norm: 11.3980  memory: 16.57GiB(34.88%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank3]:2025-11-07 23:13:30,210 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2313_real_step100_rank3.svg
[rank3]:> Batch Time: 620.64 ms, GPU Bubble Ratio: 59.09%, 56.78%, 66.16%, 26.64%
[rank0]:2025-11-07 23:15:39,402 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 23:15:41,988 - INFO - Avg. fwd time: 7.2135 / Avg. bwd time: 18.9440 / Avg. batch time: 544.5017 (ms) / GPU bubble ratio: 61.57%
[rank3]:2025-11-07 23:15:41,962 - INFO - Avg. fwd time: 11.4369 / Avg. bwd time: 45.1788 / Avg. batch time: 512.6998 (ms) / GPU bubble ratio: 11.66%
[rank1]:2025-11-07 23:15:42,019 - INFO - Avg. fwd time: 9.2196 / Avg. bwd time: 24.1463 / Avg. batch time: 583.7098 (ms) / GPU bubble ratio: 54.27%
[rank1]:2025-11-07 23:15:42,074 - INFO -  step: 150  loss: -4.0000  grad_norm:  8.1452  memory: 14.64GiB(30.82%)  tps: 6,201  tflops: 47.23  mfu: 15.14%
[rank2]:2025-11-07 23:15:42,071 - INFO -  step: 150  loss: -4.0000  grad_norm:  8.1452  memory: 11.81GiB(24.85%)  tps: 6,201  tflops: 47.23  mfu: 15.14%
[rank3]:2025-11-07 23:15:42,084 - INFO -  step: 150  loss:  6.2832  grad_norm:  8.1452  memory: 26.98GiB(56.79%)  tps: 6,202  tflops: 47.23  mfu: 15.14%
[rank0]:2025-11-07 23:15:42,049 - INFO - Avg. fwd time: 7.9704 / Avg. bwd time: 23.5447 / Avg. batch time: 620.4453 (ms) / GPU bubble ratio: 59.36%
[rank0]:2025-11-07 23:15:42,085 - INFO -  step: 150  loss: -4.0000  grad_norm:  8.1452  memory: 16.57GiB(34.88%)  tps: 6,201  tflops: 47.23  mfu: 15.14%
[rank0]:2025-11-07 23:17:51,478 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-07 23:17:53,890 - INFO - Avg. fwd time: 7.2106 / Avg. bwd time: 18.9542 / Avg. batch time: 544.9236 (ms) / GPU bubble ratio: 61.59%
[rank3]:2025-11-07 23:17:53,815 - INFO - Avg. fwd time: 11.4153 / Avg. bwd time: 45.2713 / Avg. batch time: 513.2573 (ms) / GPU bubble ratio: 11.64%
[rank1]:2025-11-07 23:17:53,928 - INFO - Avg. fwd time: 9.2271 / Avg. bwd time: 24.1878 / Avg. batch time: 584.1640 (ms) / GPU bubble ratio: 54.24%
[rank0]:2025-11-07 23:17:53,936 - INFO - Avg. fwd time: 7.9724 / Avg. bwd time: 23.5580 / Avg. batch time: 620.8458 (ms) / GPU bubble ratio: 59.37%
[rank1]:2025-11-07 23:17:54,117 - INFO -  step: 200  loss: -4.0000  grad_norm:  6.3709  memory: 14.64GiB(30.82%)  tps: 6,204  tflops: 47.25  mfu: 15.14%
[rank2]:2025-11-07 23:17:54,113 - INFO -  step: 200  loss: -4.0000  grad_norm:  6.3709  memory: 11.81GiB(24.85%)  tps: 6,204  tflops: 47.25  mfu: 15.14%
[rank3]:2025-11-07 23:17:54,125 - INFO -  step: 200  loss:  4.8678  grad_norm:  6.3709  memory: 26.98GiB(56.79%)  tps: 6,204  tflops: 47.25  mfu: 15.15%
[rank0]:2025-11-07 23:17:54,127 - INFO -  step: 200  loss: -4.0000  grad_norm:  6.3709  memory: 16.57GiB(34.88%)  tps: 6,204  tflops: 47.25  mfu: 15.14%
[rank3]:2025-11-07 23:17:54,283 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2317_real_step200_rank3.svg
[rank3]:> Batch Time: 622.69 ms, GPU Bubble Ratio: 59.14%, 56.71%, 66.24%, 26.73%
[rank0]:2025-11-07 23:20:03,313 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:20:05,670 - INFO - Avg. fwd time: 11.3933 / Avg. bwd time: 45.2988 / Avg. batch time: 513.2895 (ms) / GPU bubble ratio: 11.64%
[rank2]:2025-11-07 23:20:05,742 - INFO - Avg. fwd time: 7.2073 / Avg. bwd time: 18.9605 / Avg. batch time: 544.9836 (ms) / GPU bubble ratio: 61.59%
[rank1]:2025-11-07 23:20:05,782 - INFO - Avg. fwd time: 9.2313 / Avg. bwd time: 24.2157 / Avg. batch time: 584.2427 (ms) / GPU bubble ratio: 54.20%
[rank0]:2025-11-07 23:20:05,790 - INFO - Avg. fwd time: 7.9729 / Avg. bwd time: 23.5657 / Avg. batch time: 620.8895 (ms) / GPU bubble ratio: 59.36%
[rank2]:2025-11-07 23:20:05,963 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8500  memory: 11.81GiB(24.85%)  tps: 6,213  tflops: 47.32  mfu: 15.17%
[rank1]:2025-11-07 23:20:05,967 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8500  memory: 14.64GiB(30.82%)  tps: 6,213  tflops: 47.32  mfu: 15.17%
[rank3]:2025-11-07 23:20:05,975 - INFO -  step: 250  loss:  4.5699  grad_norm:  3.8500  memory: 26.98GiB(56.79%)  tps: 6,213  tflops: 47.32  mfu: 15.17%
[rank0]:2025-11-07 23:20:05,977 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8500  memory: 16.57GiB(34.88%)  tps: 6,213  tflops: 47.32  mfu: 15.17%
[rank0]:2025-11-07 23:22:14,496 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-07 23:22:17,116 - INFO - Avg. fwd time: 9.2319 / Avg. bwd time: 24.2311 / Avg. batch time: 583.7077 (ms) / GPU bubble ratio: 54.14%
[rank2]:2025-11-07 23:22:17,085 - INFO - Avg. fwd time: 7.2040 / Avg. bwd time: 18.9616 / Avg. batch time: 544.4573 (ms) / GPU bubble ratio: 61.55%
[rank3]:2025-11-07 23:22:17,057 - INFO - Avg. fwd time: 11.3788 / Avg. bwd time: 45.2596 / Avg. batch time: 512.8495 (ms) / GPU bubble ratio: 11.65%
[rank0]:2025-11-07 23:22:17,147 - INFO - Avg. fwd time: 7.9736 / Avg. bwd time: 23.5693 / Avg. batch time: 620.3280 (ms) / GPU bubble ratio: 59.32%
[rank1]:2025-11-07 23:22:17,172 - INFO -  step: 300  loss: -4.0000  grad_norm:  6.6310  memory: 14.64GiB(30.82%)  tps: 6,244  tflops: 47.55  mfu: 15.24%
[rank2]:2025-11-07 23:22:17,168 - INFO -  step: 300  loss: -4.0000  grad_norm:  6.6310  memory: 11.81GiB(24.85%)  tps: 6,244  tflops: 47.55  mfu: 15.24%
[rank3]:2025-11-07 23:22:17,181 - INFO -  step: 300  loss:  4.7660  grad_norm:  6.6310  memory: 26.98GiB(56.79%)  tps: 6,244  tflops: 47.55  mfu: 15.24%
[rank0]:2025-11-07 23:22:17,182 - INFO -  step: 300  loss: -4.0000  grad_norm:  6.6310  memory: 16.57GiB(34.88%)  tps: 6,244  tflops: 47.55  mfu: 15.24%
[rank3]:2025-11-07 23:22:17,334 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2322_real_step300_rank3.svg
[rank3]:> Batch Time: 616.67 ms, GPU Bubble Ratio: 58.79%, 56.40%, 65.94%, 26.91%
[rank0]:2025-11-07 23:24:26,155 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:24:28,506 - INFO - Avg. fwd time: 11.3755 / Avg. bwd time: 45.2413 / Avg. batch time: 512.6749 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:24:28,578 - INFO - Avg. fwd time: 7.2026 / Avg. bwd time: 18.9646 / Avg. batch time: 544.3128 (ms) / GPU bubble ratio: 61.54%
[rank1]:2025-11-07 23:24:28,618 - INFO - Avg. fwd time: 9.2327 / Avg. bwd time: 24.2459 / Avg. batch time: 583.5623 (ms) / GPU bubble ratio: 54.10%
[rank0]:2025-11-07 23:24:28,625 - INFO - Avg. fwd time: 7.9740 / Avg. bwd time: 23.5733 / Avg. batch time: 620.1686 (ms) / GPU bubble ratio: 59.30%
[rank2]:2025-11-07 23:24:28,799 - INFO -  step: 350  loss: -4.0000  grad_norm:  7.3616  memory: 11.81GiB(24.85%)  tps: 6,223  tflops: 47.40  mfu: 15.19%
[rank1]:2025-11-07 23:24:28,803 - INFO -  step: 350  loss: -4.0000  grad_norm:  7.3616  memory: 14.64GiB(30.82%)  tps: 6,223  tflops: 47.40  mfu: 15.19%
[rank0]:2025-11-07 23:24:28,813 - INFO -  step: 350  loss: -4.0000  grad_norm:  7.3616  memory: 16.57GiB(34.88%)  tps: 6,223  tflops: 47.40  mfu: 15.19%
[rank3]:2025-11-07 23:24:28,812 - INFO -  step: 350  loss:  4.9435  grad_norm:  7.3616  memory: 26.98GiB(56.79%)  tps: 6,224  tflops: 47.40  mfu: 15.19%
[rank0]:2025-11-07 23:26:37,732 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:26:40,092 - INFO - Avg. fwd time: 11.3727 / Avg. bwd time: 45.2390 / Avg. batch time: 512.6294 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:26:40,164 - INFO - Avg. fwd time: 7.2019 / Avg. bwd time: 18.9679 / Avg. batch time: 544.2090 (ms) / GPU bubble ratio: 61.53%
[rank1]:2025-11-07 23:26:40,204 - INFO - Avg. fwd time: 9.2355 / Avg. bwd time: 24.2611 / Avg. batch time: 583.4761 (ms) / GPU bubble ratio: 54.07%
[rank0]:2025-11-07 23:26:40,212 - INFO - Avg. fwd time: 7.9752 / Avg. bwd time: 23.5783 / Avg. batch time: 620.0717 (ms) / GPU bubble ratio: 59.29%
[rank2]:2025-11-07 23:26:40,385 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.7212  memory: 11.81GiB(24.85%)  tps: 6,226  tflops: 47.42  mfu: 15.20%
[rank1]:2025-11-07 23:26:40,389 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.7212  memory: 14.64GiB(30.82%)  tps: 6,226  tflops: 47.42  mfu: 15.20%
[rank0]:2025-11-07 23:26:40,400 - INFO -  step: 400  loss: -4.0000  grad_norm:  1.7212  memory: 16.57GiB(34.88%)  tps: 6,226  tflops: 47.42  mfu: 15.20%
[rank3]:2025-11-07 23:26:40,398 - INFO -  step: 400  loss:  4.9884  grad_norm:  1.7212  memory: 26.98GiB(56.79%)  tps: 6,226  tflops: 47.42  mfu: 15.20%
[rank3]:2025-11-07 23:26:40,580 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2326_real_step400_rank3.svg
[rank3]:> Batch Time: 619.70 ms, GPU Bubble Ratio: 58.94%, 56.52%, 66.10%, 26.78%
[rank0]:2025-11-07 23:28:49,581 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:28:52,112 - INFO - Avg. fwd time: 11.3717 / Avg. bwd time: 45.2449 / Avg. batch time: 512.6683 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:28:52,139 - INFO - Avg. fwd time: 7.2005 / Avg. bwd time: 18.9691 / Avg. batch time: 544.2762 (ms) / GPU bubble ratio: 61.53%
[rank1]:2025-11-07 23:28:52,171 - INFO - Avg. fwd time: 9.2357 / Avg. bwd time: 24.2679 / Avg. batch time: 583.5553 (ms) / GPU bubble ratio: 54.07%
[rank1]:2025-11-07 23:28:52,227 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.3927  memory: 14.64GiB(30.82%)  tps: 6,214  tflops: 47.33  mfu: 15.17%
[rank3]:2025-11-07 23:28:52,236 - INFO -  step: 450  loss:  4.6392  grad_norm:  2.3927  memory: 26.98GiB(56.79%)  tps: 6,214  tflops: 47.33  mfu: 15.17%
[rank2]:2025-11-07 23:28:52,223 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.3927  memory: 11.81GiB(24.85%)  tps: 6,214  tflops: 47.32  mfu: 15.17%
[rank0]:2025-11-07 23:28:52,202 - INFO - Avg. fwd time: 7.9743 / Avg. bwd time: 23.5807 / Avg. batch time: 620.1501 (ms) / GPU bubble ratio: 59.29%
[rank0]:2025-11-07 23:28:52,237 - INFO -  step: 450  loss: -4.0000  grad_norm:  2.3927  memory: 16.57GiB(34.88%)  tps: 6,214  tflops: 47.33  mfu: 15.17%
[rank0]:2025-11-07 23:31:00,997 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:31:03,347 - INFO - Avg. fwd time: 11.3695 / Avg. bwd time: 45.2442 / Avg. batch time: 512.6435 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:31:03,420 - INFO - Avg. fwd time: 7.1989 / Avg. bwd time: 18.9694 / Avg. batch time: 544.2151 (ms) / GPU bubble ratio: 61.53%
[rank0]:2025-11-07 23:31:03,468 - INFO - Avg. fwd time: 7.9710 / Avg. bwd time: 23.5823 / Avg. batch time: 620.0824 (ms) / GPU bubble ratio: 59.29%
[rank1]:2025-11-07 23:31:03,462 - INFO - Avg. fwd time: 9.2351 / Avg. bwd time: 24.2709 / Avg. batch time: 583.4928 (ms) / GPU bubble ratio: 54.06%
[rank2]:2025-11-07 23:31:03,641 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.6664  memory: 11.81GiB(24.85%)  tps: 6,234  tflops: 47.48  mfu: 15.22%
[rank1]:2025-11-07 23:31:03,645 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.6664  memory: 14.64GiB(30.82%)  tps: 6,234  tflops: 47.48  mfu: 15.22%
[rank3]:2025-11-07 23:31:03,653 - INFO -  step: 500  loss:  4.4544  grad_norm:  1.6664  memory: 26.98GiB(56.79%)  tps: 6,234  tflops: 47.48  mfu: 15.22%
[rank0]:2025-11-07 23:31:03,655 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.6664  memory: 16.57GiB(34.88%)  tps: 6,234  tflops: 47.48  mfu: 15.22%
[rank3]:2025-11-07 23:31:03,805 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2331_real_step500_rank3.svg
[rank3]:> Batch Time: 620.70 ms, GPU Bubble Ratio: 59.04%, 56.67%, 66.17%, 26.86%
[rank0]:2025-11-07 23:33:13,150 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:33:15,491 - INFO - Avg. fwd time: 11.3703 / Avg. bwd time: 45.2645 / Avg. batch time: 512.8128 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:33:15,563 - INFO - Avg. fwd time: 7.1984 / Avg. bwd time: 18.9709 / Avg. batch time: 544.3926 (ms) / GPU bubble ratio: 61.54%
[rank0]:2025-11-07 23:33:15,611 - INFO - Avg. fwd time: 7.9716 / Avg. bwd time: 23.5860 / Avg. batch time: 620.2460 (ms) / GPU bubble ratio: 59.30%
[rank1]:2025-11-07 23:33:15,602 - INFO - Avg. fwd time: 9.2367 / Avg. bwd time: 24.2750 / Avg. batch time: 583.6583 (ms) / GPU bubble ratio: 54.07%
[rank2]:2025-11-07 23:33:15,783 - INFO -  step: 550  loss: -4.0000  grad_norm:  3.5597  memory: 11.81GiB(24.85%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank3]:2025-11-07 23:33:15,795 - INFO -  step: 550  loss:  4.2830  grad_norm:  3.5597  memory: 26.98GiB(56.79%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank0]:2025-11-07 23:33:15,797 - INFO -  step: 550  loss: -4.0000  grad_norm:  3.5597  memory: 16.57GiB(34.88%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank1]:2025-11-07 23:33:15,786 - INFO -  step: 550  loss: -4.0000  grad_norm:  3.5597  memory: 14.64GiB(30.82%)  tps: 6,199  tflops: 47.22  mfu: 15.13%

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 07. (ê¸ˆ) 23:35:12 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_slimalpaca_1600.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=slimorca,alpaca_cleaned --training.steps=1600 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-07 23:35:18,633 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-07 23:35:18,629 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-07 23:35:18,751 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank2]:2025-11-07 23:35:18,758 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-07 23:35:18,830 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 23:35:18,833 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:35:18,837 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-07 23:35:18,838 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-07 23:35:18,856 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-07 23:35:18,859 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-07 23:35:19,039 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-07 23:35:19,041 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-07 23:35:19,027 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-07 23:35:19,030 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:35:19,240 - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:2025-11-07 23:35:22,108 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-07 23:35:24,273 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-07 23:35:24,440 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 23:35:24,480 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:35:24,481 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-07 23:35:24,505 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-07 23:35:24,505 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-07 23:35:24,429 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-07 23:35:24,473 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:35:24,501 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-07 23:35:24,501 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 23:35:24,697 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:35:24,697 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-07 23:35:24,698 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-07 23:35:24,696 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:35:24,696 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-07 23:35:24,697 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-07 23:35:24,961 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-07 23:35:25,000 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:35:25,026 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-07 23:35:25,027 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-07 23:35:25,250 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:35:25,251 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-07 23:35:25,251 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run kx7k4wkd
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/20251107-2335/wandb/run-20251107_233525-kx7k4wkd
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/kx7k4wkd
[rank3]:2025-11-07 23:35:26,776 - INFO - WandB logging enabled
[rank3]:2025-11-07 23:35:26,777 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-07 23:35:26,815 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:35:26,844 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-07 23:35:26,844 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-07 23:35:27,073 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-07 23:35:27,056 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:35:27,056 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-07 23:35:27,057 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-07 23:35:27,072 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:35:27,072 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank0]:2025-11-07 23:35:27,072 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:35:27,073 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 1600 (warmup 100)
[rank0]:2025-11-07 23:35:27,073 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-07 23:35:27,072 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:35:29,730 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-11-07 23:35:29,730 - INFO - Finished loading the checkpoint in 2.66 seconds.
[rank0]:2025-11-07 23:35:29,730 - INFO - Training starts at step 1
[rank2]:2025-11-07 23:35:33,025 - INFO -  step:  1  loss: -4.0000  grad_norm: 30.3606  memory:  9.99GiB(21.03%)  tps: 2,042  tflops: 15.55  mfu: 4.98%
[rank2]:2025-11-07 23:35:33,025 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-07 23:35:33,030 - INFO -  step:  1  loss: -4.0000  grad_norm: 30.3606  memory: 12.38GiB(26.05%)  tps: 1,915  tflops: 14.58  mfu: 4.67%
[rank1]:2025-11-07 23:35:33,031 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:35:33,064 - INFO -  step:  1  loss: -4.0000  grad_norm: 30.3606  memory: 12.80GiB(26.95%)  tps: 1,909  tflops: 14.54  mfu: 4.66%
[rank3]:2025-11-07 23:35:33,037 - INFO -  step:  1  loss: 13.8151  grad_norm: 30.3606  memory: 24.19GiB(50.91%)  tps: 2,634  tflops: 20.06  mfu: 6.43%
[rank3]:2025-11-07 23:35:33,037 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:35:33,064 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:37:39,953 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:37:42,300 - INFO - Avg. fwd time: 11.5692 / Avg. bwd time: 45.3050 / Avg. batch time: 515.0054 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-07 23:37:42,374 - INFO - Avg. fwd time: 7.2274 / Avg. bwd time: 18.8966 / Avg. batch time: 547.2708 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-07 23:37:42,414 - INFO - Avg. fwd time: 9.2338 / Avg. bwd time: 24.1004 / Avg. batch time: 587.0036 (ms) / GPU bubble ratio: 54.57%
[rank0]:2025-11-07 23:37:42,421 - INFO - Avg. fwd time: 8.0036 / Avg. bwd time: 23.5162 / Avg. batch time: 624.5368 (ms) / GPU bubble ratio: 59.62%
[rank2]:2025-11-07 23:37:42,597 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.2127  memory: 11.81GiB(24.85%)  tps: 6,196  tflops: 47.19  mfu: 15.12%
[rank1]:2025-11-07 23:37:42,601 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.2127  memory: 14.64GiB(30.82%)  tps: 6,196  tflops: 47.19  mfu: 15.13%
[rank0]:2025-11-07 23:37:42,612 - INFO -  step: 50  loss: -4.0000  grad_norm:  9.2127  memory: 16.57GiB(34.88%)  tps: 6,197  tflops: 47.20  mfu: 15.13%
[rank3]:2025-11-07 23:37:42,610 - INFO -  step: 50  loss:  7.1174  grad_norm:  9.2127  memory: 26.98GiB(56.79%)  tps: 6,196  tflops: 47.19  mfu: 15.12%
[rank0]:2025-11-07 23:39:52,103 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:39:54,444 - INFO - Avg. fwd time: 11.4786 / Avg. bwd time: 45.4021 / Avg. batch time: 514.9028 (ms) / GPU bubble ratio: 11.62%
[rank0]:2025-11-07 23:39:54,565 - INFO - Avg. fwd time: 8.0148 / Avg. bwd time: 23.5588 / Avg. batch time: 622.9619 (ms) / GPU bubble ratio: 59.45%
[rank1]:2025-11-07 23:39:54,557 - INFO - Avg. fwd time: 9.2332 / Avg. bwd time: 24.1707 / Avg. batch time: 585.9127 (ms) / GPU bubble ratio: 54.39%
[rank2]:2025-11-07 23:39:54,517 - INFO - Avg. fwd time: 7.2113 / Avg. bwd time: 18.9366 / Avg. batch time: 546.6228 (ms) / GPU bubble ratio: 61.73%
[rank0]:2025-11-07 23:39:54,755 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.0338  memory: 16.57GiB(34.88%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank1]:2025-11-07 23:39:54,744 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.0338  memory: 14.64GiB(30.82%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank2]:2025-11-07 23:39:54,740 - INFO -  step: 100  loss: -4.0000  grad_norm:  8.0338  memory: 11.81GiB(24.85%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank3]:2025-11-07 23:39:54,754 - INFO -  step: 100  loss:  5.0390  grad_norm:  8.0338  memory: 26.98GiB(56.79%)  tps: 6,199  tflops: 47.22  mfu: 15.13%
[rank3]:2025-11-07 23:39:54,945 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2339_real_step100_rank3.svg
[rank3]:> Batch Time: 620.75 ms, GPU Bubble Ratio: 58.98%, 56.75%, 66.17%, 26.77%
[rank0]:2025-11-07 23:42:04,041 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:42:06,591 - INFO - Avg. fwd time: 11.4247 / Avg. bwd time: 45.4006 / Avg. batch time: 514.3959 (ms) / GPU bubble ratio: 11.62%
[rank1]:2025-11-07 23:42:06,648 - INFO - Avg. fwd time: 9.2396 / Avg. bwd time: 24.2022 / Avg. batch time: 585.3687 (ms) / GPU bubble ratio: 54.30%
[rank2]:2025-11-07 23:42:06,617 - INFO - Avg. fwd time: 7.2112 / Avg. bwd time: 18.9573 / Avg. batch time: 546.1998 (ms) / GPU bubble ratio: 61.67%
[rank3]:2025-11-07 23:42:06,712 - INFO -  step: 150  loss:  5.0488  grad_norm: 12.5292  memory: 26.98GiB(56.79%)  tps: 6,208  tflops: 47.28  mfu: 15.15%
[rank0]:2025-11-07 23:42:06,677 - INFO - Avg. fwd time: 8.0119 / Avg. bwd time: 23.5771 / Avg. batch time: 622.2675 (ms) / GPU bubble ratio: 59.39%
[rank0]:2025-11-07 23:42:06,713 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.5292  memory: 16.57GiB(34.88%)  tps: 6,208  tflops: 47.28  mfu: 15.15%
[rank1]:2025-11-07 23:42:06,702 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.5292  memory: 14.64GiB(30.82%)  tps: 6,208  tflops: 47.28  mfu: 15.15%
[rank2]:2025-11-07 23:42:06,699 - INFO -  step: 150  loss: -4.0000  grad_norm: 12.5292  memory: 11.81GiB(24.85%)  tps: 6,208  tflops: 47.28  mfu: 15.15%
[rank0]:2025-11-07 23:44:15,765 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:44:18,103 - INFO - Avg. fwd time: 11.4077 / Avg. bwd time: 45.3645 / Avg. batch time: 513.9480 (ms) / GPU bubble ratio: 11.63%
[rank1]:2025-11-07 23:44:18,214 - INFO - Avg. fwd time: 9.2394 / Avg. bwd time: 24.2100 / Avg. batch time: 584.7388 (ms) / GPU bubble ratio: 54.24%
[rank0]:2025-11-07 23:44:18,222 - INFO - Avg. fwd time: 8.0101 / Avg. bwd time: 23.5818 / Avg. batch time: 621.5745 (ms) / GPU bubble ratio: 59.34%
[rank2]:2025-11-07 23:44:18,175 - INFO - Avg. fwd time: 7.2087 / Avg. bwd time: 18.9640 / Avg. batch time: 545.6226 (ms) / GPU bubble ratio: 61.63%
[rank1]:2025-11-07 23:44:18,401 - INFO -  step: 200  loss: -4.0000  grad_norm:  7.6969  memory: 14.64GiB(30.82%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank0]:2025-11-07 23:44:18,413 - INFO -  step: 200  loss: -4.0000  grad_norm:  7.6969  memory: 16.57GiB(34.88%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank2]:2025-11-07 23:44:18,398 - INFO -  step: 200  loss: -4.0000  grad_norm:  7.6969  memory: 11.81GiB(24.85%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank3]:2025-11-07 23:44:18,411 - INFO -  step: 200  loss:  4.9077  grad_norm:  7.6969  memory: 26.98GiB(56.79%)  tps: 6,220  tflops: 47.38  mfu: 15.18%
[rank3]:2025-11-07 23:44:18,567 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2344_real_step200_rank3.svg
[rank3]:> Batch Time: 619.23 ms, GPU Bubble Ratio: 58.91%, 56.66%, 66.06%, 26.75%
[rank0]:2025-11-07 23:46:27,605 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:46:29,944 - INFO - Avg. fwd time: 11.3994 / Avg. bwd time: 45.3564 / Avg. batch time: 513.7992 (ms) / GPU bubble ratio: 11.63%
[rank1]:2025-11-07 23:46:30,056 - INFO - Avg. fwd time: 9.2412 / Avg. bwd time: 24.2195 / Avg. batch time: 584.5943 (ms) / GPU bubble ratio: 54.21%
[rank2]:2025-11-07 23:46:30,016 - INFO - Avg. fwd time: 7.2089 / Avg. bwd time: 18.9709 / Avg. batch time: 545.5118 (ms) / GPU bubble ratio: 61.61%
[rank0]:2025-11-07 23:46:30,063 - INFO - Avg. fwd time: 8.0083 / Avg. bwd time: 23.5890 / Avg. batch time: 621.3974 (ms) / GPU bubble ratio: 59.32%
[rank1]:2025-11-07 23:46:30,242 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.8298  memory: 14.64GiB(30.82%)  tps: 6,214  tflops: 47.32  mfu: 15.17%
[rank2]:2025-11-07 23:46:30,238 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.8298  memory: 11.81GiB(24.85%)  tps: 6,214  tflops: 47.32  mfu: 15.17%
[rank3]:2025-11-07 23:46:30,251 - INFO -  step: 250  loss:  4.4847  grad_norm:  2.8298  memory: 26.98GiB(56.79%)  tps: 6,214  tflops: 47.33  mfu: 15.17%
[rank0]:2025-11-07 23:46:30,253 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.8298  memory: 16.57GiB(34.88%)  tps: 6,214  tflops: 47.32  mfu: 15.17%
[rank0]:2025-11-07 23:48:39,586 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:48:42,161 - INFO - Avg. fwd time: 11.3929 / Avg. bwd time: 45.3552 / Avg. batch time: 513.7226 (ms) / GPU bubble ratio: 11.63%
[rank2]:2025-11-07 23:48:42,188 - INFO - Avg. fwd time: 7.2119 / Avg. bwd time: 18.9795 / Avg. batch time: 545.3679 (ms) / GPU bubble ratio: 61.58%
[rank1]:2025-11-07 23:48:42,219 - INFO - Avg. fwd time: 9.2465 / Avg. bwd time: 24.2299 / Avg. batch time: 584.4260 (ms) / GPU bubble ratio: 54.18%
[rank3]:2025-11-07 23:48:42,289 - INFO -  step: 300  loss:  5.0069  grad_norm:  2.1734  memory: 26.98GiB(56.79%)  tps: 6,204  tflops: 47.25  mfu: 15.15%
[rank0]:2025-11-07 23:48:42,252 - INFO - Avg. fwd time: 8.0105 / Avg. bwd time: 23.5977 / Avg. batch time: 621.2174 (ms) / GPU bubble ratio: 59.30%
[rank0]:2025-11-07 23:48:42,289 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.1734  memory: 16.57GiB(34.88%)  tps: 6,204  tflops: 47.25  mfu: 15.15%
[rank2]:2025-11-07 23:48:42,274 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.1734  memory: 11.81GiB(24.85%)  tps: 6,204  tflops: 47.25  mfu: 15.15%
[rank1]:2025-11-07 23:48:42,278 - INFO -  step: 300  loss: -4.0000  grad_norm:  2.1734  memory: 14.64GiB(30.82%)  tps: 6,204  tflops: 47.25  mfu: 15.15%
[rank3]:2025-11-07 23:48:42,444 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2348_real_step300_rank3.svg
[rank3]:> Batch Time: 621.78 ms, GPU Bubble Ratio: 59.00%, 56.69%, 66.11%, 26.93%
[rank0]:2025-11-07 23:50:52,223 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:50:54,582 - INFO - Avg. fwd time: 11.3957 / Avg. bwd time: 45.3807 / Avg. batch time: 513.9473 (ms) / GPU bubble ratio: 11.62%
[rank2]:2025-11-07 23:50:54,654 - INFO - Avg. fwd time: 7.2137 / Avg. bwd time: 18.9857 / Avg. batch time: 545.6450 (ms) / GPU bubble ratio: 61.59%
[rank1]:2025-11-07 23:50:54,693 - INFO - Avg. fwd time: 9.2488 / Avg. bwd time: 24.2372 / Avg. batch time: 584.6890 (ms) / GPU bubble ratio: 54.18%
[rank0]:2025-11-07 23:50:54,702 - INFO - Avg. fwd time: 8.0119 / Avg. bwd time: 23.6036 / Avg. batch time: 621.4697 (ms) / GPU bubble ratio: 59.30%
[rank2]:2025-11-07 23:50:54,875 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.3779  memory: 11.81GiB(24.85%)  tps: 6,178  tflops: 47.05  mfu: 15.08%
[rank3]:2025-11-07 23:50:54,887 - INFO -  step: 350  loss:  4.8720  grad_norm:  1.3779  memory: 26.98GiB(56.79%)  tps: 6,178  tflops: 47.05  mfu: 15.08%
[rank1]:2025-11-07 23:50:54,879 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.3779  memory: 14.64GiB(30.82%)  tps: 6,178  tflops: 47.05  mfu: 15.08%
[rank0]:2025-11-07 23:50:54,890 - INFO -  step: 350  loss: -4.0000  grad_norm:  1.3779  memory: 16.57GiB(34.88%)  tps: 6,178  tflops: 47.05  mfu: 15.08%

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 07. (ê¸ˆ) 23:52:56 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_slimalpaca_1600.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=slimorca --training.steps=1000 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-07 23:53:02,897 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank0]:2025-11-07 23:53:03,108 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-07 23:53:03,111 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:53:03,115 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-07 23:53:03,116 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-07 23:53:03,153 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-07 23:53:03,150 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-07 23:53:03,162 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-07 23:53:03,506 - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:2025-11-07 23:53:03,515 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-07 23:53:03,517 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-07 23:53:03,504 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-07 23:53:03,506 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-07 23:53:03,498 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-07 23:53:03,501 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-07 23:53:06,579 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-07 23:53:06,726 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-07 23:53:06,766 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:53:06,767 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-07 23:53:06,793 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-07 23:53:06,793 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 23:53:07,007 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-07 23:53:07,007 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-07 23:53:07,007 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-07 23:53:07,101 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-07 23:53:07,137 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:53:07,165 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-07 23:53:07,165 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-07 23:53:07,070 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-07 23:53:07,107 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:53:07,134 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-07 23:53:07,134 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-07 23:53:07,351 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-07 23:53:07,351 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-07 23:53:07,352 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-07 23:53:07,389 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-07 23:53:07,389 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-07 23:53:07,390 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run y8lv4hig
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/20251107-2353/wandb/run-20251107_235308-y8lv4hig
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/y8lv4hig
[rank3]:2025-11-07 23:53:09,459 - INFO - WandB logging enabled
[rank3]:2025-11-07 23:53:09,460 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-07 23:53:09,500 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:53:09,529 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-07 23:53:09,530 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-07 23:53:09,770 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank0]:2025-11-07 23:53:09,770 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-07 23:53:09,770 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-07 23:53:09,753 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-07 23:53:09,754 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-07 23:53:09,755 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-07 23:53:09,770 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-07 23:53:09,770 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-07 23:53:09,770 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 1000 (warmup 100)
[rank0]:2025-11-07 23:53:09,771 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank0]:2025-11-07 23:53:12,582 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-07 23:53:12,582 - INFO - Finished loading the checkpoint in 2.81 seconds.
[rank0]:2025-11-07 23:53:12,582 - INFO - Training starts at step 1
[rank1]:2025-11-07 23:53:15,853 - INFO -  step:  1  loss: -4.0000  grad_norm: 38.9559  memory: 12.38GiB(26.05%)  tps: 1,880  tflops: 14.32  mfu: 4.59%
[rank1]:2025-11-07 23:53:15,854 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-07 23:53:15,848 - INFO -  step:  1  loss: -4.0000  grad_norm: 38.9559  memory:  9.99GiB(21.03%)  tps: 1,875  tflops: 14.28  mfu: 4.58%
[rank2]:2025-11-07 23:53:15,848 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-07 23:53:15,859 - INFO -  step:  1  loss:  5.3713  grad_norm: 38.9559  memory: 24.19GiB(50.91%)  tps: 2,578  tflops: 19.63  mfu: 6.29%
[rank3]:2025-11-07 23:53:15,860 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:53:15,894 - INFO -  step:  1  loss: -4.0000  grad_norm: 38.9559  memory: 12.80GiB(26.95%)  tps: 1,795  tflops: 13.67  mfu: 4.38%
[rank0]:2025-11-07 23:53:15,895 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-07 23:55:24,431 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:55:26,821 - INFO - Avg. fwd time: 11.8825 / Avg. bwd time: 45.8340 / Avg. batch time: 521.7689 (ms) / GPU bubble ratio: 11.51%
[rank2]:2025-11-07 23:55:26,899 - INFO - Avg. fwd time: 7.2688 / Avg. bwd time: 18.9278 / Avg. batch time: 554.0452 (ms) / GPU bubble ratio: 62.17%
[rank1]:2025-11-07 23:55:26,939 - INFO - Avg. fwd time: 9.2637 / Avg. bwd time: 24.0435 / Avg. batch time: 593.6883 (ms) / GPU bubble ratio: 55.12%
[rank0]:2025-11-07 23:55:26,946 - INFO - Avg. fwd time: 8.0305 / Avg. bwd time: 23.5249 / Avg. batch time: 631.1108 (ms) / GPU bubble ratio: 60.00%
[rank2]:2025-11-07 23:55:27,124 - INFO -  step: 50  loss: -4.0000  grad_norm:  7.9503  memory: 11.81GiB(24.85%)  tps: 6,116  tflops: 46.58  mfu: 14.93%
[rank1]:2025-11-07 23:55:27,127 - INFO -  step: 50  loss: -4.0000  grad_norm:  7.9503  memory: 14.64GiB(30.82%)  tps: 6,116  tflops: 46.58  mfu: 14.93%
[rank3]:2025-11-07 23:55:27,136 - INFO -  step: 50  loss:  3.3489  grad_norm:  7.9503  memory: 26.98GiB(56.79%)  tps: 6,116  tflops: 46.58  mfu: 14.93%
[rank0]:2025-11-07 23:55:27,138 - INFO -  step: 50  loss: -4.0000  grad_norm:  7.9503  memory: 16.57GiB(34.88%)  tps: 6,117  tflops: 46.59  mfu: 14.93%
[rank0]:2025-11-07 23:57:39,436 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:57:41,879 - INFO - Avg. fwd time: 11.9313 / Avg. bwd time: 46.2169 / Avg. batch time: 525.1192 (ms) / GPU bubble ratio: 11.41%
[rank2]:2025-11-07 23:57:41,957 - INFO - Avg. fwd time: 7.2687 / Avg. bwd time: 18.9817 / Avg. batch time: 557.0286 (ms) / GPU bubble ratio: 62.30%
[rank1]:2025-11-07 23:57:41,997 - INFO - Avg. fwd time: 9.2715 / Avg. bwd time: 24.1198 / Avg. batch time: 596.3106 (ms) / GPU bubble ratio: 55.20%
[rank0]:2025-11-07 23:57:42,004 - INFO - Avg. fwd time: 8.0351 / Avg. bwd time: 23.5646 / Avg. batch time: 633.2851 (ms) / GPU bubble ratio: 60.08%
[rank3]:2025-11-07 23:57:42,197 - INFO -  step: 100  loss:  1.2855  grad_norm:  0.6911  memory: 26.98GiB(56.79%)  tps: 6,066  tflops: 46.20  mfu: 14.81%
[rank2]:2025-11-07 23:57:42,183 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.6911  memory: 11.81GiB(24.85%)  tps: 6,066  tflops: 46.20  mfu: 14.81%
[rank1]:2025-11-07 23:57:42,186 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.6911  memory: 14.64GiB(30.82%)  tps: 6,066  tflops: 46.20  mfu: 14.81%
[rank0]:2025-11-07 23:57:42,198 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.6911  memory: 16.57GiB(34.88%)  tps: 6,065  tflops: 46.20  mfu: 14.81%
[rank3]:2025-11-07 23:57:42,381 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251107_2357_real_step100_rank3.svg
[rank3]:> Batch Time: 635.22 ms, GPU Bubble Ratio: 59.82%, 57.71%, 66.75%, 26.03%
[rank0]:2025-11-07 23:59:54,685 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-07 23:59:57,316 - INFO - Avg. fwd time: 11.9536 / Avg. bwd time: 46.3724 / Avg. batch time: 526.5031 (ms) / GPU bubble ratio: 11.38%
[rank0]:2025-11-07 23:59:57,402 - INFO - Avg. fwd time: 8.0414 / Avg. bwd time: 23.5820 / Avg. batch time: 634.5757 (ms) / GPU bubble ratio: 60.13%
[rank1]:2025-11-07 23:59:57,373 - INFO - Avg. fwd time: 9.2766 / Avg. bwd time: 24.1606 / Avg. batch time: 597.7258 (ms) / GPU bubble ratio: 55.25%
[rank2]:2025-11-07 23:59:57,342 - INFO - Avg. fwd time: 7.2708 / Avg. bwd time: 19.0059 / Avg. batch time: 558.5365 (ms) / GPU bubble ratio: 62.36%
[rank2]:2025-11-07 23:59:57,424 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.7034  memory: 11.81GiB(24.85%)  tps: 6,057  tflops: 46.13  mfu: 14.79%
[rank3]:2025-11-07 23:59:57,438 - INFO -  step: 150  loss:  1.3680  grad_norm:  0.7034  memory: 26.98GiB(56.79%)  tps: 6,058  tflops: 46.14  mfu: 14.79%
[rank0]:2025-11-07 23:59:57,438 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.7034  memory: 16.57GiB(34.88%)  tps: 6,057  tflops: 46.13  mfu: 14.79%
[rank1]:2025-11-07 23:59:57,427 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.7034  memory: 14.64GiB(30.82%)  tps: 6,057  tflops: 46.13  mfu: 14.79%
[rank0]:2025-11-08 00:02:09,650 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:02:12,036 - INFO - Avg. fwd time: 11.9569 / Avg. bwd time: 46.4166 / Avg. batch time: 526.8609 (ms) / GPU bubble ratio: 11.36%
[rank2]:2025-11-08 00:02:12,115 - INFO - Avg. fwd time: 7.2707 / Avg. bwd time: 19.0157 / Avg. batch time: 558.7739 (ms) / GPU bubble ratio: 62.37%
[rank1]:2025-11-08 00:02:12,154 - INFO - Avg. fwd time: 9.2774 / Avg. bwd time: 24.1783 / Avg. batch time: 597.9235 (ms) / GPU bubble ratio: 55.24%
[rank0]:2025-11-08 00:02:12,161 - INFO - Avg. fwd time: 8.0502 / Avg. bwd time: 23.5874 / Avg. batch time: 634.7216 (ms) / GPU bubble ratio: 60.12%
[rank2]:2025-11-08 00:02:12,338 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.5236  memory: 11.81GiB(24.85%)  tps: 6,072  tflops: 46.25  mfu: 14.82%
[rank1]:2025-11-08 00:02:12,342 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.5236  memory: 14.64GiB(30.82%)  tps: 6,072  tflops: 46.25  mfu: 14.82%
[rank3]:2025-11-08 00:02:12,351 - INFO -  step: 200  loss:  1.1327  grad_norm:  0.5236  memory: 26.98GiB(56.79%)  tps: 6,072  tflops: 46.25  mfu: 14.82%
[rank0]:2025-11-08 00:02:12,353 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.5236  memory: 16.57GiB(34.88%)  tps: 6,072  tflops: 46.25  mfu: 14.82%
[rank3]:2025-11-08 00:02:12,503 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0002_real_step200_rank3.svg
[rank3]:> Batch Time: 634.69 ms, GPU Bubble Ratio: 59.85%, 57.66%, 66.73%, 26.05%
[rank0]:2025-11-08 00:04:24,664 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:04:27,056 - INFO - Avg. fwd time: 11.9510 / Avg. bwd time: 46.4104 / Avg. batch time: 526.7451 (ms) / GPU bubble ratio: 11.36%
[rank0]:2025-11-08 00:04:27,180 - INFO - Avg. fwd time: 8.0468 / Avg. bwd time: 23.5880 / Avg. batch time: 634.5733 (ms) / GPU bubble ratio: 60.12%
[rank2]:2025-11-08 00:04:27,133 - INFO - Avg. fwd time: 7.2691 / Avg. bwd time: 19.0187 / Avg. batch time: 558.6896 (ms) / GPU bubble ratio: 62.36%
[rank1]:2025-11-08 00:04:27,173 - INFO - Avg. fwd time: 9.2751 / Avg. bwd time: 24.1876 / Avg. batch time: 597.8140 (ms) / GPU bubble ratio: 55.22%
[rank0]:2025-11-08 00:04:27,371 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4688  memory: 16.57GiB(34.88%)  tps: 6,067  tflops: 46.21  mfu: 14.81%
[rank3]:2025-11-08 00:04:27,369 - INFO -  step: 250  loss:  1.0893  grad_norm:  0.4688  memory: 26.98GiB(56.79%)  tps: 6,067  tflops: 46.21  mfu: 14.81%
[rank2]:2025-11-08 00:04:27,356 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4688  memory: 11.81GiB(24.85%)  tps: 6,067  tflops: 46.21  mfu: 14.81%
[rank1]:2025-11-08 00:04:27,360 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4688  memory: 14.64GiB(30.82%)  tps: 6,067  tflops: 46.21  mfu: 14.81%
[rank0]:2025-11-08 00:06:40,112 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:06:42,764 - INFO - Avg. fwd time: 11.9536 / Avg. bwd time: 46.4451 / Avg. batch time: 527.0346 (ms) / GPU bubble ratio: 11.36%
[rank2]:2025-11-08 00:06:42,792 - INFO - Avg. fwd time: 7.2694 / Avg. bwd time: 19.0233 / Avg. batch time: 558.9164 (ms) / GPU bubble ratio: 62.37%
[rank0]:2025-11-08 00:06:42,857 - INFO - Avg. fwd time: 8.0457 / Avg. bwd time: 23.5917 / Avg. batch time: 634.7709 (ms) / GPU bubble ratio: 60.13%
[rank1]:2025-11-08 00:06:42,826 - INFO - Avg. fwd time: 9.2747 / Avg. bwd time: 24.1993 / Avg. batch time: 598.0235 (ms) / GPU bubble ratio: 55.22%
[rank2]:2025-11-08 00:06:42,878 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4884  memory: 11.81GiB(24.85%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:06:42,893 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4884  memory: 16.57GiB(34.88%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank3]:2025-11-08 00:06:42,891 - INFO -  step: 300  loss:  1.1584  grad_norm:  0.4884  memory: 26.98GiB(56.79%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank1]:2025-11-08 00:06:42,882 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4884  memory: 14.64GiB(30.82%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank3]:2025-11-08 00:06:43,045 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0006_real_step300_rank3.svg
[rank3]:> Batch Time: 636.19 ms, GPU Bubble Ratio: 59.93%, 57.71%, 66.79%, 25.96%
[rank0]:2025-11-08 00:08:55,503 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:08:57,912 - INFO - Avg. fwd time: 11.9579 / Avg. bwd time: 46.4699 / Avg. batch time: 527.2635 (ms) / GPU bubble ratio: 11.35%
[rank2]:2025-11-08 00:08:57,987 - INFO - Avg. fwd time: 7.2700 / Avg. bwd time: 19.0260 / Avg. batch time: 559.1926 (ms) / GPU bubble ratio: 62.38%
[rank0]:2025-11-08 00:08:58,034 - INFO - Avg. fwd time: 8.0444 / Avg. bwd time: 23.5931 / Avg. batch time: 635.0186 (ms) / GPU bubble ratio: 60.14%
[rank1]:2025-11-08 00:08:58,027 - INFO - Avg. fwd time: 9.2761 / Avg. bwd time: 24.2092 / Avg. batch time: 598.2911 (ms) / GPU bubble ratio: 55.23%
[rank3]:2025-11-08 00:08:58,223 - INFO -  step: 350  loss:  1.2319  grad_norm:  0.6396  memory: 26.98GiB(56.79%)  tps: 6,053  tflops: 46.10  mfu: 14.78%
[rank2]:2025-11-08 00:08:58,208 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6396  memory: 11.81GiB(24.85%)  tps: 6,053  tflops: 46.10  mfu: 14.78%
[rank0]:2025-11-08 00:08:58,223 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6396  memory: 16.57GiB(34.88%)  tps: 6,053  tflops: 46.10  mfu: 14.78%
[rank1]:2025-11-08 00:08:58,212 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.6396  memory: 14.64GiB(30.82%)  tps: 6,053  tflops: 46.10  mfu: 14.78%
[rank0]:2025-11-08 00:11:10,266 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:11:12,673 - INFO - Avg. fwd time: 11.9546 / Avg. bwd time: 46.4588 / Avg. batch time: 527.1441 (ms) / GPU bubble ratio: 11.35%
[rank0]:2025-11-08 00:11:12,797 - INFO - Avg. fwd time: 8.0407 / Avg. bwd time: 23.5901 / Avg. batch time: 634.8061 (ms) / GPU bubble ratio: 60.14%
[rank2]:2025-11-08 00:11:12,749 - INFO - Avg. fwd time: 7.2694 / Avg. bwd time: 19.0255 / Avg. batch time: 559.0129 (ms) / GPU bubble ratio: 62.37%
[rank1]:2025-11-08 00:11:12,789 - INFO - Avg. fwd time: 9.2737 / Avg. bwd time: 24.2145 / Avg. batch time: 598.1015 (ms) / GPU bubble ratio: 55.21%
[rank0]:2025-11-08 00:11:12,986 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4579  memory: 16.57GiB(34.88%)  tps: 6,079  tflops: 46.30  mfu: 14.84%
[rank2]:2025-11-08 00:11:12,971 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4579  memory: 11.81GiB(24.85%)  tps: 6,079  tflops: 46.30  mfu: 14.84%
[rank3]:2025-11-08 00:11:12,986 - INFO -  step: 400  loss:  1.0602  grad_norm:  0.4579  memory: 26.98GiB(56.79%)  tps: 6,079  tflops: 46.30  mfu: 14.84%
[rank1]:2025-11-08 00:11:12,975 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4579  memory: 14.64GiB(30.82%)  tps: 6,079  tflops: 46.30  mfu: 14.84%
[rank3]:2025-11-08 00:11:13,172 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0011_real_step400_rank3.svg
[rank3]:> Batch Time: 633.16 ms, GPU Bubble Ratio: 59.81%, 57.55%, 66.67%, 26.05%
[rank0]:2025-11-08 00:13:25,819 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 00:13:28,451 - INFO - Avg. fwd time: 7.2687 / Avg. bwd time: 19.0257 / Avg. batch time: 559.0378 (ms) / GPU bubble ratio: 62.37%
[rank3]:2025-11-08 00:13:28,422 - INFO - Avg. fwd time: 11.9524 / Avg. bwd time: 46.4593 / Avg. batch time: 527.1277 (ms) / GPU bubble ratio: 11.35%
[rank1]:2025-11-08 00:13:28,484 - INFO - Avg. fwd time: 9.2726 / Avg. bwd time: 24.2194 / Avg. batch time: 598.1150 (ms) / GPU bubble ratio: 55.20%
[rank0]:2025-11-08 00:13:28,516 - INFO - Avg. fwd time: 8.0396 / Avg. bwd time: 23.5894 / Avg. batch time: 634.8180 (ms) / GPU bubble ratio: 60.14%
[rank0]:2025-11-08 00:13:28,553 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4998  memory: 16.57GiB(34.88%)  tps: 6,043  tflops: 46.02  mfu: 14.75%
[rank2]:2025-11-08 00:13:28,538 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4998  memory: 11.81GiB(24.85%)  tps: 6,043  tflops: 46.02  mfu: 14.75%
[rank3]:2025-11-08 00:13:28,552 - INFO -  step: 450  loss:  1.1144  grad_norm:  0.4998  memory: 26.98GiB(56.79%)  tps: 6,043  tflops: 46.02  mfu: 14.75%
[rank1]:2025-11-08 00:13:28,542 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.4998  memory: 14.64GiB(30.82%)  tps: 6,043  tflops: 46.02  mfu: 14.75%
[rank0]:2025-11-08 00:15:41,349 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:15:43,753 - INFO - Avg. fwd time: 11.9556 / Avg. bwd time: 46.4760 / Avg. batch time: 527.2860 (ms) / GPU bubble ratio: 11.35%
[rank2]:2025-11-08 00:15:43,830 - INFO - Avg. fwd time: 7.2689 / Avg. bwd time: 19.0273 / Avg. batch time: 559.1627 (ms) / GPU bubble ratio: 62.38%
[rank1]:2025-11-08 00:15:43,870 - INFO - Avg. fwd time: 9.2727 / Avg. bwd time: 24.2259 / Avg. batch time: 598.2360 (ms) / GPU bubble ratio: 55.20%
[rank0]:2025-11-08 00:15:43,876 - INFO - Avg. fwd time: 8.0389 / Avg. bwd time: 23.5896 / Avg. batch time: 634.9328 (ms) / GPU bubble ratio: 60.15%
[rank2]:2025-11-08 00:15:44,052 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4813  memory: 11.81GiB(24.85%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank3]:2025-11-08 00:15:44,065 - INFO -  step: 500  loss:  1.0053  grad_norm:  0.4813  memory: 26.98GiB(56.79%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank1]:2025-11-08 00:15:44,055 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4813  memory: 14.64GiB(30.82%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:15:44,066 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.4813  memory: 16.57GiB(34.88%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank3]:2025-11-08 00:15:44,251 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0015_real_step500_rank3.svg
[rank3]:> Batch Time: 635.19 ms, GPU Bubble Ratio: 59.90%, 57.58%, 66.74%, 25.94%
[rank0]:2025-11-08 00:17:56,836 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:17:59,283 - INFO - Avg. fwd time: 11.9548 / Avg. bwd time: 46.4790 / Avg. batch time: 527.3017 (ms) / GPU bubble ratio: 11.35%
[rank2]:2025-11-08 00:17:59,355 - INFO - Avg. fwd time: 7.2683 / Avg. bwd time: 19.0271 / Avg. batch time: 559.2146 (ms) / GPU bubble ratio: 62.38%
[rank0]:2025-11-08 00:17:59,403 - INFO - Avg. fwd time: 8.0364 / Avg. bwd time: 23.5882 / Avg. batch time: 634.9664 (ms) / GPU bubble ratio: 60.16%
[rank1]:2025-11-08 00:17:59,395 - INFO - Avg. fwd time: 9.2711 / Avg. bwd time: 24.2295 / Avg. batch time: 598.2749 (ms) / GPU bubble ratio: 55.20%
[rank2]:2025-11-08 00:17:59,574 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.5040  memory: 11.81GiB(24.85%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:17:59,588 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.5040  memory: 16.57GiB(34.88%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank3]:2025-11-08 00:17:59,590 - INFO -  step: 550  loss:  1.1564  grad_norm:  0.5040  memory: 26.98GiB(56.79%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank1]:2025-11-08 00:17:59,578 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.5040  memory: 14.64GiB(30.82%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:20:12,128 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 00:20:14,785 - INFO - Avg. fwd time: 7.2676 / Avg. bwd time: 19.0267 / Avg. batch time: 559.1541 (ms) / GPU bubble ratio: 62.38%
[rank3]:2025-11-08 00:20:14,757 - INFO - Avg. fwd time: 11.9525 / Avg. bwd time: 46.4787 / Avg. batch time: 527.2774 (ms) / GPU bubble ratio: 11.35%
[rank2]:2025-11-08 00:20:14,871 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4491  memory: 11.81GiB(24.85%)  tps: 6,055  tflops: 46.12  mfu: 14.78%
[rank3]:2025-11-08 00:20:14,884 - INFO -  step: 600  loss:  1.0341  grad_norm:  0.4491  memory: 26.98GiB(56.79%)  tps: 6,055  tflops: 46.12  mfu: 14.78%
[rank0]:2025-11-08 00:20:14,849 - INFO - Avg. fwd time: 8.0343 / Avg. bwd time: 23.5870 / Avg. batch time: 634.8977 (ms) / GPU bubble ratio: 60.16%
[rank0]:2025-11-08 00:20:14,885 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4491  memory: 16.57GiB(34.88%)  tps: 6,055  tflops: 46.11  mfu: 14.78%
[rank1]:2025-11-08 00:20:14,818 - INFO - Avg. fwd time: 9.2696 / Avg. bwd time: 24.2322 / Avg. batch time: 598.2071 (ms) / GPU bubble ratio: 55.20%
[rank1]:2025-11-08 00:20:14,874 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4491  memory: 14.64GiB(30.82%)  tps: 6,055  tflops: 46.12  mfu: 14.78%
[rank3]:2025-11-08 00:20:15,050 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0020_real_step600_rank3.svg
[rank3]:> Batch Time: 634.69 ms, GPU Bubble Ratio: 59.90%, 57.65%, 66.76%, 26.05%
[rank0]:2025-11-08 00:22:28,014 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:22:30,421 - INFO - Avg. fwd time: 11.9532 / Avg. bwd time: 46.4882 / Avg. batch time: 527.3570 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:22:30,497 - INFO - Avg. fwd time: 7.2674 / Avg. bwd time: 19.0271 / Avg. batch time: 559.2630 (ms) / GPU bubble ratio: 62.39%
[rank0]:2025-11-08 00:22:30,545 - INFO - Avg. fwd time: 8.0333 / Avg. bwd time: 23.5865 / Avg. batch time: 634.9982 (ms) / GPU bubble ratio: 60.16%
[rank1]:2025-11-08 00:22:30,537 - INFO - Avg. fwd time: 9.2697 / Avg. bwd time: 24.2344 / Avg. batch time: 598.3075 (ms) / GPU bubble ratio: 55.20%
[rank2]:2025-11-08 00:22:30,715 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4567  memory: 11.81GiB(24.85%)  tps: 6,030  tflops: 45.93  mfu: 14.72%
[rank0]:2025-11-08 00:22:30,729 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4567  memory: 16.57GiB(34.88%)  tps: 6,030  tflops: 45.93  mfu: 14.72%
[rank3]:2025-11-08 00:22:30,727 - INFO -  step: 650  loss:  1.0859  grad_norm:  0.4567  memory: 26.98GiB(56.79%)  tps: 6,031  tflops: 45.93  mfu: 14.72%
[rank1]:2025-11-08 00:22:30,718 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.4567  memory: 14.64GiB(30.82%)  tps: 6,030  tflops: 45.93  mfu: 14.72%
[rank0]:2025-11-08 00:24:43,623 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:24:46,050 - INFO - Avg. fwd time: 11.9521 / Avg. bwd time: 46.4902 / Avg. batch time: 527.3614 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:24:46,123 - INFO - Avg. fwd time: 7.2669 / Avg. bwd time: 19.0266 / Avg. batch time: 559.2379 (ms) / GPU bubble ratio: 62.39%
[rank0]:2025-11-08 00:24:46,172 - INFO - Avg. fwd time: 8.0333 / Avg. bwd time: 23.5854 / Avg. batch time: 634.9676 (ms) / GPU bubble ratio: 60.16%
[rank1]:2025-11-08 00:24:46,164 - INFO - Avg. fwd time: 9.2695 / Avg. bwd time: 24.2358 / Avg. batch time: 598.2762 (ms) / GPU bubble ratio: 55.20%
[rank2]:2025-11-08 00:24:46,343 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4229  memory: 11.81GiB(24.85%)  tps: 6,040  tflops: 46.00  mfu: 14.74%
[rank1]:2025-11-08 00:24:46,347 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4229  memory: 14.64GiB(30.82%)  tps: 6,040  tflops: 46.00  mfu: 14.74%
[rank3]:2025-11-08 00:24:46,356 - INFO -  step: 700  loss:  1.0615  grad_norm:  0.4229  memory: 26.98GiB(56.79%)  tps: 6,040  tflops: 46.00  mfu: 14.74%
[rank0]:2025-11-08 00:24:46,358 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4229  memory: 16.57GiB(34.88%)  tps: 6,040  tflops: 46.00  mfu: 14.74%
[rank3]:2025-11-08 00:24:46,546 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0024_real_step700_rank3.svg
[rank3]:> Batch Time: 633.16 ms, GPU Bubble Ratio: 59.80%, 57.58%, 66.70%, 26.04%
[rank0]:2025-11-08 00:26:59,101 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:27:01,749 - INFO - Avg. fwd time: 11.9504 / Avg. bwd time: 46.4876 / Avg. batch time: 527.3249 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:27:01,779 - INFO - Avg. fwd time: 7.2665 / Avg. bwd time: 19.0263 / Avg. batch time: 559.2302 (ms) / GPU bubble ratio: 62.39%
[rank1]:2025-11-08 00:27:01,812 - INFO - Avg. fwd time: 9.2688 / Avg. bwd time: 24.2347 / Avg. batch time: 598.2584 (ms) / GPU bubble ratio: 55.20%
[rank3]:2025-11-08 00:27:01,881 - INFO -  step: 750  loss:  1.1530  grad_norm:  0.4975  memory: 26.98GiB(56.79%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank2]:2025-11-08 00:27:01,868 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4975  memory: 11.81GiB(24.85%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:27:01,846 - INFO - Avg. fwd time: 8.0319 / Avg. bwd time: 23.5839 / Avg. batch time: 634.9513 (ms) / GPU bubble ratio: 60.17%
[rank1]:2025-11-08 00:27:01,872 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4975  memory: 14.64GiB(30.82%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:27:01,882 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4975  memory: 16.57GiB(34.88%)  tps: 6,045  tflops: 46.04  mfu: 14.76%
[rank0]:2025-11-08 00:29:15,065 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:29:17,504 - INFO - Avg. fwd time: 11.9513 / Avg. bwd time: 46.4924 / Avg. batch time: 527.3693 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:29:17,583 - INFO - Avg. fwd time: 7.2663 / Avg. bwd time: 19.0267 / Avg. batch time: 559.2560 (ms) / GPU bubble ratio: 62.39%
[rank0]:2025-11-08 00:29:17,628 - INFO - Avg. fwd time: 8.0309 / Avg. bwd time: 23.5835 / Avg. batch time: 634.9735 (ms) / GPU bubble ratio: 60.17%
[rank1]:2025-11-08 00:29:17,622 - INFO - Avg. fwd time: 9.2691 / Avg. bwd time: 24.2341 / Avg. batch time: 598.2789 (ms) / GPU bubble ratio: 55.20%
[rank3]:2025-11-08 00:29:17,813 - INFO -  step: 800  loss:  0.9872  grad_norm:  0.4783  memory: 26.98GiB(56.79%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank2]:2025-11-08 00:29:17,800 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4783  memory: 11.81GiB(24.85%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank0]:2025-11-08 00:29:17,814 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4783  memory: 16.57GiB(34.88%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank1]:2025-11-08 00:29:17,804 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4783  memory: 14.64GiB(30.82%)  tps: 6,027  tflops: 45.90  mfu: 14.71%
[rank3]:2025-11-08 00:29:17,973 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0029_real_step800_rank3.svg
[rank3]:> Batch Time: 636.20 ms, GPU Bubble Ratio: 59.94%, 57.74%, 66.82%, 26.00%
[rank0]:2025-11-08 00:31:31,084 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:31:33,539 - INFO - Avg. fwd time: 11.9533 / Avg. bwd time: 46.5044 / Avg. batch time: 527.4822 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:31:33,614 - INFO - Avg. fwd time: 7.2664 / Avg. bwd time: 19.0274 / Avg. batch time: 559.3877 (ms) / GPU bubble ratio: 62.40%
[rank1]:2025-11-08 00:31:33,654 - INFO - Avg. fwd time: 9.2698 / Avg. bwd time: 24.2333 / Avg. batch time: 598.4065 (ms) / GPU bubble ratio: 55.21%
[rank0]:2025-11-08 00:31:33,662 - INFO - Avg. fwd time: 8.0307 / Avg. bwd time: 23.5832 / Avg. batch time: 635.1011 (ms) / GPU bubble ratio: 60.18%
[rank2]:2025-11-08 00:31:33,830 - INFO -  step: 850  loss: -4.0000  grad_norm:  0.4645  memory: 11.81GiB(24.85%)  tps: 6,022  tflops: 45.87  mfu: 14.70%
[rank1]:2025-11-08 00:31:33,833 - INFO -  step: 850  loss: -4.0000  grad_norm:  0.4645  memory: 14.64GiB(30.82%)  tps: 6,022  tflops: 45.87  mfu: 14.70%
[rank3]:2025-11-08 00:31:33,842 - INFO -  step: 850  loss:  1.1240  grad_norm:  0.4645  memory: 26.98GiB(56.79%)  tps: 6,022  tflops: 45.87  mfu: 14.70%
[rank0]:2025-11-08 00:31:33,845 - INFO -  step: 850  loss: -4.0000  grad_norm:  0.4645  memory: 16.57GiB(34.88%)  tps: 6,022  tflops: 45.87  mfu: 14.70%
[rank0]:2025-11-08 00:33:46,779 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:33:49,445 - INFO - Avg. fwd time: 11.9534 / Avg. bwd time: 46.5027 / Avg. batch time: 527.4682 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:33:49,476 - INFO - Avg. fwd time: 7.2662 / Avg. bwd time: 19.0273 / Avg. batch time: 559.3562 (ms) / GPU bubble ratio: 62.39%
[rank1]:2025-11-08 00:33:49,512 - INFO - Avg. fwd time: 9.2694 / Avg. bwd time: 24.2306 / Avg. batch time: 598.3708 (ms) / GPU bubble ratio: 55.21%
[rank3]:2025-11-08 00:33:49,582 - INFO -  step: 900  loss:  1.1226  grad_norm:  0.4752  memory: 26.98GiB(56.79%)  tps: 6,035  tflops: 45.97  mfu: 14.73%
[rank0]:2025-11-08 00:33:49,547 - INFO - Avg. fwd time: 8.0289 / Avg. bwd time: 23.5821 / Avg. batch time: 635.0656 (ms) / GPU bubble ratio: 60.18%
[rank0]:2025-11-08 00:33:49,582 - INFO -  step: 900  loss: -4.0000  grad_norm:  0.4752  memory: 16.57GiB(34.88%)  tps: 6,035  tflops: 45.97  mfu: 14.73%
[rank2]:2025-11-08 00:33:49,568 - INFO -  step: 900  loss: -4.0000  grad_norm:  0.4752  memory: 11.81GiB(24.85%)  tps: 6,035  tflops: 45.96  mfu: 14.73%
[rank1]:2025-11-08 00:33:49,572 - INFO -  step: 900  loss: -4.0000  grad_norm:  0.4752  memory: 14.64GiB(30.82%)  tps: 6,035  tflops: 45.97  mfu: 14.73%
[rank3]:2025-11-08 00:33:49,745 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0033_real_step900_rank3.svg
[rank3]:> Batch Time: 632.67 ms, GPU Bubble Ratio: 59.80%, 57.61%, 66.65%, 25.92%
[rank0]:2025-11-08 00:36:02,301 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:36:04,712 - INFO - Avg. fwd time: 11.9483 / Avg. bwd time: 46.5028 / Avg. batch time: 527.4261 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:36:04,786 - INFO - Avg. fwd time: 7.2661 / Avg. bwd time: 19.0274 / Avg. batch time: 559.3337 (ms) / GPU bubble ratio: 62.39%
[rank0]:2025-11-08 00:36:04,834 - INFO - Avg. fwd time: 8.0283 / Avg. bwd time: 23.5817 / Avg. batch time: 635.0395 (ms) / GPU bubble ratio: 60.18%
[rank1]:2025-11-08 00:36:04,825 - INFO - Avg. fwd time: 9.2692 / Avg. bwd time: 24.2288 / Avg. batch time: 598.3441 (ms) / GPU bubble ratio: 55.21%
[rank1]:2025-11-08 00:36:05,004 - INFO -  step: 950  loss: -4.0000  grad_norm:  0.4747  memory: 14.64GiB(30.82%)  tps: 6,049  tflops: 46.07  mfu: 14.77%
[rank3]:2025-11-08 00:36:05,013 - INFO -  step: 950  loss:  0.9228  grad_norm:  0.4747  memory: 26.98GiB(56.79%)  tps: 6,049  tflops: 46.07  mfu: 14.77%
[rank0]:2025-11-08 00:36:05,015 - INFO -  step: 950  loss: -4.0000  grad_norm:  0.4747  memory: 16.57GiB(34.88%)  tps: 6,049  tflops: 46.07  mfu: 14.77%
[rank2]:2025-11-08 00:36:05,000 - INFO -  step: 950  loss: -4.0000  grad_norm:  0.4747  memory: 11.81GiB(24.85%)  tps: 6,049  tflops: 46.07  mfu: 14.77%
[rank0]:2025-11-08 00:38:18,298 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:38:20,747 - INFO - Avg. fwd time: 11.9446 / Avg. bwd time: 46.5128 / Avg. batch time: 527.4754 (ms) / GPU bubble ratio: 11.34%
[rank2]:2025-11-08 00:38:20,822 - INFO - Avg. fwd time: 7.2663 / Avg. bwd time: 19.0281 / Avg. batch time: 559.3706 (ms) / GPU bubble ratio: 62.39%
[rank1]:2025-11-08 00:38:20,862 - INFO - Avg. fwd time: 9.2695 / Avg. bwd time: 24.2279 / Avg. batch time: 598.3804 (ms) / GPU bubble ratio: 55.22%
[rank0]:2025-11-08 00:38:20,869 - INFO - Avg. fwd time: 8.0284 / Avg. bwd time: 23.5815 / Avg. batch time: 635.0768 (ms) / GPU bubble ratio: 60.18%
[rank2]:2025-11-08 00:38:21,036 - INFO -  step: 1000  loss: -4.0000  grad_norm:  0.3999  memory: 11.81GiB(24.85%)  tps: 6,022  tflops: 45.86  mfu: 14.70%
[rank2]:2025-11-08 00:38:21,036 - INFO -  final step: 1000  loss: -4.0000  grad_norm:  0.3999  tps: 6,339  tflops: 48.28  mfu: 14.28%
[rank2]:2025-11-08 00:38:21,036 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 00:38:21,037 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 1000.
[rank1]:2025-11-08 00:38:21,039 - INFO -  step: 1000  loss: -4.0000  grad_norm:  0.3999  memory: 14.64GiB(30.82%)  tps: 6,022  tflops: 45.86  mfu: 14.70%
[rank1]:2025-11-08 00:38:21,040 - INFO -  final step: 1000  loss: -4.0000  grad_norm:  0.3999  tps: 6,339  tflops: 48.28  mfu: 14.29%
[rank1]:2025-11-08 00:38:21,040 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 00:38:21,040 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 1000.
[rank0]:2025-11-08 00:38:21,050 - INFO -  step: 1000  loss: -4.0000  grad_norm:  0.3999  memory: 16.57GiB(34.88%)  tps: 6,022  tflops: 45.86  mfu: 14.70%
[rank0]:2025-11-08 00:38:21,050 - INFO -  final step: 1000  loss: -4.0000  grad_norm:  0.3999  tps: 6,338  tflops: 48.27  mfu: 14.28%
[rank0]:2025-11-08 00:38:21,051 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 00:38:21,051 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 1000.
[rank3]:2025-11-08 00:38:21,049 - INFO -  step: 1000  loss:  0.9974  grad_norm:  0.3999  memory: 26.98GiB(56.79%)  tps: 6,022  tflops: 45.87  mfu: 14.70%
[rank3]:2025-11-08 00:38:21,050 - INFO -  final step: 1000  loss:  0.9974  grad_norm:  0.3999  tps: 6,345  tflops: 48.32  mfu: 14.37%
[rank3]:2025-11-08 00:38:21,050 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 00:38:21,052 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 1000.
[rank0]:2025-11-08 00:38:23,215 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 00:38:23,234 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-08 00:38:23,234 - INFO - Destroying the purge thread.
[rank1]:2025-11-08 00:38:23,234 - INFO - Destroying the purge thread.
[rank3]:2025-11-08 00:38:23,401 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0038_real_final1000_rank3.svg
[rank3]:> Batch Time: 634.69 ms, GPU Bubble Ratio: 59.87%, 57.66%, 66.75%, 26.09%
[rank2]:2025-11-08 00:38:23,449 - INFO - Process group destroyed
[rank1]:2025-11-08 00:38:23,467 - INFO - Process group destroyed
[rank3]:2025-11-08 00:38:23,590 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/pipeline_schedule/251108_0038_thry_final1000_rank3.svg
[rank3]:> Batch Time: 301.48 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 00:38:23,591 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 19-20, summary, console lines 237-246
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–…â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.58236
[rank3]:wandb:               final/avg_loss 0.99745
[rank3]:wandb:             final/avg_mfu(%) 14.36668
[rank3]:wandb:             final/avg_tflops 48.32161
[rank3]:wandb:    final/avg_throughput(tps) 6344.57185
[rank3]:wandb:              final/grad_norm 0.39988
[rank3]:wandb:               final/max_loss 0.99745
[rank3]:wandb:                    grad_norm 0.39988
[rank3]:wandb: loss_metrics/global_avg_loss 0.99745
[rank3]:wandb: loss_metrics/global_max_loss 0.99745
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/y8lv4hig
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1/20251107-2353/wandb/run-20251107_235308-y8lv4hig/logs
[rank3]:2025-11-08 00:38:24,977 - INFO - Process group destroyed
[rank0]:2025-11-08 00:38:25,234 - INFO - Training completed
[rank0]:2025-11-08 00:38:25,235 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 00:38:25,596 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.2', 'layers.0', 'layers.1', 'layers.3', 'tok_embeddings'}
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.11', 'layers.10', 'layers.9'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.6', 'layers.5', 'layers.8', 'layers.7'}
[rank3]:Stage 3: Modules to keep: {'output', 'layers.14', 'layers.13', 'norm', 'layers.15'}
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: slimorca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 2025
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed2025_slimalpaca_1600_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
