
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 22:22:01 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42_codealpaca.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4 --training.dataset=codealpaca  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 22:22:07,516 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank2]:2025-11-09 22:22:07,505 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:"
[rank0]:2025-11-09 22:22:07,716 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 22:22:07,730 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 22:22:07,735 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 22:22:07,736 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-09 22:22:07,720 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank3]:2025-11-09 22:22:07,661 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank2]:2025-11-09 22:22:07,744 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 22:22:07,746 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 22:22:07,930 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 22:22:07,933 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 22:22:07,900 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 22:22:07,903 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 22:22:08,126 - INFO - Preparing codealpaca dataset from sahil2801/CodeAlpaca-20k
[rank0]:2025-11-09 22:22:10,858 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 22:22:11,033 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 22:22:11,052 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 22:22:11,073 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 22:22:11,075 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 22:22:11,100 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 22:22:11,100 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 22:22:11,090 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 22:22:11,116 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 22:22:11,116 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 22:22:11,287 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 22:22:11,287 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 22:22:11,288 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-09 22:22:11,298 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 22:22:11,298 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 22:22:11,299 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-09 22:22:11,573 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 22:22:11,612 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 22:22:11,639 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 22:22:11,639 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 22:22:11,822 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 22:22:11,822 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 22:22:11,823 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run umw4chiz
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_codealpaca_dm1/20251109-2222/wandb/run-20251109_222212-umw4chiz
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/umw4chiz
[rank3]:2025-11-09 22:22:13,271 - INFO - WandB logging enabled
[rank3]:2025-11-09 22:22:13,272 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 22:22:13,309 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 22:22:13,337 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 22:22:13,337 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-09 22:22:13,544 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 22:22:13,544 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 22:22:13,545 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank0]:2025-11-09 22:22:13,561 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank0]:2025-11-09 22:22:13,561 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 22:22:13,562 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 22:22:13,562 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-09 22:22:13,561 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 22:22:13,561 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 22:22:13,561 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 22:22:16,150 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 22:22:16,150 - INFO - Finished loading the checkpoint in 2.59 seconds.
[rank0]:2025-11-09 22:22:16,150 - INFO - Training starts at step 1
[rank1]:2025-11-09 22:22:19,150 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6787  memory: 12.38GiB(26.05%)  tps: 2,174  tflops: 16.55  mfu: 5.31%
[rank1]:2025-11-09 22:22:19,150 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 22:22:19,159 - INFO -  step:  1  loss: 10.1253  grad_norm: 147.6787  memory: 24.19GiB(50.91%)  tps: 2,803  tflops: 21.34  mfu: 6.84%
[rank3]:2025-11-09 22:22:19,160 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 22:22:19,147 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6787  memory:  9.99GiB(21.03%)  tps: 2,034  tflops: 15.49  mfu: 4.96%
[rank2]:2025-11-09 22:22:19,147 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 22:22:19,188 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6787  memory: 12.80GiB(26.95%)  tps: 2,019  tflops: 15.38  mfu: 4.93%
[rank0]:2025-11-09 22:22:19,188 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 22:24:15,912 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:24:18,060 - INFO - Avg. fwd time: 10.6269 / Avg. bwd time: 41.9407 / Avg. batch time: 480.2335 (ms) / GPU bubble ratio: 12.43%
[rank2]:2025-11-09 22:24:18,128 - INFO - Avg. fwd time: 7.0361 / Avg. bwd time: 18.5707 / Avg. batch time: 511.7268 (ms) / GPU bubble ratio: 59.97%
[rank1]:2025-11-09 22:24:18,168 - INFO - Avg. fwd time: 9.0225 / Avg. bwd time: 23.8553 / Avg. batch time: 550.6872 (ms) / GPU bubble ratio: 52.24%
[rank0]:2025-11-09 22:24:18,175 - INFO - Avg. fwd time: 7.9847 / Avg. bwd time: 23.7184 / Avg. batch time: 588.2876 (ms) / GPU bubble ratio: 56.89%
[rank1]:2025-11-09 22:24:18,344 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1215  memory: 14.64GiB(30.82%)  tps: 6,735  tflops: 51.30  mfu: 16.44%
[rank3]:2025-11-09 22:24:18,353 - INFO -  step: 50  loss: 10.5127  grad_norm: 24.1215  memory: 26.98GiB(56.79%)  tps: 6,735  tflops: 51.30  mfu: 16.44%
[rank0]:2025-11-09 22:24:18,356 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1215  memory: 16.57GiB(34.88%)  tps: 6,737  tflops: 51.31  mfu: 16.45%
[rank2]:2025-11-09 22:24:18,341 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1215  memory: 11.81GiB(24.85%)  tps: 6,735  tflops: 51.30  mfu: 16.44%
[rank0]:2025-11-09 22:26:19,909 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:26:22,129 - INFO - Avg. fwd time: 10.7178 / Avg. bwd time: 42.7051 / Avg. batch time: 486.9750 (ms) / GPU bubble ratio: 12.24%
[rank1]:2025-11-09 22:26:22,238 - INFO - Avg. fwd time: 9.0552 / Avg. bwd time: 23.9892 / Avg. batch time: 556.7924 (ms) / GPU bubble ratio: 52.52%
[rank2]:2025-11-09 22:26:22,198 - INFO - Avg. fwd time: 7.0414 / Avg. bwd time: 18.6693 / Avg. batch time: 518.1061 (ms) / GPU bubble ratio: 60.30%
[rank0]:2025-11-09 22:26:22,244 - INFO - Avg. fwd time: 8.0195 / Avg. bwd time: 23.8102 / Avg. batch time: 594.0324 (ms) / GPU bubble ratio: 57.13%
[rank1]:2025-11-09 22:26:22,423 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1284  memory: 14.64GiB(30.82%)  tps: 6,602  tflops: 50.28  mfu: 16.12%
[rank2]:2025-11-09 22:26:22,419 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1284  memory: 11.81GiB(24.85%)  tps: 6,602  tflops: 50.28  mfu: 16.12%
[rank0]:2025-11-09 22:26:22,435 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1284  memory: 16.57GiB(34.88%)  tps: 6,602  tflops: 50.28  mfu: 16.12%
[rank3]:2025-11-09 22:26:22,431 - INFO -  step: 100  loss:  4.7187  grad_norm: 24.1284  memory: 26.98GiB(56.79%)  tps: 6,602  tflops: 50.29  mfu: 16.12%
[rank3]:2025-11-09 22:26:22,620 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2226_real_step100_rank3.svg
[rank3]:> Batch Time: 599.58 ms, GPU Bubble Ratio: 57.10%, 55.57%, 65.46%, 27.75%
[rank0]:2025-11-09 22:28:24,437 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:28:26,824 - INFO - Avg. fwd time: 10.7362 / Avg. bwd time: 43.0184 / Avg. batch time: 489.5902 (ms) / GPU bubble ratio: 12.16%
[rank0]:2025-11-09 22:28:26,909 - INFO - Avg. fwd time: 8.0303 / Avg. bwd time: 23.8442 / Avg. batch time: 596.6053 (ms) / GPU bubble ratio: 57.26%
[rank1]:2025-11-09 22:28:26,879 - INFO - Avg. fwd time: 9.0648 / Avg. bwd time: 24.0393 / Avg. batch time: 559.4757 (ms) / GPU bubble ratio: 52.66%
[rank2]:2025-11-09 22:28:26,850 - INFO - Avg. fwd time: 7.0400 / Avg. bwd time: 18.7040 / Avg. batch time: 520.8544 (ms) / GPU bubble ratio: 60.46%
[rank2]:2025-11-09 22:28:26,931 - INFO -  step: 150  loss: -4.0000  grad_norm: 67.2569  memory: 11.81GiB(24.85%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank3]:2025-11-09 22:28:26,943 - INFO -  step: 150  loss:  4.1137  grad_norm: 67.2569  memory: 26.98GiB(56.79%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank0]:2025-11-09 22:28:26,945 - INFO -  step: 150  loss: -4.0000  grad_norm: 67.2569  memory: 16.57GiB(34.88%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank1]:2025-11-09 22:28:26,934 - INFO -  step: 150  loss: -4.0000  grad_norm: 67.2569  memory: 14.64GiB(30.82%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank3]:2025-11-09 22:28:41,613 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 22:28:41,824 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:28:41,877 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 22:28:41,851 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:30:28,297 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:30:30,498 - INFO - Avg. fwd time: 10.7336 / Avg. bwd time: 43.0981 / Avg. batch time: 490.1820 (ms) / GPU bubble ratio: 12.14%
[rank2]:2025-11-09 22:30:30,571 - INFO - Avg. fwd time: 7.0368 / Avg. bwd time: 18.7338 / Avg. batch time: 521.3611 (ms) / GPU bubble ratio: 60.46%
[rank0]:2025-11-09 22:30:30,617 - INFO - Avg. fwd time: 8.0392 / Avg. bwd time: 23.8685 / Avg. batch time: 597.0482 (ms) / GPU bubble ratio: 57.25%
[rank1]:2025-11-09 22:30:30,610 - INFO - Avg. fwd time: 9.0651 / Avg. bwd time: 24.0780 / Avg. batch time: 559.9609 (ms) / GPU bubble ratio: 52.65%
[rank1]:2025-11-09 22:30:30,790 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.2379  memory: 14.64GiB(30.82%)  tps: 6,614  tflops: 50.37  mfu: 16.15%
[rank3]:2025-11-09 22:30:30,799 - INFO -  step: 200  loss:  1.0131  grad_norm: 16.2379  memory: 26.98GiB(56.79%)  tps: 6,614  tflops: 50.38  mfu: 16.15%
[rank2]:2025-11-09 22:30:30,787 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.2379  memory: 11.81GiB(24.85%)  tps: 6,614  tflops: 50.37  mfu: 16.15%
[rank0]:2025-11-09 22:30:30,802 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.2379  memory: 16.57GiB(34.88%)  tps: 6,614  tflops: 50.37  mfu: 16.15%
[rank3]:2025-11-09 22:30:30,944 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2230_real_step200_rank3.svg
[rank3]:> Batch Time: 597.56 ms, GPU Bubble Ratio: 56.90%, 55.37%, 65.24%, 27.74%
[rank0]:2025-11-09 22:32:32,080 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:32:34,269 - INFO - Avg. fwd time: 10.7209 / Avg. bwd time: 43.1269 / Avg. batch time: 490.2930 (ms) / GPU bubble ratio: 12.14%
[rank2]:2025-11-09 22:32:34,341 - INFO - Avg. fwd time: 7.0321 / Avg. bwd time: 18.7631 / Avg. batch time: 521.5306 (ms) / GPU bubble ratio: 60.43%
[rank0]:2025-11-09 22:32:34,388 - INFO - Avg. fwd time: 8.0420 / Avg. bwd time: 23.8851 / Avg. batch time: 597.1775 (ms) / GPU bubble ratio: 57.23%
[rank1]:2025-11-09 22:32:34,381 - INFO - Avg. fwd time: 9.0618 / Avg. bwd time: 24.1106 / Avg. batch time: 560.1178 (ms) / GPU bubble ratio: 52.62%
[rank1]:2025-11-09 22:32:34,559 - INFO -  step: 250  loss: -4.0000  grad_norm:  4.3593  memory: 14.64GiB(30.82%)  tps: 6,619  tflops: 50.41  mfu: 16.16%
[rank3]:2025-11-09 22:32:34,568 - INFO -  step: 250  loss:  0.3100  grad_norm:  4.3593  memory: 26.98GiB(56.79%)  tps: 6,619  tflops: 50.41  mfu: 16.16%
[rank2]:2025-11-09 22:32:34,556 - INFO -  step: 250  loss: -4.0000  grad_norm:  4.3593  memory: 11.81GiB(24.85%)  tps: 6,619  tflops: 50.41  mfu: 16.16%
[rank0]:2025-11-09 22:32:34,570 - INFO -  step: 250  loss: -4.0000  grad_norm:  4.3593  memory: 16.57GiB(34.88%)  tps: 6,619  tflops: 50.41  mfu: 16.16%
[rank0]:2025-11-09 22:34:36,394 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:34:38,816 - INFO - Avg. fwd time: 10.7265 / Avg. bwd time: 43.1948 / Avg. batch time: 490.8802 (ms) / GPU bubble ratio: 12.12%
[rank2]:2025-11-09 22:34:38,842 - INFO - Avg. fwd time: 7.0309 / Avg. bwd time: 18.7819 / Avg. batch time: 522.0690 (ms) / GPU bubble ratio: 60.45%
[rank1]:2025-11-09 22:34:38,872 - INFO - Avg. fwd time: 9.0625 / Avg. bwd time: 24.1288 / Avg. batch time: 560.6540 (ms) / GPU bubble ratio: 52.64%
[rank0]:2025-11-09 22:34:38,903 - INFO - Avg. fwd time: 8.0476 / Avg. bwd time: 23.8959 / Avg. batch time: 597.7097 (ms) / GPU bubble ratio: 57.25%
[rank0]:2025-11-09 22:34:38,939 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.2250  memory: 16.57GiB(34.88%)  tps: 6,587  tflops: 50.17  mfu: 16.08%
[rank3]:2025-11-09 22:34:38,937 - INFO -  step: 300  loss:  0.2092  grad_norm:  1.2250  memory: 26.98GiB(56.79%)  tps: 6,587  tflops: 50.17  mfu: 16.08%
[rank2]:2025-11-09 22:34:38,924 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.2250  memory: 11.81GiB(24.85%)  tps: 6,587  tflops: 50.17  mfu: 16.08%
[rank1]:2025-11-09 22:34:38,928 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.2250  memory: 14.64GiB(30.82%)  tps: 6,587  tflops: 50.17  mfu: 16.08%
[rank3]:2025-11-09 22:34:39,082 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2234_real_step300_rank3.svg
[rank3]:> Batch Time: 601.08 ms, GPU Bubble Ratio: 57.10%, 55.59%, 65.46%, 27.67%
[rank3]:2025-11-09 22:35:08,614 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 22:35:08,822 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 22:35:08,848 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:35:08,875 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:36:40,987 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:36:43,186 - INFO - Avg. fwd time: 10.7286 / Avg. bwd time: 43.2513 / Avg. batch time: 491.3468 (ms) / GPU bubble ratio: 12.11%
[rank2]:2025-11-09 22:36:43,259 - INFO - Avg. fwd time: 7.0305 / Avg. bwd time: 18.7964 / Avg. batch time: 522.5873 (ms) / GPU bubble ratio: 60.46%
[rank1]:2025-11-09 22:36:43,299 - INFO - Avg. fwd time: 9.0634 / Avg. bwd time: 24.1434 / Avg. batch time: 561.1831 (ms) / GPU bubble ratio: 52.66%
[rank0]:2025-11-09 22:36:43,306 - INFO - Avg. fwd time: 8.0509 / Avg. bwd time: 23.9042 / Avg. batch time: 598.2314 (ms) / GPU bubble ratio: 57.27%
[rank2]:2025-11-09 22:36:43,473 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7597  memory: 11.81GiB(24.85%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank1]:2025-11-09 22:36:43,476 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7597  memory: 14.64GiB(30.82%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank0]:2025-11-09 22:36:43,488 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.7597  memory: 16.57GiB(34.88%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank3]:2025-11-09 22:36:43,485 - INFO -  step: 350  loss:  0.1788  grad_norm:  0.7597  memory: 26.98GiB(56.79%)  tps: 6,577  tflops: 50.10  mfu: 16.06%
[rank0]:2025-11-09 22:38:45,186 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:38:47,400 - INFO - Avg. fwd time: 10.7285 / Avg. bwd time: 43.2867 / Avg. batch time: 491.6263 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:38:47,472 - INFO - Avg. fwd time: 7.0295 / Avg. bwd time: 18.8081 / Avg. batch time: 522.8334 (ms) / GPU bubble ratio: 60.47%
[rank0]:2025-11-09 22:38:47,519 - INFO - Avg. fwd time: 8.0522 / Avg. bwd time: 23.9110 / Avg. batch time: 598.4705 (ms) / GPU bubble ratio: 57.27%
[rank1]:2025-11-09 22:38:47,511 - INFO - Avg. fwd time: 9.0631 / Avg. bwd time: 24.1523 / Avg. batch time: 561.4288 (ms) / GPU bubble ratio: 52.67%
[rank3]:2025-11-09 22:38:47,701 - INFO -  step: 400  loss:  0.1547  grad_norm:  0.6775  memory: 26.98GiB(56.79%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank2]:2025-11-09 22:38:47,688 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.6775  memory: 11.81GiB(24.85%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank0]:2025-11-09 22:38:47,704 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.6775  memory: 16.57GiB(34.88%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank1]:2025-11-09 22:38:47,692 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.6775  memory: 14.64GiB(30.82%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank3]:2025-11-09 22:38:47,845 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2238_real_step400_rank3.svg
[rank3]:> Batch Time: 598.06 ms, GPU Bubble Ratio: 56.94%, 55.46%, 65.28%, 27.65%
[rank0]:2025-11-09 22:40:48,868 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:40:51,255 - INFO - Avg. fwd time: 10.7245 / Avg. bwd time: 43.2689 / Avg. batch time: 491.4463 (ms) / GPU bubble ratio: 12.11%
[rank2]:2025-11-09 22:40:51,283 - INFO - Avg. fwd time: 7.0285 / Avg. bwd time: 18.8134 / Avg. batch time: 522.6828 (ms) / GPU bubble ratio: 60.45%
[rank1]:2025-11-09 22:40:51,314 - INFO - Avg. fwd time: 9.0616 / Avg. bwd time: 24.1586 / Avg. batch time: 561.2712 (ms) / GPU bubble ratio: 52.65%
[rank3]:2025-11-09 22:40:51,379 - INFO -  step: 450  loss:  0.1619  grad_norm:  0.5441  memory: 26.98GiB(56.79%)  tps: 6,624  tflops: 50.45  mfu: 16.17%
[rank2]:2025-11-09 22:40:51,367 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5441  memory: 11.81GiB(24.85%)  tps: 6,624  tflops: 50.45  mfu: 16.17%
[rank1]:2025-11-09 22:40:51,370 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5441  memory: 14.64GiB(30.82%)  tps: 6,624  tflops: 50.45  mfu: 16.17%
[rank0]:2025-11-09 22:40:51,345 - INFO - Avg. fwd time: 8.0490 / Avg. bwd time: 23.9157 / Avg. batch time: 598.3048 (ms) / GPU bubble ratio: 57.26%
[rank0]:2025-11-09 22:40:51,381 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.5441  memory: 16.57GiB(34.88%)  tps: 6,624  tflops: 50.45  mfu: 16.17%
[rank3]:2025-11-09 22:41:36,439 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 22:41:36,651 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 22:41:36,678 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:41:36,704 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:42:53,012 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:42:55,245 - INFO - Avg. fwd time: 10.7267 / Avg. bwd time: 43.2854 / Avg. batch time: 491.5969 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:42:55,316 - INFO - Avg. fwd time: 7.0281 / Avg. bwd time: 18.8180 / Avg. batch time: 522.8082 (ms) / GPU bubble ratio: 60.45%
[rank0]:2025-11-09 22:42:55,362 - INFO - Avg. fwd time: 8.0471 / Avg. bwd time: 23.9188 / Avg. batch time: 598.4186 (ms) / GPU bubble ratio: 57.27%
[rank1]:2025-11-09 22:42:55,355 - INFO - Avg. fwd time: 9.0614 / Avg. bwd time: 24.1603 / Avg. batch time: 561.3909 (ms) / GPU bubble ratio: 52.66%
[rank2]:2025-11-09 22:42:55,533 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.0974  memory: 11.81GiB(24.85%)  tps: 6,598  tflops: 50.25  mfu: 16.11%
[rank3]:2025-11-09 22:42:55,545 - INFO -  step: 500  loss:  0.1544  grad_norm:  1.0974  memory: 26.98GiB(56.79%)  tps: 6,598  tflops: 50.25  mfu: 16.11%
[rank0]:2025-11-09 22:42:55,549 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.0974  memory: 16.57GiB(34.88%)  tps: 6,598  tflops: 50.25  mfu: 16.11%
[rank1]:2025-11-09 22:42:55,537 - INFO -  step: 500  loss: -4.0000  grad_norm:  1.0974  memory: 14.64GiB(30.82%)  tps: 6,598  tflops: 50.25  mfu: 16.11%
[rank3]:2025-11-09 22:42:55,689 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2242_real_step500_rank3.svg
[rank3]:> Batch Time: 599.08 ms, GPU Bubble Ratio: 56.99%, 55.48%, 65.32%, 27.54%
[rank0]:2025-11-09 22:44:57,342 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:44:59,536 - INFO - Avg. fwd time: 10.7300 / Avg. bwd time: 43.3028 / Avg. batch time: 491.7619 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:44:59,610 - INFO - Avg. fwd time: 7.0280 / Avg. bwd time: 18.8229 / Avg. batch time: 522.9986 (ms) / GPU bubble ratio: 60.46%
[rank1]:2025-11-09 22:44:59,649 - INFO - Avg. fwd time: 9.0620 / Avg. bwd time: 24.1647 / Avg. batch time: 561.5820 (ms) / GPU bubble ratio: 52.67%
[rank0]:2025-11-09 22:44:59,656 - INFO - Avg. fwd time: 8.0477 / Avg. bwd time: 23.9223 / Avg. batch time: 598.6063 (ms) / GPU bubble ratio: 57.27%
[rank2]:2025-11-09 22:44:59,821 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2286  memory: 11.81GiB(24.85%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 22:44:59,833 - INFO -  step: 550  loss:  0.1564  grad_norm:  0.2286  memory: 26.98GiB(56.79%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank1]:2025-11-09 22:44:59,824 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2286  memory: 14.64GiB(30.82%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 22:44:59,835 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2286  memory: 16.57GiB(34.88%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 22:47:01,568 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:47:03,984 - INFO - Avg. fwd time: 10.7336 / Avg. bwd time: 43.3167 / Avg. batch time: 491.9011 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:47:04,012 - INFO - Avg. fwd time: 7.0282 / Avg. bwd time: 18.8270 / Avg. batch time: 523.1158 (ms) / GPU bubble ratio: 60.46%
[rank2]:2025-11-09 22:47:04,097 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4385  memory: 11.81GiB(24.85%)  tps: 6,592  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 22:47:04,076 - INFO - Avg. fwd time: 8.0470 / Avg. bwd time: 23.9252 / Avg. batch time: 598.7274 (ms) / GPU bubble ratio: 57.28%
[rank3]:2025-11-09 22:47:04,109 - INFO -  step: 600  loss:  0.1470  grad_norm:  0.4385  memory: 26.98GiB(56.79%)  tps: 6,592  tflops: 50.21  mfu: 16.09%
[rank1]:2025-11-09 22:47:04,044 - INFO - Avg. fwd time: 9.0632 / Avg. bwd time: 24.1691 / Avg. batch time: 561.7052 (ms) / GPU bubble ratio: 52.67%
[rank1]:2025-11-09 22:47:04,101 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4385  memory: 14.64GiB(30.82%)  tps: 6,592  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 22:47:04,112 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.4385  memory: 16.57GiB(34.88%)  tps: 6,592  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 22:47:04,254 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2247_real_step600_rank3.svg
[rank3]:> Batch Time: 599.56 ms, GPU Bubble Ratio: 57.08%, 55.58%, 65.36%, 27.65%
[rank3]:2025-11-09 22:48:03,824 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 22:48:04,033 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:48:04,086 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 22:48:04,060 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:49:05,037 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:49:07,234 - INFO - Avg. fwd time: 10.7316 / Avg. bwd time: 43.2959 / Avg. batch time: 491.7170 (ms) / GPU bubble ratio: 12.10%
[rank0]:2025-11-09 22:49:07,350 - INFO - Avg. fwd time: 8.0439 / Avg. bwd time: 23.9254 / Avg. batch time: 598.5492 (ms) / GPU bubble ratio: 57.27%
[rank2]:2025-11-09 22:49:07,305 - INFO - Avg. fwd time: 7.0277 / Avg. bwd time: 18.8286 / Avg. batch time: 522.9546 (ms) / GPU bubble ratio: 60.45%
[rank1]:2025-11-09 22:49:07,345 - INFO - Avg. fwd time: 9.0626 / Avg. bwd time: 24.1698 / Avg. batch time: 561.5349 (ms) / GPU bubble ratio: 52.65%
[rank3]:2025-11-09 22:49:07,528 - INFO -  step: 650  loss:  0.1664  grad_norm:  0.7249  memory: 26.98GiB(56.79%)  tps: 6,638  tflops: 50.55  mfu: 16.20%
[rank0]:2025-11-09 22:49:07,531 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.7249  memory: 16.57GiB(34.88%)  tps: 6,638  tflops: 50.55  mfu: 16.20%
[rank2]:2025-11-09 22:49:07,516 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.7249  memory: 11.81GiB(24.85%)  tps: 6,638  tflops: 50.55  mfu: 16.20%
[rank1]:2025-11-09 22:49:07,519 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.7249  memory: 14.64GiB(30.82%)  tps: 6,638  tflops: 50.55  mfu: 16.20%
[rank0]:2025-11-09 22:51:08,589 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:51:10,809 - INFO - Avg. fwd time: 10.7294 / Avg. bwd time: 43.2834 / Avg. batch time: 491.5938 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:51:10,880 - INFO - Avg. fwd time: 7.0272 / Avg. bwd time: 18.8314 / Avg. batch time: 522.8124 (ms) / GPU bubble ratio: 60.43%
[rank1]:2025-11-09 22:51:10,919 - INFO - Avg. fwd time: 9.0620 / Avg. bwd time: 24.1713 / Avg. batch time: 561.3848 (ms) / GPU bubble ratio: 52.64%
[rank0]:2025-11-09 22:51:10,926 - INFO - Avg. fwd time: 8.0425 / Avg. bwd time: 23.9263 / Avg. batch time: 598.3943 (ms) / GPU bubble ratio: 57.26%
[rank3]:2025-11-09 22:51:11,108 - INFO -  step: 700  loss:  0.1414  grad_norm:  1.0693  memory: 26.98GiB(56.79%)  tps: 6,629  tflops: 50.49  mfu: 16.18%
[rank0]:2025-11-09 22:51:11,110 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.0693  memory: 16.57GiB(34.88%)  tps: 6,629  tflops: 50.49  mfu: 16.18%
[rank2]:2025-11-09 22:51:11,095 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.0693  memory: 11.81GiB(24.85%)  tps: 6,629  tflops: 50.49  mfu: 16.18%
[rank1]:2025-11-09 22:51:11,099 - INFO -  step: 700  loss: -4.0000  grad_norm:  1.0693  memory: 14.64GiB(30.82%)  tps: 6,629  tflops: 50.49  mfu: 16.18%
[rank3]:2025-11-09 22:51:11,253 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2251_real_step700_rank3.svg
[rank3]:> Batch Time: 596.56 ms, GPU Bubble Ratio: 56.88%, 55.34%, 65.19%, 27.80%
[rank0]:2025-11-09 22:53:12,892 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:53:15,287 - INFO - Avg. fwd time: 10.7316 / Avg. bwd time: 43.2935 / Avg. batch time: 491.6919 (ms) / GPU bubble ratio: 12.10%
[rank1]:2025-11-09 22:53:15,348 - INFO - Avg. fwd time: 9.0627 / Avg. bwd time: 24.1740 / Avg. batch time: 561.5088 (ms) / GPU bubble ratio: 52.65%
[rank1]:2025-11-09 22:53:15,406 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4127  memory: 14.64GiB(30.82%)  tps: 6,590  tflops: 50.19  mfu: 16.09%
[rank2]:2025-11-09 22:53:15,315 - INFO - Avg. fwd time: 7.0274 / Avg. bwd time: 18.8352 / Avg. batch time: 522.9320 (ms) / GPU bubble ratio: 60.43%
[rank2]:2025-11-09 22:53:15,403 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4127  memory: 11.81GiB(24.85%)  tps: 6,590  tflops: 50.19  mfu: 16.09%
[rank3]:2025-11-09 22:53:15,415 - INFO -  step: 750  loss:  0.1582  grad_norm:  0.4127  memory: 26.98GiB(56.79%)  tps: 6,590  tflops: 50.19  mfu: 16.09%
[rank0]:2025-11-09 22:53:15,381 - INFO - Avg. fwd time: 8.0430 / Avg. bwd time: 23.9283 / Avg. batch time: 598.5177 (ms) / GPU bubble ratio: 57.27%
[rank0]:2025-11-09 22:53:15,417 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4127  memory: 16.57GiB(34.88%)  tps: 6,590  tflops: 50.19  mfu: 16.09%
[rank3]:2025-11-09 22:54:31,138 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 22:54:31,376 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 22:54:31,350 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:54:31,403 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 22:55:17,436 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:55:19,672 - INFO - Avg. fwd time: 10.7350 / Avg. bwd time: 43.3103 / Avg. batch time: 491.8546 (ms) / GPU bubble ratio: 12.10%
[rank2]:2025-11-09 22:55:19,743 - INFO - Avg. fwd time: 7.0279 / Avg. bwd time: 18.8381 / Avg. batch time: 523.0798 (ms) / GPU bubble ratio: 60.44%
[rank0]:2025-11-09 22:55:19,789 - INFO - Avg. fwd time: 8.0438 / Avg. bwd time: 23.9304 / Avg. batch time: 598.6755 (ms) / GPU bubble ratio: 57.27%
[rank1]:2025-11-09 22:55:19,783 - INFO - Avg. fwd time: 9.0640 / Avg. bwd time: 24.1771 / Avg. batch time: 561.6651 (ms) / GPU bubble ratio: 52.65%
[rank1]:2025-11-09 22:55:19,959 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.7213  memory: 14.64GiB(30.82%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank1]:2025-11-09 22:55:19,959 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.7213  tps: 7,004  tflops: 53.34  mfu: 15.50%
[rank1]:2025-11-09 22:55:19,959 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 22:55:19,960 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 22:55:19,955 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.7213  memory: 11.81GiB(24.85%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank2]:2025-11-09 22:55:19,956 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.7213  tps: 7,002  tflops: 53.33  mfu: 15.48%
[rank2]:2025-11-09 22:55:19,956 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 22:55:19,956 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 22:55:19,968 - INFO -  step: 800  loss:  0.1328  grad_norm:  0.7213  memory: 26.98GiB(56.79%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank3]:2025-11-09 22:55:19,968 - INFO -  final step: 800  loss:  0.1328  grad_norm:  0.7213  tps: 7,010  tflops: 53.39  mfu: 15.59%
[rank3]:2025-11-09 22:55:19,969 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 22:55:19,969 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 22:55:19,971 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.7213  memory: 16.57GiB(34.88%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank0]:2025-11-09 22:55:19,971 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.7213  tps: 7,002  tflops: 53.33  mfu: 15.47%
[rank0]:2025-11-09 22:55:19,971 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 22:55:19,972 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 22:55:21,964 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 22:55:21,983 - INFO - Process group destroyed
[rank2]:2025-11-09 22:55:21,964 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 22:55:21,952 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 22:55:21,964 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-09 22:55:22,106 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2255_real_final800_rank3.svg
[rank3]:> Batch Time: 598.56 ms, GPU Bubble Ratio: 57.00%, 55.49%, 65.32%, 27.76%
[rank3]:2025-11-09 22:55:22,246 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1/pipeline_schedule/251109_2255_thry_final800_rank3.svg
[rank3]:> Batch Time: 287.55 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 22:55:22,247 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank2]:2025-11-09 22:55:22,450 - INFO - Process group destroyed
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–„â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–„â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.3372
[rank3]:wandb:               final/avg_loss 0.13283
[rank3]:wandb:             final/avg_mfu(%) 15.5873
[rank3]:wandb:             final/avg_tflops 53.39037
[rank3]:wandb:    final/avg_throughput(tps) 7010.09379
[rank3]:wandb:              final/grad_norm 0.72128
[rank3]:wandb:               final/max_loss 0.13283
[rank3]:wandb:                    grad_norm 0.72128
[rank3]:wandb: loss_metrics/global_avg_loss 0.13283
[rank3]:wandb: loss_metrics/global_max_loss 0.13283
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed42_codealpaca_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/umw4chiz
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_codealpaca_dm1/20251109-2222/wandb/run-20251109_222212-umw4chiz/logs
[rank3]:2025-11-09 22:55:23,461 - INFO - Process group destroyed
[rank0]:2025-11-09 22:55:23,965 - INFO - Training completed
[rank0]:2025-11-09 22:55:23,965 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 22:55:24,010 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.1', 'layers.3', 'layers.2', 'layers.0'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.12', 'layers.11', 'layers.9'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.6', 'layers.8', 'layers.7', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.14', 'layers.13', 'layers.15', 'norm', 'output'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.1
[rank3]:	- training:
[rank3]:		- dataset: codealpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed42_codealpaca_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
