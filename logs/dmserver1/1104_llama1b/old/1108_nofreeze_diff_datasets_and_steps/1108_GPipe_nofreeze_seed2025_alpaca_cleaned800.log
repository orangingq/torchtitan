
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 01:24:40 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-08 01:24:46,450 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-08 01:24:46,449 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank1]:2025-11-08 01:24:46,534 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-08 01:24:46,562 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-08 01:24:46,701 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 01:24:46,703 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 01:24:46,705 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 01:24:46,706 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-08 01:24:46,699 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 01:24:46,701 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 01:24:46,845 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 01:24:46,848 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-08 01:24:46,836 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 01:24:46,840 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 01:24:47,112 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 01:24:49,948 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-08 01:24:50,091 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-08 01:24:50,134 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:24:50,097 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 01:24:50,136 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:24:50,138 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-08 01:24:50,171 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-08 01:24:50,172 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-08 01:24:50,162 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-08 01:24:50,163 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-08 01:24:50,350 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 01:24:50,350 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 01:24:50,351 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-08 01:24:50,380 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:24:50,380 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 01:24:50,381 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-08 01:24:51,255 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 01:24:51,293 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 01:24:51,318 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-08 01:24:51,318 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-08 01:24:51,500 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 01:24:51,500 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 01:24:51,501 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run elfeglcm
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0124/wandb/run-20251108_012451-elfeglcm
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/elfeglcm
[rank3]:2025-11-08 01:24:52,660 - INFO - WandB logging enabled
[rank3]:2025-11-08 01:24:52,661 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 01:24:52,700 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 01:24:52,729 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-08 01:24:52,730 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-08 01:24:52,931 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-08 01:24:52,914 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 01:24:52,915 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 01:24:52,916 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 01:24:52,931 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 01:24:52,931 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 01:24:52,931 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 01:24:52,932 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 01:24:52,932 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-08 01:24:52,931 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 01:24:55,495 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 01:24:55,495 - INFO - Finished loading the checkpoint in 2.56 seconds.
[rank0]:2025-11-08 01:24:55,495 - INFO - Training starts at step 1
[rank2]:2025-11-08 01:24:58,664 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory:  9.99GiB(21.03%)  tps: 1,921  tflops: 14.63  mfu: 4.69%
[rank2]:2025-11-08 01:24:58,664 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 01:24:58,667 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory: 12.38GiB(26.05%)  tps: 2,222  tflops: 16.92  mfu: 5.42%
[rank1]:2025-11-08 01:24:58,667 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 01:24:58,673 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5958  memory: 24.19GiB(50.91%)  tps: 2,744  tflops: 20.90  mfu: 6.70%
[rank0]:2025-11-08 01:24:58,702 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5958  memory: 12.80GiB(26.95%)  tps: 1,913  tflops: 14.57  mfu: 4.67%
[rank0]:2025-11-08 01:24:58,703 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 01:24:58,674 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 01:27:01,951 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:27:04,227 - INFO - Avg. fwd time: 11.3806 / Avg. bwd time: 44.9316 / Avg. batch time: 510.4711 (ms) / GPU bubble ratio: 11.75%
[rank2]:2025-11-08 01:27:04,300 - INFO - Avg. fwd time: 7.1488 / Avg. bwd time: 18.8208 / Avg. batch time: 542.4375 (ms) / GPU bubble ratio: 61.70%
[rank1]:2025-11-08 01:27:04,339 - INFO - Avg. fwd time: 9.0900 / Avg. bwd time: 23.8923 / Avg. batch time: 581.7576 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-08 01:27:04,347 - INFO - Avg. fwd time: 7.9296 / Avg. bwd time: 23.4020 / Avg. batch time: 618.7959 (ms) / GPU bubble ratio: 59.49%
[rank2]:2025-11-08 01:27:04,523 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.4587  memory: 11.81GiB(24.85%)  tps: 6,379  tflops: 48.58  mfu: 15.57%
[rank3]:2025-11-08 01:27:04,536 - INFO -  step: 50  loss:  4.3859  grad_norm: 14.4587  memory: 26.98GiB(56.79%)  tps: 6,379  tflops: 48.58  mfu: 15.57%
[rank1]:2025-11-08 01:27:04,527 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.4587  memory: 14.64GiB(30.82%)  tps: 6,379  tflops: 48.58  mfu: 15.57%
[rank0]:2025-11-08 01:27:04,538 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.4587  memory: 16.57GiB(34.88%)  tps: 6,380  tflops: 48.59  mfu: 15.57%
[rank0]:2025-11-08 01:29:11,071 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:29:13,389 - INFO - Avg. fwd time: 11.3651 / Avg. bwd time: 45.2551 / Avg. batch time: 512.7740 (ms) / GPU bubble ratio: 11.66%
[rank2]:2025-11-08 01:29:13,463 - INFO - Avg. fwd time: 7.1471 / Avg. bwd time: 18.8959 / Avg. batch time: 544.3520 (ms) / GPU bubble ratio: 61.73%
[rank0]:2025-11-08 01:29:13,510 - INFO - Avg. fwd time: 7.9053 / Avg. bwd time: 23.4516 / Avg. batch time: 619.8565 (ms) / GPU bubble ratio: 59.53%
[rank1]:2025-11-08 01:29:13,503 - INFO - Avg. fwd time: 9.0903 / Avg. bwd time: 23.9823 / Avg. batch time: 583.2485 (ms) / GPU bubble ratio: 54.64%
[rank1]:2025-11-08 01:29:13,690 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4683  memory: 14.64GiB(30.82%)  tps: 6,342  tflops: 48.30  mfu: 15.48%
[rank3]:2025-11-08 01:29:13,699 - INFO -  step: 100  loss:  0.5731  grad_norm:  0.4683  memory: 26.98GiB(56.79%)  tps: 6,342  tflops: 48.31  mfu: 15.48%
[rank2]:2025-11-08 01:29:13,686 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4683  memory: 11.81GiB(24.85%)  tps: 6,342  tflops: 48.31  mfu: 15.48%
[rank0]:2025-11-08 01:29:13,701 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4683  memory: 16.57GiB(34.88%)  tps: 6,342  tflops: 48.30  mfu: 15.48%
[rank3]:2025-11-08 01:29:13,873 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0129_real_step100_rank3.svg
[rank3]:> Batch Time: 621.09 ms, GPU Bubble Ratio: 59.25%, 57.12%, 66.22%, 26.52%
[rank0]:2025-11-08 01:31:20,318 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:31:22,816 - INFO - Avg. fwd time: 11.3700 / Avg. bwd time: 45.3296 / Avg. batch time: 513.3639 (ms) / GPU bubble ratio: 11.64%
[rank2]:2025-11-08 01:31:22,842 - INFO - Avg. fwd time: 7.1486 / Avg. bwd time: 18.9232 / Avg. batch time: 545.0596 (ms) / GPU bubble ratio: 61.73%
[rank1]:2025-11-08 01:31:22,873 - INFO - Avg. fwd time: 9.0928 / Avg. bwd time: 24.0176 / Avg. batch time: 583.8503 (ms) / GPU bubble ratio: 54.63%
[rank1]:2025-11-08 01:31:22,927 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3946  memory: 14.64GiB(30.82%)  tps: 6,339  tflops: 48.28  mfu: 15.47%
[rank3]:2025-11-08 01:31:22,936 - INFO -  step: 150  loss:  0.5163  grad_norm:  0.3946  memory: 26.98GiB(56.79%)  tps: 6,339  tflops: 48.28  mfu: 15.47%
[rank0]:2025-11-08 01:31:22,902 - INFO - Avg. fwd time: 7.8989 / Avg. bwd time: 23.4697 / Avg. batch time: 620.3418 (ms) / GPU bubble ratio: 59.55%
[rank0]:2025-11-08 01:31:22,938 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3946  memory: 16.57GiB(34.88%)  tps: 6,339  tflops: 48.28  mfu: 15.47%
[rank2]:2025-11-08 01:31:22,924 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3946  memory: 11.81GiB(24.85%)  tps: 6,339  tflops: 48.28  mfu: 15.47%
[rank0]:2025-11-08 01:33:29,904 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:33:32,231 - INFO - Avg. fwd time: 11.3698 / Avg. bwd time: 45.4155 / Avg. batch time: 514.0268 (ms) / GPU bubble ratio: 11.62%
[rank2]:2025-11-08 01:33:32,307 - INFO - Avg. fwd time: 7.1510 / Avg. bwd time: 18.9380 / Avg. batch time: 545.6329 (ms) / GPU bubble ratio: 61.75%
[rank0]:2025-11-08 01:33:32,354 - INFO - Avg. fwd time: 7.8960 / Avg. bwd time: 23.4785 / Avg. batch time: 620.8216 (ms) / GPU bubble ratio: 59.57%
[rank1]:2025-11-08 01:33:32,347 - INFO - Avg. fwd time: 9.0956 / Avg. bwd time: 24.0404 / Avg. batch time: 584.3787 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-08 01:33:32,547 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3174  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank2]:2025-11-08 01:33:32,532 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3174  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank1]:2025-11-08 01:33:32,536 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3174  memory: 14.64GiB(30.82%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank3]:2025-11-08 01:33:32,544 - INFO -  step: 200  loss:  0.5890  grad_norm:  0.3174  memory: 26.98GiB(56.79%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank3]:2025-11-08 01:33:32,695 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0133_real_step200_rank3.svg
[rank3]:> Batch Time: 621.06 ms, GPU Bubble Ratio: 59.28%, 57.11%, 66.23%, 26.55%
[rank0]:2025-11-08 01:35:40,027 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:35:42,331 - INFO - Avg. fwd time: 11.3800 / Avg. bwd time: 45.4934 / Avg. batch time: 514.7266 (ms) / GPU bubble ratio: 11.61%
[rank0]:2025-11-08 01:35:42,456 - INFO - Avg. fwd time: 7.8975 / Avg. bwd time: 23.4865 / Avg. batch time: 621.5539 (ms) / GPU bubble ratio: 59.61%
[rank2]:2025-11-08 01:35:42,408 - INFO - Avg. fwd time: 7.1533 / Avg. bwd time: 18.9502 / Avg. batch time: 546.3944 (ms) / GPU bubble ratio: 61.78%
[rank1]:2025-11-08 01:35:42,448 - INFO - Avg. fwd time: 9.1016 / Avg. bwd time: 24.0581 / Avg. batch time: 585.1339 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-08 01:35:42,646 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3843  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-08 01:35:42,631 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3843  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank1]:2025-11-08 01:35:42,635 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3843  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-08 01:35:42,643 - INFO -  step: 250  loss:  0.5974  grad_norm:  0.3843  memory: 26.98GiB(56.79%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-08 01:37:49,696 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 01:37:52,252 - INFO - Avg. fwd time: 7.1549 / Avg. bwd time: 18.9570 / Avg. batch time: 546.6614 (ms) / GPU bubble ratio: 61.79%
[rank3]:2025-11-08 01:37:52,226 - INFO - Avg. fwd time: 11.3804 / Avg. bwd time: 45.5349 / Avg. batch time: 515.0513 (ms) / GPU bubble ratio: 11.60%
[rank1]:2025-11-08 01:37:52,283 - INFO - Avg. fwd time: 9.1057 / Avg. bwd time: 24.0698 / Avg. batch time: 585.3994 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-08 01:37:52,312 - INFO - Avg. fwd time: 7.9001 / Avg. bwd time: 23.4926 / Avg. batch time: 621.7993 (ms) / GPU bubble ratio: 59.61%
[rank0]:2025-11-08 01:37:52,348 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3417  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-08 01:37:52,334 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3417  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-08 01:37:52,346 - INFO -  step: 300  loss:  0.5876  grad_norm:  0.3417  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank1]:2025-11-08 01:37:52,337 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3417  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-08 01:37:52,495 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0137_real_step300_rank3.svg
[rank3]:> Batch Time: 622.11 ms, GPU Bubble Ratio: 59.32%, 57.12%, 66.28%, 26.61%
[rank0]:2025-11-08 01:39:59,450 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:40:01,750 - INFO - Avg. fwd time: 11.3808 / Avg. bwd time: 45.5600 / Avg. batch time: 515.2498 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-08 01:40:01,826 - INFO - Avg. fwd time: 7.1548 / Avg. bwd time: 18.9585 / Avg. batch time: 546.8987 (ms) / GPU bubble ratio: 61.80%
[rank0]:2025-11-08 01:40:01,874 - INFO - Avg. fwd time: 7.8981 / Avg. bwd time: 23.4958 / Avg. batch time: 622.0198 (ms) / GPU bubble ratio: 59.62%
[rank1]:2025-11-08 01:40:01,869 - INFO - Avg. fwd time: 9.1064 / Avg. bwd time: 24.0772 / Avg. batch time: 585.6341 (ms) / GPU bubble ratio: 54.67%
[rank1]:2025-11-08 01:40:02,052 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3113  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-08 01:40:02,048 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3113  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-08 01:40:02,062 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3113  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-08 01:40:02,061 - INFO -  step: 350  loss:  0.5431  grad_norm:  0.3113  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-08 01:42:08,433 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:42:10,739 - INFO - Avg. fwd time: 11.3720 / Avg. bwd time: 45.5390 / Avg. batch time: 515.0018 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-08 01:42:10,818 - INFO - Avg. fwd time: 7.1537 / Avg. bwd time: 18.9577 / Avg. batch time: 546.5962 (ms) / GPU bubble ratio: 61.78%
[rank0]:2025-11-08 01:42:10,865 - INFO - Avg. fwd time: 7.8961 / Avg. bwd time: 23.4959 / Avg. batch time: 621.6917 (ms) / GPU bubble ratio: 59.60%
[rank1]:2025-11-08 01:42:10,858 - INFO - Avg. fwd time: 9.1050 / Avg. bwd time: 24.0804 / Avg. batch time: 585.3236 (ms) / GPU bubble ratio: 54.64%
[rank2]:2025-11-08 01:42:11,039 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2980  memory: 11.81GiB(24.85%)  tps: 6,351  tflops: 48.37  mfu: 15.50%
[rank0]:2025-11-08 01:42:11,054 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2980  memory: 16.57GiB(34.88%)  tps: 6,351  tflops: 48.37  mfu: 15.50%
[rank1]:2025-11-08 01:42:11,043 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2980  memory: 14.64GiB(30.82%)  tps: 6,351  tflops: 48.37  mfu: 15.50%
[rank3]:2025-11-08 01:42:11,052 - INFO -  step: 400  loss:  0.5346  grad_norm:  0.2980  memory: 26.98GiB(56.79%)  tps: 6,351  tflops: 48.37  mfu: 15.50%
[rank3]:2025-11-08 01:42:11,203 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0142_real_step400_rank3.svg
[rank3]:> Batch Time: 619.09 ms, GPU Bubble Ratio: 59.17%, 56.97%, 66.18%, 26.70%
[rank3]:2025-11-08 01:42:19,896 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-08 01:42:20,119 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 01:42:20,171 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-08 01:42:20,146 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 01:44:17,732 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:44:20,241 - INFO - Avg. fwd time: 11.3665 / Avg. bwd time: 45.5420 / Avg. batch time: 514.9796 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-08 01:44:20,268 - INFO - Avg. fwd time: 7.1529 / Avg. bwd time: 18.9569 / Avg. batch time: 546.6001 (ms) / GPU bubble ratio: 61.79%
[rank0]:2025-11-08 01:44:20,331 - INFO - Avg. fwd time: 7.8960 / Avg. bwd time: 23.4975 / Avg. batch time: 621.6916 (ms) / GPU bubble ratio: 59.60%
[rank1]:2025-11-08 01:44:20,300 - INFO - Avg. fwd time: 9.1031 / Avg. bwd time: 24.0856 / Avg. batch time: 585.3309 (ms) / GPU bubble ratio: 54.64%
[rank1]:2025-11-08 01:44:20,356 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2894  memory: 14.64GiB(30.82%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank3]:2025-11-08 01:44:20,366 - INFO -  step: 450  loss:  0.5710  grad_norm:  0.2894  memory: 26.98GiB(56.79%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank2]:2025-11-08 01:44:20,353 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2894  memory: 11.81GiB(24.85%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank0]:2025-11-08 01:44:20,367 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2894  memory: 16.57GiB(34.88%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank0]:2025-11-08 01:46:27,303 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:46:29,614 - INFO - Avg. fwd time: 11.3664 / Avg. bwd time: 45.5548 / Avg. batch time: 515.0765 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-08 01:46:29,691 - INFO - Avg. fwd time: 7.1531 / Avg. bwd time: 18.9579 / Avg. batch time: 546.6635 (ms) / GPU bubble ratio: 61.79%
[rank1]:2025-11-08 01:46:29,731 - INFO - Avg. fwd time: 9.1040 / Avg. bwd time: 24.0913 / Avg. batch time: 585.4068 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-08 01:46:29,739 - INFO - Avg. fwd time: 7.8956 / Avg. bwd time: 23.4995 / Avg. batch time: 621.7649 (ms) / GPU bubble ratio: 59.61%
[rank3]:2025-11-08 01:46:29,925 - INFO -  step: 500  loss:  0.3989  grad_norm:  0.2310  memory: 26.98GiB(56.79%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank2]:2025-11-08 01:46:29,912 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2310  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank1]:2025-11-08 01:46:29,916 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2310  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank0]:2025-11-08 01:46:29,927 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2310  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.16  mfu: 15.43%
[rank3]:2025-11-08 01:46:30,074 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0146_real_step500_rank3.svg
[rank3]:> Batch Time: 622.16 ms, GPU Bubble Ratio: 59.31%, 57.09%, 66.29%, 26.42%
[rank0]:2025-11-08 01:48:37,168 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:48:39,453 - INFO - Avg. fwd time: 11.3688 / Avg. bwd time: 45.5714 / Avg. batch time: 515.2265 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-08 01:48:39,530 - INFO - Avg. fwd time: 7.1541 / Avg. bwd time: 18.9600 / Avg. batch time: 546.8364 (ms) / GPU bubble ratio: 61.80%
[rank0]:2025-11-08 01:48:39,577 - INFO - Avg. fwd time: 7.8972 / Avg. bwd time: 23.5015 / Avg. batch time: 621.9562 (ms) / GPU bubble ratio: 59.61%
[rank1]:2025-11-08 01:48:39,570 - INFO - Avg. fwd time: 9.1052 / Avg. bwd time: 24.0980 / Avg. batch time: 585.5986 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-08 01:48:39,762 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2729  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-08 01:48:39,760 - INFO -  step: 550  loss:  0.4460  grad_norm:  0.2729  memory: 26.98GiB(56.79%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank1]:2025-11-08 01:48:39,752 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2729  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-08 01:48:39,748 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2729  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-08 01:50:46,852 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:50:49,390 - INFO - Avg. fwd time: 11.3704 / Avg. bwd time: 45.5844 / Avg. batch time: 515.3404 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-08 01:50:49,418 - INFO - Avg. fwd time: 7.1547 / Avg. bwd time: 18.9618 / Avg. batch time: 546.9197 (ms) / GPU bubble ratio: 61.80%
[rank2]:2025-11-08 01:50:49,503 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2758  memory: 11.81GiB(24.85%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank1]:2025-11-08 01:50:49,450 - INFO - Avg. fwd time: 9.1070 / Avg. bwd time: 24.1061 / Avg. batch time: 585.7030 (ms) / GPU bubble ratio: 54.63%
[rank1]:2025-11-08 01:50:49,506 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2758  memory: 14.64GiB(30.82%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-08 01:50:49,481 - INFO - Avg. fwd time: 7.8970 / Avg. bwd time: 23.5037 / Avg. batch time: 622.0616 (ms) / GPU bubble ratio: 59.62%
[rank0]:2025-11-08 01:50:49,517 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2758  memory: 16.57GiB(34.88%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-08 01:50:49,516 - INFO -  step: 600  loss:  0.4446  grad_norm:  0.2758  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-08 01:50:49,665 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0150_real_step600_rank3.svg
[rank3]:> Batch Time: 623.16 ms, GPU Bubble Ratio: 59.33%, 57.10%, 66.34%, 26.51%
[rank0]:2025-11-08 01:52:56,950 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:52:59,236 - INFO - Avg. fwd time: 11.3739 / Avg. bwd time: 45.6021 / Avg. batch time: 515.5103 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-08 01:52:59,315 - INFO - Avg. fwd time: 7.1554 / Avg. bwd time: 18.9633 / Avg. batch time: 547.1116 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-08 01:52:59,355 - INFO - Avg. fwd time: 9.1083 / Avg. bwd time: 24.1126 / Avg. batch time: 585.9099 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-08 01:52:59,362 - INFO - Avg. fwd time: 7.8986 / Avg. bwd time: 23.5059 / Avg. batch time: 622.2681 (ms) / GPU bubble ratio: 59.63%
[rank2]:2025-11-08 01:52:59,533 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2592  memory: 11.81GiB(24.85%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-08 01:52:59,545 - INFO -  step: 650  loss:  0.3715  grad_norm:  0.2592  memory: 26.98GiB(56.79%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank1]:2025-11-08 01:52:59,537 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2592  memory: 14.64GiB(30.82%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-08 01:52:59,548 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2592  memory: 16.57GiB(34.88%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-08 01:55:06,395 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:55:08,710 - INFO - Avg. fwd time: 11.3728 / Avg. bwd time: 45.6027 / Avg. batch time: 515.5023 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-08 01:55:08,785 - INFO - Avg. fwd time: 7.1557 / Avg. bwd time: 18.9636 / Avg. batch time: 547.0772 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-08 01:55:08,826 - INFO - Avg. fwd time: 9.1094 / Avg. bwd time: 24.1178 / Avg. batch time: 585.8956 (ms) / GPU bubble ratio: 54.63%
[rank0]:2025-11-08 01:55:08,833 - INFO - Avg. fwd time: 7.8984 / Avg. bwd time: 23.5063 / Avg. batch time: 622.2531 (ms) / GPU bubble ratio: 59.62%
[rank2]:2025-11-08 01:55:09,005 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3122  memory: 11.81GiB(24.85%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-08 01:55:09,017 - INFO -  step: 700  loss:  0.4395  grad_norm:  0.3122  memory: 26.98GiB(56.79%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank1]:2025-11-08 01:55:09,009 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3122  memory: 14.64GiB(30.82%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank0]:2025-11-08 01:55:09,020 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3122  memory: 16.57GiB(34.88%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-08 01:55:09,166 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0155_real_step700_rank3.svg
[rank3]:> Batch Time: 621.14 ms, GPU Bubble Ratio: 59.26%, 57.03%, 66.26%, 26.69%
[rank0]:2025-11-08 01:57:15,934 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 01:57:18,473 - INFO - Avg. fwd time: 7.1558 / Avg. bwd time: 18.9638 / Avg. batch time: 547.0646 (ms) / GPU bubble ratio: 61.80%
[rank3]:2025-11-08 01:57:18,445 - INFO - Avg. fwd time: 11.3735 / Avg. bwd time: 45.5990 / Avg. batch time: 515.4745 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-08 01:57:18,506 - INFO - Avg. fwd time: 9.1091 / Avg. bwd time: 24.1219 / Avg. batch time: 585.8961 (ms) / GPU bubble ratio: 54.63%
[rank0]:2025-11-08 01:57:18,539 - INFO - Avg. fwd time: 7.8980 / Avg. bwd time: 23.5067 / Avg. batch time: 622.2536 (ms) / GPU bubble ratio: 59.62%
[rank0]:2025-11-08 01:57:18,574 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3442  memory: 16.57GiB(34.88%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank2]:2025-11-08 01:57:18,560 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3442  memory: 11.81GiB(24.85%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank3]:2025-11-08 01:57:18,573 - INFO -  step: 750  loss:  0.4113  grad_norm:  0.3442  memory: 26.98GiB(56.79%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank1]:2025-11-08 01:57:18,564 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3442  memory: 14.64GiB(30.82%)  tps: 6,323  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-08 01:59:25,456 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:59:27,782 - INFO - Avg. fwd time: 11.3723 / Avg. bwd time: 45.6000 / Avg. batch time: 515.4720 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-08 01:59:27,857 - INFO - Avg. fwd time: 7.1559 / Avg. bwd time: 18.9641 / Avg. batch time: 547.0410 (ms) / GPU bubble ratio: 61.80%
[rank1]:2025-11-08 01:59:27,897 - INFO - Avg. fwd time: 9.1090 / Avg. bwd time: 24.1258 / Avg. batch time: 585.8788 (ms) / GPU bubble ratio: 54.62%
[rank0]:2025-11-08 01:59:27,904 - INFO - Avg. fwd time: 7.8984 / Avg. bwd time: 23.5075 / Avg. batch time: 622.2373 (ms) / GPU bubble ratio: 59.62%
[rank2]:2025-11-08 01:59:28,076 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3017  memory: 11.81GiB(24.85%)  tps: 6,325  tflops: 48.17  mfu: 15.44%
[rank2]:2025-11-08 01:59:28,076 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3017  tps: 6,702  tflops: 51.04  mfu: 14.81%
[rank2]:2025-11-08 01:59:28,076 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 01:59:28,077 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-08 01:59:28,088 - INFO -  step: 800  loss:  0.4255  grad_norm:  0.3017  memory: 26.98GiB(56.79%)  tps: 6,325  tflops: 48.17  mfu: 15.44%
[rank3]:2025-11-08 01:59:28,089 - INFO -  final step: 800  loss:  0.4255  grad_norm:  0.3017  tps: 6,710  tflops: 51.11  mfu: 14.93%
[rank3]:2025-11-08 01:59:28,089 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 01:59:28,090 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-08 01:59:28,080 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3017  memory: 14.64GiB(30.82%)  tps: 6,325  tflops: 48.17  mfu: 15.44%
[rank1]:2025-11-08 01:59:28,080 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3017  tps: 6,706  tflops: 51.07  mfu: 14.85%
[rank1]:2025-11-08 01:59:28,080 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 01:59:28,081 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 01:59:28,090 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3017  memory: 16.57GiB(34.88%)  tps: 6,325  tflops: 48.17  mfu: 15.44%
[rank0]:2025-11-08 01:59:28,091 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3017  tps: 6,702  tflops: 51.04  mfu: 14.81%
[rank0]:2025-11-08 01:59:28,091 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 01:59:28,091 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-08 01:59:30,128 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 01:59:30,115 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 01:59:30,128 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-08 01:59:30,128 - INFO - Destroying the purge thread.
[rank3]:2025-11-08 01:59:30,271 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0159_real_final800_rank3.svg
[rank3]:> Batch Time: 621.62 ms, GPU Bubble Ratio: 59.30%, 57.12%, 66.28%, 26.56%
[rank3]:2025-11-08 01:59:30,405 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0159_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.31 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 01:59:30,406 - INFO - Destroying the purge thread.
[rank2]:2025-11-08 01:59:30,481 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank1]:2025-11-08 01:59:30,450 - INFO - Process group destroyed
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44159
[rank3]:wandb:               final/avg_loss 0.42549
[rank3]:wandb:             final/avg_mfu(%) 14.9282
[rank3]:wandb:             final/avg_tflops 51.10771
[rank3]:wandb:    final/avg_throughput(tps) 6710.38352
[rank3]:wandb:              final/grad_norm 0.30166
[rank3]:wandb:               final/max_loss 0.42549
[rank3]:wandb:                    grad_norm 0.30166
[rank3]:wandb: loss_metrics/global_avg_loss 0.42549
[rank3]:wandb: loss_metrics/global_max_loss 0.42549
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/elfeglcm
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0124/wandb/run-20251108_012451-elfeglcm/logs
[rank3]:2025-11-08 01:59:31,990 - INFO - Process group destroyed
[rank0]:2025-11-08 01:59:32,128 - INFO - Training completed
[rank0]:2025-11-08 01:59:32,129 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 01:59:32,467 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.12', 'layers.11', 'layers.10'}
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.0', 'tok_embeddings', 'layers.3', 'layers.2'}
[rank1]:Stage 1: Modules to keep: {'layers.8', 'layers.6', 'layers.7', 'layers.4', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'norm', 'output', 'layers.15', 'layers.14'}
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 2025
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 02:51:54 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=42 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=2e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:2025-11-08 02:52:00,424 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank3]:2025-11-08 02:52:00,425 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-08 02:52:00,514 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-08 02:52:00,668 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-08 02:52:00,740 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 02:52:00,742 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 02:52:00,724 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 02:52:00,726 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 02:52:00,795 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 02:52:00,798 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 02:52:00,802 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 02:52:00,803 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-08 02:52:00,907 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 02:52:00,910 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 02:52:01,200 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 02:52:03,956 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-08 02:52:04,083 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 02:52:04,104 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 02:52:04,144 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 02:52:04,145 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-08 02:52:04,171 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-08 02:52:04,171 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-08 02:52:04,121 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 02:52:04,148 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-08 02:52:04,148 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-08 02:52:04,168 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 02:52:04,209 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 02:52:04,238 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-08 02:52:04,238 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-08 02:52:04,360 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 02:52:04,360 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 02:52:04,361 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-08 02:52:04,331 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 02:52:04,331 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 02:52:04,331 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-08 02:52:04,444 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 02:52:04,444 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 02:52:04,445 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run zfohkr70
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0252/wandb/run-20251108_025205-zfohkr70
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/zfohkr70
[rank3]:2025-11-08 02:52:06,235 - INFO - WandB logging enabled
[rank3]:2025-11-08 02:52:06,236 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 02:52:06,272 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 02:52:06,300 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-08 02:52:06,300 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-08 02:52:06,484 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 02:52:06,485 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 02:52:06,485 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 02:52:06,500 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 02:52:06,501 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 02:52:06,501 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 02:52:06,502 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 02:52:06,502 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank0]:2025-11-08 02:52:06,502 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800.
[rank2]:2025-11-08 02:52:06,501 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-08 02:52:06,502 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank3]:2025-11-08 02:52:06,500 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank1]:2025-11-08 02:52:06,502 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-08 02:52:06,504 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank0]:2025-11-08 02:52:09,009 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 02:52:09,009 - INFO - Finished loading the checkpoint in 2.51 seconds.
[rank0]:2025-11-08 02:52:09,009 - INFO - Training starts at step 1
[rank3]:2025-11-08 02:52:12,096 - INFO -  step:  1  loss:  0.3946  grad_norm:  0.2337  memory: 24.19GiB(50.91%)  tps: 2,815  tflops: 21.44  mfu: 6.87%
[rank3]:2025-11-08 02:52:12,096 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-08 02:52:12,085 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2337  memory:  9.99GiB(21.03%)  tps: 2,057  tflops: 15.67  mfu: 5.02%
[rank2]:2025-11-08 02:52:12,086 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 02:52:12,091 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2337  memory: 12.38GiB(26.05%)  tps: 2,079  tflops: 15.83  mfu: 5.07%
[rank1]:2025-11-08 02:52:12,091 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 02:52:12,123 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2337  memory: 12.80GiB(26.95%)  tps: 2,053  tflops: 15.64  mfu: 5.01%
[rank0]:2025-11-08 02:52:12,123 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 02:54:14,618 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:54:16,895 - INFO - Avg. fwd time: 11.2532 / Avg. bwd time: 44.5573 / Avg. batch time: 506.3840 (ms) / GPU bubble ratio: 11.83%
[rank2]:2025-11-08 02:54:16,967 - INFO - Avg. fwd time: 7.1236 / Avg. bwd time: 18.8346 / Avg. batch time: 538.3791 (ms) / GPU bubble ratio: 61.43%
[rank1]:2025-11-08 02:54:17,007 - INFO - Avg. fwd time: 9.0375 / Avg. bwd time: 23.9844 / Avg. batch time: 577.7963 (ms) / GPU bubble ratio: 54.28%
[rank0]:2025-11-08 02:54:17,015 - INFO - Avg. fwd time: 7.8529 / Avg. bwd time: 23.4298 / Avg. batch time: 614.8035 (ms) / GPU bubble ratio: 59.29%
[rank3]:2025-11-08 02:54:17,203 - INFO -  step: 50  loss:  0.3716  grad_norm:  0.2778  memory: 26.98GiB(56.79%)  tps: 6,417  tflops: 48.87  mfu: 15.66%
[rank2]:2025-11-08 02:54:17,191 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2778  memory: 11.81GiB(24.85%)  tps: 6,417  tflops: 48.87  mfu: 15.66%
[rank1]:2025-11-08 02:54:17,194 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2778  memory: 14.64GiB(30.82%)  tps: 6,417  tflops: 48.87  mfu: 15.67%
[rank0]:2025-11-08 02:54:17,205 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2778  memory: 16.57GiB(34.88%)  tps: 6,418  tflops: 48.88  mfu: 15.67%
[rank0]:2025-11-08 02:56:23,965 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:56:26,288 - INFO - Avg. fwd time: 11.3502 / Avg. bwd time: 45.1052 / Avg. batch time: 511.4264 (ms) / GPU bubble ratio: 11.69%
[rank2]:2025-11-08 02:56:26,364 - INFO - Avg. fwd time: 7.1364 / Avg. bwd time: 18.8901 / Avg. batch time: 542.9775 (ms) / GPU bubble ratio: 61.65%
[rank0]:2025-11-08 02:56:26,412 - INFO - Avg. fwd time: 7.8676 / Avg. bwd time: 23.4771 / Avg. batch time: 618.6781 (ms) / GPU bubble ratio: 59.47%
[rank1]:2025-11-08 02:56:26,404 - INFO - Avg. fwd time: 9.0691 / Avg. bwd time: 24.0724 / Avg. batch time: 582.0491 (ms) / GPU bubble ratio: 54.45%
[rank3]:2025-11-08 02:56:26,601 - INFO -  step: 100  loss:  0.3975  grad_norm:  0.2860  memory: 26.98GiB(56.79%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank2]:2025-11-08 02:56:26,588 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2860  memory: 11.81GiB(24.85%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank0]:2025-11-08 02:56:26,603 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2860  memory: 16.57GiB(34.88%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank1]:2025-11-08 02:56:26,592 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2860  memory: 14.64GiB(30.82%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank3]:2025-11-08 02:56:26,775 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0256_real_step100_rank3.svg
[rank3]:> Batch Time: 622.20 ms, GPU Bubble Ratio: 59.32%, 57.12%, 66.33%, 26.57%
[rank0]:2025-11-08 02:58:33,878 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:58:36,414 - INFO - Avg. fwd time: 7.1438 / Avg. bwd time: 18.9167 / Avg. batch time: 545.3030 (ms) / GPU bubble ratio: 61.77%
[rank1]:2025-11-08 02:58:36,444 - INFO - Avg. fwd time: 9.0792 / Avg. bwd time: 24.1107 / Avg. batch time: 584.2691 (ms) / GPU bubble ratio: 54.56%
[rank3]:2025-11-08 02:58:36,388 - INFO - Avg. fwd time: 11.3994 / Avg. bwd time: 45.3347 / Avg. batch time: 513.6314 (ms) / GPU bubble ratio: 11.63%
[rank0]:2025-11-08 02:58:36,474 - INFO - Avg. fwd time: 7.8800 / Avg. bwd time: 23.5003 / Avg. batch time: 620.8077 (ms) / GPU bubble ratio: 59.56%
[rank0]:2025-11-08 02:58:36,510 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2905  memory: 16.57GiB(34.88%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank2]:2025-11-08 02:58:36,495 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2905  memory: 11.81GiB(24.85%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank1]:2025-11-08 02:58:36,499 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2905  memory: 14.64GiB(30.82%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank3]:2025-11-08 02:58:36,508 - INFO -  step: 150  loss:  0.3708  grad_norm:  0.2905  memory: 26.98GiB(56.79%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank0]:2025-11-08 03:00:43,855 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:00:46,192 - INFO - Avg. fwd time: 11.4205 / Avg. bwd time: 45.4510 / Avg. batch time: 514.7075 (ms) / GPU bubble ratio: 11.61%
[rank2]:2025-11-08 03:00:46,267 - INFO - Avg. fwd time: 7.1476 / Avg. bwd time: 18.9300 / Avg. batch time: 546.2935 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-08 03:00:46,307 - INFO - Avg. fwd time: 9.0851 / Avg. bwd time: 24.1252 / Avg. batch time: 585.1916 (ms) / GPU bubble ratio: 54.60%
[rank0]:2025-11-08 03:00:46,315 - INFO - Avg. fwd time: 7.8834 / Avg. bwd time: 23.5101 / Avg. batch time: 621.6948 (ms) / GPU bubble ratio: 59.60%
[rank2]:2025-11-08 03:00:46,491 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3068  memory: 11.81GiB(24.85%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank0]:2025-11-08 03:00:46,506 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3068  memory: 16.57GiB(34.88%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank1]:2025-11-08 03:00:46,495 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3068  memory: 14.64GiB(30.82%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank3]:2025-11-08 03:00:46,504 - INFO -  step: 200  loss:  0.4435  grad_norm:  0.3068  memory: 26.98GiB(56.79%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank3]:2025-11-08 03:00:46,664 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0300_real_step200_rank3.svg
[rank3]:> Batch Time: 625.19 ms, GPU Bubble Ratio: 59.48%, 57.34%, 66.46%, 26.41%
[rank0]:2025-11-08 03:02:54,283 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:02:56,604 - INFO - Avg. fwd time: 11.4457 / Avg. bwd time: 45.5524 / Avg. batch time: 515.7177 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-08 03:02:56,680 - INFO - Avg. fwd time: 7.1519 / Avg. bwd time: 18.9407 / Avg. batch time: 547.3761 (ms) / GPU bubble ratio: 61.87%
[rank0]:2025-11-08 03:02:56,727 - INFO - Avg. fwd time: 7.8853 / Avg. bwd time: 23.5180 / Avg. batch time: 622.7167 (ms) / GPU bubble ratio: 59.66%
[rank1]:2025-11-08 03:02:56,720 - INFO - Avg. fwd time: 9.0920 / Avg. bwd time: 24.1360 / Avg. batch time: 586.2389 (ms) / GPU bubble ratio: 54.66%
[rank3]:2025-11-08 03:02:56,916 - INFO -  step: 250  loss:  0.4660  grad_norm:  0.3200  memory: 26.98GiB(56.79%)  tps: 6,282  tflops: 47.84  mfu: 15.33%
[rank2]:2025-11-08 03:02:56,903 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3200  memory: 11.81GiB(24.85%)  tps: 6,282  tflops: 47.84  mfu: 15.33%
[rank0]:2025-11-08 03:02:56,918 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3200  memory: 16.57GiB(34.88%)  tps: 6,282  tflops: 47.84  mfu: 15.33%
[rank1]:2025-11-08 03:02:56,907 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3200  memory: 14.64GiB(30.82%)  tps: 6,282  tflops: 47.84  mfu: 15.33%
[rank0]:2025-11-08 03:05:04,413 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:05:06,946 - INFO - Avg. fwd time: 11.4583 / Avg. bwd time: 45.6146 / Avg. batch time: 516.3109 (ms) / GPU bubble ratio: 11.57%
[rank0]:2025-11-08 03:05:07,033 - INFO - Avg. fwd time: 7.8886 / Avg. bwd time: 23.5240 / Avg. batch time: 623.2048 (ms) / GPU bubble ratio: 59.68%
[rank2]:2025-11-08 03:05:06,973 - INFO - Avg. fwd time: 7.1545 / Avg. bwd time: 18.9477 / Avg. batch time: 547.9203 (ms) / GPU bubble ratio: 61.89%
[rank2]:2025-11-08 03:05:07,055 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3077  memory: 11.81GiB(24.85%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank1]:2025-11-08 03:05:07,003 - INFO - Avg. fwd time: 9.0957 / Avg. bwd time: 24.1407 / Avg. batch time: 586.7464 (ms) / GPU bubble ratio: 54.68%
[rank1]:2025-11-08 03:05:07,058 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3077  memory: 14.64GiB(30.82%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank3]:2025-11-08 03:05:07,068 - INFO -  step: 300  loss:  0.4694  grad_norm:  0.3077  memory: 26.98GiB(56.79%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank0]:2025-11-08 03:05:07,069 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3077  memory: 16.57GiB(34.88%)  tps: 6,294  tflops: 47.94  mfu: 15.36%
[rank3]:2025-11-08 03:05:07,218 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0305_real_step300_rank3.svg
[rank3]:> Batch Time: 624.19 ms, GPU Bubble Ratio: 59.42%, 57.22%, 66.38%, 26.45%
[rank0]:2025-11-08 03:07:14,529 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:07:16,833 - INFO - Avg. fwd time: 11.4633 / Avg. bwd time: 45.6379 / Avg. batch time: 516.5304 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-08 03:07:16,909 - INFO - Avg. fwd time: 7.1562 / Avg. bwd time: 18.9525 / Avg. batch time: 548.1912 (ms) / GPU bubble ratio: 61.90%
[rank0]:2025-11-08 03:07:16,957 - INFO - Avg. fwd time: 7.8885 / Avg. bwd time: 23.5276 / Avg. batch time: 623.4413 (ms) / GPU bubble ratio: 59.69%
[rank1]:2025-11-08 03:07:16,949 - INFO - Avg. fwd time: 9.0985 / Avg. bwd time: 24.1429 / Avg. batch time: 586.9951 (ms) / GPU bubble ratio: 54.70%
[rank1]:2025-11-08 03:07:17,136 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3250  memory: 14.64GiB(30.82%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank3]:2025-11-08 03:07:17,145 - INFO -  step: 350  loss:  0.4529  grad_norm:  0.3250  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank2]:2025-11-08 03:07:17,132 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3250  memory: 11.81GiB(24.85%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank0]:2025-11-08 03:07:17,147 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3250  memory: 16.57GiB(34.88%)  tps: 6,298  tflops: 47.97  mfu: 15.37%
[rank0]:2025-11-08 03:09:24,569 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:09:26,885 - INFO - Avg. fwd time: 11.4621 / Avg. bwd time: 45.6730 / Avg. batch time: 516.7994 (ms) / GPU bubble ratio: 11.56%
[rank2]:2025-11-08 03:09:26,960 - INFO - Avg. fwd time: 7.1577 / Avg. bwd time: 18.9562 / Avg. batch time: 548.4254 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-08 03:09:27,000 - INFO - Avg. fwd time: 9.1013 / Avg. bwd time: 24.1437 / Avg. batch time: 587.2081 (ms) / GPU bubble ratio: 54.71%
[rank0]:2025-11-08 03:09:27,007 - INFO - Avg. fwd time: 7.8884 / Avg. bwd time: 23.5293 / Avg. batch time: 623.6402 (ms) / GPU bubble ratio: 59.70%
[rank0]:2025-11-08 03:09:27,199 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3102  memory: 16.57GiB(34.88%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank1]:2025-11-08 03:09:27,188 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3102  memory: 14.64GiB(30.82%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank3]:2025-11-08 03:09:27,196 - INFO -  step: 400  loss:  0.4553  grad_norm:  0.3102  memory: 26.98GiB(56.79%)  tps: 6,299  tflops: 47.98  mfu: 15.38%
[rank2]:2025-11-08 03:09:27,184 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3102  memory: 11.81GiB(24.85%)  tps: 6,299  tflops: 47.97  mfu: 15.38%
[rank3]:2025-11-08 03:09:27,346 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0309_real_step400_rank3.svg
[rank3]:> Batch Time: 625.16 ms, GPU Bubble Ratio: 59.53%, 57.33%, 66.44%, 26.43%
[rank3]:2025-11-08 03:09:36,115 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-08 03:09:36,337 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 03:09:36,388 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-08 03:09:36,363 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 03:11:34,290 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:11:36,838 - INFO - Avg. fwd time: 7.1573 / Avg. bwd time: 18.9570 / Avg. batch time: 548.4762 (ms) / GPU bubble ratio: 61.91%
[rank3]:2025-11-08 03:11:36,811 - INFO - Avg. fwd time: 11.4604 / Avg. bwd time: 45.6782 / Avg. batch time: 516.8217 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-08 03:11:36,869 - INFO - Avg. fwd time: 9.1001 / Avg. bwd time: 24.1399 / Avg. batch time: 587.2375 (ms) / GPU bubble ratio: 54.72%
[rank2]:2025-11-08 03:11:36,921 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2965  memory: 11.81GiB(24.85%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-08 03:11:36,934 - INFO -  step: 450  loss:  0.4664  grad_norm:  0.2965  memory: 26.98GiB(56.79%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-08 03:11:36,900 - INFO - Avg. fwd time: 7.8870 / Avg. bwd time: 23.5288 / Avg. batch time: 623.6583 (ms) / GPU bubble ratio: 59.70%
[rank0]:2025-11-08 03:11:36,936 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2965  memory: 16.57GiB(34.88%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-08 03:11:36,925 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2965  memory: 14.64GiB(30.82%)  tps: 6,314  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-08 03:13:43,877 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:13:46,203 - INFO - Avg. fwd time: 11.4599 / Avg. bwd time: 45.6753 / Avg. batch time: 516.7903 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-08 03:13:46,277 - INFO - Avg. fwd time: 7.1573 / Avg. bwd time: 18.9576 / Avg. batch time: 548.4113 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-08 03:13:46,316 - INFO - Avg. fwd time: 9.0995 / Avg. bwd time: 24.1375 / Avg. batch time: 587.1567 (ms) / GPU bubble ratio: 54.71%
[rank0]:2025-11-08 03:13:46,324 - INFO - Avg. fwd time: 7.8859 / Avg. bwd time: 23.5287 / Avg. batch time: 623.5676 (ms) / GPU bubble ratio: 59.70%
[rank2]:2025-11-08 03:13:46,496 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2487  memory: 11.81GiB(24.85%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-08 03:13:46,509 - INFO -  step: 500  loss:  0.3004  grad_norm:  0.2487  memory: 26.98GiB(56.79%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank1]:2025-11-08 03:13:46,500 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2487  memory: 14.64GiB(30.82%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-08 03:13:46,511 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2487  memory: 16.57GiB(34.88%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-08 03:13:46,661 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0313_real_step500_rank3.svg
[rank3]:> Batch Time: 621.64 ms, GPU Bubble Ratio: 59.32%, 57.14%, 66.27%, 26.47%
[rank0]:2025-11-08 03:15:53,788 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:15:56,085 - INFO - Avg. fwd time: 11.4649 / Avg. bwd time: 45.6751 / Avg. batch time: 516.8293 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-08 03:15:56,158 - INFO - Avg. fwd time: 7.1583 / Avg. bwd time: 18.9587 / Avg. batch time: 548.4767 (ms) / GPU bubble ratio: 61.91%
[rank0]:2025-11-08 03:15:56,206 - INFO - Avg. fwd time: 7.8861 / Avg. bwd time: 23.5293 / Avg. batch time: 623.6216 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-08 03:15:56,198 - INFO - Avg. fwd time: 9.1019 / Avg. bwd time: 24.1369 / Avg. batch time: 587.2173 (ms) / GPU bubble ratio: 54.72%
[rank1]:2025-11-08 03:15:56,380 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3244  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank3]:2025-11-08 03:15:56,388 - INFO -  step: 550  loss:  0.3680  grad_norm:  0.3244  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank2]:2025-11-08 03:15:56,376 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3244  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-08 03:15:56,390 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3244  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.04  mfu: 15.40%
[rank0]:2025-11-08 03:18:03,677 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:18:06,212 - INFO - Avg. fwd time: 11.4687 / Avg. bwd time: 45.6816 / Avg. batch time: 516.9126 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-08 03:18:06,239 - INFO - Avg. fwd time: 7.1591 / Avg. bwd time: 18.9601 / Avg. batch time: 548.5353 (ms) / GPU bubble ratio: 61.91%
[rank0]:2025-11-08 03:18:06,302 - INFO - Avg. fwd time: 7.8860 / Avg. bwd time: 23.5299 / Avg. batch time: 623.6714 (ms) / GPU bubble ratio: 59.70%
[rank1]:2025-11-08 03:18:06,271 - INFO - Avg. fwd time: 9.1033 / Avg. bwd time: 24.1361 / Avg. batch time: 587.2741 (ms) / GPU bubble ratio: 54.72%
[rank1]:2025-11-08 03:18:06,327 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3274  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-08 03:18:06,336 - INFO -  step: 600  loss:  0.3749  grad_norm:  0.3274  memory: 26.98GiB(56.79%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank2]:2025-11-08 03:18:06,323 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3274  memory: 11.81GiB(24.85%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-08 03:18:06,338 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3274  memory: 16.57GiB(34.88%)  tps: 6,304  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-08 03:18:06,484 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0318_real_step600_rank3.svg
[rank3]:> Batch Time: 624.11 ms, GPU Bubble Ratio: 59.46%, 57.27%, 66.40%, 26.53%
[rank0]:2025-11-08 03:20:13,625 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:20:15,927 - INFO - Avg. fwd time: 11.4706 / Avg. bwd time: 45.6823 / Avg. batch time: 516.9312 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-08 03:20:16,040 - INFO - Avg. fwd time: 9.1036 / Avg. bwd time: 24.1339 / Avg. batch time: 587.3094 (ms) / GPU bubble ratio: 54.73%
[rank2]:2025-11-08 03:20:16,000 - INFO - Avg. fwd time: 7.1593 / Avg. bwd time: 18.9598 / Avg. batch time: 548.5735 (ms) / GPU bubble ratio: 61.91%
[rank0]:2025-11-08 03:20:16,047 - INFO - Avg. fwd time: 7.8857 / Avg. bwd time: 23.5293 / Avg. batch time: 623.7005 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-08 03:20:16,220 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3153  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank3]:2025-11-08 03:20:16,229 - INFO -  step: 650  loss:  0.3213  grad_norm:  0.3153  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank2]:2025-11-08 03:20:16,216 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3153  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank0]:2025-11-08 03:20:16,231 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3153  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank0]:2025-11-08 03:22:23,487 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:22:25,810 - INFO - Avg. fwd time: 11.4729 / Avg. bwd time: 45.6865 / Avg. batch time: 516.9835 (ms) / GPU bubble ratio: 11.55%
[rank2]:2025-11-08 03:22:25,887 - INFO - Avg. fwd time: 7.1596 / Avg. bwd time: 18.9597 / Avg. batch time: 548.6006 (ms) / GPU bubble ratio: 61.91%
[rank0]:2025-11-08 03:22:25,935 - INFO - Avg. fwd time: 7.8860 / Avg. bwd time: 23.5288 / Avg. batch time: 623.7216 (ms) / GPU bubble ratio: 59.71%
[rank1]:2025-11-08 03:22:25,928 - INFO - Avg. fwd time: 9.1044 / Avg. bwd time: 24.1321 / Avg. batch time: 587.3387 (ms) / GPU bubble ratio: 54.73%
[rank1]:2025-11-08 03:22:26,113 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3821  memory: 14.64GiB(30.82%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank2]:2025-11-08 03:22:26,110 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3821  memory: 11.81GiB(24.85%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank0]:2025-11-08 03:22:26,124 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3821  memory: 16.57GiB(34.88%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank3]:2025-11-08 03:22:26,122 - INFO -  step: 700  loss:  0.3964  grad_norm:  0.3821  memory: 26.98GiB(56.79%)  tps: 6,307  tflops: 48.03  mfu: 15.40%
[rank3]:2025-11-08 03:22:26,289 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0322_real_step700_rank3.svg
[rank3]:> Batch Time: 624.12 ms, GPU Bubble Ratio: 59.47%, 57.30%, 66.41%, 26.49%
[rank0]:2025-11-08 03:24:33,745 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:24:36,289 - INFO - Avg. fwd time: 7.1600 / Avg. bwd time: 18.9604 / Avg. batch time: 548.7465 (ms) / GPU bubble ratio: 61.92%
[rank3]:2025-11-08 03:24:36,261 - INFO - Avg. fwd time: 11.4765 / Avg. bwd time: 45.6980 / Avg. batch time: 517.1053 (ms) / GPU bubble ratio: 11.55%
[rank0]:2025-11-08 03:24:36,354 - INFO - Avg. fwd time: 7.8850 / Avg. bwd time: 23.5291 / Avg. batch time: 623.8699 (ms) / GPU bubble ratio: 59.72%
[rank1]:2025-11-08 03:24:36,322 - INFO - Avg. fwd time: 9.1052 / Avg. bwd time: 24.1330 / Avg. batch time: 587.4929 (ms) / GPU bubble ratio: 54.74%
[rank1]:2025-11-08 03:24:36,379 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3952  memory: 14.64GiB(30.82%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank2]:2025-11-08 03:24:36,376 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3952  memory: 11.81GiB(24.85%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank3]:2025-11-08 03:24:36,389 - INFO -  step: 750  loss:  0.3725  grad_norm:  0.3952  memory: 26.98GiB(56.79%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-08 03:24:36,390 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3952  memory: 16.57GiB(34.88%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-08 03:26:43,687 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:26:46,024 - INFO - Avg. fwd time: 11.4783 / Avg. bwd time: 45.7021 / Avg. batch time: 517.1526 (ms) / GPU bubble ratio: 11.55%
[rank1]:2025-11-08 03:26:46,138 - INFO - Avg. fwd time: 9.1062 / Avg. bwd time: 24.1348 / Avg. batch time: 587.5283 (ms) / GPU bubble ratio: 54.74%
[rank2]:2025-11-08 03:26:46,098 - INFO - Avg. fwd time: 7.1604 / Avg. bwd time: 18.9610 / Avg. batch time: 548.7745 (ms) / GPU bubble ratio: 61.92%
[rank0]:2025-11-08 03:26:46,145 - INFO - Avg. fwd time: 7.8848 / Avg. bwd time: 23.5293 / Avg. batch time: 623.9003 (ms) / GPU bubble ratio: 59.72%
[rank1]:2025-11-08 03:26:46,321 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3783  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank1]:2025-11-08 03:26:46,321 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3783  tps: 6,689  tflops: 50.94  mfu: 14.80%
[rank1]:2025-11-08 03:26:46,321 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 03:26:46,322 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank1]:  warnings.warn(
[rank2]:2025-11-08 03:26:46,317 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3783  memory: 11.81GiB(24.85%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-08 03:26:46,317 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3783  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank2]:2025-11-08 03:26:46,317 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 03:26:46,318 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank2]:  warnings.warn(
[rank3]:2025-11-08 03:26:46,329 - INFO -  step: 800  loss:  0.4011  grad_norm:  0.3783  memory: 26.98GiB(56.79%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-08 03:26:46,330 - INFO -  final step: 800  loss:  0.4011  grad_norm:  0.3783  tps: 6,695  tflops: 50.99  mfu: 14.90%
[rank3]:2025-11-08 03:26:46,331 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 03:26:46,332 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank3]:  warnings.warn(
[rank0]:2025-11-08 03:26:46,331 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3783  memory: 16.57GiB(34.88%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-08 03:26:46,331 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3783  tps: 6,688  tflops: 50.94  mfu: 14.80%
[rank0]:2025-11-08 03:26:46,331 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 03:26:46,332 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank0]:  warnings.warn(
[rank0]:2025-11-08 03:26:48,364 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 03:26:48,376 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-08 03:26:48,376 - INFO - Destroying the purge thread.
[rank2]:2025-11-08 03:26:48,376 - INFO - Destroying the purge thread.
[rank1]:2025-11-08 03:26:48,547 - INFO - Process group destroyed
[rank2]:2025-11-08 03:26:48,557 - INFO - Process group destroyed
[rank3]:2025-11-08 03:26:48,519 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0326_real_final800_rank3.svg
[rank3]:> Batch Time: 623.64 ms, GPU Bubble Ratio: 59.42%, 57.23%, 66.38%, 26.53%
[rank3]:2025-11-08 03:26:48,660 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0326_thry_final800_rank3.svg
[rank3]:> Batch Time: 294.15 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 03:26:48,661 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 227-238
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–â–ƒâ–ƒâ–ƒâ–„â–…â–„â–…â–„â–„â–‚â–…â–…â–…â–‡â–ˆâ–‡
[rank3]:wandb: loss_metrics/global_avg_loss â–…â–„â–…â–„â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–â–„â–„â–‚â–…â–„â–…
[rank3]:wandb: loss_metrics/global_max_loss â–…â–„â–…â–„â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–â–„â–„â–‚â–…â–„â–…
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44708
[rank3]:wandb:               final/avg_loss 0.40113
[rank3]:wandb:             final/avg_mfu(%) 14.90492
[rank3]:wandb:             final/avg_tflops 50.99297
[rank3]:wandb:    final/avg_throughput(tps) 6695.31821
[rank3]:wandb:              final/grad_norm 0.37834
[rank3]:wandb:               final/max_loss 0.40113
[rank3]:wandb:                    grad_norm 0.37834
[rank3]:wandb: loss_metrics/global_avg_loss 0.40113
[rank3]:wandb: loss_metrics/global_max_loss 0.40113
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/zfohkr70
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0252/wandb/run-20251108_025205-zfohkr70/logs
[rank3]:2025-11-08 03:26:50,044 - INFO - Process group destroyed
[rank0]:2025-11-08 03:26:50,376 - INFO - Training completed
[rank0]:2025-11-08 03:26:50,377 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 03:26:50,497 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.1', 'tok_embeddings', 'layers.0', 'layers.2', 'layers.3'}
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.11', 'layers.10', 'layers.12'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.6', 'layers.5', 'layers.8', 'layers.7'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.14', 'layers.13', 'layers.15', 'output'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
