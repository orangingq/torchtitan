
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 00:42:50 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed2025_alpaca800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=alpaca --training.steps=800 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank2]:2025-11-08 00:42:56,750 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank1]:2025-11-08 00:42:56,817 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-08 00:42:56,750 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-08 00:42:56,872 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank2]:2025-11-08 00:42:56,987 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 00:42:56,989 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 00:42:56,994 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 00:42:56,996 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-08 00:42:57,104 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 00:42:57,107 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 00:42:57,162 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 00:42:57,164 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 00:42:57,168 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 00:42:57,169 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-08 00:42:57,534 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank2]:2025-11-08 00:43:00,391 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 00:43:00,438 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-08 00:43:00,450 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 00:43:00,479 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-08 00:43:00,479 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-08 00:43:00,567 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 00:43:00,605 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 00:43:00,633 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-08 00:43:00,633 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-08 00:43:00,591 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 00:43:00,631 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 00:43:00,633 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-08 00:43:00,662 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 00:43:00,662 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 00:43:00,663 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-08 00:43:00,659 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-08 00:43:00,660 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-08 00:43:00,839 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 00:43:00,839 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 00:43:00,840 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-08 00:43:00,854 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 00:43:00,854 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 00:43:00,855 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run xyvbudnz
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/20251108-0043/wandb/run-20251108_004301-xyvbudnz
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/xyvbudnz
[rank3]:2025-11-08 00:43:02,606 - INFO - WandB logging enabled
[rank3]:2025-11-08 00:43:02,606 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 00:43:02,645 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 00:43:02,674 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-08 00:43:02,675 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-08 00:43:02,884 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-08 00:43:02,884 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-08 00:43:02,866 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 00:43:02,867 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 00:43:02,868 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 00:43:02,883 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 00:43:02,883 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank0]:2025-11-08 00:43:02,884 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 00:43:02,884 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 00:43:02,884 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank0]:2025-11-08 00:43:05,415 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 00:43:05,415 - INFO - Finished loading the checkpoint in 2.53 seconds.
[rank0]:2025-11-08 00:43:05,416 - INFO - Training starts at step 1
[rank1]:2025-11-08 00:43:08,514 - INFO -  step:  1  loss: -4.0000  grad_norm: 174.8880  memory: 12.38GiB(26.05%)  tps: 2,072  tflops: 15.78  mfu: 5.06%
[rank1]:2025-11-08 00:43:08,515 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-08 00:43:08,511 - INFO -  step:  1  loss: -4.0000  grad_norm: 174.8880  memory:  9.99GiB(21.03%)  tps: 2,033  tflops: 15.48  mfu: 4.96%
[rank2]:2025-11-08 00:43:08,512 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 00:43:08,521 - INFO -  step:  1  loss: 10.3018  grad_norm: 174.8880  memory: 24.19GiB(50.91%)  tps: 2,789  tflops: 21.24  mfu: 6.81%
[rank3]:2025-11-08 00:43:08,522 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 00:43:08,550 - INFO -  step:  1  loss: -4.0000  grad_norm: 174.8880  memory: 12.80GiB(26.95%)  tps: 2,069  tflops: 15.76  mfu: 5.05%
[rank0]:2025-11-08 00:43:08,550 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 00:45:09,520 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:45:11,762 - INFO - Avg. fwd time: 11.1225 / Avg. bwd time: 44.1001 / Avg. batch time: 501.6749 (ms) / GPU bubble ratio: 11.94%
[rank0]:2025-11-08 00:45:11,881 - INFO - Avg. fwd time: 7.7762 / Avg. bwd time: 23.3199 / Avg. batch time: 609.5470 (ms) / GPU bubble ratio: 59.19%
[rank2]:2025-11-08 00:45:11,834 - INFO - Avg. fwd time: 7.0949 / Avg. bwd time: 18.7282 / Avg. batch time: 533.4851 (ms) / GPU bubble ratio: 61.28%
[rank1]:2025-11-08 00:45:11,873 - INFO - Avg. fwd time: 8.9993 / Avg. bwd time: 23.7711 / Avg. batch time: 572.5695 (ms) / GPU bubble ratio: 54.21%
[rank0]:2025-11-08 00:45:12,068 - INFO -  step: 50  loss: -4.0000  grad_norm: 20.8036  memory: 16.57GiB(34.88%)  tps: 6,500  tflops: 49.50  mfu: 15.87%
[rank2]:2025-11-08 00:45:12,053 - INFO -  step: 50  loss: -4.0000  grad_norm: 20.8036  memory: 11.81GiB(24.85%)  tps: 6,498  tflops: 49.49  mfu: 15.86%
[rank1]:2025-11-08 00:45:12,057 - INFO -  step: 50  loss: -4.0000  grad_norm: 20.8036  memory: 14.64GiB(30.82%)  tps: 6,498  tflops: 49.49  mfu: 15.86%
[rank3]:2025-11-08 00:45:12,066 - INFO -  step: 50  loss:  5.3698  grad_norm: 20.8036  memory: 26.98GiB(56.79%)  tps: 6,498  tflops: 49.49  mfu: 15.86%
[rank0]:2025-11-08 00:47:16,837 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:47:19,111 - INFO - Avg. fwd time: 11.1541 / Avg. bwd time: 44.5677 / Avg. batch time: 505.5400 (ms) / GPU bubble ratio: 11.82%
[rank2]:2025-11-08 00:47:19,186 - INFO - Avg. fwd time: 7.0912 / Avg. bwd time: 18.8183 / Avg. batch time: 536.9704 (ms) / GPU bubble ratio: 61.40%
[rank0]:2025-11-08 00:47:19,233 - INFO - Avg. fwd time: 7.7875 / Avg. bwd time: 23.4045 / Avg. batch time: 612.2990 (ms) / GPU bubble ratio: 59.25%
[rank1]:2025-11-08 00:47:19,226 - INFO - Avg. fwd time: 9.0081 / Avg. bwd time: 23.8942 / Avg. batch time: 575.7399 (ms) / GPU bubble ratio: 54.28%
[rank2]:2025-11-08 00:47:19,410 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2563  memory: 11.81GiB(24.85%)  tps: 6,432  tflops: 48.99  mfu: 15.70%
[rank0]:2025-11-08 00:47:19,424 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2563  memory: 16.57GiB(34.88%)  tps: 6,432  tflops: 48.99  mfu: 15.70%
[rank1]:2025-11-08 00:47:19,413 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2563  memory: 14.64GiB(30.82%)  tps: 6,432  tflops: 48.99  mfu: 15.70%
[rank3]:2025-11-08 00:47:19,422 - INFO -  step: 100  loss:  0.3001  grad_norm:  0.2563  memory: 26.98GiB(56.79%)  tps: 6,432  tflops: 48.99  mfu: 15.70%
[rank3]:2025-11-08 00:47:19,647 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0047_real_step100_rank3.svg
[rank3]:> Batch Time: 615.14 ms, GPU Bubble Ratio: 58.98%, 56.86%, 66.06%, 26.89%
[rank0]:2025-11-08 00:49:24,240 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:49:26,687 - INFO - Avg. fwd time: 11.1591 / Avg. bwd time: 44.6765 / Avg. batch time: 506.4089 (ms) / GPU bubble ratio: 11.79%
[rank2]:2025-11-08 00:49:26,713 - INFO - Avg. fwd time: 7.0933 / Avg. bwd time: 18.8571 / Avg. batch time: 538.0209 (ms) / GPU bubble ratio: 61.41%
[rank1]:2025-11-08 00:49:26,743 - INFO - Avg. fwd time: 9.0135 / Avg. bwd time: 23.9585 / Avg. batch time: 576.7343 (ms) / GPU bubble ratio: 54.26%
[rank3]:2025-11-08 00:49:26,807 - INFO -  step: 150  loss:  0.2966  grad_norm:  0.2362  memory: 26.98GiB(56.79%)  tps: 6,431  tflops: 48.98  mfu: 15.70%
[rank0]:2025-11-08 00:49:26,773 - INFO - Avg. fwd time: 7.7930 / Avg. bwd time: 23.4423 / Avg. batch time: 613.1742 (ms) / GPU bubble ratio: 59.25%
[rank0]:2025-11-08 00:49:26,808 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2362  memory: 16.57GiB(34.88%)  tps: 6,431  tflops: 48.98  mfu: 15.70%
[rank2]:2025-11-08 00:49:26,794 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2362  memory: 11.81GiB(24.85%)  tps: 6,431  tflops: 48.98  mfu: 15.70%
[rank1]:2025-11-08 00:49:26,798 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2362  memory: 14.64GiB(30.82%)  tps: 6,431  tflops: 48.98  mfu: 15.70%
[rank0]:2025-11-08 00:51:31,351 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:51:33,623 - INFO - Avg. fwd time: 11.1592 / Avg. bwd time: 44.7055 / Avg. batch time: 506.6275 (ms) / GPU bubble ratio: 11.79%
[rank2]:2025-11-08 00:51:33,695 - INFO - Avg. fwd time: 7.0940 / Avg. bwd time: 18.8709 / Avg. batch time: 538.1350 (ms) / GPU bubble ratio: 61.40%
[rank0]:2025-11-08 00:51:33,743 - INFO - Avg. fwd time: 7.7945 / Avg. bwd time: 23.4539 / Avg. batch time: 613.2041 (ms) / GPU bubble ratio: 59.23%
[rank1]:2025-11-08 00:51:33,735 - INFO - Avg. fwd time: 9.0124 / Avg. bwd time: 23.9861 / Avg. batch time: 576.8190 (ms) / GPU bubble ratio: 54.23%
[rank1]:2025-11-08 00:51:33,920 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2039  memory: 14.64GiB(30.82%)  tps: 6,444  tflops: 49.08  mfu: 15.73%
[rank3]:2025-11-08 00:51:33,929 - INFO -  step: 200  loss:  0.2644  grad_norm:  0.2039  memory: 26.98GiB(56.79%)  tps: 6,444  tflops: 49.08  mfu: 15.73%
[rank0]:2025-11-08 00:51:33,931 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2039  memory: 16.57GiB(34.88%)  tps: 6,444  tflops: 49.08  mfu: 15.73%
[rank2]:2025-11-08 00:51:33,917 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.2039  memory: 11.81GiB(24.85%)  tps: 6,444  tflops: 49.08  mfu: 15.73%
[rank3]:2025-11-08 00:51:34,093 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0051_real_step200_rank3.svg
[rank3]:> Batch Time: 615.61 ms, GPU Bubble Ratio: 59.03%, 56.88%, 66.09%, 26.89%
[rank0]:2025-11-08 00:53:39,329 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:53:41,586 - INFO - Avg. fwd time: 11.1647 / Avg. bwd time: 44.8072 / Avg. batch time: 507.4771 (ms) / GPU bubble ratio: 11.76%
[rank2]:2025-11-08 00:53:41,658 - INFO - Avg. fwd time: 7.0958 / Avg. bwd time: 18.8821 / Avg. batch time: 539.0425 (ms) / GPU bubble ratio: 61.45%
[rank0]:2025-11-08 00:53:41,706 - INFO - Avg. fwd time: 7.8006 / Avg. bwd time: 23.4639 / Avg. batch time: 614.0738 (ms) / GPU bubble ratio: 59.27%
[rank1]:2025-11-08 00:53:41,698 - INFO - Avg. fwd time: 9.0130 / Avg. bwd time: 24.0100 / Avg. batch time: 577.7178 (ms) / GPU bubble ratio: 54.27%
[rank2]:2025-11-08 00:53:41,879 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.2010  memory: 11.81GiB(24.85%)  tps: 6,402  tflops: 48.76  mfu: 15.63%
[rank1]:2025-11-08 00:53:41,883 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.2010  memory: 14.64GiB(30.82%)  tps: 6,402  tflops: 48.76  mfu: 15.63%
[rank3]:2025-11-08 00:53:41,892 - INFO -  step: 250  loss:  0.2805  grad_norm:  0.2010  memory: 26.98GiB(56.79%)  tps: 6,402  tflops: 48.76  mfu: 15.63%
[rank0]:2025-11-08 00:53:41,894 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.2010  memory: 16.57GiB(34.88%)  tps: 6,402  tflops: 48.76  mfu: 15.63%
[rank0]:2025-11-08 00:55:47,411 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 00:55:49,912 - INFO - Avg. fwd time: 11.1685 / Avg. bwd time: 44.9057 / Avg. batch time: 508.2903 (ms) / GPU bubble ratio: 11.74%
[rank2]:2025-11-08 00:55:49,938 - INFO - Avg. fwd time: 7.0969 / Avg. bwd time: 18.8909 / Avg. batch time: 539.7907 (ms) / GPU bubble ratio: 61.48%
[rank3]:2025-11-08 00:55:50,033 - INFO -  step: 300  loss:  0.2601  grad_norm:  0.1738  memory: 26.98GiB(56.79%)  tps: 6,393  tflops: 48.69  mfu: 15.61%
[rank0]:2025-11-08 00:55:49,999 - INFO - Avg. fwd time: 7.8051 / Avg. bwd time: 23.4724 / Avg. batch time: 614.8275 (ms) / GPU bubble ratio: 59.30%
[rank0]:2025-11-08 00:55:50,035 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.1738  memory: 16.57GiB(34.88%)  tps: 6,393  tflops: 48.69  mfu: 15.61%
[rank2]:2025-11-08 00:55:50,020 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.1738  memory: 11.81GiB(24.85%)  tps: 6,393  tflops: 48.69  mfu: 15.61%
[rank1]:2025-11-08 00:55:49,969 - INFO - Avg. fwd time: 9.0153 / Avg. bwd time: 24.0325 / Avg. batch time: 578.4822 (ms) / GPU bubble ratio: 54.30%
[rank1]:2025-11-08 00:55:50,024 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.1738  memory: 14.64GiB(30.82%)  tps: 6,393  tflops: 48.69  mfu: 15.61%
[rank3]:2025-11-08 00:55:50,186 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0055_real_step300_rank3.svg
[rank3]:> Batch Time: 618.62 ms, GPU Bubble Ratio: 59.19%, 56.99%, 66.21%, 26.60%
[rank0]:2025-11-08 00:57:55,173 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 00:57:57,500 - INFO - Avg. fwd time: 7.0968 / Avg. bwd time: 18.8941 / Avg. batch time: 539.9936 (ms) / GPU bubble ratio: 61.49%
[rank3]:2025-11-08 00:57:57,428 - INFO - Avg. fwd time: 11.1686 / Avg. bwd time: 44.9269 / Avg. batch time: 508.4524 (ms) / GPU bubble ratio: 11.74%
[rank0]:2025-11-08 00:57:57,547 - INFO - Avg. fwd time: 7.8070 / Avg. bwd time: 23.4741 / Avg. batch time: 615.0225 (ms) / GPU bubble ratio: 59.31%
[rank1]:2025-11-08 00:57:57,540 - INFO - Avg. fwd time: 9.0145 / Avg. bwd time: 24.0438 / Avg. batch time: 578.6902 (ms) / GPU bubble ratio: 54.30%
[rank2]:2025-11-08 00:57:57,719 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.1785  memory: 11.81GiB(24.85%)  tps: 6,415  tflops: 48.86  mfu: 15.66%
[rank0]:2025-11-08 00:57:57,734 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.1785  memory: 16.57GiB(34.88%)  tps: 6,415  tflops: 48.86  mfu: 15.66%
[rank3]:2025-11-08 00:57:57,731 - INFO -  step: 350  loss:  0.2660  grad_norm:  0.1785  memory: 26.98GiB(56.79%)  tps: 6,415  tflops: 48.86  mfu: 15.66%
[rank1]:2025-11-08 00:57:57,723 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.1785  memory: 14.64GiB(30.82%)  tps: 6,415  tflops: 48.86  mfu: 15.66%
[rank0]:2025-11-08 01:00:02,773 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:00:05,055 - INFO - Avg. fwd time: 11.1692 / Avg. bwd time: 44.9457 / Avg. batch time: 508.6010 (ms) / GPU bubble ratio: 11.73%
[rank2]:2025-11-08 01:00:05,131 - INFO - Avg. fwd time: 7.0963 / Avg. bwd time: 18.8968 / Avg. batch time: 540.0918 (ms) / GPU bubble ratio: 61.50%
[rank0]:2025-11-08 01:00:05,178 - INFO - Avg. fwd time: 7.8090 / Avg. bwd time: 23.4759 / Avg. batch time: 615.1090 (ms) / GPU bubble ratio: 59.31%
[rank1]:2025-11-08 01:00:05,170 - INFO - Avg. fwd time: 9.0128 / Avg. bwd time: 24.0539 / Avg. batch time: 578.7908 (ms) / GPU bubble ratio: 54.30%
[rank2]:2025-11-08 01:00:05,352 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1895  memory: 11.81GiB(24.85%)  tps: 6,418  tflops: 48.88  mfu: 15.67%
[rank0]:2025-11-08 01:00:05,366 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1895  memory: 16.57GiB(34.88%)  tps: 6,418  tflops: 48.88  mfu: 15.67%
[rank1]:2025-11-08 01:00:05,355 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1895  memory: 14.64GiB(30.82%)  tps: 6,418  tflops: 48.88  mfu: 15.67%
[rank3]:2025-11-08 01:00:05,364 - INFO -  step: 400  loss:  0.2675  grad_norm:  0.1895  memory: 26.98GiB(56.79%)  tps: 6,419  tflops: 48.88  mfu: 15.67%
[rank3]:2025-11-08 01:00:05,512 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0100_real_step400_rank3.svg
[rank3]:> Batch Time: 617.12 ms, GPU Bubble Ratio: 59.16%, 56.94%, 66.17%, 26.78%
[rank3]:2025-11-08 01:00:18,667 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-11-08 01:00:18,879 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-11-08 01:00:18,906 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-11-08 01:00:18,931 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-11-08 01:02:10,854 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:02:13,317 - INFO - Avg. fwd time: 11.1668 / Avg. bwd time: 44.9896 / Avg. batch time: 508.9296 (ms) / GPU bubble ratio: 11.73%
[rank2]:2025-11-08 01:02:13,344 - INFO - Avg. fwd time: 7.0963 / Avg. bwd time: 18.9009 / Avg. batch time: 540.4483 (ms) / GPU bubble ratio: 61.52%
[rank2]:2025-11-08 01:02:13,428 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.1753  memory: 11.81GiB(24.85%)  tps: 6,396  tflops: 48.71  mfu: 15.61%
[rank3]:2025-11-08 01:02:13,442 - INFO -  step: 450  loss:  0.2582  grad_norm:  0.1753  memory: 26.98GiB(56.79%)  tps: 6,396  tflops: 48.72  mfu: 15.61%
[rank0]:2025-11-08 01:02:13,407 - INFO - Avg. fwd time: 7.8105 / Avg. bwd time: 23.4790 / Avg. batch time: 615.4809 (ms) / GPU bubble ratio: 59.33%
[rank0]:2025-11-08 01:02:13,443 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.1753  memory: 16.57GiB(34.88%)  tps: 6,396  tflops: 48.71  mfu: 15.61%
[rank1]:2025-11-08 01:02:13,376 - INFO - Avg. fwd time: 9.0125 / Avg. bwd time: 24.0658 / Avg. batch time: 579.1702 (ms) / GPU bubble ratio: 54.31%
[rank1]:2025-11-08 01:02:13,432 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.1753  memory: 14.64GiB(30.82%)  tps: 6,396  tflops: 48.71  mfu: 15.61%
[rank0]:2025-11-08 01:04:19,094 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:04:21,396 - INFO - Avg. fwd time: 11.1667 / Avg. bwd time: 45.0309 / Avg. batch time: 509.2563 (ms) / GPU bubble ratio: 11.72%
[rank2]:2025-11-08 01:04:21,471 - INFO - Avg. fwd time: 7.0971 / Avg. bwd time: 18.9043 / Avg. batch time: 540.7474 (ms) / GPU bubble ratio: 61.53%
[rank0]:2025-11-08 01:04:21,519 - INFO - Avg. fwd time: 7.8111 / Avg. bwd time: 23.4815 / Avg. batch time: 615.7918 (ms) / GPU bubble ratio: 59.35%
[rank1]:2025-11-08 01:04:21,512 - INFO - Avg. fwd time: 9.0139 / Avg. bwd time: 24.0746 / Avg. batch time: 579.4860 (ms) / GPU bubble ratio: 54.32%
[rank0]:2025-11-08 01:04:21,706 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1902  memory: 16.57GiB(34.88%)  tps: 6,387  tflops: 48.64  mfu: 15.59%
[rank3]:2025-11-08 01:04:21,704 - INFO -  step: 500  loss:  0.2702  grad_norm:  0.1902  memory: 26.98GiB(56.79%)  tps: 6,387  tflops: 48.64  mfu: 15.59%
[rank1]:2025-11-08 01:04:21,695 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1902  memory: 14.64GiB(30.82%)  tps: 6,387  tflops: 48.64  mfu: 15.59%
[rank2]:2025-11-08 01:04:21,692 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1902  memory: 11.81GiB(24.85%)  tps: 6,387  tflops: 48.64  mfu: 15.59%
[rank3]:2025-11-08 01:04:21,854 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0104_real_step500_rank3.svg
[rank3]:> Batch Time: 620.16 ms, GPU Bubble Ratio: 59.34%, 57.10%, 66.31%, 26.77%
[rank0]:2025-11-08 01:06:27,512 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:06:29,792 - INFO - Avg. fwd time: 11.1673 / Avg. bwd time: 45.0705 / Avg. batch time: 509.5744 (ms) / GPU bubble ratio: 11.71%
[rank1]:2025-11-08 01:06:29,905 - INFO - Avg. fwd time: 9.0156 / Avg. bwd time: 24.0840 / Avg. batch time: 579.8456 (ms) / GPU bubble ratio: 54.33%
[rank2]:2025-11-08 01:06:29,864 - INFO - Avg. fwd time: 7.0978 / Avg. bwd time: 18.9078 / Avg. batch time: 541.0912 (ms) / GPU bubble ratio: 61.55%
[rank0]:2025-11-08 01:06:29,912 - INFO - Avg. fwd time: 7.8114 / Avg. bwd time: 23.4840 / Avg. batch time: 616.1456 (ms) / GPU bubble ratio: 59.37%
[rank0]:2025-11-08 01:06:30,098 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1928  memory: 16.57GiB(34.88%)  tps: 6,380  tflops: 48.59  mfu: 15.58%
[rank1]:2025-11-08 01:06:30,088 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1928  memory: 14.64GiB(30.82%)  tps: 6,380  tflops: 48.59  mfu: 15.58%
[rank2]:2025-11-08 01:06:30,084 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1928  memory: 11.81GiB(24.85%)  tps: 6,380  tflops: 48.60  mfu: 15.58%
[rank3]:2025-11-08 01:06:30,096 - INFO -  step: 550  loss:  0.3013  grad_norm:  0.1928  memory: 26.98GiB(56.79%)  tps: 6,381  tflops: 48.60  mfu: 15.58%
[rank0]:2025-11-08 01:08:35,777 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:08:38,285 - INFO - Avg. fwd time: 11.1675 / Avg. bwd time: 45.1087 / Avg. batch time: 509.8770 (ms) / GPU bubble ratio: 11.70%
[rank2]:2025-11-08 01:08:38,312 - INFO - Avg. fwd time: 7.0987 / Avg. bwd time: 18.9109 / Avg. batch time: 541.3698 (ms) / GPU bubble ratio: 61.56%
[rank1]:2025-11-08 01:08:38,345 - INFO - Avg. fwd time: 9.0183 / Avg. bwd time: 24.0919 / Avg. batch time: 580.1373 (ms) / GPU bubble ratio: 54.34%
[rank3]:2025-11-08 01:08:38,410 - INFO -  step: 600  loss:  0.2188  grad_norm:  0.1734  memory: 26.98GiB(56.79%)  tps: 6,384  tflops: 48.63  mfu: 15.59%
[rank2]:2025-11-08 01:08:38,397 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1734  memory: 11.81GiB(24.85%)  tps: 6,384  tflops: 48.62  mfu: 15.58%
[rank0]:2025-11-08 01:08:38,376 - INFO - Avg. fwd time: 7.8121 / Avg. bwd time: 23.4859 / Avg. batch time: 616.4315 (ms) / GPU bubble ratio: 59.38%
[rank0]:2025-11-08 01:08:38,411 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1734  memory: 16.57GiB(34.88%)  tps: 6,384  tflops: 48.62  mfu: 15.58%
[rank1]:2025-11-08 01:08:38,401 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1734  memory: 14.64GiB(30.82%)  tps: 6,384  tflops: 48.62  mfu: 15.58%
[rank3]:2025-11-08 01:08:38,562 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0108_real_step600_rank3.svg
[rank3]:> Batch Time: 620.61 ms, GPU Bubble Ratio: 59.39%, 57.08%, 66.30%, 26.45%
[rank0]:2025-11-08 01:10:43,911 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:10:46,159 - INFO - Avg. fwd time: 11.1715 / Avg. bwd time: 45.1175 / Avg. batch time: 509.9764 (ms) / GPU bubble ratio: 11.70%
[rank2]:2025-11-08 01:10:46,232 - INFO - Avg. fwd time: 7.0989 / Avg. bwd time: 18.9121 / Avg. batch time: 541.4944 (ms) / GPU bubble ratio: 61.57%
[rank0]:2025-11-08 01:10:46,280 - INFO - Avg. fwd time: 7.8109 / Avg. bwd time: 23.4860 / Avg. batch time: 616.5541 (ms) / GPU bubble ratio: 59.39%
[rank1]:2025-11-08 01:10:46,272 - INFO - Avg. fwd time: 9.0190 / Avg. bwd time: 24.0959 / Avg. batch time: 580.2674 (ms) / GPU bubble ratio: 54.35%
[rank1]:2025-11-08 01:10:46,451 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.7191  memory: 14.64GiB(30.82%)  tps: 6,398  tflops: 48.72  mfu: 15.62%
[rank3]:2025-11-08 01:10:46,460 - INFO -  step: 650  loss:  0.2314  grad_norm:  1.7191  memory: 26.98GiB(56.79%)  tps: 6,398  tflops: 48.73  mfu: 15.62%
[rank2]:2025-11-08 01:10:46,447 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.7191  memory: 11.81GiB(24.85%)  tps: 6,398  tflops: 48.72  mfu: 15.62%
[rank0]:2025-11-08 01:10:46,462 - INFO -  step: 650  loss: -4.0000  grad_norm:  1.7191  memory: 16.57GiB(34.88%)  tps: 6,397  tflops: 48.72  mfu: 15.62%
[rank0]:2025-11-08 01:12:51,912 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:12:54,201 - INFO - Avg. fwd time: 11.1715 / Avg. bwd time: 45.1324 / Avg. batch time: 510.0948 (ms) / GPU bubble ratio: 11.70%
[rank2]:2025-11-08 01:12:54,279 - INFO - Avg. fwd time: 7.0990 / Avg. bwd time: 18.9135 / Avg. batch time: 541.5887 (ms) / GPU bubble ratio: 61.58%
[rank1]:2025-11-08 01:12:54,318 - INFO - Avg. fwd time: 9.0199 / Avg. bwd time: 24.0986 / Avg. batch time: 580.3642 (ms) / GPU bubble ratio: 54.35%
[rank0]:2025-11-08 01:12:54,326 - INFO - Avg. fwd time: 7.8096 / Avg. bwd time: 23.4858 / Avg. batch time: 616.6442 (ms) / GPU bubble ratio: 59.40%
[rank0]:2025-11-08 01:12:54,511 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1804  memory: 16.57GiB(34.88%)  tps: 6,398  tflops: 48.73  mfu: 15.62%
[rank1]:2025-11-08 01:12:54,500 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1804  memory: 14.64GiB(30.82%)  tps: 6,398  tflops: 48.73  mfu: 15.62%
[rank3]:2025-11-08 01:12:54,509 - INFO -  step: 700  loss:  0.2096  grad_norm:  0.1804  memory: 26.98GiB(56.79%)  tps: 6,398  tflops: 48.73  mfu: 15.62%
[rank2]:2025-11-08 01:12:54,496 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1804  memory: 11.81GiB(24.85%)  tps: 6,398  tflops: 48.73  mfu: 15.62%
[rank3]:2025-11-08 01:12:54,660 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0112_real_step700_rank3.svg
[rank3]:> Batch Time: 618.59 ms, GPU Bubble Ratio: 59.26%, 57.03%, 66.25%, 26.65%
[rank0]:2025-11-08 01:15:00,272 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:15:02,751 - INFO - Avg. fwd time: 11.1717 / Avg. bwd time: 45.1546 / Avg. batch time: 510.2728 (ms) / GPU bubble ratio: 11.69%
[rank2]:2025-11-08 01:15:02,779 - INFO - Avg. fwd time: 7.0995 / Avg. bwd time: 18.9156 / Avg. batch time: 541.7882 (ms) / GPU bubble ratio: 61.59%
[rank0]:2025-11-08 01:15:02,844 - INFO - Avg. fwd time: 7.8090 / Avg. bwd time: 23.4864 / Avg. batch time: 616.8366 (ms) / GPU bubble ratio: 59.41%
[rank0]:2025-11-08 01:15:02,880 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1700  memory: 16.57GiB(34.88%)  tps: 6,382  tflops: 48.60  mfu: 15.58%
[rank1]:2025-11-08 01:15:02,811 - INFO - Avg. fwd time: 9.0207 / Avg. bwd time: 24.1012 / Avg. batch time: 580.5619 (ms) / GPU bubble ratio: 54.36%
[rank1]:2025-11-08 01:15:02,869 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1700  memory: 14.64GiB(30.82%)  tps: 6,382  tflops: 48.60  mfu: 15.58%
[rank3]:2025-11-08 01:15:02,878 - INFO -  step: 750  loss:  0.2696  grad_norm:  0.1700  memory: 26.98GiB(56.79%)  tps: 6,382  tflops: 48.60  mfu: 15.58%
[rank2]:2025-11-08 01:15:02,865 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1700  memory: 11.81GiB(24.85%)  tps: 6,382  tflops: 48.60  mfu: 15.58%
[rank0]:2025-11-08 01:17:08,576 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 01:17:10,881 - INFO - Avg. fwd time: 11.1731 / Avg. bwd time: 45.1733 / Avg. batch time: 510.4324 (ms) / GPU bubble ratio: 11.69%
[rank2]:2025-11-08 01:17:10,956 - INFO - Avg. fwd time: 7.0998 / Avg. bwd time: 18.9173 / Avg. batch time: 541.9309 (ms) / GPU bubble ratio: 61.59%
[rank0]:2025-11-08 01:17:11,007 - INFO - Avg. fwd time: 7.8089 / Avg. bwd time: 23.4870 / Avg. batch time: 616.9709 (ms) / GPU bubble ratio: 59.42%
[rank1]:2025-11-08 01:17:10,996 - INFO - Avg. fwd time: 9.0214 / Avg. bwd time: 24.1034 / Avg. batch time: 580.7019 (ms) / GPU bubble ratio: 54.37%
[rank3]:2025-11-08 01:17:11,187 - INFO -  step: 800  loss:  0.2132  grad_norm:  0.1936  memory: 26.98GiB(56.79%)  tps: 6,385  tflops: 48.63  mfu: 15.59%
[rank3]:2025-11-08 01:17:11,187 - INFO -  final step: 800  loss:  0.2132  grad_norm:  0.1936  tps: 6,798  tflops: 51.78  mfu: 15.13%
[rank3]:2025-11-08 01:17:11,188 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 01:17:11,188 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-08 01:17:11,174 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1936  memory: 11.81GiB(24.85%)  tps: 6,385  tflops: 48.63  mfu: 15.59%
[rank2]:2025-11-08 01:17:11,174 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1936  tps: 6,791  tflops: 51.72  mfu: 15.02%
[rank2]:2025-11-08 01:17:11,174 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 01:17:11,175 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 01:17:11,189 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1936  memory: 16.57GiB(34.88%)  tps: 6,385  tflops: 48.63  mfu: 15.59%
[rank0]:2025-11-08 01:17:11,189 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1936  tps: 6,792  tflops: 51.73  mfu: 15.02%
[rank0]:2025-11-08 01:17:11,189 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 01:17:11,190 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-08 01:17:11,178 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1936  memory: 14.64GiB(30.82%)  tps: 6,385  tflops: 48.63  mfu: 15.59%
[rank1]:2025-11-08 01:17:11,178 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1936  tps: 6,791  tflops: 51.73  mfu: 15.02%
[rank1]:2025-11-08 01:17:11,178 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 01:17:11,179 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-08 01:17:13,227 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 01:17:13,214 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 01:17:13,227 - INFO - Sleeping 2 seconds for other ranks to complete
[rank1]:2025-11-08 01:17:13,227 - INFO - Destroying the purge thread.
[rank3]:2025-11-08 01:17:13,371 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0117_real_final800_rank3.svg
[rank3]:> Batch Time: 617.11 ms, GPU Bubble Ratio: 59.16%, 56.96%, 66.16%, 26.84%
[rank2]:2025-11-08 01:17:13,399 - INFO - Process group destroyed
[rank1]:2025-11-08 01:17:13,385 - INFO - Process group destroyed
[rank3]:2025-11-08 01:17:13,517 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/pipeline_schedule/251108_0117_thry_final800_rank3.svg
[rank3]:> Batch Time: 289.62 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 01:17:13,518 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.41
[rank3]:wandb:               final/avg_loss 0.21322
[rank3]:wandb:             final/avg_mfu(%) 15.12534
[rank3]:wandb:             final/avg_tflops 51.77758
[rank3]:wandb:    final/avg_throughput(tps) 6798.33691
[rank3]:wandb:              final/grad_norm 0.19364
[rank3]:wandb:               final/max_loss 0.21322
[rank3]:wandb:                    grad_norm 0.19364
[rank3]:wandb: loss_metrics/global_avg_loss 0.21322
[rank3]:wandb: loss_metrics/global_max_loss 0.21322
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed2025_alpaca800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/xyvbudnz
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca800_dm1/20251108-0043/wandb/run-20251108_004301-xyvbudnz/logs
[rank3]:2025-11-08 01:17:14,880 - INFO - Process group destroyed
[rank0]:2025-11-08 01:17:15,228 - INFO - Training completed
[rank0]:2025-11-08 01:17:15,228 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 01:17:15,390 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.12', 'layers.11', 'layers.10'}
[rank1]:Stage 1: Modules to keep: {'layers.5', 'layers.6', 'layers.4', 'layers.7', 'layers.8'}
[rank0]:Stage 0: Modules to keep: {'layers.3', 'tok_embeddings', 'layers.0', 'layers.1', 'layers.2'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.15', 'layers.14', 'norm', 'output'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 2025
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed2025_alpaca800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
