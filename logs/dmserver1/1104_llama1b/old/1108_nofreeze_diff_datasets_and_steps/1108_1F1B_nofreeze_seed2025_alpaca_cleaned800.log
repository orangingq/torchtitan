
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 01:59:34 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x 1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=2025 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=5e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-08 01:59:40,758 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-08 01:59:40,783 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-08 01:59:40,758 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank0]:2025-11-08 01:59:41,046 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 01:59:41,048 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 01:59:41,051 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 01:59:41,053 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-08 01:59:41,036 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank1]:2025-11-08 01:59:41,028 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 01:59:41,031 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 01:59:41,060 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 01:59:41,062 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-08 01:59:41,195 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 01:59:41,198 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 01:59:41,439 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 01:59:44,177 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-08 01:59:44,327 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 01:59:44,364 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:59:44,365 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-08 01:59:44,389 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-08 01:59:44,390 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-11-08 01:59:44,351 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 01:59:44,388 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 01:59:44,424 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-08 01:59:44,464 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 01:59:44,493 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-08 01:59:44,493 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-11-08 01:59:44,415 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-08 01:59:44,415 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-08 01:59:44,580 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:59:44,580 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 01:59:44,581 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-08 01:59:44,678 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 01:59:44,678 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 01:59:44,679 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-08 01:59:44,599 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 01:59:44,599 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 01:59:44,600 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run zxm410p7
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0159/wandb/run-20251108_015945-zxm410p7
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/zxm410p7
[rank3]:2025-11-08 01:59:46,533 - INFO - WandB logging enabled
[rank3]:2025-11-08 01:59:46,534 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 01:59:46,575 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 01:59:46,604 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-08 01:59:46,605 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-11-08 01:59:46,797 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 01:59:46,814 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 01:59:46,816 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 01:59:46,816 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 01:59:46,817 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank2]:2025-11-08 01:59:46,814 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-08 01:59:46,815 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-08 01:59:46,797 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 01:59:46,798 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 01:59:46,814 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 01:59:49,286 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 01:59:49,286 - INFO - Finished loading the checkpoint in 2.47 seconds.
[rank0]:2025-11-08 01:59:49,286 - INFO - Training starts at step 1
[rank0]:2025-11-08 01:59:52,481 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5943  memory:  9.19GiB(19.34%)  tps: 2,019  tflops: 15.37  mfu: 4.93%
[rank0]:2025-11-08 01:59:52,481 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 01:59:52,446 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5943  memory:  6.76GiB(14.24%)  tps: 2,033  tflops: 15.49  mfu: 4.96%
[rank1]:2025-11-08 01:59:52,446 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-08 01:59:52,443 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5943  memory:  4.63GiB(9.75%)  tps: 2,054  tflops: 15.64  mfu: 5.01%
[rank2]:2025-11-08 01:59:52,443 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-08 01:59:52,453 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5943  memory: 12.97GiB(27.30%)  tps: 2,788  tflops: 21.24  mfu: 6.81%
[rank3]:2025-11-08 01:59:52,453 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 02:01:56,521 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:01:58,810 - INFO - Avg. fwd time: 13.4778 / Avg. bwd time: 41.9358 / Avg. batch time: 515.6885 (ms) / GPU bubble ratio: 14.04%
[rank2]:2025-11-08 02:01:58,849 - INFO - Avg. fwd time: 7.1954 / Avg. bwd time: 18.7752 / Avg. batch time: 547.5347 (ms) / GPU bubble ratio: 62.05%
[rank1]:2025-11-08 02:01:58,884 - INFO - Avg. fwd time: 9.2746 / Avg. bwd time: 23.7091 / Avg. batch time: 586.3600 (ms) / GPU bubble ratio: 55.00%
[rank0]:2025-11-08 02:01:58,880 - INFO - Avg. fwd time: 7.3069 / Avg. bwd time: 23.3100 / Avg. batch time: 623.3463 (ms) / GPU bubble ratio: 60.71%
[rank1]:2025-11-08 02:01:59,119 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.1733  memory:  9.03GiB(19.01%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank3]:2025-11-08 02:01:59,128 - INFO -  step: 50  loss:  4.3985  grad_norm: 14.1733  memory: 16.39GiB(34.50%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank0]:2025-11-08 02:01:59,130 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.1733  memory: 12.97GiB(27.31%)  tps: 6,339  tflops: 48.28  mfu: 15.47%
[rank2]:2025-11-08 02:01:59,116 - INFO -  step: 50  loss: -4.0000  grad_norm: 14.1733  memory:  6.43GiB(13.53%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank0]:2025-11-08 02:04:06,444 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:04:08,774 - INFO - Avg. fwd time: 13.5783 / Avg. bwd time: 42.1096 / Avg. batch time: 517.7949 (ms) / GPU bubble ratio: 13.96%
[rank0]:2025-11-08 02:04:08,844 - INFO - Avg. fwd time: 7.3086 / Avg. bwd time: 23.3684 / Avg. batch time: 624.2915 (ms) / GPU bubble ratio: 60.69%
[rank2]:2025-11-08 02:04:08,813 - INFO - Avg. fwd time: 7.1918 / Avg. bwd time: 18.8468 / Avg. batch time: 549.2463 (ms) / GPU bubble ratio: 62.07%
[rank1]:2025-11-08 02:04:08,848 - INFO - Avg. fwd time: 9.2996 / Avg. bwd time: 23.8172 / Avg. batch time: 587.7376 (ms) / GPU bubble ratio: 54.92%
[rank1]:2025-11-08 02:04:09,086 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4314  memory:  9.03GiB(19.01%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-08 02:04:09,095 - INFO -  step: 100  loss:  0.5661  grad_norm:  0.4314  memory: 16.39GiB(34.50%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank0]:2025-11-08 02:04:09,097 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4314  memory: 12.97GiB(27.31%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank2]:2025-11-08 02:04:09,083 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4314  memory:  6.43GiB(13.53%)  tps: 6,303  tflops: 48.01  mfu: 15.39%
[rank3]:2025-11-08 02:04:09,268 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0204_real_step100_rank3.svg
[rank3]:> Batch Time: 625.06 ms, GPU Bubble Ratio: 60.39%, 57.28%, 66.46%, 28.29%
[rank0]:2025-11-08 02:06:16,656 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:06:19,167 - INFO - Avg. fwd time: 13.6216 / Avg. bwd time: 42.1848 / Avg. batch time: 518.7198 (ms) / GPU bubble ratio: 13.93%
[rank2]:2025-11-08 02:06:19,193 - INFO - Avg. fwd time: 7.1939 / Avg. bwd time: 18.8735 / Avg. batch time: 550.2796 (ms) / GPU bubble ratio: 62.10%
[rank1]:2025-11-08 02:06:19,222 - INFO - Avg. fwd time: 9.3103 / Avg. bwd time: 23.8563 / Avg. batch time: 588.6776 (ms) / GPU bubble ratio: 54.93%
[rank0]:2025-11-08 02:06:19,252 - INFO - Avg. fwd time: 7.3074 / Avg. bwd time: 23.3875 / Avg. batch time: 625.0988 (ms) / GPU bubble ratio: 60.72%
[rank0]:2025-11-08 02:06:19,288 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3712  memory: 12.97GiB(27.31%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank3]:2025-11-08 02:06:19,285 - INFO -  step: 150  loss:  0.5080  grad_norm:  0.3712  memory: 16.39GiB(34.50%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank2]:2025-11-08 02:06:19,273 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3712  memory:  6.43GiB(13.53%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank1]:2025-11-08 02:06:19,277 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.3712  memory:  9.03GiB(19.01%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank0]:2025-11-08 02:08:26,045 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:08:28,358 - INFO - Avg. fwd time: 13.5990 / Avg. bwd time: 42.1555 / Avg. batch time: 518.2937 (ms) / GPU bubble ratio: 13.94%
[rank2]:2025-11-08 02:08:28,398 - INFO - Avg. fwd time: 7.1923 / Avg. bwd time: 18.8794 / Avg. batch time: 549.7532 (ms) / GPU bubble ratio: 62.06%
[rank1]:2025-11-08 02:08:28,433 - INFO - Avg. fwd time: 9.3066 / Avg. bwd time: 23.8614 / Avg. batch time: 588.0957 (ms) / GPU bubble ratio: 54.88%
[rank0]:2025-11-08 02:08:28,429 - INFO - Avg. fwd time: 7.3031 / Avg. bwd time: 23.3886 / Avg. batch time: 624.4510 (ms) / GPU bubble ratio: 60.68%
[rank3]:2025-11-08 02:08:28,677 - INFO -  step: 200  loss:  0.5855  grad_norm:  0.9785  memory: 16.39GiB(34.50%)  tps: 6,331  tflops: 48.22  mfu: 15.46%
[rank2]:2025-11-08 02:08:28,665 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.9785  memory:  6.43GiB(13.53%)  tps: 6,331  tflops: 48.22  mfu: 15.46%
[rank1]:2025-11-08 02:08:28,669 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.9785  memory:  9.03GiB(19.01%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank0]:2025-11-08 02:08:28,679 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.9785  memory: 12.97GiB(27.31%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank3]:2025-11-08 02:08:28,828 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0208_real_step200_rank3.svg
[rank3]:> Batch Time: 621.02 ms, GPU Bubble Ratio: 60.23%, 57.25%, 66.32%, 28.43%
[rank0]:2025-11-08 02:10:35,665 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:10:37,953 - INFO - Avg. fwd time: 13.5860 / Avg. bwd time: 42.1318 / Avg. batch time: 517.9963 (ms) / GPU bubble ratio: 13.95%
[rank0]:2025-11-08 02:10:38,023 - INFO - Avg. fwd time: 7.3012 / Avg. bwd time: 23.3894 / Avg. batch time: 624.1192 (ms) / GPU bubble ratio: 60.66%
[rank2]:2025-11-08 02:10:37,992 - INFO - Avg. fwd time: 7.1910 / Avg. bwd time: 18.8827 / Avg. batch time: 549.4976 (ms) / GPU bubble ratio: 62.04%
[rank1]:2025-11-08 02:10:38,027 - INFO - Avg. fwd time: 9.3045 / Avg. bwd time: 23.8664 / Avg. batch time: 587.8055 (ms) / GPU bubble ratio: 54.85%
[rank2]:2025-11-08 02:10:38,257 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3490  memory:  6.43GiB(13.53%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank1]:2025-11-08 02:10:38,260 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3490  memory:  9.03GiB(19.01%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-08 02:10:38,269 - INFO -  step: 250  loss:  0.5917  grad_norm:  0.3490  memory: 16.39GiB(34.50%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-08 02:10:38,272 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3490  memory: 12.97GiB(27.31%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank0]:2025-11-08 02:12:45,639 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:12:48,188 - INFO - Avg. fwd time: 13.6015 / Avg. bwd time: 42.1536 / Avg. batch time: 518.2887 (ms) / GPU bubble ratio: 13.94%
[rank2]:2025-11-08 02:12:48,214 - INFO - Avg. fwd time: 7.1920 / Avg. bwd time: 18.8877 / Avg. batch time: 549.7277 (ms) / GPU bubble ratio: 62.05%
[rank1]:2025-11-08 02:12:48,244 - INFO - Avg. fwd time: 9.3059 / Avg. bwd time: 23.8740 / Avg. batch time: 588.0129 (ms) / GPU bubble ratio: 54.86%
[rank0]:2025-11-08 02:12:48,274 - INFO - Avg. fwd time: 7.3011 / Avg. bwd time: 23.3924 / Avg. batch time: 624.3008 (ms) / GPU bubble ratio: 60.67%
[rank0]:2025-11-08 02:12:48,310 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3069  memory: 12.97GiB(27.31%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank2]:2025-11-08 02:12:48,296 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3069  memory:  6.43GiB(13.53%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank1]:2025-11-08 02:12:48,299 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3069  memory:  9.03GiB(19.01%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-08 02:12:48,308 - INFO -  step: 300  loss:  0.5848  grad_norm:  0.3069  memory: 16.39GiB(34.50%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-08 02:12:48,459 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0212_real_step300_rank3.svg
[rank3]:> Batch Time: 625.53 ms, GPU Bubble Ratio: 60.47%, 57.40%, 66.51%, 28.32%
[rank0]:2025-11-08 02:14:56,065 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:14:58,389 - INFO - Avg. fwd time: 13.6175 / Avg. bwd time: 42.1810 / Avg. batch time: 518.6312 (ms) / GPU bubble ratio: 13.93%
[rank0]:2025-11-08 02:14:58,460 - INFO - Avg. fwd time: 7.3017 / Avg. bwd time: 23.3960 / Avg. batch time: 624.6590 (ms) / GPU bubble ratio: 60.69%
[rank2]:2025-11-08 02:14:58,429 - INFO - Avg. fwd time: 7.1927 / Avg. bwd time: 18.8921 / Avg. batch time: 550.1134 (ms) / GPU bubble ratio: 62.07%
[rank1]:2025-11-08 02:14:58,464 - INFO - Avg. fwd time: 9.3069 / Avg. bwd time: 23.8810 / Avg. batch time: 588.3856 (ms) / GPU bubble ratio: 54.88%
[rank2]:2025-11-08 02:14:58,696 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3125  memory:  6.43GiB(13.53%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank1]:2025-11-08 02:14:58,700 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3125  memory:  9.03GiB(19.01%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank3]:2025-11-08 02:14:58,709 - INFO -  step: 350  loss:  0.5400  grad_norm:  0.3125  memory: 16.39GiB(34.50%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank0]:2025-11-08 02:14:58,711 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3125  memory: 12.97GiB(27.31%)  tps: 6,282  tflops: 47.85  mfu: 15.34%
[rank0]:2025-11-08 02:17:06,527 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:17:08,899 - INFO - Avg. fwd time: 7.1936 / Avg. bwd time: 18.8962 / Avg. batch time: 550.4787 (ms) / GPU bubble ratio: 62.08%
[rank3]:2025-11-08 02:17:08,858 - INFO - Avg. fwd time: 13.6367 / Avg. bwd time: 42.2138 / Avg. batch time: 519.0428 (ms) / GPU bubble ratio: 13.92%
[rank0]:2025-11-08 02:17:08,930 - INFO - Avg. fwd time: 7.3027 / Avg. bwd time: 23.3993 / Avg. batch time: 624.9985 (ms) / GPU bubble ratio: 60.70%
[rank1]:2025-11-08 02:17:08,934 - INFO - Avg. fwd time: 9.3076 / Avg. bwd time: 23.8872 / Avg. batch time: 588.7379 (ms) / GPU bubble ratio: 54.89%
[rank2]:2025-11-08 02:17:09,168 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2958  memory:  6.43GiB(13.53%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank1]:2025-11-08 02:17:09,172 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2958  memory:  9.03GiB(19.01%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank3]:2025-11-08 02:17:09,181 - INFO -  step: 400  loss:  0.5363  grad_norm:  0.2958  memory: 16.39GiB(34.50%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank0]:2025-11-08 02:17:09,183 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.2958  memory: 12.97GiB(27.31%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank3]:2025-11-08 02:17:09,330 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0217_real_step400_rank3.svg
[rank3]:> Batch Time: 628.06 ms, GPU Bubble Ratio: 60.61%, 57.60%, 66.64%, 28.41%
[rank3]:2025-11-08 02:17:18,093 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 02:17:18,374 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-08 02:17:18,322 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-08 02:17:18,348 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 02:19:16,563 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:19:19,089 - INFO - Avg. fwd time: 13.6409 / Avg. bwd time: 42.2223 / Avg. batch time: 519.1469 (ms) / GPU bubble ratio: 13.92%
[rank2]:2025-11-08 02:19:19,116 - INFO - Avg. fwd time: 7.1928 / Avg. bwd time: 18.8982 / Avg. batch time: 550.6112 (ms) / GPU bubble ratio: 62.09%
[rank1]:2025-11-08 02:19:19,148 - INFO - Avg. fwd time: 9.3048 / Avg. bwd time: 23.8895 / Avg. batch time: 588.8588 (ms) / GPU bubble ratio: 54.90%
[rank1]:2025-11-08 02:19:19,204 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3006  memory:  9.03GiB(19.01%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-08 02:19:19,178 - INFO - Avg. fwd time: 7.3020 / Avg. bwd time: 23.4006 / Avg. batch time: 625.1088 (ms) / GPU bubble ratio: 60.71%
[rank0]:2025-11-08 02:19:19,214 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3006  memory: 12.97GiB(27.31%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank2]:2025-11-08 02:19:19,200 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3006  memory:  6.43GiB(13.53%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank3]:2025-11-08 02:19:19,213 - INFO -  step: 450  loss:  0.5755  grad_norm:  0.3006  memory: 16.39GiB(34.50%)  tps: 6,300  tflops: 47.98  mfu: 15.38%
[rank0]:2025-11-08 02:21:26,499 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:21:28,825 - INFO - Avg. fwd time: 13.6419 / Avg. bwd time: 42.2226 / Avg. batch time: 519.1571 (ms) / GPU bubble ratio: 13.92%
[rank0]:2025-11-08 02:21:28,895 - INFO - Avg. fwd time: 7.3021 / Avg. bwd time: 23.4004 / Avg. batch time: 625.0673 (ms) / GPU bubble ratio: 60.70%
[rank2]:2025-11-08 02:21:28,865 - INFO - Avg. fwd time: 7.1924 / Avg. bwd time: 18.8990 / Avg. batch time: 550.5907 (ms) / GPU bubble ratio: 62.09%
[rank1]:2025-11-08 02:21:28,900 - INFO - Avg. fwd time: 9.3022 / Avg. bwd time: 23.8893 / Avg. batch time: 588.8261 (ms) / GPU bubble ratio: 54.90%
[rank2]:2025-11-08 02:21:29,128 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2321  memory:  6.43GiB(13.53%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-08 02:21:29,141 - INFO -  step: 500  loss:  0.3995  grad_norm:  0.2321  memory: 16.39GiB(34.50%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank1]:2025-11-08 02:21:29,132 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2321  memory:  9.03GiB(19.01%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-08 02:21:29,143 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2321  memory: 12.97GiB(27.31%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-08 02:21:29,295 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0221_real_step500_rank3.svg
[rank3]:> Batch Time: 625.03 ms, GPU Bubble Ratio: 60.45%, 57.46%, 66.50%, 28.23%
[rank0]:2025-11-08 02:23:36,837 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:23:39,142 - INFO - Avg. fwd time: 13.6485 / Avg. bwd time: 42.2348 / Avg. batch time: 519.3040 (ms) / GPU bubble ratio: 13.91%
[rank2]:2025-11-08 02:23:39,183 - INFO - Avg. fwd time: 7.1931 / Avg. bwd time: 18.9009 / Avg. batch time: 550.7645 (ms) / GPU bubble ratio: 62.10%
[rank1]:2025-11-08 02:23:39,218 - INFO - Avg. fwd time: 9.3012 / Avg. bwd time: 23.8907 / Avg. batch time: 588.9898 (ms) / GPU bubble ratio: 54.92%
[rank0]:2025-11-08 02:23:39,213 - INFO - Avg. fwd time: 7.3028 / Avg. bwd time: 23.4017 / Avg. batch time: 625.2241 (ms) / GPU bubble ratio: 60.71%
[rank1]:2025-11-08 02:23:39,446 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2770  memory:  9.03GiB(19.01%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-08 02:23:39,456 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2770  memory: 12.97GiB(27.31%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank2]:2025-11-08 02:23:39,442 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.2770  memory:  6.43GiB(13.53%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-08 02:23:39,454 - INFO -  step: 550  loss:  0.4412  grad_norm:  0.2770  memory: 16.39GiB(34.50%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-08 02:25:47,192 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:25:49,773 - INFO - Avg. fwd time: 7.1936 / Avg. bwd time: 18.9026 / Avg. batch time: 550.9425 (ms) / GPU bubble ratio: 62.11%
[rank3]:2025-11-08 02:25:49,745 - INFO - Avg. fwd time: 13.6582 / Avg. bwd time: 42.2506 / Avg. batch time: 519.5093 (ms) / GPU bubble ratio: 13.91%
[rank1]:2025-11-08 02:25:49,805 - INFO - Avg. fwd time: 9.2976 / Avg. bwd time: 23.8919 / Avg. batch time: 589.1606 (ms) / GPU bubble ratio: 54.93%
[rank2]:2025-11-08 02:25:49,857 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2894  memory:  6.43GiB(13.53%)  tps: 6,281  tflops: 47.84  mfu: 15.33%
[rank0]:2025-11-08 02:25:49,836 - INFO - Avg. fwd time: 7.3035 / Avg. bwd time: 23.4036 / Avg. batch time: 625.3901 (ms) / GPU bubble ratio: 60.72%
[rank0]:2025-11-08 02:25:49,872 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2894  memory: 12.97GiB(27.31%)  tps: 6,281  tflops: 47.84  mfu: 15.33%
[rank3]:2025-11-08 02:25:49,870 - INFO -  step: 600  loss:  0.4438  grad_norm:  0.2894  memory: 16.39GiB(34.50%)  tps: 6,282  tflops: 47.84  mfu: 15.33%
[rank1]:2025-11-08 02:25:49,861 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2894  memory:  9.03GiB(19.01%)  tps: 6,281  tflops: 47.84  mfu: 15.33%
[rank3]:2025-11-08 02:25:50,018 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0225_real_step600_rank3.svg
[rank3]:> Batch Time: 627.05 ms, GPU Bubble Ratio: 60.54%, 57.61%, 66.58%, 28.26%
[rank0]:2025-11-08 02:27:58,018 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:28:00,319 - INFO - Avg. fwd time: 13.6674 / Avg. bwd time: 42.2687 / Avg. batch time: 519.7275 (ms) / GPU bubble ratio: 13.90%
[rank1]:2025-11-08 02:28:00,393 - INFO - Avg. fwd time: 9.2963 / Avg. bwd time: 23.8937 / Avg. batch time: 589.3979 (ms) / GPU bubble ratio: 54.95%
[rank2]:2025-11-08 02:28:00,358 - INFO - Avg. fwd time: 7.1944 / Avg. bwd time: 18.9041 / Avg. batch time: 551.1838 (ms) / GPU bubble ratio: 62.12%
[rank0]:2025-11-08 02:28:00,389 - INFO - Avg. fwd time: 7.3053 / Avg. bwd time: 23.4053 / Avg. batch time: 625.6302 (ms) / GPU bubble ratio: 60.73%
[rank1]:2025-11-08 02:28:00,622 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2686  memory:  9.03GiB(19.01%)  tps: 6,265  tflops: 47.71  mfu: 15.29%
[rank2]:2025-11-08 02:28:00,618 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2686  memory:  6.43GiB(13.53%)  tps: 6,265  tflops: 47.71  mfu: 15.29%
[rank0]:2025-11-08 02:28:00,632 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.2686  memory: 12.97GiB(27.31%)  tps: 6,265  tflops: 47.71  mfu: 15.29%
[rank3]:2025-11-08 02:28:00,630 - INFO -  step: 650  loss:  0.3706  grad_norm:  0.2686  memory: 16.39GiB(34.50%)  tps: 6,265  tflops: 47.72  mfu: 15.29%
[rank0]:2025-11-08 02:30:08,014 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:30:10,334 - INFO - Avg. fwd time: 13.6672 / Avg. bwd time: 42.2714 / Avg. batch time: 519.7458 (ms) / GPU bubble ratio: 13.90%
[rank2]:2025-11-08 02:30:10,373 - INFO - Avg. fwd time: 7.1944 / Avg. bwd time: 18.9046 / Avg. batch time: 551.1780 (ms) / GPU bubble ratio: 62.12%
[rank0]:2025-11-08 02:30:10,404 - INFO - Avg. fwd time: 7.3048 / Avg. bwd time: 23.4045 / Avg. batch time: 625.6125 (ms) / GPU bubble ratio: 60.73%
[rank1]:2025-11-08 02:30:10,408 - INFO - Avg. fwd time: 9.2923 / Avg. bwd time: 23.8932 / Avg. batch time: 589.3849 (ms) / GPU bubble ratio: 54.96%
[rank3]:2025-11-08 02:30:10,650 - INFO -  step: 700  loss:  0.4384  grad_norm:  0.3158  memory: 16.39GiB(34.50%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank1]:2025-11-08 02:30:10,641 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3158  memory:  9.03GiB(19.01%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank2]:2025-11-08 02:30:10,638 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3158  memory:  6.43GiB(13.53%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank0]:2025-11-08 02:30:10,652 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3158  memory: 12.97GiB(27.31%)  tps: 6,301  tflops: 47.99  mfu: 15.38%
[rank3]:2025-11-08 02:30:10,798 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0230_real_step700_rank3.svg
[rank3]:> Batch Time: 624.53 ms, GPU Bubble Ratio: 60.44%, 57.49%, 66.47%, 28.41%
[rank0]:2025-11-08 02:32:18,320 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 02:32:20,842 - INFO - Avg. fwd time: 13.6688 / Avg. bwd time: 42.2740 / Avg. batch time: 519.7800 (ms) / GPU bubble ratio: 13.90%
[rank2]:2025-11-08 02:32:20,871 - INFO - Avg. fwd time: 7.1944 / Avg. bwd time: 18.9050 / Avg. batch time: 551.2303 (ms) / GPU bubble ratio: 62.12%
[rank1]:2025-11-08 02:32:20,903 - INFO - Avg. fwd time: 9.2885 / Avg. bwd time: 23.8926 / Avg. batch time: 589.4323 (ms) / GPU bubble ratio: 54.97%
[rank1]:2025-11-08 02:32:20,960 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3578  memory:  9.03GiB(19.01%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank2]:2025-11-08 02:32:20,957 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3578  memory:  6.43GiB(13.53%)  tps: 6,286  tflops: 47.88  mfu: 15.34%
[rank0]:2025-11-08 02:32:20,936 - INFO - Avg. fwd time: 7.3046 / Avg. bwd time: 23.4045 / Avg. batch time: 625.6572 (ms) / GPU bubble ratio: 60.73%
[rank0]:2025-11-08 02:32:20,971 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3578  memory: 12.97GiB(27.31%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank3]:2025-11-08 02:32:20,969 - INFO -  step: 750  loss:  0.4110  grad_norm:  0.3578  memory: 16.39GiB(34.50%)  tps: 6,286  tflops: 47.88  mfu: 15.35%
[rank0]:2025-11-08 02:34:28,803 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 02:34:31,182 - INFO - Avg. fwd time: 7.1949 / Avg. bwd time: 18.9060 / Avg. batch time: 551.3268 (ms) / GPU bubble ratio: 62.13%
[rank3]:2025-11-08 02:34:31,141 - INFO - Avg. fwd time: 13.6736 / Avg. bwd time: 42.2804 / Avg. batch time: 519.8920 (ms) / GPU bubble ratio: 13.90%
[rank1]:2025-11-08 02:34:31,217 - INFO - Avg. fwd time: 9.2860 / Avg. bwd time: 23.8926 / Avg. batch time: 589.5269 (ms) / GPU bubble ratio: 54.98%
[rank0]:2025-11-08 02:34:31,212 - INFO - Avg. fwd time: 7.3048 / Avg. bwd time: 23.4051 / Avg. batch time: 625.7514 (ms) / GPU bubble ratio: 60.74%
[rank1]:2025-11-08 02:34:31,450 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3066  memory:  9.03GiB(19.01%)  tps: 6,278  tflops: 47.81  mfu: 15.33%
[rank1]:2025-11-08 02:34:31,450 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3066  tps: 6,673  tflops: 50.82  mfu: 14.76%
[rank1]:2025-11-08 02:34:31,450 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 02:34:31,451 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-08 02:34:31,446 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3066  memory:  6.43GiB(13.53%)  tps: 6,278  tflops: 47.81  mfu: 15.33%
[rank2]:2025-11-08 02:34:31,447 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3066  tps: 6,673  tflops: 50.82  mfu: 14.76%
[rank2]:2025-11-08 02:34:31,447 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 02:34:31,447 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 02:34:31,461 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3066  memory: 12.97GiB(27.31%)  tps: 6,278  tflops: 47.81  mfu: 15.32%
[rank0]:2025-11-08 02:34:31,461 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3066  tps: 6,673  tflops: 50.82  mfu: 14.76%
[rank0]:2025-11-08 02:34:31,461 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 02:34:31,462 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-08 02:34:31,459 - INFO -  step: 800  loss:  0.4265  grad_norm:  0.3066  memory: 16.39GiB(34.50%)  tps: 6,278  tflops: 47.81  mfu: 15.33%
[rank3]:2025-11-08 02:34:31,460 - INFO -  final step: 800  loss:  0.4265  grad_norm:  0.3066  tps: 6,680  tflops: 50.87  mfu: 14.87%
[rank3]:2025-11-08 02:34:31,460 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 02:34:31,461 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-08 02:34:33,793 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank1]:2025-11-08 02:34:33,812 - INFO - Destroying the purge thread.
[rank2]:2025-11-08 02:34:33,812 - INFO - Destroying the purge thread.
[rank2]:2025-11-08 02:34:33,844 - INFO - Process group destroyed
[rank0]:2025-11-08 02:34:33,812 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-08 02:34:33,956 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0234_real_final800_rank3.svg
[rank3]:> Batch Time: 625.05 ms, GPU Bubble Ratio: 60.44%, 57.47%, 66.49%, 28.33%
[rank3]:2025-11-08 02:34:34,098 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0234_thry_final800_rank3.svg
[rank3]:> Batch Time: 308.50 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 02:34:34,099 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank1]:2025-11-08 02:34:34,334 - INFO - Process group destroyed
[rank3]:wandb: uploading history steps 16-16, summary, console lines 231-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.45276
[rank3]:wandb:               final/avg_loss 0.42648
[rank3]:wandb:             final/avg_mfu(%) 14.86748
[rank3]:wandb:             final/avg_tflops 50.87485
[rank3]:wandb:    final/avg_throughput(tps) 6679.80877
[rank3]:wandb:              final/grad_norm 0.30664
[rank3]:wandb:               final/max_loss 0.42648
[rank3]:wandb:                    grad_norm 0.30664
[rank3]:wandb: loss_metrics/global_avg_loss 0.42648
[rank3]:wandb: loss_metrics/global_max_loss 0.42648
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/zxm410p7
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0159/wandb/run-20251108_015945-zxm410p7/logs
[rank3]:2025-11-08 02:34:35,448 - INFO - Process group destroyed
[rank0]:2025-11-08 02:34:35,818 - INFO - Training completed
[rank0]:2025-11-08 02:34:35,819 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 02:34:35,884 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.0', 'layers.1', 'tok_embeddings', 'layers.3', 'layers.2'}
[rank1]:Stage 1: Modules to keep: {'layers.8', 'layers.7', 'layers.6', 'layers.5', 'layers.4'}
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.9', 'layers.11', 'layers.10'}
[rank3]:Stage 3: Modules to keep: {'layers.15', 'output', 'layers.14', 'norm', 'layers.13'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 2025
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 08. (í† ) 03:26:52 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x 1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --training.seed=42 --training.dataset=alpaca_cleaned --training.steps=800 --optimizer.lr=2e-5  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-08 03:26:58,556 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank1]:2025-11-08 03:26:58,581 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank2]:2025-11-08 03:26:58,753 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-08 03:26:58,825 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-08 03:26:58,827 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 03:26:58,832 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-08 03:26:58,833 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-08 03:26:58,846 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-08 03:26:58,829 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-08 03:26:58,831 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-08 03:26:58,946 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-08 03:26:58,949 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-08 03:26:59,045 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-08 03:26:59,048 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-08 03:26:59,221 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-08 03:27:02,037 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-08 03:27:02,190 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-08 03:27:02,234 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 03:27:02,235 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank1]:2025-11-08 03:27:02,194 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-08 03:27:02,235 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 03:27:02,261 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-08 03:27:02,261 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-08 03:27:02,261 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-08 03:27:02,262 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-11-08 03:27:02,273 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-08 03:27:02,310 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 03:27:02,337 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-08 03:27:02,337 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-11-08 03:27:02,453 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-08 03:27:02,453 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-08 03:27:02,454 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-08 03:27:02,472 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-08 03:27:02,472 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-08 03:27:02,473 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank2]:2025-11-08 03:27:02,521 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-08 03:27:02,521 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-08 03:27:02,522 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 3hygvbj4
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0327/wandb/run-20251108_032703-3hygvbj4
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/3hygvbj4
[rank3]:2025-11-08 03:27:04,443 - INFO - WandB logging enabled
[rank3]:2025-11-08 03:27:04,444 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-08 03:27:04,481 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 03:27:04,502 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-08 03:27:04,502 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-08 03:27:04,713 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank0]:2025-11-08 03:27:04,714 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-08 03:27:04,714 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-08 03:27:04,714 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank0]:2025-11-08 03:27:04,715 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800.
[rank3]:2025-11-08 03:27:04,691 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-08 03:27:04,691 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-08 03:27:04,692 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-08 03:27:04,713 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-08 03:27:04,714 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank1]:2025-11-08 03:27:04,713 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-08 03:27:04,713 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-08 03:27:04,714 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank2]:2025-11-08 03:27:04,714 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1.
[rank0]:2025-11-08 03:27:07,093 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-08 03:27:07,093 - INFO - Finished loading the checkpoint in 2.38 seconds.
[rank0]:2025-11-08 03:27:07,093 - INFO - Training starts at step 1
[rank3]:2025-11-08 03:27:10,252 - INFO -  step:  1  loss:  0.3934  grad_norm:  0.2309  memory: 12.97GiB(27.30%)  tps: 2,840  tflops: 21.63  mfu: 6.93%
[rank3]:2025-11-08 03:27:10,253 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-08 03:27:10,242 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2309  memory:  4.63GiB(9.75%)  tps: 2,066  tflops: 15.73  mfu: 5.04%
[rank1]:2025-11-08 03:27:10,245 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2309  memory:  6.76GiB(14.24%)  tps: 2,045  tflops: 15.58  mfu: 4.99%
[rank2]:2025-11-08 03:27:10,242 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-08 03:27:10,246 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 03:27:10,284 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.2309  memory:  9.19GiB(19.34%)  tps: 2,035  tflops: 15.50  mfu: 4.97%
[rank0]:2025-11-08 03:27:10,284 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-08 03:29:14,689 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:29:16,980 - INFO - Avg. fwd time: 13.5612 / Avg. bwd time: 42.0709 / Avg. batch time: 517.4726 (ms) / GPU bubble ratio: 13.99%
[rank2]:2025-11-08 03:29:17,019 - INFO - Avg. fwd time: 7.1854 / Avg. bwd time: 18.8479 / Avg. batch time: 549.4628 (ms) / GPU bubble ratio: 62.10%
[rank0]:2025-11-08 03:29:17,050 - INFO - Avg. fwd time: 7.3081 / Avg. bwd time: 23.3649 / Avg. batch time: 625.3461 (ms) / GPU bubble ratio: 60.76%
[rank1]:2025-11-08 03:29:17,054 - INFO - Avg. fwd time: 9.2219 / Avg. bwd time: 23.7749 / Avg. batch time: 588.3047 (ms) / GPU bubble ratio: 55.13%
[rank3]:2025-11-08 03:29:17,300 - INFO -  step: 50  loss:  0.3733  grad_norm:  0.2829  memory: 16.39GiB(34.50%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-08 03:29:17,302 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2829  memory: 12.97GiB(27.31%)  tps: 6,321  tflops: 48.14  mfu: 15.43%
[rank2]:2025-11-08 03:29:17,287 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2829  memory:  6.43GiB(13.53%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-08 03:29:17,291 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.2829  memory:  9.03GiB(19.01%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank0]:2025-11-08 03:31:25,114 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:31:27,446 - INFO - Avg. fwd time: 13.6639 / Avg. bwd time: 42.3046 / Avg. batch time: 520.0522 (ms) / GPU bubble ratio: 13.90%
[rank2]:2025-11-08 03:31:27,486 - INFO - Avg. fwd time: 7.1948 / Avg. bwd time: 18.8958 / Avg. batch time: 551.5943 (ms) / GPU bubble ratio: 62.16%
[rank1]:2025-11-08 03:31:27,521 - INFO - Avg. fwd time: 9.2458 / Avg. bwd time: 23.8502 / Avg. batch time: 590.0882 (ms) / GPU bubble ratio: 55.13%
[rank0]:2025-11-08 03:31:27,517 - INFO - Avg. fwd time: 7.3081 / Avg. bwd time: 23.3962 / Avg. batch time: 626.6584 (ms) / GPU bubble ratio: 60.80%
[rank3]:2025-11-08 03:31:27,769 - INFO -  step: 100  loss:  0.3952  grad_norm:  0.2836  memory: 16.39GiB(34.50%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank0]:2025-11-08 03:31:27,772 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2836  memory: 12.97GiB(27.31%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank2]:2025-11-08 03:31:27,757 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2836  memory:  6.43GiB(13.53%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank1]:2025-11-08 03:31:27,760 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.2836  memory:  9.03GiB(19.01%)  tps: 6,279  tflops: 47.82  mfu: 15.33%
[rank3]:2025-11-08 03:31:27,946 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0331_real_step100_rank3.svg
[rank3]:> Batch Time: 628.56 ms, GPU Bubble Ratio: 60.64%, 57.69%, 66.61%, 28.31%
[rank0]:2025-11-08 03:33:35,587 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:33:38,102 - INFO - Avg. fwd time: 13.6802 / Avg. bwd time: 42.3681 / Avg. batch time: 520.6585 (ms) / GPU bubble ratio: 13.88%
[rank1]:2025-11-08 03:33:38,158 - INFO - Avg. fwd time: 9.2488 / Avg. bwd time: 23.8692 / Avg. batch time: 590.6838 (ms) / GPU bubble ratio: 55.15%
[rank2]:2025-11-08 03:33:38,128 - INFO - Avg. fwd time: 7.1956 / Avg. bwd time: 18.9085 / Avg. batch time: 552.2929 (ms) / GPU bubble ratio: 62.19%
[rank2]:2025-11-08 03:33:38,209 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2993  memory:  6.43GiB(13.53%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank3]:2025-11-08 03:33:38,221 - INFO -  step: 150  loss:  0.3677  grad_norm:  0.2993  memory: 16.39GiB(34.50%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank0]:2025-11-08 03:33:38,187 - INFO - Avg. fwd time: 7.3061 / Avg. bwd time: 23.4037 / Avg. batch time: 627.1101 (ms) / GPU bubble ratio: 60.82%
[rank0]:2025-11-08 03:33:38,223 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2993  memory: 12.97GiB(27.31%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank1]:2025-11-08 03:33:38,212 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.2993  memory:  9.03GiB(19.01%)  tps: 6,280  tflops: 47.83  mfu: 15.33%
[rank0]:2025-11-08 03:35:45,929 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:35:48,266 - INFO - Avg. fwd time: 13.6899 / Avg. bwd time: 42.3854 / Avg. batch time: 520.8568 (ms) / GPU bubble ratio: 13.87%
[rank0]:2025-11-08 03:35:48,336 - INFO - Avg. fwd time: 7.3048 / Avg. bwd time: 23.4048 / Avg. batch time: 627.0884 (ms) / GPU bubble ratio: 60.82%
[rank1]:2025-11-08 03:35:48,340 - INFO - Avg. fwd time: 9.2513 / Avg. bwd time: 23.8759 / Avg. batch time: 590.7260 (ms) / GPU bubble ratio: 55.14%
[rank2]:2025-11-08 03:35:48,305 - INFO - Avg. fwd time: 7.1956 / Avg. bwd time: 18.9118 / Avg. batch time: 552.3799 (ms) / GPU bubble ratio: 62.19%
[rank2]:2025-11-08 03:35:48,576 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3080  memory:  6.43GiB(13.53%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank3]:2025-11-08 03:35:48,588 - INFO -  step: 200  loss:  0.4425  grad_norm:  0.3080  memory: 16.39GiB(34.50%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank0]:2025-11-08 03:35:48,591 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3080  memory: 12.97GiB(27.31%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank1]:2025-11-08 03:35:48,579 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3080  memory:  9.03GiB(19.01%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank3]:2025-11-08 03:35:48,740 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0335_real_step200_rank3.svg
[rank3]:> Batch Time: 626.05 ms, GPU Bubble Ratio: 60.51%, 57.57%, 66.53%, 28.15%
[rank0]:2025-11-08 03:37:56,797 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:37:59,152 - INFO - Avg. fwd time: 7.1961 / Avg. bwd time: 18.9161 / Avg. batch time: 552.8346 (ms) / GPU bubble ratio: 62.21%
[rank3]:2025-11-08 03:37:59,112 - INFO - Avg. fwd time: 13.7138 / Avg. bwd time: 42.4139 / Avg. batch time: 521.2669 (ms) / GPU bubble ratio: 13.86%
[rank0]:2025-11-08 03:37:59,183 - INFO - Avg. fwd time: 7.3058 / Avg. bwd time: 23.4079 / Avg. batch time: 627.4846 (ms) / GPU bubble ratio: 60.84%
[rank1]:2025-11-08 03:37:59,186 - INFO - Avg. fwd time: 9.2588 / Avg. bwd time: 23.8855 / Avg. batch time: 591.1560 (ms) / GPU bubble ratio: 55.15%
[rank1]:2025-11-08 03:37:59,420 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3153  memory:  9.03GiB(19.01%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank2]:2025-11-08 03:37:59,417 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3153  memory:  6.43GiB(13.53%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank3]:2025-11-08 03:37:59,429 - INFO -  step: 250  loss:  0.4652  grad_norm:  0.3153  memory: 16.39GiB(34.50%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank0]:2025-11-08 03:37:59,432 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.3153  memory: 12.97GiB(27.31%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank0]:2025-11-08 03:40:07,520 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:40:10,106 - INFO - Avg. fwd time: 7.1968 / Avg. bwd time: 18.9204 / Avg. batch time: 553.1154 (ms) / GPU bubble ratio: 62.23%
[rank3]:2025-11-08 03:40:10,079 - INFO - Avg. fwd time: 13.7320 / Avg. bwd time: 42.4391 / Avg. batch time: 521.6131 (ms) / GPU bubble ratio: 13.85%
[rank0]:2025-11-08 03:40:10,166 - INFO - Avg. fwd time: 7.3064 / Avg. bwd time: 23.4112 / Avg. batch time: 627.7316 (ms) / GPU bubble ratio: 60.85%
[rank1]:2025-11-08 03:40:10,136 - INFO - Avg. fwd time: 9.2668 / Avg. bwd time: 23.8950 / Avg. batch time: 591.4244 (ms) / GPU bubble ratio: 55.14%
[rank1]:2025-11-08 03:40:10,191 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3185  memory:  9.03GiB(19.01%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank2]:2025-11-08 03:40:10,187 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3185  memory:  6.43GiB(13.53%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank3]:2025-11-08 03:40:10,200 - INFO -  step: 300  loss:  0.4683  grad_norm:  0.3185  memory: 16.39GiB(34.50%)  tps: 6,265  tflops: 47.71  mfu: 15.29%
[rank0]:2025-11-08 03:40:10,202 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.3185  memory: 12.97GiB(27.31%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank3]:2025-11-08 03:40:10,349 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0340_real_step300_rank3.svg
[rank3]:> Batch Time: 628.06 ms, GPU Bubble Ratio: 60.59%, 57.60%, 66.60%, 28.24%
[rank0]:2025-11-08 03:42:18,234 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:42:20,527 - INFO - Avg. fwd time: 13.7381 / Avg. bwd time: 42.4479 / Avg. batch time: 521.7270 (ms) / GPU bubble ratio: 13.85%
[rank1]:2025-11-08 03:42:20,602 - INFO - Avg. fwd time: 9.2724 / Avg. bwd time: 23.9007 / Avg. batch time: 591.5724 (ms) / GPU bubble ratio: 55.14%
[rank2]:2025-11-08 03:42:20,567 - INFO - Avg. fwd time: 7.1968 / Avg. bwd time: 18.9225 / Avg. batch time: 553.2690 (ms) / GPU bubble ratio: 62.23%
[rank0]:2025-11-08 03:42:20,598 - INFO - Avg. fwd time: 7.3051 / Avg. bwd time: 23.4124 / Avg. batch time: 627.8621 (ms) / GPU bubble ratio: 60.86%
[rank3]:2025-11-08 03:42:20,844 - INFO -  step: 350  loss:  0.4519  grad_norm:  0.3227  memory: 16.39GiB(34.50%)  tps: 6,271  tflops: 47.76  mfu: 15.31%
[rank0]:2025-11-08 03:42:20,847 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3227  memory: 12.97GiB(27.31%)  tps: 6,270  tflops: 47.76  mfu: 15.31%
[rank1]:2025-11-08 03:42:20,836 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3227  memory:  9.03GiB(19.01%)  tps: 6,270  tflops: 47.76  mfu: 15.31%
[rank2]:2025-11-08 03:42:20,832 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3227  memory:  6.43GiB(13.53%)  tps: 6,270  tflops: 47.76  mfu: 15.31%
[rank0]:2025-11-08 03:44:28,458 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:44:30,780 - INFO - Avg. fwd time: 13.7315 / Avg. bwd time: 42.4356 / Avg. batch time: 521.5711 (ms) / GPU bubble ratio: 13.85%
[rank2]:2025-11-08 03:44:30,820 - INFO - Avg. fwd time: 7.1961 / Avg. bwd time: 18.9227 / Avg. batch time: 553.0607 (ms) / GPU bubble ratio: 62.22%
[rank0]:2025-11-08 03:44:30,851 - INFO - Avg. fwd time: 7.3047 / Avg. bwd time: 23.4121 / Avg. batch time: 627.6266 (ms) / GPU bubble ratio: 60.85%
[rank1]:2025-11-08 03:44:30,856 - INFO - Avg. fwd time: 9.2759 / Avg. bwd time: 23.9036 / Avg. batch time: 591.3541 (ms) / GPU bubble ratio: 55.11%
[rank2]:2025-11-08 03:44:31,088 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3131  memory:  6.43GiB(13.53%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank3]:2025-11-08 03:44:31,101 - INFO -  step: 400  loss:  0.4564  grad_norm:  0.3131  memory: 16.39GiB(34.50%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank1]:2025-11-08 03:44:31,092 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3131  memory:  9.03GiB(19.01%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank0]:2025-11-08 03:44:31,103 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3131  memory: 12.97GiB(27.31%)  tps: 6,289  tflops: 47.90  mfu: 15.35%
[rank3]:2025-11-08 03:44:31,254 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0344_real_step400_rank3.svg
[rank3]:> Batch Time: 626.06 ms, GPU Bubble Ratio: 60.49%, 57.46%, 66.53%, 28.36%
[rank3]:2025-11-08 03:44:40,034 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-08 03:44:40,262 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-08 03:44:40,289 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 03:44:40,314 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-08 03:46:38,636 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:46:41,181 - INFO - Avg. fwd time: 13.7184 / Avg. bwd time: 42.4322 / Avg. batch time: 521.4396 (ms) / GPU bubble ratio: 13.85%
[rank2]:2025-11-08 03:46:41,208 - INFO - Avg. fwd time: 7.1952 / Avg. bwd time: 18.9236 / Avg. batch time: 552.9590 (ms) / GPU bubble ratio: 62.21%
[rank1]:2025-11-08 03:46:41,239 - INFO - Avg. fwd time: 9.2801 / Avg. bwd time: 23.9068 / Avg. batch time: 591.2487 (ms) / GPU bubble ratio: 55.10%
[rank1]:2025-11-08 03:46:41,295 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3046  memory:  9.03GiB(19.01%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank3]:2025-11-08 03:46:41,304 - INFO -  step: 450  loss:  0.4694  grad_norm:  0.3046  memory: 16.39GiB(34.50%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank0]:2025-11-08 03:46:41,270 - INFO - Avg. fwd time: 7.3036 / Avg. bwd time: 23.4137 / Avg. batch time: 627.5123 (ms) / GPU bubble ratio: 60.84%
[rank0]:2025-11-08 03:46:41,306 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3046  memory: 12.97GiB(27.31%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank2]:2025-11-08 03:46:41,291 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3046  memory:  6.43GiB(13.53%)  tps: 6,292  tflops: 47.92  mfu: 15.36%
[rank0]:2025-11-08 03:48:49,337 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:48:51,667 - INFO - Avg. fwd time: 13.7209 / Avg. bwd time: 42.4438 / Avg. batch time: 521.5532 (ms) / GPU bubble ratio: 13.85%
[rank2]:2025-11-08 03:48:51,706 - INFO - Avg. fwd time: 7.1948 / Avg. bwd time: 18.9245 / Avg. batch time: 553.0409 (ms) / GPU bubble ratio: 62.22%
[rank1]:2025-11-08 03:48:51,741 - INFO - Avg. fwd time: 9.2846 / Avg. bwd time: 23.9103 / Avg. batch time: 591.3264 (ms) / GPU bubble ratio: 55.09%
[rank0]:2025-11-08 03:48:51,737 - INFO - Avg. fwd time: 7.3035 / Avg. bwd time: 23.4147 / Avg. batch time: 627.5838 (ms) / GPU bubble ratio: 60.84%
[rank1]:2025-11-08 03:48:51,975 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2494  memory:  9.03GiB(19.01%)  tps: 6,269  tflops: 47.74  mfu: 15.30%
[rank3]:2025-11-08 03:48:51,984 - INFO -  step: 500  loss:  0.3007  grad_norm:  0.2494  memory: 16.39GiB(34.50%)  tps: 6,269  tflops: 47.74  mfu: 15.30%
[rank0]:2025-11-08 03:48:51,986 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2494  memory: 12.97GiB(27.31%)  tps: 6,269  tflops: 47.74  mfu: 15.30%
[rank2]:2025-11-08 03:48:51,971 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.2494  memory:  6.43GiB(13.53%)  tps: 6,269  tflops: 47.74  mfu: 15.30%
[rank3]:2025-11-08 03:48:52,135 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0348_real_step500_rank3.svg
[rank3]:> Batch Time: 628.55 ms, GPU Bubble Ratio: 60.64%, 57.57%, 66.64%, 28.18%
[rank0]:2025-11-08 03:51:00,365 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:51:02,719 - INFO - Avg. fwd time: 7.1956 / Avg. bwd time: 18.9262 / Avg. batch time: 553.3015 (ms) / GPU bubble ratio: 62.23%
[rank1]:2025-11-08 03:51:02,754 - INFO - Avg. fwd time: 9.2890 / Avg. bwd time: 23.9140 / Avg. batch time: 591.5852 (ms) / GPU bubble ratio: 55.10%
[rank3]:2025-11-08 03:51:02,679 - INFO - Avg. fwd time: 13.7329 / Avg. bwd time: 42.4614 / Avg. batch time: 521.7895 (ms) / GPU bubble ratio: 13.84%
[rank0]:2025-11-08 03:51:02,750 - INFO - Avg. fwd time: 7.3045 / Avg. bwd time: 23.4165 / Avg. batch time: 627.8399 (ms) / GPU bubble ratio: 60.86%
[rank0]:2025-11-08 03:51:02,994 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3194  memory: 12.97GiB(27.31%)  tps: 6,253  tflops: 47.62  mfu: 15.26%
[rank2]:2025-11-08 03:51:02,979 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3194  memory:  6.43GiB(13.53%)  tps: 6,253  tflops: 47.62  mfu: 15.26%
[rank1]:2025-11-08 03:51:02,983 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3194  memory:  9.03GiB(19.01%)  tps: 6,253  tflops: 47.62  mfu: 15.26%
[rank3]:2025-11-08 03:51:02,992 - INFO -  step: 550  loss:  0.3638  grad_norm:  0.3194  memory: 16.39GiB(34.50%)  tps: 6,253  tflops: 47.63  mfu: 15.26%
[rank0]:2025-11-08 03:53:11,284 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 03:53:13,841 - INFO - Avg. fwd time: 13.7433 / Avg. bwd time: 42.4740 / Avg. batch time: 521.9731 (ms) / GPU bubble ratio: 13.84%
[rank2]:2025-11-08 03:53:13,869 - INFO - Avg. fwd time: 7.1958 / Avg. bwd time: 18.9276 / Avg. batch time: 553.4573 (ms) / GPU bubble ratio: 62.24%
[rank1]:2025-11-08 03:53:13,901 - INFO - Avg. fwd time: 9.2916 / Avg. bwd time: 23.9167 / Avg. batch time: 591.7384 (ms) / GPU bubble ratio: 55.10%
[rank0]:2025-11-08 03:53:13,933 - INFO - Avg. fwd time: 7.3053 / Avg. bwd time: 23.4181 / Avg. batch time: 627.9927 (ms) / GPU bubble ratio: 60.86%
[rank0]:2025-11-08 03:53:13,969 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3305  memory: 12.97GiB(27.31%)  tps: 6,255  tflops: 47.64  mfu: 15.27%
[rank2]:2025-11-08 03:53:13,954 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3305  memory:  6.43GiB(13.53%)  tps: 6,255  tflops: 47.64  mfu: 15.27%
[rank1]:2025-11-08 03:53:13,958 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3305  memory:  9.03GiB(19.01%)  tps: 6,255  tflops: 47.64  mfu: 15.27%
[rank3]:2025-11-08 03:53:13,967 - INFO -  step: 600  loss:  0.3743  grad_norm:  0.3305  memory: 16.39GiB(34.50%)  tps: 6,255  tflops: 47.64  mfu: 15.27%
[rank3]:2025-11-08 03:53:14,116 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0353_real_step600_rank3.svg
[rank3]:> Batch Time: 629.58 ms, GPU Bubble Ratio: 60.68%, 57.69%, 66.69%, 28.23%
[rank0]:2025-11-08 03:55:22,143 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-08 03:55:24,477 - INFO - Avg. fwd time: 7.1959 / Avg. bwd time: 18.9280 / Avg. batch time: 553.5401 (ms) / GPU bubble ratio: 62.24%
[rank1]:2025-11-08 03:55:24,512 - INFO - Avg. fwd time: 9.2936 / Avg. bwd time: 23.9174 / Avg. batch time: 591.8172 (ms) / GPU bubble ratio: 55.11%
[rank3]:2025-11-08 03:55:24,437 - INFO - Avg. fwd time: 13.7462 / Avg. bwd time: 42.4789 / Avg. batch time: 522.0332 (ms) / GPU bubble ratio: 13.84%
[rank0]:2025-11-08 03:55:24,507 - INFO - Avg. fwd time: 7.3054 / Avg. bwd time: 23.4184 / Avg. batch time: 628.0675 (ms) / GPU bubble ratio: 60.87%
[rank0]:2025-11-08 03:55:24,750 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3167  memory: 12.97GiB(27.31%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank2]:2025-11-08 03:55:24,736 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3167  memory:  6.43GiB(13.53%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank1]:2025-11-08 03:55:24,740 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3167  memory:  9.03GiB(19.01%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank3]:2025-11-08 03:55:24,748 - INFO -  step: 650  loss:  0.3202  grad_norm:  0.3167  memory: 16.39GiB(34.50%)  tps: 6,264  tflops: 47.71  mfu: 15.29%
[rank0]:2025-11-08 03:57:32,595 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-08 03:57:34,983 - INFO - Avg. fwd time: 9.2943 / Avg. bwd time: 23.9163 / Avg. batch time: 591.7798 (ms) / GPU bubble ratio: 55.10%
[rank3]:2025-11-08 03:57:34,909 - INFO - Avg. fwd time: 13.7472 / Avg. bwd time: 42.4773 / Avg. batch time: 522.0263 (ms) / GPU bubble ratio: 13.84%
[rank0]:2025-11-08 03:57:34,980 - INFO - Avg. fwd time: 7.3052 / Avg. bwd time: 23.4176 / Avg. batch time: 628.0262 (ms) / GPU bubble ratio: 60.86%
[rank2]:2025-11-08 03:57:34,949 - INFO - Avg. fwd time: 7.1957 / Avg. bwd time: 18.9276 / Avg. batch time: 553.5085 (ms) / GPU bubble ratio: 62.24%
[rank2]:2025-11-08 03:57:35,215 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3835  memory:  6.43GiB(13.53%)  tps: 6,278  tflops: 47.82  mfu: 15.33%
[rank1]:2025-11-08 03:57:35,219 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3835  memory:  9.03GiB(19.01%)  tps: 6,278  tflops: 47.82  mfu: 15.33%
[rank3]:2025-11-08 03:57:35,228 - INFO -  step: 700  loss:  0.3946  grad_norm:  0.3835  memory: 16.39GiB(34.50%)  tps: 6,278  tflops: 47.82  mfu: 15.33%
[rank0]:2025-11-08 03:57:35,230 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.3835  memory: 12.97GiB(27.31%)  tps: 6,278  tflops: 47.82  mfu: 15.33%
[rank3]:2025-11-08 03:57:35,376 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0357_real_step700_rank3.svg
[rank3]:> Batch Time: 627.06 ms, GPU Bubble Ratio: 60.57%, 57.53%, 66.57%, 28.45%
[rank0]:2025-11-08 03:59:43,376 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-08 03:59:45,965 - INFO - Avg. fwd time: 9.2953 / Avg. bwd time: 23.9168 / Avg. batch time: 591.8167 (ms) / GPU bubble ratio: 55.10%
[rank3]:2025-11-08 03:59:45,904 - INFO - Avg. fwd time: 13.7477 / Avg. bwd time: 42.4794 / Avg. batch time: 522.0477 (ms) / GPU bubble ratio: 13.84%
[rank2]:2025-11-08 03:59:45,932 - INFO - Avg. fwd time: 7.1959 / Avg. bwd time: 18.9282 / Avg. batch time: 553.5481 (ms) / GPU bubble ratio: 62.24%
[rank0]:2025-11-08 03:59:45,997 - INFO - Avg. fwd time: 7.3052 / Avg. bwd time: 23.4178 / Avg. batch time: 628.0616 (ms) / GPU bubble ratio: 60.87%
[rank1]:2025-11-08 03:59:46,022 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3997  memory:  9.03GiB(19.01%)  tps: 6,263  tflops: 47.70  mfu: 15.29%
[rank3]:2025-11-08 03:59:46,031 - INFO -  step: 750  loss:  0.3707  grad_norm:  0.3997  memory: 16.39GiB(34.50%)  tps: 6,263  tflops: 47.70  mfu: 15.29%
[rank2]:2025-11-08 03:59:46,019 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3997  memory:  6.43GiB(13.53%)  tps: 6,263  tflops: 47.70  mfu: 15.29%
[rank0]:2025-11-08 03:59:46,033 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.3997  memory: 12.97GiB(27.31%)  tps: 6,263  tflops: 47.70  mfu: 15.29%
[rank0]:2025-11-08 04:01:54,215 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-08 04:01:56,553 - INFO - Avg. fwd time: 13.7529 / Avg. bwd time: 42.4857 / Avg. batch time: 522.1398 (ms) / GPU bubble ratio: 13.83%
[rank1]:2025-11-08 04:01:56,629 - INFO - Avg. fwd time: 9.2954 / Avg. bwd time: 23.9171 / Avg. batch time: 591.8879 (ms) / GPU bubble ratio: 55.11%
[rank2]:2025-11-08 04:01:56,594 - INFO - Avg. fwd time: 7.1962 / Avg. bwd time: 18.9289 / Avg. batch time: 553.6225 (ms) / GPU bubble ratio: 62.25%
[rank0]:2025-11-08 04:01:56,624 - INFO - Avg. fwd time: 7.3049 / Avg. bwd time: 23.4180 / Avg. batch time: 628.1310 (ms) / GPU bubble ratio: 60.87%
[rank2]:2025-11-08 04:01:56,857 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3822  memory:  6.43GiB(13.53%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank2]:2025-11-08 04:01:56,857 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3822  tps: 6,649  tflops: 50.64  mfu: 14.71%
[rank2]:2025-11-08 04:01:56,857 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-08 04:01:56,857 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank2]:  warnings.warn(
[rank0]:2025-11-08 04:01:56,871 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3822  memory: 12.97GiB(27.31%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank0]:2025-11-08 04:01:56,871 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3822  tps: 6,649  tflops: 50.64  mfu: 14.71%
[rank0]:2025-11-08 04:01:56,871 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-08 04:01:56,872 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank0]:  warnings.warn(
[rank1]:2025-11-08 04:01:56,860 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.3822  memory:  9.03GiB(19.01%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank1]:2025-11-08 04:01:56,860 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.3822  tps: 6,649  tflops: 50.64  mfu: 14.71%
[rank1]:2025-11-08 04:01:56,860 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-08 04:01:56,861 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank1]:  warnings.warn(
[rank3]:2025-11-08 04:01:56,869 - INFO -  step: 800  loss:  0.4006  grad_norm:  0.3822  memory: 16.39GiB(34.50%)  tps: 6,261  tflops: 47.69  mfu: 15.28%
[rank3]:2025-11-08 04:01:56,870 - INFO -  final step: 800  loss:  0.4006  grad_norm:  0.3822  tps: 6,656  tflops: 50.69  mfu: 14.82%
[rank3]:2025-11-08 04:01:56,870 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-08 04:01:56,871 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank3]:  warnings.warn(
[rank1]:2025-11-08 04:01:59,107 - INFO - Destroying the purge thread.
[rank2]:2025-11-08 04:01:59,107 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 04:01:59,095 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-08 04:01:59,106 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-08 04:01:59,249 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0401_real_final800_rank3.svg
[rank3]:> Batch Time: 627.56 ms, GPU Bubble Ratio: 60.58%, 57.60%, 66.59%, 28.25%
[rank1]:2025-11-08 04:01:59,294 - INFO - Process group destroyed
[rank2]:2025-11-08 04:01:59,275 - INFO - Process group destroyed
[rank3]:2025-11-08 04:01:59,392 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/pipeline_schedule/251108_0401_thry_final800_rank3.svg
[rank3]:> Batch Time: 309.35 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-08 04:01:59,393 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 227-238
[rank0]:2025-11-08 04:02:01,107 - INFO - Training completed
[rank0]:2025-11-08 04:02:01,107 - INFO - Destroying the purge thread.
[rank0]:2025-11-08 04:02:01,266 - INFO - Process group destroyed
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–â–ƒâ–ƒâ–„â–„â–„â–…â–…â–„â–„â–‚â–…â–…â–…â–‡â–ˆâ–‡
[rank3]:wandb: loss_metrics/global_avg_loss â–…â–„â–…â–„â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–â–„â–„â–‚â–…â–„â–…
[rank3]:wandb: loss_metrics/global_max_loss â–…â–„â–…â–„â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–â–„â–„â–‚â–…â–„â–…
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.46159
[rank3]:wandb:               final/avg_loss 0.4006
[rank3]:wandb:             final/avg_mfu(%) 14.8221
[rank3]:wandb:             final/avg_tflops 50.69244
[rank3]:wandb:    final/avg_throughput(tps) 6655.85868
[rank3]:wandb:              final/grad_norm 0.38221
[rank3]:wandb:               final/max_loss 0.4006
[rank3]:wandb:                    grad_norm 0.38221
[rank3]:wandb: loss_metrics/global_avg_loss 0.4006
[rank3]:wandb: loss_metrics/global_max_loss 0.4006
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/3hygvbj4
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1/20251108-0327/wandb/run-20251108_032703-3hygvbj4/logs
[rank3]:2025-11-08 04:02:02,792 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'layers.0', 'tok_embeddings', 'layers.2', 'layers.3', 'layers.1'}
[rank1]:Stage 1: Modules to keep: {'layers.7', 'layers.8', 'layers.4', 'layers.6', 'layers.5'}
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.12', 'layers.11', 'layers.10'}
[rank3]:Stage 3: Modules to keep: {'norm', 'layers.15', 'output', 'layers.13', 'layers.14'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_1F1B_nofreeze_seed2025_alpaca_cleaned800_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
