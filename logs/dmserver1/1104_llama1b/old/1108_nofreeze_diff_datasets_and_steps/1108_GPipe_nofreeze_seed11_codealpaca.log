
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 22:55:25 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed11_codealpaca.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4 --training.dataset=codealpaca  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank0]:2025-11-09 22:55:32,015 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank2]:2025-11-09 22:55:32,024 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:"
[rank3]:2025-11-09 22:55:32,056 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank1]:2025-11-09 22:55:32,150 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank2]:2025-11-09 22:55:32,236 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 22:55:32,238 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 22:55:32,316 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 22:55:32,318 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 22:55:32,320 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 22:55:32,321 - INFO - Loading tokenizer from tokenizer.json
[rank3]:2025-11-09 22:55:32,358 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 22:55:32,361 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 22:55:32,398 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 22:55:32,401 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 22:55:32,706 - INFO - Preparing codealpaca dataset from sahil2801/CodeAlpaca-20k
[rank0]:2025-11-09 22:55:40,484 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 22:55:40,635 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 22:55:40,672 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 22:55:40,673 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-09 22:55:40,702 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 22:55:40,741 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 22:55:40,769 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 22:55:40,769 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 22:55:40,750 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 22:55:40,790 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 22:55:40,699 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 22:55:40,699 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 22:55:40,818 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 22:55:40,818 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 22:55:40,952 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 22:55:40,952 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 22:55:40,953 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-09 22:55:40,905 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 22:55:40,905 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 22:55:40,906 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 22:55:41,001 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 22:55:41,001 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 22:55:41,002 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run dkflsloh
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_codealpaca_dm1/20251109-2255/wandb/run-20251109_225541-dkflsloh
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/dkflsloh
[rank3]:2025-11-09 22:55:42,984 - INFO - WandB logging enabled
[rank3]:2025-11-09 22:55:42,985 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 22:55:43,021 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 22:55:43,049 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 22:55:43,049 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 22:55:43,250 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 22:55:43,251 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 22:55:43,251 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank0]:2025-11-09 22:55:43,251 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 22:55:43,251 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 22:55:43,252 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-09 22:55:43,233 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 22:55:43,234 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 22:55:43,235 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 22:55:43,250 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 22:55:45,615 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 22:55:45,615 - INFO - Finished loading the checkpoint in 2.36 seconds.
[rank0]:2025-11-09 22:55:45,615 - INFO - Training starts at step 1
[rank2]:2025-11-09 22:55:48,652 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory:  9.99GiB(21.03%)  tps: 2,071  tflops: 15.77  mfu: 5.06%
[rank2]:2025-11-09 22:55:48,652 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 22:55:48,657 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory: 12.38GiB(26.05%)  tps: 2,083  tflops: 15.86  mfu: 5.08%
[rank0]:2025-11-09 22:55:48,697 - INFO -  step:  1  loss: -4.0000  grad_norm: 147.6782  memory: 12.80GiB(26.95%)  tps: 2,042  tflops: 15.55  mfu: 4.98%
[rank1]:2025-11-09 22:55:48,657 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 22:55:48,662 - INFO -  step:  1  loss: 10.1253  grad_norm: 147.6782  memory: 24.19GiB(50.91%)  tps: 2,906  tflops: 22.13  mfu: 7.09%
[rank0]:2025-11-09 22:55:48,697 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 22:55:48,663 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 22:57:46,886 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:57:49,037 - INFO - Avg. fwd time: 10.7582 / Avg. bwd time: 42.7921 / Avg. batch time: 488.1123 (ms) / GPU bubble ratio: 12.23%
[rank2]:2025-11-09 22:57:49,106 - INFO - Avg. fwd time: 7.0623 / Avg. bwd time: 18.6432 / Avg. batch time: 519.8062 (ms) / GPU bubble ratio: 60.44%
[rank0]:2025-11-09 22:57:49,152 - INFO - Avg. fwd time: 8.0402 / Avg. bwd time: 23.7983 / Avg. batch time: 596.5893 (ms) / GPU bubble ratio: 57.31%
[rank1]:2025-11-09 22:57:49,145 - INFO - Avg. fwd time: 9.0900 / Avg. bwd time: 23.9297 / Avg. batch time: 558.9313 (ms) / GPU bubble ratio: 52.74%
[rank2]:2025-11-09 22:57:49,321 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1027  memory: 11.81GiB(24.85%)  tps: 6,653  tflops: 50.67  mfu: 16.24%
[rank0]:2025-11-09 22:57:49,336 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1027  memory: 16.57GiB(34.88%)  tps: 6,655  tflops: 50.68  mfu: 16.24%
[rank3]:2025-11-09 22:57:49,333 - INFO -  step: 50  loss: 10.4976  grad_norm: 24.1027  memory: 26.98GiB(56.79%)  tps: 6,653  tflops: 50.67  mfu: 16.24%
[rank1]:2025-11-09 22:57:49,325 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.1027  memory: 14.64GiB(30.82%)  tps: 6,653  tflops: 50.67  mfu: 16.24%
[rank0]:2025-11-09 22:59:50,836 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 22:59:53,066 - INFO - Avg. fwd time: 10.7737 / Avg. bwd time: 43.1157 / Avg. batch time: 490.7104 (ms) / GPU bubble ratio: 12.14%
[rank2]:2025-11-09 22:59:53,139 - INFO - Avg. fwd time: 7.0511 / Avg. bwd time: 18.7076 / Avg. batch time: 521.9697 (ms) / GPU bubble ratio: 60.52%
[rank0]:2025-11-09 22:59:53,185 - INFO - Avg. fwd time: 8.0358 / Avg. bwd time: 23.8440 / Avg. batch time: 597.9482 (ms) / GPU bubble ratio: 57.35%
[rank1]:2025-11-09 22:59:53,178 - INFO - Avg. fwd time: 9.0851 / Avg. bwd time: 24.0004 / Avg. batch time: 560.7215 (ms) / GPU bubble ratio: 52.80%
[rank2]:2025-11-09 22:59:53,357 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1463  memory: 11.81GiB(24.85%)  tps: 6,605  tflops: 50.30  mfu: 16.12%
[rank0]:2025-11-09 22:59:53,371 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1463  memory: 16.57GiB(34.88%)  tps: 6,605  tflops: 50.30  mfu: 16.12%
[rank1]:2025-11-09 22:59:53,361 - INFO -  step: 100  loss: -4.0000  grad_norm: 24.1463  memory: 14.64GiB(30.82%)  tps: 6,605  tflops: 50.30  mfu: 16.12%
[rank3]:2025-11-09 22:59:53,369 - INFO -  step: 100  loss:  4.7201  grad_norm: 24.1463  memory: 26.98GiB(56.79%)  tps: 6,605  tflops: 50.30  mfu: 16.12%
[rank3]:2025-11-09 22:59:53,542 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2259_real_step100_rank3.svg
[rank3]:> Batch Time: 599.00 ms, GPU Bubble Ratio: 57.11%, 55.67%, 65.42%, 27.74%
[rank0]:2025-11-09 23:01:55,266 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 23:01:57,680 - INFO - Avg. fwd time: 7.0441 / Avg. bwd time: 18.7330 / Avg. batch time: 523.2059 (ms) / GPU bubble ratio: 60.59%
[rank3]:2025-11-09 23:01:57,654 - INFO - Avg. fwd time: 10.7764 / Avg. bwd time: 43.2621 / Avg. batch time: 491.8681 (ms) / GPU bubble ratio: 12.11%
[rank2]:2025-11-09 23:01:57,761 - INFO -  step: 150  loss: -4.0000  grad_norm: 33.9905  memory: 11.81GiB(24.85%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank1]:2025-11-09 23:01:57,710 - INFO - Avg. fwd time: 9.0841 / Avg. bwd time: 24.0347 / Avg. batch time: 561.8833 (ms) / GPU bubble ratio: 52.85%
[rank1]:2025-11-09 23:01:57,765 - INFO -  step: 150  loss: -4.0000  grad_norm: 33.9905  memory: 14.64GiB(30.82%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank3]:2025-11-09 23:01:57,773 - INFO -  step: 150  loss:  3.3520  grad_norm: 33.9905  memory: 26.98GiB(56.79%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank0]:2025-11-09 23:01:57,740 - INFO - Avg. fwd time: 8.0405 / Avg. bwd time: 23.8612 / Avg. batch time: 599.0063 (ms) / GPU bubble ratio: 57.39%
[rank0]:2025-11-09 23:01:57,775 - INFO -  step: 150  loss: -4.0000  grad_norm: 33.9905  memory: 16.57GiB(34.88%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank3]:2025-11-09 23:02:12,448 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:02:12,659 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:02:12,685 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:02:12,711 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:03:59,777 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:04:02,004 - INFO - Avg. fwd time: 10.7747 / Avg. bwd time: 43.3561 / Avg. batch time: 492.5894 (ms) / GPU bubble ratio: 12.09%
[rank2]:2025-11-09 23:04:02,075 - INFO - Avg. fwd time: 7.0408 / Avg. bwd time: 18.7668 / Avg. batch time: 523.8378 (ms) / GPU bubble ratio: 60.59%
[rank1]:2025-11-09 23:04:02,115 - INFO - Avg. fwd time: 9.0824 / Avg. bwd time: 24.0765 / Avg. batch time: 562.5226 (ms) / GPU bubble ratio: 52.84%
[rank0]:2025-11-09 23:04:02,122 - INFO - Avg. fwd time: 8.0485 / Avg. bwd time: 23.8827 / Avg. batch time: 599.6125 (ms) / GPU bubble ratio: 57.40%
[rank2]:2025-11-09 23:04:02,295 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.0072  memory: 11.81GiB(24.85%)  tps: 6,578  tflops: 50.10  mfu: 16.06%
[rank1]:2025-11-09 23:04:02,299 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.0072  memory: 14.64GiB(30.82%)  tps: 6,578  tflops: 50.10  mfu: 16.06%
[rank0]:2025-11-09 23:04:02,310 - INFO -  step: 200  loss: -4.0000  grad_norm: 16.0072  memory: 16.57GiB(34.88%)  tps: 6,578  tflops: 50.10  mfu: 16.06%
[rank3]:2025-11-09 23:04:02,307 - INFO -  step: 200  loss:  0.9783  grad_norm: 16.0072  memory: 26.98GiB(56.79%)  tps: 6,578  tflops: 50.10  mfu: 16.06%
[rank3]:2025-11-09 23:04:02,456 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2304_real_step200_rank3.svg
[rank3]:> Batch Time: 602.09 ms, GPU Bubble Ratio: 57.15%, 55.64%, 65.43%, 27.81%
[rank0]:2025-11-09 23:06:04,046 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:06:06,233 - INFO - Avg. fwd time: 10.7631 / Avg. bwd time: 43.3839 / Avg. batch time: 492.7072 (ms) / GPU bubble ratio: 12.08%
[rank2]:2025-11-09 23:06:06,305 - INFO - Avg. fwd time: 7.0357 / Avg. bwd time: 18.7977 / Avg. batch time: 524.0136 (ms) / GPU bubble ratio: 60.56%
[rank1]:2025-11-09 23:06:06,345 - INFO - Avg. fwd time: 9.0779 / Avg. bwd time: 24.1081 / Avg. batch time: 562.7000 (ms) / GPU bubble ratio: 52.82%
[rank0]:2025-11-09 23:06:06,352 - INFO - Avg. fwd time: 8.0544 / Avg. bwd time: 23.8971 / Avg. batch time: 599.7679 (ms) / GPU bubble ratio: 57.38%
[rank2]:2025-11-09 23:06:06,520 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.7589  memory: 11.81GiB(24.85%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank1]:2025-11-09 23:06:06,523 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.7589  memory: 14.64GiB(30.82%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank0]:2025-11-09 23:06:06,534 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.7589  memory: 16.57GiB(34.88%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank3]:2025-11-09 23:06:06,532 - INFO -  step: 250  loss:  0.3045  grad_norm:  3.7589  memory: 26.98GiB(56.79%)  tps: 6,595  tflops: 50.23  mfu: 16.10%
[rank0]:2025-11-09 23:08:08,343 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:08:10,767 - INFO - Avg. fwd time: 10.7550 / Avg. bwd time: 43.4066 / Avg. batch time: 492.8168 (ms) / GPU bubble ratio: 12.08%
[rank2]:2025-11-09 23:08:10,793 - INFO - Avg. fwd time: 7.0326 / Avg. bwd time: 18.8142 / Avg. batch time: 524.0623 (ms) / GPU bubble ratio: 60.54%
[rank2]:2025-11-09 23:08:10,876 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.7158  memory: 11.81GiB(24.85%)  tps: 6,588  tflops: 50.17  mfu: 16.08%
[rank1]:2025-11-09 23:08:10,824 - INFO - Avg. fwd time: 9.0754 / Avg. bwd time: 24.1247 / Avg. batch time: 562.7558 (ms) / GPU bubble ratio: 52.80%
[rank1]:2025-11-09 23:08:10,880 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.7158  memory: 14.64GiB(30.82%)  tps: 6,588  tflops: 50.17  mfu: 16.08%
[rank0]:2025-11-09 23:08:10,855 - INFO - Avg. fwd time: 8.0585 / Avg. bwd time: 23.9038 / Avg. batch time: 599.8153 (ms) / GPU bubble ratio: 57.37%
[rank0]:2025-11-09 23:08:10,891 - INFO -  step: 300  loss: -4.0000  grad_norm:  1.7158  memory: 16.57GiB(34.88%)  tps: 6,588  tflops: 50.17  mfu: 16.08%
[rank3]:2025-11-09 23:08:10,889 - INFO -  step: 300  loss:  0.1993  grad_norm:  1.7158  memory: 26.98GiB(56.79%)  tps: 6,588  tflops: 50.17  mfu: 16.08%
[rank3]:2025-11-09 23:08:11,036 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2308_real_step300_rank3.svg
[rank3]:> Batch Time: 601.56 ms, GPU Bubble Ratio: 57.18%, 55.66%, 65.44%, 27.68%
[rank3]:2025-11-09 23:08:40,587 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:08:40,795 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:08:40,821 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:08:40,847 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:10:12,897 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:10:15,098 - INFO - Avg. fwd time: 10.7508 / Avg. bwd time: 43.4349 / Avg. batch time: 493.0040 (ms) / GPU bubble ratio: 12.07%
[rank2]:2025-11-09 23:10:15,169 - INFO - Avg. fwd time: 7.0310 / Avg. bwd time: 18.8249 / Avg. batch time: 524.2908 (ms) / GPU bubble ratio: 60.55%
[rank1]:2025-11-09 23:10:15,209 - INFO - Avg. fwd time: 9.0744 / Avg. bwd time: 24.1366 / Avg. batch time: 562.9898 (ms) / GPU bubble ratio: 52.81%
[rank0]:2025-11-09 23:10:15,218 - INFO - Avg. fwd time: 8.0590 / Avg. bwd time: 23.9090 / Avg. batch time: 600.0392 (ms) / GPU bubble ratio: 57.38%
[rank2]:2025-11-09 23:10:15,385 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3508  memory: 11.81GiB(24.85%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank1]:2025-11-09 23:10:15,389 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3508  memory: 14.64GiB(30.82%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank3]:2025-11-09 23:10:15,398 - INFO -  step: 350  loss:  0.1771  grad_norm:  0.3508  memory: 26.98GiB(56.79%)  tps: 6,580  tflops: 50.11  mfu: 16.06%
[rank0]:2025-11-09 23:10:15,400 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.3508  memory: 16.57GiB(34.88%)  tps: 6,579  tflops: 50.11  mfu: 16.06%
[rank0]:2025-11-09 23:12:17,414 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:12:19,652 - INFO - Avg. fwd time: 10.7489 / Avg. bwd time: 43.4597 / Avg. batch time: 493.1912 (ms) / GPU bubble ratio: 12.07%
[rank2]:2025-11-09 23:12:19,723 - INFO - Avg. fwd time: 7.0297 / Avg. bwd time: 18.8353 / Avg. batch time: 524.4385 (ms) / GPU bubble ratio: 60.54%
[rank1]:2025-11-09 23:12:19,762 - INFO - Avg. fwd time: 9.0730 / Avg. bwd time: 24.1454 / Avg. batch time: 563.1381 (ms) / GPU bubble ratio: 52.81%
[rank0]:2025-11-09 23:12:19,769 - INFO - Avg. fwd time: 8.0649 / Avg. bwd time: 23.9150 / Avg. batch time: 600.1906 (ms) / GPU bubble ratio: 57.37%
[rank2]:2025-11-09 23:12:19,939 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.9292  memory: 11.81GiB(24.85%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank1]:2025-11-09 23:12:19,943 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.9292  memory: 14.64GiB(30.82%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank0]:2025-11-09 23:12:19,955 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.9292  memory: 16.57GiB(34.88%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank3]:2025-11-09 23:12:19,951 - INFO -  step: 400  loss:  0.1551  grad_norm:  0.9292  memory: 26.98GiB(56.79%)  tps: 6,577  tflops: 50.09  mfu: 16.06%
[rank3]:2025-11-09 23:12:20,102 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2312_real_step400_rank3.svg
[rank3]:> Batch Time: 601.11 ms, GPU Bubble Ratio: 57.11%, 55.68%, 65.38%, 27.71%
[rank0]:2025-11-09 23:14:21,731 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-11-09 23:14:24,171 - INFO - Avg. fwd time: 9.0712 / Avg. bwd time: 24.1479 / Avg. batch time: 563.0898 (ms) / GPU bubble ratio: 52.80%
[rank2]:2025-11-09 23:14:24,139 - INFO - Avg. fwd time: 7.0285 / Avg. bwd time: 18.8410 / Avg. batch time: 524.3952 (ms) / GPU bubble ratio: 60.53%
[rank3]:2025-11-09 23:14:24,113 - INFO - Avg. fwd time: 10.7455 / Avg. bwd time: 43.4541 / Avg. batch time: 493.1174 (ms) / GPU bubble ratio: 12.07%
[rank0]:2025-11-09 23:14:24,202 - INFO - Avg. fwd time: 8.0666 / Avg. bwd time: 23.9176 / Avg. batch time: 600.1410 (ms) / GPU bubble ratio: 57.36%
[rank1]:2025-11-09 23:14:24,227 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2919  memory: 14.64GiB(30.82%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank2]:2025-11-09 23:14:24,224 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2919  memory: 11.81GiB(24.85%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 23:14:24,236 - INFO -  step: 450  loss:  0.1599  grad_norm:  0.2919  memory: 26.98GiB(56.79%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 23:14:24,238 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2919  memory: 16.57GiB(34.88%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 23:15:09,248 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:15:09,489 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:15:09,462 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:15:09,518 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:16:25,860 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:16:28,080 - INFO - Avg. fwd time: 10.7430 / Avg. bwd time: 43.4530 / Avg. batch time: 493.0841 (ms) / GPU bubble ratio: 12.07%
[rank2]:2025-11-09 23:16:28,151 - INFO - Avg. fwd time: 7.0277 / Avg. bwd time: 18.8439 / Avg. batch time: 524.3304 (ms) / GPU bubble ratio: 60.53%
[rank1]:2025-11-09 23:16:28,190 - INFO - Avg. fwd time: 9.0698 / Avg. bwd time: 24.1484 / Avg. batch time: 563.0172 (ms) / GPU bubble ratio: 52.80%
[rank0]:2025-11-09 23:16:28,197 - INFO - Avg. fwd time: 8.0682 / Avg. bwd time: 23.9186 / Avg. batch time: 600.0628 (ms) / GPU bubble ratio: 57.36%
[rank1]:2025-11-09 23:16:28,370 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.7230  memory: 14.64GiB(30.82%)  tps: 6,599  tflops: 50.26  mfu: 16.11%
[rank3]:2025-11-09 23:16:28,378 - INFO -  step: 500  loss:  0.1551  grad_norm:  0.7230  memory: 26.98GiB(56.79%)  tps: 6,599  tflops: 50.26  mfu: 16.11%
[rank2]:2025-11-09 23:16:28,366 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.7230  memory: 11.81GiB(24.85%)  tps: 6,599  tflops: 50.26  mfu: 16.11%
[rank0]:2025-11-09 23:16:28,381 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.7230  memory: 16.57GiB(34.88%)  tps: 6,599  tflops: 50.26  mfu: 16.11%
[rank3]:2025-11-09 23:16:28,527 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2316_real_step500_rank3.svg
[rank3]:> Batch Time: 600.06 ms, GPU Bubble Ratio: 57.07%, 55.62%, 65.38%, 27.57%
[rank0]:2025-11-09 23:18:30,298 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:18:32,497 - INFO - Avg. fwd time: 10.7430 / Avg. bwd time: 43.4586 / Avg. batch time: 493.1279 (ms) / GPU bubble ratio: 12.07%
[rank1]:2025-11-09 23:18:32,609 - INFO - Avg. fwd time: 9.0695 / Avg. bwd time: 24.1514 / Avg. batch time: 563.0833 (ms) / GPU bubble ratio: 52.80%
[rank0]:2025-11-09 23:18:32,616 - INFO - Avg. fwd time: 8.0689 / Avg. bwd time: 23.9206 / Avg. batch time: 600.1237 (ms) / GPU bubble ratio: 57.36%
[rank2]:2025-11-09 23:18:32,569 - INFO - Avg. fwd time: 7.0272 / Avg. bwd time: 18.8485 / Avg. batch time: 524.3976 (ms) / GPU bubble ratio: 60.53%
[rank3]:2025-11-09 23:18:32,795 - INFO -  step: 550  loss:  0.1562  grad_norm:  0.3259  memory: 26.98GiB(56.79%)  tps: 6,584  tflops: 50.15  mfu: 16.07%
[rank1]:2025-11-09 23:18:32,786 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3259  memory: 14.64GiB(30.82%)  tps: 6,584  tflops: 50.15  mfu: 16.07%
[rank0]:2025-11-09 23:18:32,798 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3259  memory: 16.57GiB(34.88%)  tps: 6,584  tflops: 50.15  mfu: 16.07%
[rank2]:2025-11-09 23:18:32,783 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3259  memory: 11.81GiB(24.85%)  tps: 6,584  tflops: 50.15  mfu: 16.07%
[rank0]:2025-11-09 23:20:35,056 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:20:37,484 - INFO - Avg. fwd time: 10.7463 / Avg. bwd time: 43.4772 / Avg. batch time: 493.3045 (ms) / GPU bubble ratio: 12.06%
[rank1]:2025-11-09 23:20:37,544 - INFO - Avg. fwd time: 9.0710 / Avg. bwd time: 24.1565 / Avg. batch time: 563.2440 (ms) / GPU bubble ratio: 52.81%
[rank0]:2025-11-09 23:20:37,576 - INFO - Avg. fwd time: 8.0678 / Avg. bwd time: 23.9238 / Avg. batch time: 600.2868 (ms) / GPU bubble ratio: 57.36%
[rank2]:2025-11-09 23:20:37,512 - INFO - Avg. fwd time: 7.0274 / Avg. bwd time: 18.8538 / Avg. batch time: 524.5500 (ms) / GPU bubble ratio: 60.53%
[rank3]:2025-11-09 23:20:37,610 - INFO -  step: 600  loss:  0.1478  grad_norm:  0.2355  memory: 26.98GiB(56.79%)  tps: 6,563  tflops: 49.99  mfu: 16.02%
[rank1]:2025-11-09 23:20:37,601 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2355  memory: 14.64GiB(30.82%)  tps: 6,563  tflops: 49.99  mfu: 16.02%
[rank0]:2025-11-09 23:20:37,612 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2355  memory: 16.57GiB(34.88%)  tps: 6,563  tflops: 49.99  mfu: 16.02%
[rank2]:2025-11-09 23:20:37,598 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.2355  memory: 11.81GiB(24.85%)  tps: 6,563  tflops: 49.99  mfu: 16.02%
[rank3]:2025-11-09 23:20:37,760 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2320_real_step600_rank3.svg
[rank3]:> Batch Time: 603.11 ms, GPU Bubble Ratio: 57.27%, 55.70%, 65.48%, 27.55%
[rank3]:2025-11-09 23:21:37,820 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:21:38,032 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:21:38,058 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:21:38,085 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:22:39,527 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:22:41,723 - INFO - Avg. fwd time: 10.7462 / Avg. bwd time: 43.4785 / Avg. batch time: 493.3139 (ms) / GPU bubble ratio: 12.06%
[rank2]:2025-11-09 23:22:41,792 - INFO - Avg. fwd time: 7.0273 / Avg. bwd time: 18.8563 / Avg. batch time: 524.5820 (ms) / GPU bubble ratio: 60.53%
[rank1]:2025-11-09 23:22:41,834 - INFO - Avg. fwd time: 9.0711 / Avg. bwd time: 24.1581 / Avg. batch time: 563.2796 (ms) / GPU bubble ratio: 52.81%
[rank0]:2025-11-09 23:22:41,841 - INFO - Avg. fwd time: 8.0691 / Avg. bwd time: 23.9252 / Avg. batch time: 600.3202 (ms) / GPU bubble ratio: 57.36%
[rank1]:2025-11-09 23:22:42,008 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.9081  memory: 14.64GiB(30.82%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank0]:2025-11-09 23:22:42,020 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.9081  memory: 16.57GiB(34.88%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank3]:2025-11-09 23:22:42,017 - INFO -  step: 650  loss:  0.1646  grad_norm:  0.9081  memory: 26.98GiB(56.79%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank2]:2025-11-09 23:22:42,005 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.9081  memory: 11.81GiB(24.85%)  tps: 6,585  tflops: 50.15  mfu: 16.07%
[rank0]:2025-11-09 23:24:43,559 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:24:45,792 - INFO - Avg. fwd time: 10.7435 / Avg. bwd time: 43.4693 / Avg. batch time: 493.2186 (ms) / GPU bubble ratio: 12.07%
[rank2]:2025-11-09 23:24:45,862 - INFO - Avg. fwd time: 7.0265 / Avg. bwd time: 18.8564 / Avg. batch time: 524.4589 (ms) / GPU bubble ratio: 60.52%
[rank1]:2025-11-09 23:24:45,902 - INFO - Avg. fwd time: 9.0696 / Avg. bwd time: 24.1554 / Avg. batch time: 563.1485 (ms) / GPU bubble ratio: 52.80%
[rank0]:2025-11-09 23:24:45,908 - INFO - Avg. fwd time: 8.0696 / Avg. bwd time: 23.9244 / Avg. batch time: 600.1865 (ms) / GPU bubble ratio: 57.35%
[rank1]:2025-11-09 23:24:46,080 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.2974  memory: 14.64GiB(30.82%)  tps: 6,603  tflops: 50.29  mfu: 16.12%
[rank3]:2025-11-09 23:24:46,089 - INFO -  step: 700  loss:  0.1420  grad_norm:  0.2974  memory: 26.98GiB(56.79%)  tps: 6,603  tflops: 50.29  mfu: 16.12%
[rank0]:2025-11-09 23:24:46,091 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.2974  memory: 16.57GiB(34.88%)  tps: 6,603  tflops: 50.29  mfu: 16.12%
[rank2]:2025-11-09 23:24:46,077 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.2974  memory: 11.81GiB(24.85%)  tps: 6,603  tflops: 50.29  mfu: 16.12%
[rank3]:2025-11-09 23:24:46,234 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2324_real_step700_rank3.svg
[rank3]:> Batch Time: 598.56 ms, GPU Bubble Ratio: 57.00%, 55.55%, 65.33%, 27.61%
[rank0]:2025-11-09 23:26:47,852 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:26:50,246 - INFO - Avg. fwd time: 10.7439 / Avg. bwd time: 43.4669 / Avg. batch time: 493.1992 (ms) / GPU bubble ratio: 12.07%
[rank1]:2025-11-09 23:26:50,307 - INFO - Avg. fwd time: 9.0694 / Avg. bwd time: 24.1555 / Avg. batch time: 563.1424 (ms) / GPU bubble ratio: 52.80%
[rank0]:2025-11-09 23:26:50,339 - INFO - Avg. fwd time: 8.0694 / Avg. bwd time: 23.9247 / Avg. batch time: 600.1751 (ms) / GPU bubble ratio: 57.35%
[rank2]:2025-11-09 23:26:50,274 - INFO - Avg. fwd time: 7.0262 / Avg. bwd time: 18.8581 / Avg. batch time: 524.4555 (ms) / GPU bubble ratio: 60.52%
[rank2]:2025-11-09 23:26:50,361 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4427  memory: 11.81GiB(24.85%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank1]:2025-11-09 23:26:50,364 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4427  memory: 14.64GiB(30.82%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank0]:2025-11-09 23:26:50,375 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4427  memory: 16.57GiB(34.88%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 23:26:50,374 - INFO -  step: 750  loss:  0.1556  grad_norm:  0.4427  memory: 26.98GiB(56.79%)  tps: 6,591  tflops: 50.20  mfu: 16.09%
[rank3]:2025-11-09 23:28:06,039 - WARNING - Dataset codealpaca is being re-looped
[rank2]:2025-11-09 23:28:06,252 - WARNING - Dataset codealpaca is being re-looped
[rank1]:2025-11-09 23:28:06,278 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:28:06,304 - WARNING - Dataset codealpaca is being re-looped
[rank0]:2025-11-09 23:28:52,286 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 23:28:54,516 - INFO - Avg. fwd time: 10.7447 / Avg. bwd time: 43.4734 / Avg. batch time: 493.2571 (ms) / GPU bubble ratio: 12.07%
[rank2]:2025-11-09 23:28:54,587 - INFO - Avg. fwd time: 7.0264 / Avg. bwd time: 18.8596 / Avg. batch time: 524.4953 (ms) / GPU bubble ratio: 60.52%
[rank1]:2025-11-09 23:28:54,626 - INFO - Avg. fwd time: 9.0699 / Avg. bwd time: 24.1572 / Avg. batch time: 563.1834 (ms) / GPU bubble ratio: 52.80%
[rank0]:2025-11-09 23:28:54,633 - INFO - Avg. fwd time: 8.0701 / Avg. bwd time: 23.9261 / Avg. batch time: 600.2123 (ms) / GPU bubble ratio: 57.35%
[rank1]:2025-11-09 23:28:54,803 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4629  memory: 14.64GiB(30.82%)  tps: 6,583  tflops: 50.14  mfu: 16.07%
[rank1]:2025-11-09 23:28:54,804 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4629  tps: 6,984  tflops: 53.19  mfu: 15.44%
[rank1]:2025-11-09 23:28:54,804 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 23:28:54,804 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 23:28:54,800 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4629  memory: 11.81GiB(24.85%)  tps: 6,583  tflops: 50.14  mfu: 16.07%
[rank2]:2025-11-09 23:28:54,800 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4629  tps: 6,984  tflops: 53.19  mfu: 15.44%
[rank2]:2025-11-09 23:28:54,800 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 23:28:54,801 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 23:28:54,814 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4629  memory: 16.57GiB(34.88%)  tps: 6,583  tflops: 50.14  mfu: 16.07%
[rank0]:2025-11-09 23:28:54,814 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4629  tps: 6,984  tflops: 53.19  mfu: 15.44%
[rank0]:2025-11-09 23:28:54,815 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 23:28:54,815 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 23:28:54,812 - INFO -  step: 800  loss:  0.1303  grad_norm:  0.4629  memory: 26.98GiB(56.79%)  tps: 6,583  tflops: 50.14  mfu: 16.07%
[rank3]:2025-11-09 23:28:54,813 - INFO -  final step: 800  loss:  0.1303  grad_norm:  0.4629  tps: 6,992  tflops: 53.25  mfu: 15.56%
[rank3]:2025-11-09 23:28:54,813 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 23:28:54,814 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 23:28:56,787 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 23:28:56,787 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 23:28:56,776 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 23:28:56,787 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-09 23:28:56,927 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2328_real_final800_rank3.svg
[rank3]:> Batch Time: 600.08 ms, GPU Bubble Ratio: 57.09%, 55.61%, 65.38%, 27.67%
[rank3]:2025-11-09 23:28:57,061 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1/pipeline_schedule/251109_2328_thry_final800_rank3.svg
[rank3]:> Batch Time: 287.83 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 23:28:57,061 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 23:28:57,208 - INFO - Process group destroyed
[rank2]:2025-11-09 23:28:57,159 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 231-240
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.34324
[rank3]:wandb:               final/avg_loss 0.13032
[rank3]:wandb:             final/avg_mfu(%) 15.56074
[rank3]:wandb:             final/avg_tflops 53.25277
[rank3]:wandb:    final/avg_throughput(tps) 6992.02734
[rank3]:wandb:              final/grad_norm 0.4629
[rank3]:wandb:               final/max_loss 0.13032
[rank3]:wandb:                    grad_norm 0.4629
[rank3]:wandb: loss_metrics/global_avg_loss 0.13032
[rank3]:wandb: loss_metrics/global_max_loss 0.13032
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed11_codealpaca_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/dkflsloh
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_codealpaca_dm1/20251109-2255/wandb/run-20251109_225541-dkflsloh/logs
[rank3]:2025-11-09 23:28:58,163 - INFO - Process group destroyed
[rank0]:2025-11-09 23:28:58,787 - INFO - Training completed
[rank0]:2025-11-09 23:28:58,788 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 23:28:59,214 - INFO - Process group destroyed
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.0', 'layers.3', 'layers.2', 'layers.1'}
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.10', 'layers.9', 'layers.11'}
[rank1]:Stage 1: Modules to keep: {'layers.8', 'layers.5', 'layers.4', 'layers.6', 'layers.7'}
[rank3]:Stage 3: Modules to keep: {'layers.15', 'layers.14', 'output', 'norm', 'layers.13'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.1
[rank3]:	- training:
[rank3]:		- dataset: codealpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 11
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed11_codealpaca_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
