
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 14:46:51 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank1]:2025-11-09 14:46:57,287 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank3]:2025-11-09 14:46:57,285 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank1]:2025-11-09 14:46:57,442 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 14:46:57,445 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 14:46:57,376 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank0]:2025-11-09 14:46:57,509 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank3]:2025-11-09 14:46:57,533 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 14:46:57,536 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 14:46:57,603 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 14:46:57,606 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 14:46:57,703 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 14:46:57,706 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 14:46:57,709 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 14:46:57,710 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-09 14:46:58,093 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank1]:2025-11-09 14:47:00,809 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 14:47:00,847 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 14:47:00,875 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 14:47:00,875 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 14:47:00,886 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-09 14:47:00,919 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 14:47:01,036 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 14:47:00,959 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 14:47:00,988 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 14:47:00,988 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 14:47:01,060 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 14:47:01,060 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 14:47:01,061 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-09 14:47:01,076 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 14:47:01,077 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 14:47:01,111 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 14:47:01,112 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 14:47:01,172 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 14:47:01,172 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 14:47:01,173 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-09 14:47:01,313 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 14:47:01,313 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 14:47:01,314 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 2sac0to8
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1/20251109-1447/wandb/run-20251109_144702-2sac0to8
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed42_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/2sac0to8
[rank3]:2025-11-09 14:47:03,184 - INFO - WandB logging enabled
[rank3]:2025-11-09 14:47:03,184 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 14:47:03,221 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 14:47:03,249 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 14:47:03,249 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 14:47:03,448 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 14:47:03,448 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 14:47:03,448 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1
[rank0]:2025-11-09 14:47:03,448 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 14:47:03,449 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 14:47:03,449 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank3]:2025-11-09 14:47:03,431 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 14:47:03,431 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 14:47:03,432 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 14:47:03,448 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 14:47:06,348 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-11-09 14:47:06,348 - INFO - Finished loading the checkpoint in 2.90 seconds.
[rank0]:2025-11-09 14:47:06,348 - INFO - Training starts at step 1
[rank1]:2025-11-09 14:47:09,431 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory: 12.38GiB(26.05%)  tps: 1,909  tflops: 14.54  mfu: 4.66%
[rank1]:2025-11-09 14:47:09,431 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 14:47:09,462 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory: 12.80GiB(26.95%)  tps: 1,954  tflops: 14.88  mfu: 4.77%
[rank0]:2025-11-09 14:47:09,462 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 14:47:09,428 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5955  memory:  9.99GiB(21.03%)  tps: 1,935  tflops: 14.74  mfu: 4.72%
[rank2]:2025-11-09 14:47:09,428 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-09 14:47:09,439 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5955  memory: 24.19GiB(50.91%)  tps: 2,636  tflops: 20.08  mfu: 6.43%
[rank3]:2025-11-09 14:47:09,439 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 14:49:10,660 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 14:49:12,928 - INFO - Avg. fwd time: 11.2175 / Avg. bwd time: 43.7795 / Avg. batch time: 499.9513 (ms) / GPU bubble ratio: 12.00%
[rank2]:2025-11-09 14:49:13,004 - INFO - Avg. fwd time: 7.1341 / Avg. bwd time: 18.7167 / Avg. batch time: 531.7313 (ms) / GPU bubble ratio: 61.11%
[rank0]:2025-11-09 14:49:13,051 - INFO - Avg. fwd time: 7.8526 / Avg. bwd time: 23.3160 / Avg. batch time: 607.6440 (ms) / GPU bubble ratio: 58.96%
[rank1]:2025-11-09 14:49:13,044 - INFO - Avg. fwd time: 9.0343 / Avg. bwd time: 23.7264 / Avg. batch time: 570.8055 (ms) / GPU bubble ratio: 54.08%
[rank2]:2025-11-09 14:49:13,226 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5445  memory: 11.81GiB(24.85%)  tps: 6,485  tflops: 49.39  mfu: 15.83%
[rank1]:2025-11-09 14:49:13,230 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5445  memory: 14.64GiB(30.82%)  tps: 6,485  tflops: 49.39  mfu: 15.83%
[rank3]:2025-11-09 14:49:13,238 - INFO -  step: 50  loss:  8.2842  grad_norm: 17.5445  memory: 26.98GiB(56.79%)  tps: 6,485  tflops: 49.39  mfu: 15.83%
[rank0]:2025-11-09 14:49:13,241 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5445  memory: 16.57GiB(34.88%)  tps: 6,486  tflops: 49.40  mfu: 15.83%
[rank0]:2025-11-09 14:51:19,805 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 14:51:22,140 - INFO - Avg. fwd time: 11.3669 / Avg. bwd time: 44.6697 / Avg. batch time: 508.1336 (ms) / GPU bubble ratio: 11.78%
[rank2]:2025-11-09 14:51:22,215 - INFO - Avg. fwd time: 7.1485 / Avg. bwd time: 18.8192 / Avg. batch time: 539.5765 (ms) / GPU bubble ratio: 61.50%
[rank0]:2025-11-09 14:51:22,262 - INFO - Avg. fwd time: 7.8684 / Avg. bwd time: 23.4027 / Avg. batch time: 614.8268 (ms) / GPU bubble ratio: 59.31%
[rank1]:2025-11-09 14:51:22,254 - INFO - Avg. fwd time: 9.0791 / Avg. bwd time: 23.8872 / Avg. batch time: 578.3440 (ms) / GPU bubble ratio: 54.40%
[rank2]:2025-11-09 14:51:22,441 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1200  memory: 11.81GiB(24.85%)  tps: 6,340  tflops: 48.29  mfu: 15.48%
[rank0]:2025-11-09 14:51:22,456 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1200  memory: 16.57GiB(34.88%)  tps: 6,340  tflops: 48.29  mfu: 15.48%
[rank3]:2025-11-09 14:51:22,454 - INFO -  step: 100  loss:  4.2830  grad_norm: 19.1200  memory: 26.98GiB(56.79%)  tps: 6,340  tflops: 48.29  mfu: 15.48%
[rank1]:2025-11-09 14:51:22,445 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1200  memory: 14.64GiB(30.82%)  tps: 6,340  tflops: 48.29  mfu: 15.48%
[rank3]:2025-11-09 14:51:22,666 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1451_real_step100_rank3.svg
[rank3]:> Batch Time: 622.58 ms, GPU Bubble Ratio: 59.36%, 57.23%, 66.37%, 26.40%
[rank0]:2025-11-09 14:53:29,802 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 14:53:32,334 - INFO - Avg. fwd time: 7.1521 / Avg. bwd time: 18.8607 / Avg. batch time: 543.3368 (ms) / GPU bubble ratio: 61.70%
[rank3]:2025-11-09 14:53:32,309 - INFO - Avg. fwd time: 11.4237 / Avg. bwd time: 45.0648 / Avg. batch time: 511.7134 (ms) / GPU bubble ratio: 11.69%
[rank1]:2025-11-09 14:53:32,364 - INFO - Avg. fwd time: 9.0936 / Avg. bwd time: 23.9548 / Avg. batch time: 582.0314 (ms) / GPU bubble ratio: 54.58%
[rank0]:2025-11-09 14:53:32,394 - INFO - Avg. fwd time: 7.8773 / Avg. bwd time: 23.4366 / Avg. batch time: 618.4243 (ms) / GPU bubble ratio: 59.49%
[rank0]:2025-11-09 14:53:32,429 - INFO -  step: 150  loss: -4.0000  grad_norm: 108.4351  memory: 16.57GiB(34.88%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank2]:2025-11-09 14:53:32,415 - INFO -  step: 150  loss: -4.0000  grad_norm: 108.4351  memory: 11.81GiB(24.85%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank3]:2025-11-09 14:53:32,427 - INFO -  step: 150  loss:  5.1829  grad_norm: 108.4351  memory: 26.98GiB(56.79%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank1]:2025-11-09 14:53:32,419 - INFO -  step: 150  loss: -4.0000  grad_norm: 108.4351  memory: 14.64GiB(30.82%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank0]:2025-11-09 14:55:39,702 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 14:55:42,045 - INFO - Avg. fwd time: 11.4454 / Avg. bwd time: 45.2287 / Avg. batch time: 513.1777 (ms) / GPU bubble ratio: 11.65%
[rank2]:2025-11-09 14:55:42,120 - INFO - Avg. fwd time: 7.1520 / Avg. bwd time: 18.8900 / Avg. batch time: 544.7154 (ms) / GPU bubble ratio: 61.75%
[rank1]:2025-11-09 14:55:42,160 - INFO - Avg. fwd time: 9.0968 / Avg. bwd time: 24.0054 / Avg. batch time: 583.4106 (ms) / GPU bubble ratio: 54.61%
[rank0]:2025-11-09 14:55:42,168 - INFO - Avg. fwd time: 7.8824 / Avg. bwd time: 23.4620 / Avg. batch time: 619.7816 (ms) / GPU bubble ratio: 59.54%
[rank3]:2025-11-09 14:55:42,358 - INFO -  step: 200  loss:  1.0639  grad_norm: 10.5842  memory: 26.98GiB(56.79%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank1]:2025-11-09 14:55:42,350 - INFO -  step: 200  loss: -4.0000  grad_norm: 10.5842  memory: 14.64GiB(30.82%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-09 14:55:42,346 - INFO -  step: 200  loss: -4.0000  grad_norm: 10.5842  memory: 11.81GiB(24.85%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 14:55:42,361 - INFO -  step: 200  loss: -4.0000  grad_norm: 10.5842  memory: 16.57GiB(34.88%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-09 14:55:42,509 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1455_real_step200_rank3.svg
[rank3]:> Batch Time: 622.66 ms, GPU Bubble Ratio: 59.32%, 57.10%, 66.28%, 26.38%
[rank0]:2025-11-09 14:57:50,066 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 14:57:52,373 - INFO - Avg. fwd time: 11.4680 / Avg. bwd time: 45.3760 / Avg. batch time: 514.5286 (ms) / GPU bubble ratio: 11.62%
[rank2]:2025-11-09 14:57:52,450 - INFO - Avg. fwd time: 7.1511 / Avg. bwd time: 18.9186 / Avg. batch time: 546.1326 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 14:57:52,490 - INFO - Avg. fwd time: 9.1012 / Avg. bwd time: 24.0473 / Avg. batch time: 584.8470 (ms) / GPU bubble ratio: 54.66%
[rank0]:2025-11-09 14:57:52,497 - INFO - Avg. fwd time: 7.8877 / Avg. bwd time: 23.4816 / Avg. batch time: 621.2045 (ms) / GPU bubble ratio: 59.60%
[rank0]:2025-11-09 14:57:52,687 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.0966  memory: 16.57GiB(34.88%)  tps: 6,286  tflops: 47.87  mfu: 15.34%
[rank2]:2025-11-09 14:57:52,672 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.0966  memory: 11.81GiB(24.85%)  tps: 6,286  tflops: 47.87  mfu: 15.34%
[rank3]:2025-11-09 14:57:52,685 - INFO -  step: 250  loss:  0.5936  grad_norm:  2.0966  memory: 26.98GiB(56.79%)  tps: 6,286  tflops: 47.87  mfu: 15.34%
[rank1]:2025-11-09 14:57:52,676 - INFO -  step: 250  loss: -4.0000  grad_norm:  2.0966  memory: 14.64GiB(30.82%)  tps: 6,286  tflops: 47.87  mfu: 15.34%
[rank0]:2025-11-09 14:59:59,969 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:00:02,506 - INFO - Avg. fwd time: 11.4753 / Avg. bwd time: 45.4553 / Avg. batch time: 515.2158 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 15:00:02,533 - INFO - Avg. fwd time: 7.1499 / Avg. bwd time: 18.9338 / Avg. batch time: 546.7646 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 15:00:02,563 - INFO - Avg. fwd time: 9.1027 / Avg. bwd time: 24.0680 / Avg. batch time: 585.4865 (ms) / GPU bubble ratio: 54.68%
[rank1]:2025-11-09 15:00:02,618 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4804  memory: 14.64GiB(30.82%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 15:00:02,593 - INFO - Avg. fwd time: 7.8882 / Avg. bwd time: 23.4908 / Avg. batch time: 621.8294 (ms) / GPU bubble ratio: 59.63%
[rank0]:2025-11-09 15:00:02,629 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4804  memory: 16.57GiB(34.88%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-09 15:00:02,615 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4804  memory: 11.81GiB(24.85%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-09 15:00:02,628 - INFO -  step: 300  loss:  0.5193  grad_norm:  0.4804  memory: 26.98GiB(56.79%)  tps: 6,304  tflops: 48.02  mfu: 15.39%
[rank3]:2025-11-09 15:00:02,777 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1500_real_step300_rank3.svg
[rank3]:> Batch Time: 623.11 ms, GPU Bubble Ratio: 59.39%, 57.21%, 66.35%, 26.45%
[rank0]:2025-11-09 15:02:09,344 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:02:11,644 - INFO - Avg. fwd time: 11.4705 / Avg. bwd time: 45.4615 / Avg. batch time: 515.2176 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 15:02:11,717 - INFO - Avg. fwd time: 7.1480 / Avg. bwd time: 18.9380 / Avg. batch time: 546.8060 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-09 15:02:11,757 - INFO - Avg. fwd time: 9.0999 / Avg. bwd time: 24.0740 / Avg. batch time: 585.5209 (ms) / GPU bubble ratio: 54.67%
[rank0]:2025-11-09 15:02:11,765 - INFO - Avg. fwd time: 7.8908 / Avg. bwd time: 23.4915 / Avg. batch time: 621.8458 (ms) / GPU bubble ratio: 59.63%
[rank3]:2025-11-09 15:02:11,951 - INFO -  step: 350  loss:  0.4789  grad_norm:  0.4213  memory: 26.98GiB(56.79%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank1]:2025-11-09 15:02:11,942 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4213  memory: 14.64GiB(30.82%)  tps: 6,335  tflops: 48.24  mfu: 15.46%
[rank0]:2025-11-09 15:02:11,953 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4213  memory: 16.57GiB(34.88%)  tps: 6,334  tflops: 48.24  mfu: 15.46%
[rank2]:2025-11-09 15:02:11,938 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4213  memory: 11.81GiB(24.85%)  tps: 6,335  tflops: 48.25  mfu: 15.46%
[rank0]:2025-11-09 15:04:18,545 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:04:20,865 - INFO - Avg. fwd time: 11.4652 / Avg. bwd time: 45.4627 / Avg. batch time: 515.1824 (ms) / GPU bubble ratio: 11.60%
[rank2]:2025-11-09 15:04:20,938 - INFO - Avg. fwd time: 7.1456 / Avg. bwd time: 18.9394 / Avg. batch time: 546.7140 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 15:04:20,986 - INFO - Avg. fwd time: 7.8937 / Avg. bwd time: 23.4922 / Avg. batch time: 621.7272 (ms) / GPU bubble ratio: 59.61%
[rank1]:2025-11-09 15:04:20,978 - INFO - Avg. fwd time: 9.0960 / Avg. bwd time: 24.0788 / Avg. batch time: 585.4189 (ms) / GPU bubble ratio: 54.67%
[rank2]:2025-11-09 15:04:21,161 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3787  memory: 11.81GiB(24.85%)  tps: 6,339  tflops: 48.28  mfu: 15.48%
[rank0]:2025-11-09 15:04:21,176 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3787  memory: 16.57GiB(34.88%)  tps: 6,339  tflops: 48.28  mfu: 15.48%
[rank3]:2025-11-09 15:04:21,173 - INFO -  step: 400  loss:  0.4672  grad_norm:  0.3787  memory: 26.98GiB(56.79%)  tps: 6,340  tflops: 48.28  mfu: 15.48%
[rank1]:2025-11-09 15:04:21,165 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3787  memory: 14.64GiB(30.82%)  tps: 6,339  tflops: 48.28  mfu: 15.48%
[rank3]:2025-11-09 15:04:21,323 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1504_real_step400_rank3.svg
[rank3]:> Batch Time: 621.11 ms, GPU Bubble Ratio: 59.26%, 57.14%, 66.31%, 26.48%
[rank3]:2025-11-09 15:04:30,054 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 15:04:30,272 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 15:04:30,299 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 15:04:30,324 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 15:06:27,925 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:06:30,472 - INFO - Avg. fwd time: 7.1441 / Avg. bwd time: 18.9418 / Avg. batch time: 546.8032 (ms) / GPU bubble ratio: 61.84%
[rank3]:2025-11-09 15:06:30,445 - INFO - Avg. fwd time: 11.4609 / Avg. bwd time: 45.4762 / Avg. batch time: 515.2485 (ms) / GPU bubble ratio: 11.60%
[rank1]:2025-11-09 15:06:30,503 - INFO - Avg. fwd time: 9.0943 / Avg. bwd time: 24.0870 / Avg. batch time: 585.5170 (ms) / GPU bubble ratio: 54.66%
[rank2]:2025-11-09 15:06:30,555 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3980  memory: 11.81GiB(24.85%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank0]:2025-11-09 15:06:30,534 - INFO - Avg. fwd time: 7.8912 / Avg. bwd time: 23.4952 / Avg. batch time: 621.8212 (ms) / GPU bubble ratio: 59.62%
[rank0]:2025-11-09 15:06:30,570 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3980  memory: 16.57GiB(34.88%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank3]:2025-11-09 15:06:30,569 - INFO -  step: 450  loss:  0.5235  grad_norm:  0.3980  memory: 26.98GiB(56.79%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank1]:2025-11-09 15:06:30,559 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3980  memory: 14.64GiB(30.82%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank0]:2025-11-09 15:08:37,587 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:08:39,908 - INFO - Avg. fwd time: 11.4611 / Avg. bwd time: 45.4953 / Avg. batch time: 515.4006 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 15:08:39,981 - INFO - Avg. fwd time: 7.1438 / Avg. bwd time: 18.9442 / Avg. batch time: 546.9213 (ms) / GPU bubble ratio: 61.84%
[rank0]:2025-11-09 15:08:40,029 - INFO - Avg. fwd time: 7.8903 / Avg. bwd time: 23.4969 / Avg. batch time: 621.9532 (ms) / GPU bubble ratio: 59.63%
[rank1]:2025-11-09 15:08:40,021 - INFO - Avg. fwd time: 9.0954 / Avg. bwd time: 24.0944 / Avg. batch time: 585.6506 (ms) / GPU bubble ratio: 54.66%
[rank2]:2025-11-09 15:08:40,201 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3215  memory: 11.81GiB(24.85%)  tps: 6,319  tflops: 48.13  mfu: 15.42%
[rank0]:2025-11-09 15:08:40,216 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3215  memory: 16.57GiB(34.88%)  tps: 6,319  tflops: 48.12  mfu: 15.42%
[rank3]:2025-11-09 15:08:40,214 - INFO -  step: 500  loss:  0.3967  grad_norm:  0.3215  memory: 26.98GiB(56.79%)  tps: 6,319  tflops: 48.13  mfu: 15.43%
[rank1]:2025-11-09 15:08:40,205 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3215  memory: 14.64GiB(30.82%)  tps: 6,319  tflops: 48.12  mfu: 15.42%
[rank3]:2025-11-09 15:08:40,363 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1508_real_step500_rank3.svg
[rank3]:> Batch Time: 622.66 ms, GPU Bubble Ratio: 59.35%, 57.08%, 66.35%, 26.60%
[rank0]:2025-11-09 15:10:47,525 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:10:49,835 - INFO - Avg. fwd time: 11.4640 / Avg. bwd time: 45.5191 / Avg. batch time: 515.6102 (ms) / GPU bubble ratio: 11.59%
[rank1]:2025-11-09 15:10:49,951 - INFO - Avg. fwd time: 9.0978 / Avg. bwd time: 24.1027 / Avg. batch time: 585.9026 (ms) / GPU bubble ratio: 54.67%
[rank2]:2025-11-09 15:10:49,911 - INFO - Avg. fwd time: 7.1444 / Avg. bwd time: 18.9472 / Avg. batch time: 547.1523 (ms) / GPU bubble ratio: 61.85%
[rank0]:2025-11-09 15:10:49,958 - INFO - Avg. fwd time: 7.8912 / Avg. bwd time: 23.4988 / Avg. batch time: 622.2068 (ms) / GPU bubble ratio: 59.64%
[rank3]:2025-11-09 15:10:50,141 - INFO -  step: 550  loss:  0.4421  grad_norm:  0.3920  memory: 26.98GiB(56.79%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank1]:2025-11-09 15:10:50,132 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3920  memory: 14.64GiB(30.82%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank2]:2025-11-09 15:10:50,128 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3920  memory: 11.81GiB(24.85%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 15:10:50,143 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3920  memory: 16.57GiB(34.88%)  tps: 6,305  tflops: 48.02  mfu: 15.39%
[rank0]:2025-11-09 15:12:57,357 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:12:59,877 - INFO - Avg. fwd time: 11.4669 / Avg. bwd time: 45.5388 / Avg. batch time: 515.7917 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 15:12:59,904 - INFO - Avg. fwd time: 7.1447 / Avg. bwd time: 18.9497 / Avg. batch time: 547.3044 (ms) / GPU bubble ratio: 61.86%
[rank0]:2025-11-09 15:12:59,968 - INFO - Avg. fwd time: 7.8912 / Avg. bwd time: 23.5008 / Avg. batch time: 622.3814 (ms) / GPU bubble ratio: 59.65%
[rank3]:2025-11-09 15:13:00,003 - INFO -  step: 600  loss:  0.4438  grad_norm:  0.3795  memory: 26.98GiB(56.79%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 15:12:59,937 - INFO - Avg. fwd time: 9.0997 / Avg. bwd time: 24.1117 / Avg. batch time: 586.0757 (ms) / GPU bubble ratio: 54.67%
[rank1]:2025-11-09 15:12:59,993 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3795  memory: 14.64GiB(30.82%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank2]:2025-11-09 15:12:59,990 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3795  memory: 11.81GiB(24.85%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 15:13:00,004 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3795  memory: 16.57GiB(34.88%)  tps: 6,308  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 15:13:00,153 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1513_real_step600_rank3.svg
[rank3]:> Batch Time: 623.64 ms, GPU Bubble Ratio: 59.44%, 57.15%, 66.39%, 26.72%
[rank0]:2025-11-09 15:15:06,916 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:15:09,207 - INFO - Avg. fwd time: 11.4655 / Avg. bwd time: 45.5380 / Avg. batch time: 515.7726 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 15:15:09,280 - INFO - Avg. fwd time: 7.1447 / Avg. bwd time: 18.9503 / Avg. batch time: 547.3079 (ms) / GPU bubble ratio: 61.86%
[rank0]:2025-11-09 15:15:09,328 - INFO - Avg. fwd time: 7.8907 / Avg. bwd time: 23.5002 / Avg. batch time: 622.3969 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 15:15:09,320 - INFO - Avg. fwd time: 9.1001 / Avg. bwd time: 24.1160 / Avg. batch time: 586.0938 (ms) / GPU bubble ratio: 54.66%
[rank2]:2025-11-09 15:15:09,496 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3755  memory: 11.81GiB(24.85%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank0]:2025-11-09 15:15:09,511 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3755  memory: 16.57GiB(34.88%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank1]:2025-11-09 15:15:09,500 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3755  memory: 14.64GiB(30.82%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank3]:2025-11-09 15:15:09,509 - INFO -  step: 650  loss:  0.3758  grad_norm:  0.3755  memory: 26.98GiB(56.79%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank0]:2025-11-09 15:17:16,165 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:17:18,479 - INFO - Avg. fwd time: 11.4609 / Avg. bwd time: 45.5323 / Avg. batch time: 515.6839 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 15:17:18,603 - INFO - Avg. fwd time: 7.8902 / Avg. bwd time: 23.4991 / Avg. batch time: 622.3007 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 15:17:18,595 - INFO - Avg. fwd time: 9.1003 / Avg. bwd time: 24.1198 / Avg. batch time: 585.9982 (ms) / GPU bubble ratio: 54.65%
[rank2]:2025-11-09 15:17:18,556 - INFO - Avg. fwd time: 7.1443 / Avg. bwd time: 18.9498 / Avg. batch time: 547.1902 (ms) / GPU bubble ratio: 61.85%
[rank0]:2025-11-09 15:17:18,789 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4270  memory: 16.57GiB(34.88%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank1]:2025-11-09 15:17:18,778 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4270  memory: 14.64GiB(30.82%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank2]:2025-11-09 15:17:18,774 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4270  memory: 11.81GiB(24.85%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 15:17:18,787 - INFO -  step: 700  loss:  0.4445  grad_norm:  0.4270  memory: 26.98GiB(56.79%)  tps: 6,337  tflops: 48.26  mfu: 15.47%
[rank3]:2025-11-09 15:17:18,936 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1517_real_step700_rank3.svg
[rank3]:> Batch Time: 622.16 ms, GPU Bubble Ratio: 59.40%, 57.14%, 66.34%, 26.81%
[rank0]:2025-11-09 15:19:25,846 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 15:19:28,391 - INFO - Avg. fwd time: 7.1442 / Avg. bwd time: 18.9506 / Avg. batch time: 547.2370 (ms) / GPU bubble ratio: 61.85%
[rank3]:2025-11-09 15:19:28,362 - INFO - Avg. fwd time: 11.4603 / Avg. bwd time: 45.5368 / Avg. batch time: 515.7136 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 15:19:28,456 - INFO - Avg. fwd time: 7.8896 / Avg. bwd time: 23.4998 / Avg. batch time: 622.3613 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-09 15:19:28,492 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4597  memory: 16.57GiB(34.88%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-09 15:19:28,477 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4597  memory: 11.81GiB(24.85%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank1]:2025-11-09 15:19:28,423 - INFO - Avg. fwd time: 9.1006 / Avg. bwd time: 24.1249 / Avg. batch time: 586.0582 (ms) / GPU bubble ratio: 54.65%
[rank1]:2025-11-09 15:19:28,481 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4597  memory: 14.64GiB(30.82%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 15:19:28,490 - INFO -  step: 750  loss:  0.4226  grad_norm:  0.4597  memory: 26.98GiB(56.79%)  tps: 6,316  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 15:21:35,529 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 15:21:37,852 - INFO - Avg. fwd time: 11.4605 / Avg. bwd time: 45.5452 / Avg. batch time: 515.7787 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 15:21:37,927 - INFO - Avg. fwd time: 7.1443 / Avg. bwd time: 18.9516 / Avg. batch time: 547.2829 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-09 15:21:37,966 - INFO - Avg. fwd time: 9.1014 / Avg. bwd time: 24.1301 / Avg. batch time: 586.1114 (ms) / GPU bubble ratio: 54.64%
[rank0]:2025-11-09 15:21:37,974 - INFO - Avg. fwd time: 7.8892 / Avg. bwd time: 23.5008 / Avg. batch time: 622.4149 (ms) / GPU bubble ratio: 59.65%
[rank2]:2025-11-09 15:21:38,146 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4057  memory: 11.81GiB(24.85%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank2]:2025-11-09 15:21:38,146 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4057  tps: 6,704  tflops: 51.06  mfu: 14.82%
[rank2]:2025-11-09 15:21:38,146 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 15:21:38,147 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 15:21:38,150 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4057  memory: 14.64GiB(30.82%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank1]:2025-11-09 15:21:38,150 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4057  tps: 6,704  tflops: 51.06  mfu: 14.81%
[rank1]:2025-11-09 15:21:38,150 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 15:21:38,151 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-09 15:21:38,159 - INFO -  step: 800  loss:  0.4281  grad_norm:  0.4057  memory: 26.98GiB(56.79%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank3]:2025-11-09 15:21:38,159 - INFO -  final step: 800  loss:  0.4281  grad_norm:  0.4057  tps: 6,712  tflops: 51.12  mfu: 14.92%
[rank3]:2025-11-09 15:21:38,160 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 15:21:38,160 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 15:21:38,160 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4057  memory: 16.57GiB(34.88%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank0]:2025-11-09 15:21:38,160 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4057  tps: 6,705  tflops: 51.07  mfu: 14.82%
[rank0]:2025-11-09 15:21:38,160 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 15:21:38,161 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 15:21:40,062 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank2]:2025-11-09 15:21:40,075 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 15:21:40,075 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 15:21:40,075 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-09 15:21:40,218 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1521_real_final800_rank3.svg
[rank3]:> Batch Time: 622.62 ms, GPU Bubble Ratio: 59.38%, 57.18%, 66.35%, 26.62%
[rank3]:2025-11-09 15:21:40,380 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_1521_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.80 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 15:21:40,381 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank2]:2025-11-09 15:21:40,513 - INFO - Process group destroyed
[rank1]:2025-11-09 15:21:40,515 - INFO - Process group destroyed
[rank3]:wandb: uploading history steps 15-16, summary, console lines 226-235
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–…â–â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–…â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44106
[rank3]:wandb:               final/avg_loss 0.42809
[rank3]:wandb:             final/avg_mfu(%) 14.91842
[rank3]:wandb:             final/avg_tflops 51.11887
[rank3]:wandb:    final/avg_throughput(tps) 6711.84856
[rank3]:wandb:              final/grad_norm 0.40568
[rank3]:wandb:               final/max_loss 0.42809
[rank3]:wandb:                    grad_norm 0.40568
[rank3]:wandb: loss_metrics/global_avg_loss 0.42809
[rank3]:wandb: loss_metrics/global_max_loss 0.42809
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed42_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/2sac0to8
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1/20251109-1447/wandb/run-20251109_144702-2sac0to8/logs
[rank3]:2025-11-09 15:21:41,519 - INFO - Process group destroyed
[rank0]:2025-11-09 15:21:42,075 - INFO - Training completed
[rank0]:2025-11-09 15:21:42,076 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 15:21:42,527 - INFO - Process group destroyed
[rank1]:Stage 1: Modules to keep: {'layers.5', 'layers.8', 'layers.7', 'layers.6', 'layers.4'}
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.11', 'layers.10', 'layers.12'}
[rank0]:Stage 0: Modules to keep: {'layers.2', 'layers.1', 'layers.0', 'tok_embeddings', 'layers.3'}
[rank3]:Stage 3: Modules to keep: {'layers.14', 'output', 'layers.15', 'norm', 'layers.13'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed42_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed42_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 20:21:21 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank1]:2025-11-09 20:21:27,365 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank2]:2025-11-09 20:21:27,385 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:"
[rank3]:2025-11-09 20:21:27,528 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank2]:2025-11-09 20:21:27,600 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 20:21:27,602 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:21:27,609 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank1]:2025-11-09 20:21:27,604 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 20:21:27,606 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-09 20:21:27,719 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 20:21:27,722 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:21:27,804 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 20:21:27,807 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 20:21:27,810 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 20:21:27,812 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-09 20:21:28,195 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 20:21:30,912 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-09 20:21:30,930 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 20:21:30,969 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:21:30,991 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 20:21:30,991 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:21:31,063 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 20:21:31,101 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:21:31,102 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 20:21:31,128 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 20:21:31,128 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 20:21:31,191 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 20:21:31,191 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 20:21:31,192 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-09 20:21:31,303 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 20:21:31,314 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 20:21:31,314 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 20:21:31,314 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 20:21:31,343 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:21:31,371 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 20:21:31,371 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 20:21:31,554 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 20:21:31,554 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 20:21:31,555 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run kj8c8tyq
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1/20251109-2021/wandb/run-20251109_202132-kj8c8tyq
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed42_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/kj8c8tyq
[rank3]:2025-11-09 20:21:33,474 - INFO - WandB logging enabled
[rank3]:2025-11-09 20:21:33,475 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 20:21:33,514 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:21:33,543 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 20:21:33,543 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 20:21:33,744 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1
[rank0]:2025-11-09 20:21:33,745 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 20:21:33,745 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 20:21:33,745 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1.
[rank0]:2025-11-09 20:21:33,746 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1/step-800.
[rank1]:2025-11-09 20:21:33,745 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 20:21:33,746 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1.
[rank3]:2025-11-09 20:21:33,728 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 20:21:33,729 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 20:21:33,729 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 20:21:33,744 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 20:21:33,745 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1.
[rank2]:2025-11-09 20:21:33,745 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 20:21:33,746 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1.
[rank0]:2025-11-09 20:21:36,260 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 20:21:36,260 - INFO - Finished loading the checkpoint in 2.51 seconds.
[rank0]:2025-11-09 20:21:36,261 - INFO - Training starts at step 1
[rank3]:2025-11-09 20:21:39,343 - INFO -  step:  1  loss:  0.4191  grad_norm:  0.3540  memory: 24.19GiB(50.91%)  tps: 2,812  tflops: 21.42  mfu: 6.87%
[rank3]:2025-11-09 20:21:39,343 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 20:21:39,339 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3540  memory: 12.38GiB(26.05%)  tps: 2,049  tflops: 15.61  mfu: 5.00%
[rank1]:2025-11-09 20:21:39,339 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 20:21:39,371 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3540  memory: 12.80GiB(26.95%)  tps: 1,981  tflops: 15.09  mfu: 4.84%
[rank0]:2025-11-09 20:21:39,371 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 20:21:39,333 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3540  memory:  9.99GiB(21.03%)  tps: 1,959  tflops: 14.92  mfu: 4.78%
[rank2]:2025-11-09 20:21:39,333 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 20:23:41,028 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:23:43,304 - INFO - Avg. fwd time: 11.1532 / Avg. bwd time: 44.0083 / Avg. batch time: 501.3594 (ms) / GPU bubble ratio: 11.98%
[rank2]:2025-11-09 20:23:43,377 - INFO - Avg. fwd time: 7.0995 / Avg. bwd time: 18.8196 / Avg. batch time: 533.3886 (ms) / GPU bubble ratio: 61.13%
[rank0]:2025-11-09 20:23:43,424 - INFO - Avg. fwd time: 7.8238 / Avg. bwd time: 23.3707 / Avg. batch time: 609.6455 (ms) / GPU bubble ratio: 59.07%
[rank1]:2025-11-09 20:23:43,416 - INFO - Avg. fwd time: 8.9887 / Avg. bwd time: 23.9076 / Avg. batch time: 572.7190 (ms) / GPU bubble ratio: 54.05%
[rank2]:2025-11-09 20:23:43,600 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.3597  memory: 11.81GiB(24.85%)  tps: 6,460  tflops: 49.20  mfu: 15.77%
[rank3]:2025-11-09 20:23:43,612 - INFO -  step: 50  loss:  0.4028  grad_norm:  0.3597  memory: 26.98GiB(56.79%)  tps: 6,460  tflops: 49.20  mfu: 15.77%
[rank0]:2025-11-09 20:23:43,614 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.3597  memory: 16.57GiB(34.88%)  tps: 6,462  tflops: 49.21  mfu: 15.77%
[rank1]:2025-11-09 20:23:43,604 - INFO -  step: 50  loss: -4.0000  grad_norm:  0.3597  memory: 14.64GiB(30.82%)  tps: 6,461  tflops: 49.20  mfu: 15.77%
[rank0]:2025-11-09 20:25:49,849 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:25:52,159 - INFO - Avg. fwd time: 11.2779 / Avg. bwd time: 44.6947 / Avg. batch time: 507.6423 (ms) / GPU bubble ratio: 11.79%
[rank2]:2025-11-09 20:25:52,235 - INFO - Avg. fwd time: 7.1186 / Avg. bwd time: 18.8902 / Avg. batch time: 539.2375 (ms) / GPU bubble ratio: 61.41%
[rank0]:2025-11-09 20:25:52,282 - INFO - Avg. fwd time: 7.8391 / Avg. bwd time: 23.4310 / Avg. batch time: 614.9390 (ms) / GPU bubble ratio: 59.32%
[rank1]:2025-11-09 20:25:52,274 - INFO - Avg. fwd time: 9.0441 / Avg. bwd time: 24.0419 / Avg. batch time: 578.4139 (ms) / GPU bubble ratio: 54.24%
[rank0]:2025-11-09 20:25:52,471 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.3647  memory: 16.57GiB(34.88%)  tps: 6,357  tflops: 48.42  mfu: 15.52%
[rank3]:2025-11-09 20:25:52,469 - INFO -  step: 100  loss:  0.4450  grad_norm:  0.3647  memory: 26.98GiB(56.79%)  tps: 6,358  tflops: 48.42  mfu: 15.52%
[rank2]:2025-11-09 20:25:52,457 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.3647  memory: 11.81GiB(24.85%)  tps: 6,357  tflops: 48.42  mfu: 15.52%
[rank1]:2025-11-09 20:25:52,461 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.3647  memory: 14.64GiB(30.82%)  tps: 6,357  tflops: 48.42  mfu: 15.52%
[rank3]:2025-11-09 20:25:52,644 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2025_real_step100_rank3.svg
[rank3]:> Batch Time: 620.58 ms, GPU Bubble Ratio: 59.29%, 56.99%, 66.27%, 26.78%
[rank0]:2025-11-09 20:27:59,220 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:28:01,713 - INFO - Avg. fwd time: 11.3234 / Avg. bwd time: 44.9782 / Avg. batch time: 510.2176 (ms) / GPU bubble ratio: 11.72%
[rank1]:2025-11-09 20:28:01,772 - INFO - Avg. fwd time: 9.0638 / Avg. bwd time: 24.1001 / Avg. batch time: 581.0559 (ms) / GPU bubble ratio: 54.34%
[rank2]:2025-11-09 20:28:01,741 - INFO - Avg. fwd time: 7.1256 / Avg. bwd time: 18.9177 / Avg. batch time: 541.9192 (ms) / GPU bubble ratio: 61.55%
[rank2]:2025-11-09 20:28:01,823 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.4015  memory: 11.81GiB(24.85%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 20:28:01,801 - INFO - Avg. fwd time: 7.8475 / Avg. bwd time: 23.4548 / Avg. batch time: 617.4842 (ms) / GPU bubble ratio: 59.45%
[rank0]:2025-11-09 20:28:01,837 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.4015  memory: 16.57GiB(34.88%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank3]:2025-11-09 20:28:01,835 - INFO -  step: 150  loss:  0.4173  grad_norm:  0.4015  memory: 26.98GiB(56.79%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank1]:2025-11-09 20:28:01,827 - INFO -  step: 150  loss: -4.0000  grad_norm:  0.4015  memory: 14.64GiB(30.82%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 20:30:09,005 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:30:11,350 - INFO - Avg. fwd time: 11.3407 / Avg. bwd time: 45.1640 / Avg. batch time: 511.8119 (ms) / GPU bubble ratio: 11.68%
[rank1]:2025-11-09 20:30:11,465 - INFO - Avg. fwd time: 9.0770 / Avg. bwd time: 24.1330 / Avg. batch time: 582.5259 (ms) / GPU bubble ratio: 54.39%
[rank2]:2025-11-09 20:30:11,426 - INFO - Avg. fwd time: 7.1322 / Avg. bwd time: 18.9373 / Avg. batch time: 543.4311 (ms) / GPU bubble ratio: 61.62%
[rank0]:2025-11-09 20:30:11,473 - INFO - Avg. fwd time: 7.8576 / Avg. bwd time: 23.4737 / Avg. batch time: 618.9349 (ms) / GPU bubble ratio: 59.50%
[rank0]:2025-11-09 20:30:11,665 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3604  memory: 16.57GiB(34.88%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank1]:2025-11-09 20:30:11,654 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3604  memory: 14.64GiB(30.82%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 20:30:11,663 - INFO -  step: 200  loss:  0.4695  grad_norm:  0.3604  memory: 26.98GiB(56.79%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank2]:2025-11-09 20:30:11,650 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.3604  memory: 11.81GiB(24.85%)  tps: 6,310  tflops: 48.06  mfu: 15.40%
[rank3]:2025-11-09 20:30:11,815 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2030_real_step200_rank3.svg
[rank3]:> Batch Time: 622.67 ms, GPU Bubble Ratio: 59.36%, 57.01%, 66.30%, 26.52%
[rank0]:2025-11-09 20:32:19,308 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:32:21,626 - INFO - Avg. fwd time: 11.3638 / Avg. bwd time: 45.3214 / Avg. batch time: 513.2409 (ms) / GPU bubble ratio: 11.64%
[rank1]:2025-11-09 20:32:21,739 - INFO - Avg. fwd time: 9.0872 / Avg. bwd time: 24.1604 / Avg. batch time: 583.9700 (ms) / GPU bubble ratio: 54.45%
[rank0]:2025-11-09 20:32:21,747 - INFO - Avg. fwd time: 7.8667 / Avg. bwd time: 23.4877 / Avg. batch time: 620.3716 (ms) / GPU bubble ratio: 59.57%
[rank2]:2025-11-09 20:32:21,699 - INFO - Avg. fwd time: 7.1385 / Avg. bwd time: 18.9531 / Avg. batch time: 544.9242 (ms) / GPU bubble ratio: 61.70%
[rank1]:2025-11-09 20:32:21,926 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4340  memory: 14.64GiB(30.82%)  tps: 6,288  tflops: 47.89  mfu: 15.35%
[rank0]:2025-11-09 20:32:21,937 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4340  memory: 16.57GiB(34.88%)  tps: 6,288  tflops: 47.89  mfu: 15.35%
[rank3]:2025-11-09 20:32:21,934 - INFO -  step: 250  loss:  0.4764  grad_norm:  0.4340  memory: 26.98GiB(56.79%)  tps: 6,288  tflops: 47.89  mfu: 15.35%
[rank2]:2025-11-09 20:32:21,922 - INFO -  step: 250  loss: -4.0000  grad_norm:  0.4340  memory: 11.81GiB(24.85%)  tps: 6,288  tflops: 47.89  mfu: 15.35%
[rank0]:2025-11-09 20:34:29,626 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:34:32,184 - INFO - Avg. fwd time: 11.3806 / Avg. bwd time: 45.4318 / Avg. batch time: 514.2501 (ms) / GPU bubble ratio: 11.62%
[rank2]:2025-11-09 20:34:32,211 - INFO - Avg. fwd time: 7.1421 / Avg. bwd time: 18.9629 / Avg. batch time: 545.8829 (ms) / GPU bubble ratio: 61.74%
[rank1]:2025-11-09 20:34:32,242 - INFO - Avg. fwd time: 9.0942 / Avg. bwd time: 24.1776 / Avg. batch time: 584.8992 (ms) / GPU bubble ratio: 54.49%
[rank1]:2025-11-09 20:34:32,297 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4095  memory: 14.64GiB(30.82%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank0]:2025-11-09 20:34:32,272 - INFO - Avg. fwd time: 7.8719 / Avg. bwd time: 23.4972 / Avg. batch time: 621.3024 (ms) / GPU bubble ratio: 59.61%
[rank0]:2025-11-09 20:34:32,308 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4095  memory: 16.57GiB(34.88%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank3]:2025-11-09 20:34:32,306 - INFO -  step: 300  loss:  0.4771  grad_norm:  0.4095  memory: 26.98GiB(56.79%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank2]:2025-11-09 20:34:32,293 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4095  memory: 11.81GiB(24.85%)  tps: 6,284  tflops: 47.86  mfu: 15.34%
[rank3]:2025-11-09 20:34:32,472 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2034_real_step300_rank3.svg
[rank3]:> Batch Time: 624.69 ms, GPU Bubble Ratio: 59.45%, 57.14%, 66.37%, 26.56%
[rank0]:2025-11-09 20:36:39,611 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:36:41,912 - INFO - Avg. fwd time: 11.3856 / Avg. bwd time: 45.4667 / Avg. batch time: 514.5600 (ms) / GPU bubble ratio: 11.61%
[rank2]:2025-11-09 20:36:41,986 - INFO - Avg. fwd time: 7.1438 / Avg. bwd time: 18.9676 / Avg. batch time: 546.2456 (ms) / GPU bubble ratio: 61.76%
[rank1]:2025-11-09 20:36:42,025 - INFO - Avg. fwd time: 9.0956 / Avg. bwd time: 24.1853 / Avg. batch time: 585.2369 (ms) / GPU bubble ratio: 54.51%
[rank0]:2025-11-09 20:36:42,033 - INFO - Avg. fwd time: 7.8752 / Avg. bwd time: 23.5020 / Avg. batch time: 621.6424 (ms) / GPU bubble ratio: 59.62%
[rank1]:2025-11-09 20:36:42,210 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4143  memory: 14.64GiB(30.82%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank3]:2025-11-09 20:36:42,219 - INFO -  step: 350  loss:  0.4518  grad_norm:  0.4143  memory: 26.98GiB(56.79%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank2]:2025-11-09 20:36:42,206 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4143  memory: 11.81GiB(24.85%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank0]:2025-11-09 20:36:42,222 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4143  memory: 16.57GiB(34.88%)  tps: 6,306  tflops: 48.03  mfu: 15.39%
[rank0]:2025-11-09 20:38:49,033 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:38:51,353 - INFO - Avg. fwd time: 11.3857 / Avg. bwd time: 45.4675 / Avg. batch time: 514.5567 (ms) / GPU bubble ratio: 11.61%
[rank1]:2025-11-09 20:38:51,466 - INFO - Avg. fwd time: 9.0957 / Avg. bwd time: 24.1843 / Avg. batch time: 585.1540 (ms) / GPU bubble ratio: 54.50%
[rank2]:2025-11-09 20:38:51,427 - INFO - Avg. fwd time: 7.1437 / Avg. bwd time: 18.9683 / Avg. batch time: 546.1898 (ms) / GPU bubble ratio: 61.75%
[rank0]:2025-11-09 20:38:51,474 - INFO - Avg. fwd time: 7.8757 / Avg. bwd time: 23.5028 / Avg. batch time: 621.5478 (ms) / GPU bubble ratio: 59.61%
[rank1]:2025-11-09 20:38:51,652 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3971  memory: 14.64GiB(30.82%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-09 20:38:51,661 - INFO -  step: 400  loss:  0.4484  grad_norm:  0.3971  memory: 26.98GiB(56.79%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank2]:2025-11-09 20:38:51,648 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3971  memory: 11.81GiB(24.85%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank0]:2025-11-09 20:38:51,663 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3971  memory: 16.57GiB(34.88%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-09 20:38:51,815 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2038_real_step400_rank3.svg
[rank3]:> Batch Time: 621.12 ms, GPU Bubble Ratio: 59.35%, 57.05%, 66.26%, 26.42%
[rank3]:2025-11-09 20:39:00,542 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 20:39:00,787 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 20:39:00,761 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 20:39:00,813 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 20:40:58,556 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:41:01,078 - INFO - Avg. fwd time: 11.3811 / Avg. bwd time: 45.4950 / Avg. batch time: 514.7307 (ms) / GPU bubble ratio: 11.60%
[rank1]:2025-11-09 20:41:01,138 - INFO - Avg. fwd time: 9.0940 / Avg. bwd time: 24.1822 / Avg. batch time: 585.3277 (ms) / GPU bubble ratio: 54.52%
[rank2]:2025-11-09 20:41:01,106 - INFO - Avg. fwd time: 7.1432 / Avg. bwd time: 18.9693 / Avg. batch time: 546.3976 (ms) / GPU bubble ratio: 61.77%
[rank0]:2025-11-09 20:41:01,169 - INFO - Avg. fwd time: 7.8739 / Avg. bwd time: 23.5044 / Avg. batch time: 621.7149 (ms) / GPU bubble ratio: 59.62%
[rank0]:2025-11-09 20:41:01,205 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3955  memory: 16.57GiB(34.88%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank1]:2025-11-09 20:41:01,194 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3955  memory: 14.64GiB(30.82%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank3]:2025-11-09 20:41:01,204 - INFO -  step: 450  loss:  0.4943  grad_norm:  0.3955  memory: 26.98GiB(56.79%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank2]:2025-11-09 20:41:01,190 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3955  memory: 11.81GiB(24.85%)  tps: 6,324  tflops: 48.16  mfu: 15.44%
[rank0]:2025-11-09 20:43:08,335 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 20:43:10,729 - INFO - Avg. fwd time: 7.1435 / Avg. bwd time: 18.9706 / Avg. batch time: 546.5931 (ms) / GPU bubble ratio: 61.78%
[rank3]:2025-11-09 20:43:10,655 - INFO - Avg. fwd time: 11.3820 / Avg. bwd time: 45.5221 / Avg. batch time: 514.9529 (ms) / GPU bubble ratio: 11.60%
[rank1]:2025-11-09 20:43:10,769 - INFO - Avg. fwd time: 9.0944 / Avg. bwd time: 24.1805 / Avg. batch time: 585.4989 (ms) / GPU bubble ratio: 54.53%
[rank0]:2025-11-09 20:43:10,777 - INFO - Avg. fwd time: 7.8744 / Avg. bwd time: 23.5065 / Avg. batch time: 621.8805 (ms) / GPU bubble ratio: 59.63%
[rank1]:2025-11-09 20:43:10,954 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3305  memory: 14.64GiB(30.82%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank0]:2025-11-09 20:43:10,964 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3305  memory: 16.57GiB(34.88%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank2]:2025-11-09 20:43:10,950 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3305  memory: 11.81GiB(24.85%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 20:43:10,962 - INFO -  step: 500  loss:  0.3653  grad_norm:  0.3305  memory: 26.98GiB(56.79%)  tps: 6,313  tflops: 48.08  mfu: 15.41%
[rank3]:2025-11-09 20:43:11,125 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2043_real_step500_rank3.svg
[rank3]:> Batch Time: 622.67 ms, GPU Bubble Ratio: 59.39%, 57.18%, 66.31%, 26.51%
[rank0]:2025-11-09 20:45:18,330 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-09 20:45:20,714 - INFO - Avg. fwd time: 7.1446 / Avg. bwd time: 18.9723 / Avg. batch time: 546.8976 (ms) / GPU bubble ratio: 61.80%
[rank3]:2025-11-09 20:45:20,638 - INFO - Avg. fwd time: 11.3850 / Avg. bwd time: 45.5538 / Avg. batch time: 515.2277 (ms) / GPU bubble ratio: 11.59%
[rank1]:2025-11-09 20:45:20,753 - INFO - Avg. fwd time: 9.0961 / Avg. bwd time: 24.1792 / Avg. batch time: 585.7819 (ms) / GPU bubble ratio: 54.56%
[rank0]:2025-11-09 20:45:20,760 - INFO - Avg. fwd time: 7.8755 / Avg. bwd time: 23.5083 / Avg. batch time: 622.1591 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 20:45:20,936 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4065  memory: 14.64GiB(30.82%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank0]:2025-11-09 20:45:20,946 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4065  memory: 16.57GiB(34.88%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank2]:2025-11-09 20:45:20,932 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.4065  memory: 11.81GiB(24.85%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank3]:2025-11-09 20:45:20,944 - INFO -  step: 550  loss:  0.4173  grad_norm:  0.4065  memory: 26.98GiB(56.79%)  tps: 6,303  tflops: 48.00  mfu: 15.39%
[rank0]:2025-11-09 20:47:28,268 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:47:30,809 - INFO - Avg. fwd time: 11.3899 / Avg. bwd time: 45.5811 / Avg. batch time: 515.4863 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 20:47:30,837 - INFO - Avg. fwd time: 7.1453 / Avg. bwd time: 18.9740 / Avg. batch time: 547.1326 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 20:47:30,869 - INFO - Avg. fwd time: 9.0984 / Avg. bwd time: 24.1777 / Avg. batch time: 586.0012 (ms) / GPU bubble ratio: 54.57%
[rank1]:2025-11-09 20:47:30,925 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3847  memory: 14.64GiB(30.82%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank0]:2025-11-09 20:47:30,900 - INFO - Avg. fwd time: 7.8758 / Avg. bwd time: 23.5106 / Avg. batch time: 622.3743 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 20:47:30,936 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3847  memory: 16.57GiB(34.88%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank3]:2025-11-09 20:47:30,934 - INFO -  step: 600  loss:  0.4201  grad_norm:  0.3847  memory: 26.98GiB(56.79%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank2]:2025-11-09 20:47:30,922 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3847  memory: 11.81GiB(24.85%)  tps: 6,302  tflops: 48.00  mfu: 15.38%
[rank3]:2025-11-09 20:47:31,095 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2047_real_step600_rank3.svg
[rank3]:> Batch Time: 624.16 ms, GPU Bubble Ratio: 59.46%, 57.22%, 66.38%, 26.44%
[rank0]:2025-11-09 20:49:38,427 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:49:40,715 - INFO - Avg. fwd time: 11.3937 / Avg. bwd time: 45.6010 / Avg. batch time: 515.6754 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 20:49:40,838 - INFO - Avg. fwd time: 7.8762 / Avg. bwd time: 23.5123 / Avg. batch time: 622.5824 (ms) / GPU bubble ratio: 59.67%
[rank2]:2025-11-09 20:49:40,791 - INFO - Avg. fwd time: 7.1462 / Avg. bwd time: 18.9756 / Avg. batch time: 547.3538 (ms) / GPU bubble ratio: 61.82%
[rank1]:2025-11-09 20:49:40,830 - INFO - Avg. fwd time: 9.1010 / Avg. bwd time: 24.1774 / Avg. batch time: 586.2141 (ms) / GPU bubble ratio: 54.59%
[rank1]:2025-11-09 20:49:41,011 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3887  memory: 14.64GiB(30.82%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank3]:2025-11-09 20:49:41,020 - INFO -  step: 650  loss:  0.3568  grad_norm:  0.3887  memory: 26.98GiB(56.79%)  tps: 6,298  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 20:49:41,022 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3887  memory: 16.57GiB(34.88%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank2]:2025-11-09 20:49:41,007 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3887  memory: 11.81GiB(24.85%)  tps: 6,297  tflops: 47.96  mfu: 15.37%
[rank0]:2025-11-09 20:51:48,251 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:51:50,567 - INFO - Avg. fwd time: 11.3952 / Avg. bwd time: 45.6182 / Avg. batch time: 515.8238 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 20:51:50,642 - INFO - Avg. fwd time: 7.1467 / Avg. bwd time: 18.9765 / Avg. batch time: 547.4793 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 20:51:50,690 - INFO - Avg. fwd time: 7.8761 / Avg. bwd time: 23.5129 / Avg. batch time: 622.6947 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 20:51:50,682 - INFO - Avg. fwd time: 9.1027 / Avg. bwd time: 24.1761 / Avg. batch time: 586.3313 (ms) / GPU bubble ratio: 54.59%
[rank2]:2025-11-09 20:51:50,861 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4389  memory: 11.81GiB(24.85%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 20:51:50,873 - INFO -  step: 700  loss:  0.4266  grad_norm:  0.4389  memory: 26.98GiB(56.79%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank0]:2025-11-09 20:51:50,876 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4389  memory: 16.57GiB(34.88%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank1]:2025-11-09 20:51:50,864 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4389  memory: 14.64GiB(30.82%)  tps: 6,309  tflops: 48.05  mfu: 15.40%
[rank3]:2025-11-09 20:51:51,027 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2051_real_step700_rank3.svg
[rank3]:> Batch Time: 622.64 ms, GPU Bubble Ratio: 59.41%, 57.14%, 66.34%, 26.55%
[rank0]:2025-11-09 20:53:57,710 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:54:00,223 - INFO - Avg. fwd time: 11.3974 / Avg. bwd time: 45.6124 / Avg. batch time: 515.7938 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 20:54:00,251 - INFO - Avg. fwd time: 7.1464 / Avg. bwd time: 18.9755 / Avg. batch time: 547.4651 (ms) / GPU bubble ratio: 61.83%
[rank1]:2025-11-09 20:54:00,284 - INFO - Avg. fwd time: 9.1020 / Avg. bwd time: 24.1727 / Avg. batch time: 586.3064 (ms) / GPU bubble ratio: 54.60%
[rank1]:2025-11-09 20:54:00,342 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4428  memory: 14.64GiB(30.82%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank3]:2025-11-09 20:54:00,352 - INFO -  step: 750  loss:  0.4034  grad_norm:  0.4428  memory: 26.98GiB(56.79%)  tps: 6,327  tflops: 48.19  mfu: 15.45%
[rank2]:2025-11-09 20:54:00,338 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4428  memory: 11.81GiB(24.85%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank0]:2025-11-09 20:54:00,316 - INFO - Avg. fwd time: 7.8761 / Avg. bwd time: 23.5126 / Avg. batch time: 622.6632 (ms) / GPU bubble ratio: 59.67%
[rank0]:2025-11-09 20:54:00,352 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4428  memory: 16.57GiB(34.88%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank0]:2025-11-09 20:56:07,113 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 20:56:09,439 - INFO - Avg. fwd time: 11.3996 / Avg. bwd time: 45.6084 / Avg. batch time: 515.7792 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 20:56:09,514 - INFO - Avg. fwd time: 7.1463 / Avg. bwd time: 18.9751 / Avg. batch time: 547.4313 (ms) / GPU bubble ratio: 61.83%
[rank0]:2025-11-09 20:56:09,562 - INFO - Avg. fwd time: 7.8767 / Avg. bwd time: 23.5130 / Avg. batch time: 622.6188 (ms) / GPU bubble ratio: 59.67%
[rank1]:2025-11-09 20:56:09,555 - INFO - Avg. fwd time: 9.1018 / Avg. bwd time: 24.1703 / Avg. batch time: 586.2656 (ms) / GPU bubble ratio: 54.60%
[rank2]:2025-11-09 20:56:09,734 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4153  memory: 11.81GiB(24.85%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank2]:2025-11-09 20:56:09,734 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4153  tps: 6,699  tflops: 51.02  mfu: 14.81%
[rank2]:2025-11-09 20:56:09,735 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 20:56:09,735 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank2]:  warnings.warn(
[rank3]:2025-11-09 20:56:09,747 - INFO -  step: 800  loss:  0.4138  grad_norm:  0.4153  memory: 26.98GiB(56.79%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank3]:2025-11-09 20:56:09,747 - INFO -  final step: 800  loss:  0.4138  grad_norm:  0.4153  tps: 6,708  tflops: 51.09  mfu: 14.93%
[rank3]:2025-11-09 20:56:09,748 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 20:56:09,749 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank3]:  warnings.warn(
[rank0]:2025-11-09 20:56:09,749 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4153  memory: 16.57GiB(34.88%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank0]:2025-11-09 20:56:09,749 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4153  tps: 6,700  tflops: 51.03  mfu: 14.81%
[rank0]:2025-11-09 20:56:09,749 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 20:56:09,750 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank0]:  warnings.warn(
[rank1]:2025-11-09 20:56:09,738 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4153  memory: 14.64GiB(30.82%)  tps: 6,331  tflops: 48.22  mfu: 15.45%
[rank1]:2025-11-09 20:56:09,738 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4153  tps: 6,701  tflops: 51.03  mfu: 14.82%
[rank1]:2025-11-09 20:56:09,738 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 20:56:09,739 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/distributed/checkpoint/filesystem.py:660: UserWarning: Detected an existing checkpoint in /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1/step-800, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.
[rank1]:  warnings.warn(
[rank1]:2025-11-09 20:56:11,812 - INFO - Destroying the purge thread.
[rank2]:2025-11-09 20:56:11,812 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 20:56:11,799 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 20:56:11,812 - INFO - Sleeping 2 seconds for other ranks to complete
[rank3]:2025-11-09 20:56:11,956 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2056_real_final800_rank3.svg
[rank3]:> Batch Time: 622.64 ms, GPU Bubble Ratio: 59.39%, 57.14%, 66.33%, 26.63%
[rank3]:2025-11-09 20:56:12,102 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1/pipeline_schedule/251109_2056_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.17 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 20:56:12,103 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank1]:2025-11-09 20:56:12,292 - INFO - Process group destroyed
[rank2]:2025-11-09 20:56:12,279 - INFO - Process group destroyed
[rank3]:wandb: uploading history steps 15-16, summary, console lines 228-239
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–‚â–ƒâ–ƒâ–…â–ƒâ–‡â–†â–†â–…â–…â–â–†â–„â–…â–ˆâ–ˆâ–†
[rank3]:wandb: loss_metrics/global_avg_loss â–„â–ƒâ–…â–„â–‡â–‡â–‡â–†â–†â–ˆâ–â–„â–„â–â–…â–ƒâ–„
[rank3]:wandb: loss_metrics/global_max_loss â–„â–ƒâ–…â–„â–‡â–‡â–‡â–†â–†â–ˆâ–â–„â–„â–â–…â–ƒâ–„
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.44258
[rank3]:wandb:               final/avg_loss 0.41384
[rank3]:wandb:             final/avg_mfu(%) 14.93175
[rank3]:wandb:             final/avg_tflops 51.08701
[rank3]:wandb:    final/avg_throughput(tps) 6707.6657
[rank3]:wandb:              final/grad_norm 0.41529
[rank3]:wandb:               final/max_loss 0.41384
[rank3]:wandb:                    grad_norm 0.41529
[rank3]:wandb: loss_metrics/global_avg_loss 0.41384
[rank3]:wandb: loss_metrics/global_max_loss 0.41384
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed42_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/kj8c8tyq
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1/20251109-2021/wandb/run-20251109_202132-kj8c8tyq/logs
[rank3]:2025-11-09 20:56:13,268 - INFO - Process group destroyed
[rank0]:2025-11-09 20:56:13,812 - INFO - Training completed
[rank0]:2025-11-09 20:56:13,813 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 20:56:14,281 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.12', 'layers.11', 'layers.9', 'layers.10'}
[rank0]:Stage 0: Modules to keep: {'layers.2', 'layers.1', 'layers.3', 'layers.0', 'tok_embeddings'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.7', 'layers.5', 'layers.6', 'layers.8'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'output', 'layers.15', 'norm', 'layers.14'}
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed42_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed42_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.1
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed42_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 22:21:36 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed42.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4 --training.dataset=codealpaca  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
