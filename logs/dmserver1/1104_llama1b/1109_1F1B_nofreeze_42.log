
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 10. (ì›”) 12:29:29 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 2,3,4,5
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1109_1F1B_nofreeze_42.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
1109: switch to alpaca_gpt4+alpaca_cleaned dataset

âœ”ï¸Running with nofreeze x 1F1B ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1109.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
1109: switch to alpaca_gpt4+alpaca_cleaned dataset
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank3]:2025-11-10 12:29:35,939 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank3]:"
[rank0]:2025-11-10 12:29:35,861 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank0]:"
[rank1]:2025-11-10 12:29:35,893 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank1]:"
[rank2]:2025-11-10 12:29:35,861 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank2]:"
[rank0]:2025-11-10 12:29:36,162 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-10 12:29:36,165 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-10 12:29:36,168 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-10 12:29:36,169 - INFO - Loading tokenizer from tokenizer.json
[rank2]:2025-11-10 12:29:36,197 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-10 12:29:36,199 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-10 12:29:36,211 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-10 12:29:36,213 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-10 12:29:36,267 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-10 12:29:36,270 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-10 12:29:36,575 - INFO - Preparing alpaca_gpt4 dataset from vicgalle/alpaca-gpt4
[rank0]:2025-11-10 12:29:39,398 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-10 12:29:39,548 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-10 12:29:39,481 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-10 12:29:39,520 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-10 12:29:39,547 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-10 12:29:39,548 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-11-10 12:29:39,465 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-10 12:29:39,505 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-10 12:29:39,538 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-10 12:29:39,539 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-11-10 12:29:39,587 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-10 12:29:39,589 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-10 12:29:39,614 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-10 12:29:39,614 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-11-10 12:29:39,761 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-10 12:29:39,761 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-10 12:29:39,762 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank2]:2025-11-10 12:29:39,715 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-10 12:29:39,715 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-10 12:29:39,716 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-10 12:29:39,804 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-10 12:29:39,804 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-10 12:29:39,805 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 02buaaqy
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1109_1F1B_nofreeze_42_dm1/20251110-1229/wandb/run-20251110_122940-02buaaqy
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1109_1F1B_nofreeze_42_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/02buaaqy
[rank3]:2025-11-10 12:29:41,958 - INFO - WandB logging enabled
[rank3]:2025-11-10 12:29:41,958 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-10 12:29:41,995 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-10 12:29:42,026 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-10 12:29:42,026 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-11-10 12:29:42,218 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-10 12:29:42,219 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-10 12:29:42,220 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-10 12:29:42,235 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-10 12:29:42,234 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1109_1F1B_nofreeze_42_dm1
[rank0]:2025-11-10 12:29:42,235 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-10 12:29:42,235 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 1024, total steps 800 (warmup 100)
[rank0]:2025-11-10 12:29:42,235 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank1]:2025-11-10 12:29:42,235 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-10 12:29:42,235 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-10 12:29:44,648 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-10 12:29:44,648 - INFO - Finished loading the checkpoint in 2.41 seconds.
[rank0]:2025-11-10 12:29:44,648 - INFO - Training starts at step 1
[rank3]:2025-11-10 12:29:49,910 - INFO -  step:  1  loss: 10.6092  grad_norm: 203.0663  memory: 21.88GiB(46.05%)  tps: 4,142  tflops: 32.38  mfu: 10.38%
[rank3]:2025-11-10 12:29:49,910 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-10 12:29:49,901 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0663  memory: 10.58GiB(22.26%)  tps: 3,156  tflops: 24.68  mfu: 7.91%
[rank1]:2025-11-10 12:29:49,902 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-10 12:29:49,939 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0663  memory: 13.10GiB(27.57%)  tps: 3,166  tflops: 24.75  mfu: 7.93%
[rank0]:2025-11-10 12:29:49,939 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-10 12:29:49,897 - INFO -  step:  1  loss: -4.0000  grad_norm: 203.0663  memory:  6.84GiB(14.41%)  tps: 3,155  tflops: 24.66  mfu: 7.90%
[rank2]:2025-11-10 12:29:49,898 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-10 12:33:43,942 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:33:48,226 - INFO - Avg. fwd time: 24.5354 / Avg. bwd time: 80.1343 / Avg. batch time: 992.9994 (ms) / GPU bubble ratio: 15.67%
[rank0]:2025-11-10 12:33:48,366 - INFO - Avg. fwd time: 15.9060 / Avg. bwd time: 40.6946 / Avg. batch time: 1191.7894 (ms) / GPU bubble ratio: 62.01%
[rank2]:2025-11-10 12:33:48,288 - INFO - Avg. fwd time: 15.4222 / Avg. bwd time: 35.2344 / Avg. batch time: 1052.7252 (ms) / GPU bubble ratio: 61.50%
[rank1]:2025-11-10 12:33:48,409 - INFO - Avg. fwd time: 19.9019 / Avg. bwd time: 45.5913 / Avg. batch time: 1126.6298 (ms) / GPU bubble ratio: 53.49%
[rank1]:2025-11-10 12:33:48,805 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.2623  memory: 12.86GiB(27.07%)  tps: 6,721  tflops: 52.54  mfu: 16.84%
[rank3]:2025-11-10 12:33:48,813 - INFO -  step: 50  loss:  9.8879  grad_norm: 24.2623  memory: 25.79GiB(54.28%)  tps: 6,721  tflops: 52.54  mfu: 16.84%
[rank0]:2025-11-10 12:33:48,818 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.2623  memory: 16.87GiB(35.50%)  tps: 6,722  tflops: 52.55  mfu: 16.84%
[rank2]:2025-11-10 12:33:48,801 - INFO -  step: 50  loss: -4.0000  grad_norm: 24.2623  memory:  8.66GiB(18.23%)  tps: 6,721  tflops: 52.54  mfu: 16.84%
[rank0]:2025-11-10 12:37:47,860 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:37:52,168 - INFO - Avg. fwd time: 24.6279 / Avg. bwd time: 80.4002 / Avg. batch time: 994.7802 (ms) / GPU bubble ratio: 15.54%
[rank2]:2025-11-10 12:37:52,229 - INFO - Avg. fwd time: 15.4490 / Avg. bwd time: 35.3458 / Avg. batch time: 1054.2787 (ms) / GPU bubble ratio: 61.46%
[rank1]:2025-11-10 12:37:52,350 - INFO - Avg. fwd time: 19.9487 / Avg. bwd time: 45.7634 / Avg. batch time: 1127.7737 (ms) / GPU bubble ratio: 53.39%
[rank0]:2025-11-10 12:37:52,308 - INFO - Avg. fwd time: 15.9747 / Avg. bwd time: 40.7934 / Avg. batch time: 1192.6461 (ms) / GPU bubble ratio: 61.92%
[rank2]:2025-11-10 12:37:52,701 - INFO -  step: 100  loss: -4.0000  grad_norm: 15.7029  memory:  8.66GiB(18.23%)  tps: 6,718  tflops: 52.51  mfu: 16.83%
[rank1]:2025-11-10 12:37:52,705 - INFO -  step: 100  loss: -4.0000  grad_norm: 15.7029  memory: 12.86GiB(27.07%)  tps: 6,718  tflops: 52.51  mfu: 16.83%
[rank0]:2025-11-10 12:37:52,718 - INFO -  step: 100  loss: -4.0000  grad_norm: 15.7029  memory: 16.87GiB(35.50%)  tps: 6,718  tflops: 52.51  mfu: 16.83%
[rank3]:2025-11-10 12:37:52,714 - INFO -  step: 100  loss:  8.2163  grad_norm: 15.7029  memory: 25.79GiB(54.28%)  tps: 6,718  tflops: 52.51  mfu: 16.83%
[rank3]:2025-11-10 12:37:52,894 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1237_real_step100_rank3.svg
[rank3]:> Batch Time: 1188.94 ms, GPU Bubble Ratio: 61.55%, 55.61%, 65.66%, 28.52%
[rank0]:2025-11-10 12:41:52,893 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:41:57,595 - INFO - Avg. fwd time: 24.6325 / Avg. bwd time: 80.4877 / Avg. batch time: 995.9833 (ms) / GPU bubble ratio: 15.56%
[rank2]:2025-11-10 12:41:57,638 - INFO - Avg. fwd time: 15.4423 / Avg. bwd time: 35.3769 / Avg. batch time: 1055.9315 (ms) / GPU bubble ratio: 61.50%
[rank1]:2025-11-10 12:41:57,691 - INFO - Avg. fwd time: 19.9473 / Avg. bwd time: 45.8157 / Avg. batch time: 1129.5754 (ms) / GPU bubble ratio: 53.42%
[rank1]:2025-11-10 12:41:57,765 - INFO -  step: 150  loss: -4.0000  grad_norm: 27.4170  memory: 12.86GiB(27.07%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank2]:2025-11-10 12:41:57,761 - INFO -  step: 150  loss: -4.0000  grad_norm: 27.4170  memory:  8.66GiB(18.23%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank0]:2025-11-10 12:41:57,740 - INFO - Avg. fwd time: 15.9779 / Avg. bwd time: 40.8206 / Avg. batch time: 1194.2863 (ms) / GPU bubble ratio: 61.95%
[rank0]:2025-11-10 12:41:57,778 - INFO -  step: 150  loss: -4.0000  grad_norm: 27.4170  memory: 16.87GiB(35.50%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank3]:2025-11-10 12:41:57,774 - INFO -  step: 150  loss:  3.4493  grad_norm: 27.4170  memory: 25.79GiB(54.28%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank0]:2025-11-10 12:45:57,910 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:46:02,270 - INFO - Avg. fwd time: 24.6386 / Avg. bwd time: 80.5372 / Avg. batch time: 996.7928 (ms) / GPU bubble ratio: 15.59%
[rank2]:2025-11-10 12:46:02,330 - INFO - Avg. fwd time: 15.4593 / Avg. bwd time: 35.4274 / Avg. batch time: 1056.8078 (ms) / GPU bubble ratio: 61.48%
[rank0]:2025-11-10 12:46:02,412 - INFO - Avg. fwd time: 15.9938 / Avg. bwd time: 40.8605 / Avg. batch time: 1195.1918 (ms) / GPU bubble ratio: 61.94%
[rank1]:2025-11-10 12:46:02,458 - INFO - Avg. fwd time: 19.9519 / Avg. bwd time: 45.8772 / Avg. batch time: 1130.4679 (ms) / GPU bubble ratio: 53.41%
[rank0]:2025-11-10 12:46:02,835 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.5540  memory: 16.87GiB(35.50%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank1]:2025-11-10 12:46:02,822 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.5540  memory: 12.86GiB(27.07%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank2]:2025-11-10 12:46:02,818 - INFO -  step: 200  loss: -4.0000  grad_norm: 15.5540  memory:  8.66GiB(18.23%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank3]:2025-11-10 12:46:02,830 - INFO -  step: 200  loss:  1.0391  grad_norm: 15.5540  memory: 25.79GiB(54.28%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank3]:2025-11-10 12:46:02,987 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1246_real_step200_rank3.svg
[rank3]:> Batch Time: 1194.06 ms, GPU Bubble Ratio: 61.62%, 55.57%, 65.59%, 28.73%
[rank0]:2025-11-10 12:50:02,209 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:50:06,492 - INFO - Avg. fwd time: 24.6441 / Avg. bwd time: 80.5308 / Avg. batch time: 996.5024 (ms) / GPU bubble ratio: 15.56%
[rank2]:2025-11-10 12:50:06,552 - INFO - Avg. fwd time: 15.4671 / Avg. bwd time: 35.4919 / Avg. batch time: 1056.5080 (ms) / GPU bubble ratio: 61.41%
[rank0]:2025-11-10 12:50:06,637 - INFO - Avg. fwd time: 16.0039 / Avg. bwd time: 40.8874 / Avg. batch time: 1194.7485 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-10 12:50:06,675 - INFO - Avg. fwd time: 19.9633 / Avg. bwd time: 45.9461 / Avg. batch time: 1130.0371 (ms) / GPU bubble ratio: 53.34%
[rank0]:2025-11-10 12:50:07,044 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.5748  memory: 16.87GiB(35.50%)  tps: 6,709  tflops: 52.45  mfu: 16.81%
[rank1]:2025-11-10 12:50:07,030 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.5748  memory: 12.86GiB(27.07%)  tps: 6,709  tflops: 52.45  mfu: 16.81%
[rank2]:2025-11-10 12:50:07,026 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.5748  memory:  8.66GiB(18.23%)  tps: 6,709  tflops: 52.45  mfu: 16.81%
[rank3]:2025-11-10 12:50:07,039 - INFO -  step: 250  loss:  0.3634  grad_norm:  3.5748  memory: 25.79GiB(54.28%)  tps: 6,709  tflops: 52.45  mfu: 16.81%
[rank0]:2025-11-10 12:54:07,213 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:54:11,922 - INFO - Avg. fwd time: 24.6424 / Avg. bwd time: 80.5506 / Avg. batch time: 996.8297 (ms) / GPU bubble ratio: 15.58%
[rank2]:2025-11-10 12:54:11,967 - INFO - Avg. fwd time: 15.4789 / Avg. bwd time: 35.5287 / Avg. batch time: 1057.1191 (ms) / GPU bubble ratio: 61.40%
[rank0]:2025-11-10 12:54:12,070 - INFO - Avg. fwd time: 16.0074 / Avg. bwd time: 40.9049 / Avg. batch time: 1195.3536 (ms) / GPU bubble ratio: 61.91%
[rank1]:2025-11-10 12:54:12,021 - INFO - Avg. fwd time: 19.9665 / Avg. bwd time: 45.9850 / Avg. batch time: 1130.6406 (ms) / GPU bubble ratio: 53.34%
[rank1]:2025-11-10 12:54:12,095 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.5285  memory: 12.86GiB(27.07%)  tps: 6,686  tflops: 52.26  mfu: 16.75%
[rank2]:2025-11-10 12:54:12,092 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.5285  memory:  8.66GiB(18.23%)  tps: 6,686  tflops: 52.26  mfu: 16.75%
[rank3]:2025-11-10 12:54:12,104 - INFO -  step: 300  loss:  0.2516  grad_norm:  0.5285  memory: 25.79GiB(54.28%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank0]:2025-11-10 12:54:12,107 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.5285  memory: 16.87GiB(35.50%)  tps: 6,686  tflops: 52.27  mfu: 16.75%
[rank3]:2025-11-10 12:54:12,269 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1254_real_step300_rank3.svg
[rank3]:> Batch Time: 1185.97 ms, GPU Bubble Ratio: 61.44%, 55.30%, 65.44%, 29.02%
[rank0]:2025-11-10 12:58:11,174 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 12:58:15,597 - INFO - Avg. fwd time: 24.6411 / Avg. bwd time: 80.5253 / Avg. batch time: 996.4619 (ms) / GPU bubble ratio: 15.57%
[rank2]:2025-11-10 12:58:15,659 - INFO - Avg. fwd time: 15.4643 / Avg. bwd time: 35.5465 / Avg. batch time: 1056.7786 (ms) / GPU bubble ratio: 61.38%
[rank0]:2025-11-10 12:58:15,738 - INFO - Avg. fwd time: 16.0098 / Avg. bwd time: 40.9107 / Avg. batch time: 1195.0784 (ms) / GPU bubble ratio: 61.90%
[rank1]:2025-11-10 12:58:15,779 - INFO - Avg. fwd time: 19.9654 / Avg. bwd time: 45.9998 / Avg. batch time: 1130.3842 (ms) / GPU bubble ratio: 53.31%
[rank3]:2025-11-10 12:58:16,146 - INFO -  step: 350  loss:  0.2108  grad_norm:  0.2031  memory: 25.79GiB(54.28%)  tps: 6,714  tflops: 52.48  mfu: 16.82%
[rank0]:2025-11-10 12:58:16,151 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2031  memory: 16.87GiB(35.50%)  tps: 6,714  tflops: 52.48  mfu: 16.82%
[rank1]:2025-11-10 12:58:16,137 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2031  memory: 12.86GiB(27.07%)  tps: 6,714  tflops: 52.48  mfu: 16.82%
[rank2]:2025-11-10 12:58:16,134 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.2031  memory:  8.66GiB(18.23%)  tps: 6,714  tflops: 52.48  mfu: 16.82%
[rank0]:2025-11-10 13:02:15,296 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:02:19,714 - INFO - Avg. fwd time: 24.6259 / Avg. bwd time: 80.4728 / Avg. batch time: 996.1406 (ms) / GPU bubble ratio: 15.60%
[rank2]:2025-11-10 13:02:19,779 - INFO - Avg. fwd time: 15.4480 / Avg. bwd time: 35.5559 / Avg. batch time: 1056.3868 (ms) / GPU bubble ratio: 61.37%
[rank0]:2025-11-10 13:02:19,858 - INFO - Avg. fwd time: 16.0135 / Avg. bwd time: 40.9151 / Avg. batch time: 1194.6397 (ms) / GPU bubble ratio: 61.88%
[rank1]:2025-11-10 13:02:19,904 - INFO - Avg. fwd time: 19.9613 / Avg. bwd time: 46.0068 / Avg. batch time: 1129.9531 (ms) / GPU bubble ratio: 53.29%
[rank2]:2025-11-10 13:02:20,261 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1732  memory:  8.66GiB(18.23%)  tps: 6,711  tflops: 52.47  mfu: 16.82%
[rank0]:2025-11-10 13:02:20,279 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1732  memory: 16.87GiB(35.50%)  tps: 6,711  tflops: 52.47  mfu: 16.82%
[rank1]:2025-11-10 13:02:20,265 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.1732  memory: 12.86GiB(27.07%)  tps: 6,711  tflops: 52.47  mfu: 16.82%
[rank3]:2025-11-10 13:02:20,273 - INFO -  step: 400  loss:  0.2045  grad_norm:  0.1732  memory: 25.79GiB(54.28%)  tps: 6,711  tflops: 52.47  mfu: 16.82%
[rank3]:2025-11-10 13:02:20,429 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1302_real_step400_rank3.svg
[rank3]:> Batch Time: 1183.03 ms, GPU Bubble Ratio: 61.39%, 55.38%, 65.44%, 28.81%
[rank3]:2025-11-10 13:02:47,956 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank2]:2025-11-10 13:02:48,377 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank1]:2025-11-10 13:02:48,426 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank0]:2025-11-10 13:02:48,470 - WARNING - Dataset alpaca_gpt4 is being re-looped
[rank0]:2025-11-10 13:06:18,902 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-10 13:06:23,712 - INFO - Avg. fwd time: 15.4293 / Avg. bwd time: 35.5631 / Avg. batch time: 1056.0942 (ms) / GPU bubble ratio: 61.37%
[rank3]:2025-11-10 13:06:23,666 - INFO - Avg. fwd time: 24.6293 / Avg. bwd time: 80.4601 / Avg. batch time: 995.8535 (ms) / GPU bubble ratio: 15.58%
[rank0]:2025-11-10 13:06:23,816 - INFO - Avg. fwd time: 16.0165 / Avg. bwd time: 40.9184 / Avg. batch time: 1194.2502 (ms) / GPU bubble ratio: 61.86%
[rank1]:2025-11-10 13:06:23,766 - INFO - Avg. fwd time: 19.9561 / Avg. bwd time: 46.0112 / Avg. batch time: 1129.5812 (ms) / GPU bubble ratio: 53.28%
[rank1]:2025-11-10 13:06:23,841 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2121  memory: 12.86GiB(27.07%)  tps: 6,726  tflops: 52.58  mfu: 16.85%
[rank0]:2025-11-10 13:06:23,854 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2121  memory: 16.87GiB(35.50%)  tps: 6,727  tflops: 52.58  mfu: 16.85%
[rank2]:2025-11-10 13:06:23,838 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.2121  memory:  8.66GiB(18.23%)  tps: 6,726  tflops: 52.58  mfu: 16.85%
[rank3]:2025-11-10 13:06:23,851 - INFO -  step: 450  loss:  0.2181  grad_norm:  0.2121  memory: 25.79GiB(54.28%)  tps: 6,726  tflops: 52.58  mfu: 16.85%
[rank0]:2025-11-10 13:10:22,510 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:10:26,816 - INFO - Avg. fwd time: 24.6258 / Avg. bwd time: 80.4442 / Avg. batch time: 995.4995 (ms) / GPU bubble ratio: 15.56%
[rank2]:2025-11-10 13:10:26,877 - INFO - Avg. fwd time: 15.4245 / Avg. bwd time: 35.5650 / Avg. batch time: 1055.7387 (ms) / GPU bubble ratio: 61.36%
[rank0]:2025-11-10 13:10:26,957 - INFO - Avg. fwd time: 16.0169 / Avg. bwd time: 40.9184 / Avg. batch time: 1193.8934 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-10 13:10:27,001 - INFO - Avg. fwd time: 19.9485 / Avg. bwd time: 46.0076 / Avg. batch time: 1129.2332 (ms) / GPU bubble ratio: 53.27%
[rank1]:2025-11-10 13:10:27,355 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1900  memory: 12.86GiB(27.07%)  tps: 6,728  tflops: 52.60  mfu: 16.86%
[rank2]:2025-11-10 13:10:27,351 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1900  memory:  8.66GiB(18.23%)  tps: 6,728  tflops: 52.60  mfu: 16.86%
[rank0]:2025-11-10 13:10:27,366 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.1900  memory: 16.87GiB(35.50%)  tps: 6,728  tflops: 52.60  mfu: 16.86%
[rank3]:2025-11-10 13:10:27,363 - INFO -  step: 500  loss:  0.2102  grad_norm:  0.1900  memory: 25.79GiB(54.28%)  tps: 6,728  tflops: 52.60  mfu: 16.86%
[rank3]:2025-11-10 13:10:27,519 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1310_real_step500_rank3.svg
[rank3]:> Batch Time: 1185.94 ms, GPU Bubble Ratio: 61.49%, 55.57%, 65.51%, 28.57%
[rank0]:2025-11-10 13:14:26,295 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:14:30,654 - INFO - Avg. fwd time: 24.6241 / Avg. bwd time: 80.4506 / Avg. batch time: 995.2774 (ms) / GPU bubble ratio: 15.54%
[rank2]:2025-11-10 13:14:30,716 - INFO - Avg. fwd time: 15.4165 / Avg. bwd time: 35.5687 / Avg. batch time: 1055.5265 (ms) / GPU bubble ratio: 61.36%
[rank0]:2025-11-10 13:14:30,791 - INFO - Avg. fwd time: 16.0142 / Avg. bwd time: 40.9183 / Avg. batch time: 1193.6496 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-10 13:14:30,840 - INFO - Avg. fwd time: 19.9412 / Avg. bwd time: 46.0023 / Avg. batch time: 1129.0013 (ms) / GPU bubble ratio: 53.27%
[rank2]:2025-11-10 13:14:31,189 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1803  memory:  8.66GiB(18.23%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank0]:2025-11-10 13:14:31,206 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1803  memory: 16.87GiB(35.50%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank1]:2025-11-10 13:14:31,193 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.1803  memory: 12.86GiB(27.07%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank3]:2025-11-10 13:14:31,201 - INFO -  step: 550  loss:  0.2197  grad_norm:  0.1803  memory: 25.79GiB(54.28%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank0]:2025-11-10 13:18:30,552 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:18:35,414 - INFO - Avg. fwd time: 24.6271 / Avg. bwd time: 80.4608 / Avg. batch time: 995.3634 (ms) / GPU bubble ratio: 15.54%
[rank2]:2025-11-10 13:18:35,460 - INFO - Avg. fwd time: 15.4081 / Avg. bwd time: 35.5740 / Avg. batch time: 1055.5544 (ms) / GPU bubble ratio: 61.36%
[rank0]:2025-11-10 13:18:35,566 - INFO - Avg. fwd time: 16.0119 / Avg. bwd time: 40.9186 / Avg. batch time: 1193.6964 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-10 13:18:35,515 - INFO - Avg. fwd time: 19.9374 / Avg. bwd time: 46.0017 / Avg. batch time: 1129.0608 (ms) / GPU bubble ratio: 53.28%
[rank2]:2025-11-10 13:18:35,588 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1803  memory:  8.66GiB(18.23%)  tps: 6,704  tflops: 52.41  mfu: 16.80%
[rank3]:2025-11-10 13:18:35,600 - INFO -  step: 600  loss:  0.2128  grad_norm:  0.1803  memory: 25.79GiB(54.28%)  tps: 6,704  tflops: 52.41  mfu: 16.80%
[rank0]:2025-11-10 13:18:35,604 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1803  memory: 16.87GiB(35.50%)  tps: 6,704  tflops: 52.41  mfu: 16.80%
[rank1]:2025-11-10 13:18:35,591 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.1803  memory: 12.86GiB(27.07%)  tps: 6,704  tflops: 52.41  mfu: 16.80%
[rank3]:2025-11-10 13:18:35,759 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1318_real_step600_rank3.svg
[rank3]:> Batch Time: 1184.94 ms, GPU Bubble Ratio: 61.46%, 55.54%, 65.54%, 28.45%
[rank0]:2025-11-10 13:22:34,595 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:22:38,901 - INFO - Avg. fwd time: 24.6267 / Avg. bwd time: 80.4575 / Avg. batch time: 995.2355 (ms) / GPU bubble ratio: 15.53%
[rank2]:2025-11-10 13:22:38,963 - INFO - Avg. fwd time: 15.3998 / Avg. bwd time: 35.5769 / Avg. batch time: 1055.4559 (ms) / GPU bubble ratio: 61.36%
[rank0]:2025-11-10 13:22:39,041 - INFO - Avg. fwd time: 16.0097 / Avg. bwd time: 40.9171 / Avg. batch time: 1193.5524 (ms) / GPU bubble ratio: 61.84%
[rank1]:2025-11-10 13:22:39,082 - INFO - Avg. fwd time: 19.9327 / Avg. bwd time: 45.9992 / Avg. batch time: 1128.9275 (ms) / GPU bubble ratio: 53.28%
[rank2]:2025-11-10 13:22:39,430 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1720  memory:  8.66GiB(18.23%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank3]:2025-11-10 13:22:39,443 - INFO -  step: 650  loss:  0.2220  grad_norm:  0.1720  memory: 25.79GiB(54.28%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank0]:2025-11-10 13:22:39,447 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1720  memory: 16.87GiB(35.50%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank1]:2025-11-10 13:22:39,434 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.1720  memory: 12.86GiB(27.07%)  tps: 6,719  tflops: 52.53  mfu: 16.84%
[rank0]:2025-11-10 13:26:38,963 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:26:43,305 - INFO - Avg. fwd time: 24.6300 / Avg. bwd time: 80.4657 / Avg. batch time: 995.3250 (ms) / GPU bubble ratio: 15.53%
[rank2]:2025-11-10 13:26:43,370 - INFO - Avg. fwd time: 15.3967 / Avg. bwd time: 35.5804 / Avg. batch time: 1055.5014 (ms) / GPU bubble ratio: 61.36%
[rank0]:2025-11-10 13:26:43,448 - INFO - Avg. fwd time: 16.0084 / Avg. bwd time: 40.9169 / Avg. batch time: 1193.6020 (ms) / GPU bubble ratio: 61.85%
[rank1]:2025-11-10 13:26:43,490 - INFO - Avg. fwd time: 19.9276 / Avg. bwd time: 45.9935 / Avg. batch time: 1128.9819 (ms) / GPU bubble ratio: 53.29%
[rank2]:2025-11-10 13:26:43,840 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1619  memory:  8.66GiB(18.23%)  tps: 6,704  tflops: 52.40  mfu: 16.80%
[rank0]:2025-11-10 13:26:43,858 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1619  memory: 16.87GiB(35.50%)  tps: 6,703  tflops: 52.40  mfu: 16.80%
[rank3]:2025-11-10 13:26:43,853 - INFO -  step: 700  loss:  0.1794  grad_norm:  0.1619  memory: 25.79GiB(54.28%)  tps: 6,704  tflops: 52.41  mfu: 16.80%
[rank1]:2025-11-10 13:26:43,844 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.1619  memory: 12.86GiB(27.07%)  tps: 6,704  tflops: 52.40  mfu: 16.80%
[rank3]:2025-11-10 13:26:44,016 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1326_real_step700_rank3.svg
[rank3]:> Batch Time: 1190.88 ms, GPU Bubble Ratio: 61.70%, 55.85%, 65.73%, 28.54%
[rank0]:2025-11-10 13:30:43,618 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:30:48,355 - INFO - Avg. fwd time: 24.6373 / Avg. bwd time: 80.4809 / Avg. batch time: 995.5308 (ms) / GPU bubble ratio: 15.53%
[rank2]:2025-11-10 13:30:48,402 - INFO - Avg. fwd time: 15.3929 / Avg. bwd time: 35.5839 / Avg. batch time: 1055.6893 (ms) / GPU bubble ratio: 61.37%
[rank1]:2025-11-10 13:30:48,458 - INFO - Avg. fwd time: 19.9221 / Avg. bwd time: 45.9904 / Avg. batch time: 1129.1256 (ms) / GPU bubble ratio: 53.30%
[rank2]:2025-11-10 13:30:48,531 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1951  memory:  8.66GiB(18.23%)  tps: 6,696  tflops: 52.34  mfu: 16.78%
[rank0]:2025-11-10 13:30:48,509 - INFO - Avg. fwd time: 16.0039 / Avg. bwd time: 40.9171 / Avg. batch time: 1193.7388 (ms) / GPU bubble ratio: 61.85%
[rank0]:2025-11-10 13:30:48,546 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1951  memory: 16.87GiB(35.50%)  tps: 6,696  tflops: 52.35  mfu: 16.78%
[rank3]:2025-11-10 13:30:48,543 - INFO -  step: 750  loss:  0.2179  grad_norm:  0.1951  memory: 25.79GiB(54.28%)  tps: 6,696  tflops: 52.35  mfu: 16.78%
[rank1]:2025-11-10 13:30:48,534 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.1951  memory: 12.86GiB(27.07%)  tps: 6,696  tflops: 52.34  mfu: 16.78%
[rank0]:2025-11-10 13:34:48,356 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-10 13:34:52,773 - INFO - Avg. fwd time: 24.6424 / Avg. bwd time: 80.5000 / Avg. batch time: 995.7954 (ms) / GPU bubble ratio: 15.53%
[rank2]:2025-11-10 13:34:52,834 - INFO - Avg. fwd time: 15.3883 / Avg. bwd time: 35.5865 / Avg. batch time: 1055.8981 (ms) / GPU bubble ratio: 61.38%
[rank0]:2025-11-10 13:34:52,909 - INFO - Avg. fwd time: 16.0022 / Avg. bwd time: 40.9184 / Avg. batch time: 1193.9249 (ms) / GPU bubble ratio: 61.86%
[rank1]:2025-11-10 13:34:52,953 - INFO - Avg. fwd time: 19.9200 / Avg. bwd time: 45.9889 / Avg. batch time: 1129.3136 (ms) / GPU bubble ratio: 53.31%
[rank1]:2025-11-10 13:34:53,309 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1873  memory: 12.86GiB(27.07%)  tps: 6,693  tflops: 52.33  mfu: 16.77%
[rank1]:2025-11-10 13:34:53,309 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1873  tps: 7,117  tflops: 55.63  mfu: 16.28%
[rank1]:2025-11-10 13:34:53,309 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-10 13:34:53,310 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-10 13:34:53,322 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1873  memory: 16.87GiB(35.50%)  tps: 6,693  tflops: 52.33  mfu: 16.77%
[rank0]:2025-11-10 13:34:53,322 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1873  tps: 7,117  tflops: 55.64  mfu: 16.28%
[rank0]:2025-11-10 13:34:53,322 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-10 13:34:53,323 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-10 13:34:53,306 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.1873  memory:  8.66GiB(18.23%)  tps: 6,694  tflops: 52.33  mfu: 16.77%
[rank2]:2025-11-10 13:34:53,306 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.1873  tps: 7,117  tflops: 55.63  mfu: 16.28%
[rank2]:2025-11-10 13:34:53,306 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-10 13:34:53,307 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank3]:2025-11-10 13:34:53,318 - INFO -  step: 800  loss:  0.2241  grad_norm:  0.1873  memory: 25.79GiB(54.28%)  tps: 6,694  tflops: 52.33  mfu: 16.77%
[rank3]:2025-11-10 13:34:53,319 - INFO -  final step: 800  loss:  0.2241  grad_norm:  0.1873  tps: 7,121  tflops: 55.67  mfu: 16.43%
[rank3]:2025-11-10 13:34:53,319 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-10 13:34:53,320 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-10 13:34:55,217 - INFO - Destroying the purge thread.
[rank0]:2025-11-10 13:34:55,200 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-10 13:34:55,218 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-10 13:34:55,217 - INFO - Destroying the purge thread.
[rank3]:2025-11-10 13:34:55,408 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1334_real_final800_rank3.svg
[rank3]:> Batch Time: 1187.97 ms, GPU Bubble Ratio: 61.54%, 55.57%, 65.57%, 28.69%
[rank3]:2025-11-10 13:34:55,593 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1/pipeline_schedule/251110_1334_thry_final800_rank3.svg
[rank3]:> Batch Time: 627.36 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-10 13:34:55,594 - INFO - Destroying the purge thread.
[rank1]:2025-11-10 13:34:55,600 - INFO - Process group destroyed
[rank2]:2025-11-10 13:34:55,645 - INFO - Process group destroyed
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 15-16, summary, console lines 228-237
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 4.60151
[rank3]:wandb:               final/avg_loss 0.22414
[rank3]:wandb:             final/avg_mfu(%) 16.42828
[rank3]:wandb:             final/avg_tflops 55.66978
[rank3]:wandb:    final/avg_throughput(tps) 7121.13781
[rank3]:wandb:              final/grad_norm 0.18733
[rank3]:wandb:               final/max_loss 0.22414
[rank3]:wandb:                    grad_norm 0.18733
[rank3]:wandb: loss_metrics/global_avg_loss 0.22414
[rank3]:wandb: loss_metrics/global_max_loss 0.22414
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1109_1F1B_nofreeze_42_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/02buaaqy
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1109_1F1B_nofreeze_42_dm1/20251110-1229/wandb/run-20251110_122940-02buaaqy/logs
[rank3]:2025-11-10 13:34:57,108 - INFO - Process group destroyed
[rank0]:2025-11-10 13:34:57,218 - INFO - Training completed
[rank0]:2025-11-10 13:34:57,218 - INFO - Destroying the purge thread.
[rank0]:2025-11-10 13:34:57,614 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.11', 'layers.12', 'layers.9'}
[rank1]:Stage 1: Modules to keep: {'layers.6', 'layers.5', 'layers.4', 'layers.8', 'layers.7'}
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.0', 'tok_embeddings', 'layers.2', 'layers.3'}
[rank3]:Stage 3: Modules to keep: {'output', 'layers.13', 'norm', 'layers.15', 'layers.14'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1109.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:1109: switch to alpaca_gpt4+alpaca_cleaned dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1109_1F1B_nofreeze_42_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1109_1F1B_nofreeze_42_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1109_1F1B_nofreeze_42_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1109_1F1B_nofreeze_42_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1109_1F1B_nofreeze_42_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1109_1F1B_nofreeze_42_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.01
[rank3]:	- training:
[rank3]:		- dataset: alpaca_gpt4
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 42
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1109_1F1B_nofreeze_42_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1109_1F1B_nofreeze_42_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
