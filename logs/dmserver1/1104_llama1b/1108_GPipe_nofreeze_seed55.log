
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 17:41:30 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed55.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
" --parallelism.pipeline_parallel_degree=4 --optimizer.lr=5e-6  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank3]:2025-11-09 17:41:36,784 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank2]:2025-11-09 17:41:36,785 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:"
[rank1]:2025-11-09 17:41:36,903 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:"
[rank0]:2025-11-09 17:41:36,919 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:"
[rank3]:2025-11-09 17:41:37,051 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 17:41:37,053 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 17:41:37,046 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 17:41:37,049 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-11-09 17:41:37,195 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 17:41:37,198 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 17:41:37,191 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 17:41:37,194 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 17:41:37,196 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 17:41:37,198 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-09 17:41:37,574 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank2]:2025-11-09 17:41:40,427 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-09 17:41:40,465 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 17:41:40,520 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 17:41:40,558 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 17:41:40,489 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 17:41:40,490 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-09 17:41:40,585 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 17:41:40,585 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 17:41:40,668 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 17:41:40,668 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 17:41:40,668 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-09 17:41:40,770 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 17:41:40,770 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 17:41:40,770 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-09 17:41:40,684 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 17:41:40,832 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 17:41:40,869 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 17:41:40,870 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-09 17:41:40,893 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 17:41:40,894 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 17:41:41,082 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 17:41:41,082 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 17:41:41,083 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 6ji2yl5r
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed55_dm1/20251109-1741/wandb/run-20251109_174141-6ji2yl5r
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed55_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/6ji2yl5r
[rank3]:2025-11-09 17:41:42,798 - INFO - WandB logging enabled
[rank3]:2025-11-09 17:41:42,798 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 17:41:42,837 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 17:41:42,865 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 17:41:42,866 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-09 17:41:43,063 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 17:41:43,064 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 17:41:43,065 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 17:41:43,079 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 17:41:43,080 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 17:41:43,079 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 17:41:43,080 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1
[rank0]:2025-11-09 17:41:43,080 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 17:41:43,080 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 17:41:43,080 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp.
[rank0]:2025-11-09 17:41:45,356 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-11-09 17:41:45,356 - INFO - Finished loading the checkpoint in 2.28 seconds.
[rank0]:2025-11-09 17:41:45,356 - INFO - Training starts at step 1
[rank3]:2025-11-09 17:41:48,484 - INFO -  step:  1  loss:  9.4364  grad_norm: 183.5944  memory: 24.19GiB(50.91%)  tps: 2,903  tflops: 22.11  mfu: 7.09%
[rank3]:2025-11-09 17:41:48,485 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 17:41:48,480 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5944  memory: 12.38GiB(26.05%)  tps: 2,068  tflops: 15.75  mfu: 5.05%
[rank1]:2025-11-09 17:41:48,480 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 17:41:48,474 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5944  memory:  9.99GiB(21.03%)  tps: 2,046  tflops: 15.58  mfu: 4.99%
[rank2]:2025-11-09 17:41:48,474 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 17:41:48,511 - INFO -  step:  1  loss: -4.0000  grad_norm: 183.5944  memory: 12.80GiB(26.95%)  tps: 2,144  tflops: 16.33  mfu: 5.23%
[rank0]:2025-11-09 17:41:48,511 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 17:43:51,881 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:43:54,160 - INFO - Avg. fwd time: 11.4710 / Avg. bwd time: 44.9706 / Avg. batch time: 511.4885 (ms) / GPU bubble ratio: 11.72%
[rank2]:2025-11-09 17:43:54,236 - INFO - Avg. fwd time: 7.1717 / Avg. bwd time: 18.8068 / Avg. batch time: 543.4306 (ms) / GPU bubble ratio: 61.76%
[rank0]:2025-11-09 17:43:54,283 - INFO - Avg. fwd time: 7.9043 / Avg. bwd time: 23.4073 / Avg. batch time: 619.8721 (ms) / GPU bubble ratio: 59.59%
[rank1]:2025-11-09 17:43:54,275 - INFO - Avg. fwd time: 9.1131 / Avg. bwd time: 23.9543 / Avg. batch time: 582.8320 (ms) / GPU bubble ratio: 54.61%
[rank2]:2025-11-09 17:43:54,460 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5366  memory: 11.81GiB(24.85%)  tps: 6,372  tflops: 48.53  mfu: 15.56%
[rank1]:2025-11-09 17:43:54,463 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5366  memory: 14.64GiB(30.82%)  tps: 6,372  tflops: 48.53  mfu: 15.56%
[rank0]:2025-11-09 17:43:54,474 - INFO -  step: 50  loss: -4.0000  grad_norm: 17.5366  memory: 16.57GiB(34.88%)  tps: 6,373  tflops: 48.54  mfu: 15.56%
[rank3]:2025-11-09 17:43:54,472 - INFO -  step: 50  loss:  8.2896  grad_norm: 17.5366  memory: 26.98GiB(56.79%)  tps: 6,372  tflops: 48.53  mfu: 15.56%
[rank0]:2025-11-09 17:46:01,285 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:46:03,617 - INFO - Avg. fwd time: 11.4800 / Avg. bwd time: 45.2841 / Avg. batch time: 513.9221 (ms) / GPU bubble ratio: 11.64%
[rank1]:2025-11-09 17:46:03,733 - INFO - Avg. fwd time: 9.1295 / Avg. bwd time: 24.0566 / Avg. batch time: 584.6235 (ms) / GPU bubble ratio: 54.59%
[rank2]:2025-11-09 17:46:03,692 - INFO - Avg. fwd time: 7.1730 / Avg. bwd time: 18.8826 / Avg. batch time: 545.4799 (ms) / GPU bubble ratio: 61.79%
[rank0]:2025-11-09 17:46:03,740 - INFO - Avg. fwd time: 7.8959 / Avg. bwd time: 23.4625 / Avg. batch time: 621.2361 (ms) / GPU bubble ratio: 59.62%
[rank1]:2025-11-09 17:46:03,922 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1233  memory: 14.64GiB(30.82%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank2]:2025-11-09 17:46:03,918 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1233  memory: 11.81GiB(24.85%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 17:46:03,930 - INFO -  step: 100  loss:  4.2821  grad_norm: 19.1233  memory: 26.98GiB(56.79%)  tps: 6,328  tflops: 48.20  mfu: 15.45%
[rank0]:2025-11-09 17:46:03,932 - INFO -  step: 100  loss: -4.0000  grad_norm: 19.1233  memory: 16.57GiB(34.88%)  tps: 6,328  tflops: 48.19  mfu: 15.45%
[rank3]:2025-11-09 17:46:04,119 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1746_real_step100_rank3.svg
[rank3]:> Batch Time: 622.62 ms, GPU Bubble Ratio: 59.37%, 57.11%, 66.31%, 26.48%
[rank0]:2025-11-09 17:48:10,976 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:48:13,464 - INFO - Avg. fwd time: 11.4778 / Avg. bwd time: 45.4096 / Avg. batch time: 514.8592 (ms) / GPU bubble ratio: 11.61%
[rank2]:2025-11-09 17:48:13,490 - INFO - Avg. fwd time: 7.1685 / Avg. bwd time: 18.9068 / Avg. batch time: 546.5449 (ms) / GPU bubble ratio: 61.83%
[rank3]:2025-11-09 17:48:13,583 - INFO -  step: 150  loss:  3.4822  grad_norm: 30.5291  memory: 26.98GiB(56.79%)  tps: 6,319  tflops: 48.12  mfu: 15.42%
[rank0]:2025-11-09 17:48:13,550 - INFO - Avg. fwd time: 7.8937 / Avg. bwd time: 23.4782 / Avg. batch time: 622.1188 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 17:48:13,586 - INFO -  step: 150  loss: -4.0000  grad_norm: 30.5291  memory: 16.57GiB(34.88%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank2]:2025-11-09 17:48:13,571 - INFO -  step: 150  loss: -4.0000  grad_norm: 30.5291  memory: 11.81GiB(24.85%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank1]:2025-11-09 17:48:13,520 - INFO - Avg. fwd time: 9.1324 / Avg. bwd time: 24.0938 / Avg. batch time: 585.6350 (ms) / GPU bubble ratio: 54.61%
[rank1]:2025-11-09 17:48:13,574 - INFO -  step: 150  loss: -4.0000  grad_norm: 30.5291  memory: 14.64GiB(30.82%)  tps: 6,318  tflops: 48.12  mfu: 15.42%
[rank0]:2025-11-09 17:50:20,391 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:50:22,715 - INFO - Avg. fwd time: 11.4746 / Avg. bwd time: 45.4440 / Avg. batch time: 515.0863 (ms) / GPU bubble ratio: 11.60%
[rank1]:2025-11-09 17:50:22,829 - INFO - Avg. fwd time: 9.1316 / Avg. bwd time: 24.1235 / Avg. batch time: 585.7696 (ms) / GPU bubble ratio: 54.58%
[rank0]:2025-11-09 17:50:22,835 - INFO - Avg. fwd time: 7.8857 / Avg. bwd time: 23.4879 / Avg. batch time: 622.1964 (ms) / GPU bubble ratio: 59.66%
[rank2]:2025-11-09 17:50:22,787 - INFO - Avg. fwd time: 7.1656 / Avg. bwd time: 18.9238 / Avg. batch time: 546.6725 (ms) / GPU bubble ratio: 61.82%
[rank3]:2025-11-09 17:50:23,026 - INFO -  step: 200  loss:  1.2217  grad_norm: 14.7265  memory: 26.98GiB(56.79%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank1]:2025-11-09 17:50:23,018 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.7265  memory: 14.64GiB(30.82%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank0]:2025-11-09 17:50:23,028 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.7265  memory: 16.57GiB(34.88%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank2]:2025-11-09 17:50:23,014 - INFO -  step: 200  loss: -4.0000  grad_norm: 14.7265  memory: 11.81GiB(24.85%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-09 17:50:23,179 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1750_real_step200_rank3.svg
[rank3]:> Batch Time: 621.11 ms, GPU Bubble Ratio: 59.30%, 56.92%, 66.21%, 26.63%
[rank0]:2025-11-09 17:52:30,142 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:52:32,446 - INFO - Avg. fwd time: 11.4599 / Avg. bwd time: 45.4815 / Avg. batch time: 515.2549 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 17:52:32,514 - INFO - Avg. fwd time: 7.1631 / Avg. bwd time: 18.9457 / Avg. batch time: 546.8990 (ms) / GPU bubble ratio: 61.81%
[rank1]:2025-11-09 17:52:32,554 - INFO - Avg. fwd time: 9.1294 / Avg. bwd time: 24.1535 / Avg. batch time: 585.9858 (ms) / GPU bubble ratio: 54.56%
[rank0]:2025-11-09 17:52:32,562 - INFO - Avg. fwd time: 7.8842 / Avg. bwd time: 23.4977 / Avg. batch time: 622.3817 (ms) / GPU bubble ratio: 59.66%
[rank1]:2025-11-09 17:52:32,740 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8255  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank2]:2025-11-09 17:52:32,736 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8255  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank3]:2025-11-09 17:52:32,750 - INFO -  step: 250  loss:  0.6583  grad_norm:  3.8255  memory: 26.98GiB(56.79%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 17:52:32,751 - INFO -  step: 250  loss: -4.0000  grad_norm:  3.8255  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.10  mfu: 15.42%
[rank0]:2025-11-09 17:54:39,603 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:54:42,134 - INFO - Avg. fwd time: 11.4458 / Avg. bwd time: 45.5088 / Avg. batch time: 515.3524 (ms) / GPU bubble ratio: 11.59%
[rank1]:2025-11-09 17:54:42,191 - INFO - Avg. fwd time: 9.1264 / Avg. bwd time: 24.1691 / Avg. batch time: 586.0007 (ms) / GPU bubble ratio: 54.55%
[rank2]:2025-11-09 17:54:42,161 - INFO - Avg. fwd time: 7.1611 / Avg. bwd time: 18.9605 / Avg. batch time: 546.9392 (ms) / GPU bubble ratio: 61.79%
[rank3]:2025-11-09 17:54:42,255 - INFO -  step: 300  loss:  0.5353  grad_norm:  0.6072  memory: 26.98GiB(56.79%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank1]:2025-11-09 17:54:42,246 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6072  memory: 14.64GiB(30.82%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank0]:2025-11-09 17:54:42,221 - INFO - Avg. fwd time: 7.8825 / Avg. bwd time: 23.5039 / Avg. batch time: 622.3752 (ms) / GPU bubble ratio: 59.66%
[rank0]:2025-11-09 17:54:42,257 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6072  memory: 16.57GiB(34.88%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank2]:2025-11-09 17:54:42,243 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.6072  memory: 11.81GiB(24.85%)  tps: 6,326  tflops: 48.18  mfu: 15.44%
[rank3]:2025-11-09 17:54:42,409 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1754_real_step300_rank3.svg
[rank3]:> Batch Time: 621.09 ms, GPU Bubble Ratio: 59.29%, 56.94%, 66.19%, 26.55%
[rank0]:2025-11-09 17:56:48,981 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:56:51,281 - INFO - Avg. fwd time: 11.4379 / Avg. bwd time: 45.5072 / Avg. batch time: 515.2651 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 17:56:51,355 - INFO - Avg. fwd time: 7.1596 / Avg. bwd time: 18.9668 / Avg. batch time: 546.9001 (ms) / GPU bubble ratio: 61.78%
[rank1]:2025-11-09 17:56:51,395 - INFO - Avg. fwd time: 9.1231 / Avg. bwd time: 24.1760 / Avg. batch time: 585.9363 (ms) / GPU bubble ratio: 54.54%
[rank0]:2025-11-09 17:56:51,400 - INFO - Avg. fwd time: 7.8810 / Avg. bwd time: 23.5068 / Avg. batch time: 622.2937 (ms) / GPU bubble ratio: 59.65%
[rank1]:2025-11-09 17:56:51,580 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4336  memory: 14.64GiB(30.82%)  tps: 6,334  tflops: 48.24  mfu: 15.46%
[rank2]:2025-11-09 17:56:51,576 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4336  memory: 11.81GiB(24.85%)  tps: 6,334  tflops: 48.24  mfu: 15.46%
[rank3]:2025-11-09 17:56:51,589 - INFO -  step: 350  loss:  0.4832  grad_norm:  0.4336  memory: 26.98GiB(56.79%)  tps: 6,334  tflops: 48.24  mfu: 15.46%
[rank0]:2025-11-09 17:56:51,590 - INFO -  step: 350  loss: -4.0000  grad_norm:  0.4336  memory: 16.57GiB(34.88%)  tps: 6,334  tflops: 48.24  mfu: 15.46%
[rank0]:2025-11-09 17:58:58,442 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 17:59:00,764 - INFO - Avg. fwd time: 11.4379 / Avg. bwd time: 45.5139 / Avg. batch time: 515.3146 (ms) / GPU bubble ratio: 11.59%
[rank2]:2025-11-09 17:59:00,835 - INFO - Avg. fwd time: 7.1584 / Avg. bwd time: 18.9717 / Avg. batch time: 546.9034 (ms) / GPU bubble ratio: 61.78%
[rank1]:2025-11-09 17:59:00,877 - INFO - Avg. fwd time: 9.1204 / Avg. bwd time: 24.1804 / Avg. batch time: 585.9132 (ms) / GPU bubble ratio: 54.53%
[rank0]:2025-11-09 17:59:00,884 - INFO - Avg. fwd time: 7.8800 / Avg. bwd time: 23.5095 / Avg. batch time: 622.2557 (ms) / GPU bubble ratio: 59.64%
[rank2]:2025-11-09 17:59:01,059 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3741  memory: 11.81GiB(24.85%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank3]:2025-11-09 17:59:01,072 - INFO -  step: 400  loss:  0.4686  grad_norm:  0.3741  memory: 26.98GiB(56.79%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank1]:2025-11-09 17:59:01,063 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3741  memory: 14.64GiB(30.82%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank0]:2025-11-09 17:59:01,074 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.3741  memory: 16.57GiB(34.88%)  tps: 6,327  tflops: 48.19  mfu: 15.44%
[rank3]:2025-11-09 17:59:01,240 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1759_real_step400_rank3.svg
[rank3]:> Batch Time: 622.09 ms, GPU Bubble Ratio: 59.34%, 57.06%, 66.24%, 26.63%
[rank3]:2025-11-09 17:59:09,972 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank1]:2025-11-09 17:59:10,214 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank2]:2025-11-09 17:59:10,188 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 17:59:10,240 - WARNING - Dataset alpaca_cleaned is being re-looped
[rank0]:2025-11-09 18:01:07,819 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:01:10,326 - INFO - Avg. fwd time: 11.4351 / Avg. bwd time: 45.5180 / Avg. batch time: 515.3189 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 18:01:10,353 - INFO - Avg. fwd time: 7.1576 / Avg. bwd time: 18.9760 / Avg. batch time: 546.9475 (ms) / GPU bubble ratio: 61.78%
[rank1]:2025-11-09 18:01:10,385 - INFO - Avg. fwd time: 9.1190 / Avg. bwd time: 24.1838 / Avg. batch time: 585.9383 (ms) / GPU bubble ratio: 54.53%
[rank0]:2025-11-09 18:01:10,416 - INFO - Avg. fwd time: 7.8784 / Avg. bwd time: 23.5126 / Avg. batch time: 622.2697 (ms) / GPU bubble ratio: 59.64%
[rank3]:2025-11-09 18:01:10,451 - INFO -  step: 450  loss:  0.5250  grad_norm:  0.3754  memory: 26.98GiB(56.79%)  tps: 6,332  tflops: 48.23  mfu: 15.46%
[rank2]:2025-11-09 18:01:10,437 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3754  memory: 11.81GiB(24.85%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank1]:2025-11-09 18:01:10,441 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3754  memory: 14.64GiB(30.82%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank0]:2025-11-09 18:01:10,451 - INFO -  step: 450  loss: -4.0000  grad_norm:  0.3754  memory: 16.57GiB(34.88%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank0]:2025-11-09 18:03:17,265 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:03:19,583 - INFO - Avg. fwd time: 11.4319 / Avg. bwd time: 45.5232 / Avg. batch time: 515.3335 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 18:03:19,657 - INFO - Avg. fwd time: 7.1572 / Avg. bwd time: 18.9782 / Avg. batch time: 546.9299 (ms) / GPU bubble ratio: 61.77%
[rank0]:2025-11-09 18:03:19,704 - INFO - Avg. fwd time: 7.8783 / Avg. bwd time: 23.5139 / Avg. batch time: 622.2317 (ms) / GPU bubble ratio: 59.64%
[rank1]:2025-11-09 18:03:19,696 - INFO - Avg. fwd time: 9.1188 / Avg. bwd time: 24.1853 / Avg. batch time: 585.9035 (ms) / GPU bubble ratio: 54.53%
[rank2]:2025-11-09 18:03:19,874 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3180  memory: 11.81GiB(24.85%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank1]:2025-11-09 18:03:19,878 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3180  memory: 14.64GiB(30.82%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-09 18:03:19,887 - INFO -  step: 500  loss:  0.3978  grad_norm:  0.3180  memory: 26.98GiB(56.79%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank0]:2025-11-09 18:03:19,889 - INFO -  step: 500  loss: -4.0000  grad_norm:  0.3180  memory: 16.57GiB(34.88%)  tps: 6,329  tflops: 48.20  mfu: 15.45%
[rank3]:2025-11-09 18:03:20,055 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1803_real_step500_rank3.svg
[rank3]:> Batch Time: 621.56 ms, GPU Bubble Ratio: 59.32%, 57.04%, 66.24%, 26.52%
[rank0]:2025-11-09 18:05:27,024 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:05:29,323 - INFO - Avg. fwd time: 11.4361 / Avg. bwd time: 45.5352 / Avg. batch time: 515.4612 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 18:05:29,433 - INFO - Avg. fwd time: 9.1186 / Avg. bwd time: 24.1862 / Avg. batch time: 586.0414 (ms) / GPU bubble ratio: 54.54%
[rank2]:2025-11-09 18:05:29,393 - INFO - Avg. fwd time: 7.1575 / Avg. bwd time: 18.9801 / Avg. batch time: 547.0899 (ms) / GPU bubble ratio: 61.78%
[rank0]:2025-11-09 18:05:29,439 - INFO - Avg. fwd time: 7.8784 / Avg. bwd time: 23.5155 / Avg. batch time: 622.3661 (ms) / GPU bubble ratio: 59.65%
[rank0]:2025-11-09 18:05:29,622 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3925  memory: 16.57GiB(34.88%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank1]:2025-11-09 18:05:29,611 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3925  memory: 14.64GiB(30.82%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank2]:2025-11-09 18:05:29,607 - INFO -  step: 550  loss: -4.0000  grad_norm:  0.3925  memory: 11.81GiB(24.85%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank3]:2025-11-09 18:05:29,620 - INFO -  step: 550  loss:  0.4425  grad_norm:  0.3925  memory: 26.98GiB(56.79%)  tps: 6,315  tflops: 48.09  mfu: 15.41%
[rank0]:2025-11-09 18:07:36,315 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:07:38,848 - INFO - Avg. fwd time: 11.4355 / Avg. bwd time: 45.5299 / Avg. batch time: 515.4107 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 18:07:38,877 - INFO - Avg. fwd time: 7.1573 / Avg. bwd time: 18.9810 / Avg. batch time: 547.0099 (ms) / GPU bubble ratio: 61.77%
[rank1]:2025-11-09 18:07:38,909 - INFO - Avg. fwd time: 9.1179 / Avg. bwd time: 24.1860 / Avg. batch time: 585.9419 (ms) / GPU bubble ratio: 54.53%
[rank3]:2025-11-09 18:07:38,975 - INFO -  step: 600  loss:  0.4451  grad_norm:  0.3686  memory: 26.98GiB(56.79%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank0]:2025-11-09 18:07:38,940 - INFO - Avg. fwd time: 7.8764 / Avg. bwd time: 23.5160 / Avg. batch time: 622.2638 (ms) / GPU bubble ratio: 59.64%
[rank0]:2025-11-09 18:07:38,977 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3686  memory: 16.57GiB(34.88%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank2]:2025-11-09 18:07:38,962 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3686  memory: 11.81GiB(24.85%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank1]:2025-11-09 18:07:38,966 - INFO -  step: 600  loss: -4.0000  grad_norm:  0.3686  memory: 14.64GiB(30.82%)  tps: 6,333  tflops: 48.23  mfu: 15.46%
[rank3]:2025-11-09 18:07:39,131 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1807_real_step600_rank3.svg
[rank3]:> Batch Time: 621.09 ms, GPU Bubble Ratio: 59.34%, 57.05%, 66.24%, 26.61%
[rank0]:2025-11-09 18:09:45,967 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:09:48,264 - INFO - Avg. fwd time: 11.4363 / Avg. bwd time: 45.5343 / Avg. batch time: 515.4494 (ms) / GPU bubble ratio: 11.58%
[rank2]:2025-11-09 18:09:48,337 - INFO - Avg. fwd time: 7.1571 / Avg. bwd time: 18.9808 / Avg. batch time: 547.0739 (ms) / GPU bubble ratio: 61.78%
[rank0]:2025-11-09 18:09:48,384 - INFO - Avg. fwd time: 7.8758 / Avg. bwd time: 23.5159 / Avg. batch time: 622.3041 (ms) / GPU bubble ratio: 59.64%
[rank1]:2025-11-09 18:09:48,376 - INFO - Avg. fwd time: 9.1172 / Avg. bwd time: 24.1830 / Avg. batch time: 585.9870 (ms) / GPU bubble ratio: 54.54%
[rank2]:2025-11-09 18:09:48,552 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3695  memory: 11.81GiB(24.85%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank1]:2025-11-09 18:09:48,556 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3695  memory: 14.64GiB(30.82%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank3]:2025-11-09 18:09:48,565 - INFO -  step: 650  loss:  0.3929  grad_norm:  0.3695  memory: 26.98GiB(56.79%)  tps: 6,322  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 18:09:48,567 - INFO -  step: 650  loss: -4.0000  grad_norm:  0.3695  memory: 16.57GiB(34.88%)  tps: 6,321  tflops: 48.15  mfu: 15.43%
[rank0]:2025-11-09 18:11:55,193 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:11:57,505 - INFO - Avg. fwd time: 11.4331 / Avg. bwd time: 45.5329 / Avg. batch time: 515.4100 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 18:11:57,622 - INFO - Avg. fwd time: 7.8758 / Avg. bwd time: 23.5153 / Avg. batch time: 622.2109 (ms) / GPU bubble ratio: 59.64%
[rank1]:2025-11-09 18:11:57,619 - INFO - Avg. fwd time: 9.1154 / Avg. bwd time: 24.1784 / Avg. batch time: 585.8966 (ms) / GPU bubble ratio: 54.54%
[rank2]:2025-11-09 18:11:57,579 - INFO - Avg. fwd time: 7.1567 / Avg. bwd time: 18.9805 / Avg. batch time: 547.0077 (ms) / GPU bubble ratio: 61.77%
[rank3]:2025-11-09 18:11:57,810 - INFO -  step: 700  loss:  0.4445  grad_norm:  0.4211  memory: 26.98GiB(56.79%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank0]:2025-11-09 18:11:57,813 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4211  memory: 16.57GiB(34.88%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank1]:2025-11-09 18:11:57,802 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4211  memory: 14.64GiB(30.82%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank2]:2025-11-09 18:11:57,798 - INFO -  step: 700  loss: -4.0000  grad_norm:  0.4211  memory: 11.81GiB(24.85%)  tps: 6,338  tflops: 48.27  mfu: 15.47%
[rank3]:2025-11-09 18:11:57,969 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1811_real_step700_rank3.svg
[rank3]:> Batch Time: 620.62 ms, GPU Bubble Ratio: 59.26%, 57.06%, 66.22%, 26.74%
[rank0]:2025-11-09 18:14:04,716 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:14:07,216 - INFO - Avg. fwd time: 11.4340 / Avg. bwd time: 45.5319 / Avg. batch time: 515.4072 (ms) / GPU bubble ratio: 11.58%
[rank1]:2025-11-09 18:14:07,278 - INFO - Avg. fwd time: 9.1140 / Avg. bwd time: 24.1757 / Avg. batch time: 585.8943 (ms) / GPU bubble ratio: 54.55%
[rank2]:2025-11-09 18:14:07,244 - INFO - Avg. fwd time: 7.1564 / Avg. bwd time: 18.9803 / Avg. batch time: 547.0244 (ms) / GPU bubble ratio: 61.78%
[rank3]:2025-11-09 18:14:07,345 - INFO -  step: 750  loss:  0.4244  grad_norm:  0.4819  memory: 26.98GiB(56.79%)  tps: 6,324  tflops: 48.17  mfu: 15.44%
[rank0]:2025-11-09 18:14:07,310 - INFO - Avg. fwd time: 7.8755 / Avg. bwd time: 23.5156 / Avg. batch time: 622.2083 (ms) / GPU bubble ratio: 59.64%
[rank0]:2025-11-09 18:14:07,346 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4819  memory: 16.57GiB(34.88%)  tps: 6,324  tflops: 48.17  mfu: 15.44%
[rank1]:2025-11-09 18:14:07,335 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4819  memory: 14.64GiB(30.82%)  tps: 6,324  tflops: 48.17  mfu: 15.44%
[rank2]:2025-11-09 18:14:07,332 - INFO -  step: 750  loss: -4.0000  grad_norm:  0.4819  memory: 11.81GiB(24.85%)  tps: 6,324  tflops: 48.17  mfu: 15.44%
[rank0]:2025-11-09 18:16:14,103 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-09 18:16:16,427 - INFO - Avg. fwd time: 11.4316 / Avg. bwd time: 45.5324 / Avg. batch time: 515.3884 (ms) / GPU bubble ratio: 11.58%
[rank0]:2025-11-09 18:16:16,543 - INFO - Avg. fwd time: 7.8765 / Avg. bwd time: 23.5167 / Avg. batch time: 622.1576 (ms) / GPU bubble ratio: 59.63%
[rank2]:2025-11-09 18:16:16,491 - INFO - Avg. fwd time: 7.1564 / Avg. bwd time: 18.9809 / Avg. batch time: 546.9859 (ms) / GPU bubble ratio: 61.77%
[rank1]:2025-11-09 18:16:16,536 - INFO - Avg. fwd time: 9.1132 / Avg. bwd time: 24.1747 / Avg. batch time: 585.8412 (ms) / GPU bubble ratio: 54.54%
[rank3]:2025-11-09 18:16:16,728 - INFO -  step: 800  loss:  0.4292  grad_norm:  0.4035  memory: 26.98GiB(56.79%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank3]:2025-11-09 18:16:16,728 - INFO -  final step: 800  loss:  0.4292  grad_norm:  0.4035  tps: 6,715  tflops: 51.14  mfu: 14.96%
[rank3]:2025-11-09 18:16:16,729 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank3]:2025-11-09 18:16:16,730 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 18:16:16,730 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4035  memory: 16.57GiB(34.88%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank0]:2025-11-09 18:16:16,730 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4035  tps: 6,709  tflops: 51.10  mfu: 14.85%
[rank0]:2025-11-09 18:16:16,730 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank0]:2025-11-09 18:16:16,730 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank2]:2025-11-09 18:16:16,715 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4035  memory: 11.81GiB(24.85%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank2]:2025-11-09 18:16:16,715 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4035  tps: 6,708  tflops: 51.09  mfu: 14.84%
[rank2]:2025-11-09 18:16:16,715 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank2]:2025-11-09 18:16:16,715 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank1]:2025-11-09 18:16:16,718 - INFO -  step: 800  loss: -4.0000  grad_norm:  0.4035  memory: 14.64GiB(30.82%)  tps: 6,332  tflops: 48.22  mfu: 15.46%
[rank1]:2025-11-09 18:16:16,718 - INFO -  final step: 800  loss: -4.0000  grad_norm:  0.4035  tps: 6,708  tflops: 51.09  mfu: 14.84%
[rank1]:2025-11-09 18:16:16,719 - INFO - Saving the checkpoint (or staging if async is enabled).
[rank1]:2025-11-09 18:16:16,719 - INFO - Saving a model only checkpoint in torch.float16 at last step, step 800.
[rank0]:2025-11-09 18:16:18,648 - INFO - [GC] GC collection invoked by checkpointer. 0.00 seconds
[rank0]:2025-11-09 18:16:18,665 - INFO - Sleeping 2 seconds for other ranks to complete
[rank2]:2025-11-09 18:16:18,664 - INFO - Destroying the purge thread.
[rank1]:2025-11-09 18:16:18,664 - INFO - Destroying the purge thread.
[rank3]:2025-11-09 18:16:18,807 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1816_real_final800_rank3.svg
[rank3]:> Batch Time: 621.59 ms, GPU Bubble Ratio: 59.32%, 57.05%, 66.24%, 26.61%
[rank3]:2025-11-09 18:16:18,942 - INFO - Pipeline schedule is saved as: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1/pipeline_schedule/251109_1816_thry_final800_rank3.svg
[rank3]:> Batch Time: 293.62 ms, GPU Bubble Ratio: 27.27%, 27.27%, 27.27%, 27.27%
[rank3]:2025-11-09 18:16:18,943 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank2]:2025-11-09 18:16:19,119 - INFO - Process group destroyed
[rank1]:2025-11-09 18:16:19,090 - INFO - Process group destroyed
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:      final/avg_end_to_end(s) â–
[rank3]:wandb:               final/avg_loss â–
[rank3]:wandb:             final/avg_mfu(%) â–
[rank3]:wandb:             final/avg_tflops â–
[rank3]:wandb:    final/avg_throughput(tps) â–
[rank3]:wandb:              final/grad_norm â–
[rank3]:wandb:               final/max_loss â–
[rank3]:wandb:                    grad_norm â–ˆâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_avg_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb: loss_metrics/global_max_loss â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:      final/avg_end_to_end(s) 2.43982
[rank3]:wandb:               final/avg_loss 0.42919
[rank3]:wandb:             final/avg_mfu(%) 14.95914
[rank3]:wandb:             final/avg_tflops 51.1448
[rank3]:wandb:    final/avg_throughput(tps) 6715.25362
[rank3]:wandb:              final/grad_norm 0.40348
[rank3]:wandb:               final/max_loss 0.42919
[rank3]:wandb:                    grad_norm 0.40348
[rank3]:wandb: loss_metrics/global_avg_loss 0.42919
[rank3]:wandb: loss_metrics/global_max_loss 0.42919
[rank3]:wandb:                          +14 ...
[rank3]:wandb: 
[rank3]:wandb: ðŸš€ View run 1108_GPipe_nofreeze_seed55_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/6ji2yl5r
[rank3]:wandb: â­ï¸ View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed55_dm1/20251109-1741/wandb/run-20251109_174141-6ji2yl5r/logs
[rank3]:2025-11-09 18:16:20,103 - INFO - Process group destroyed
[rank0]:2025-11-09 18:16:20,665 - INFO - Training completed
[rank0]:2025-11-09 18:16:20,665 - INFO - Destroying the purge thread.
[rank0]:2025-11-09 18:16:21,155 - INFO - Process group destroyed
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.9', 'layers.11', 'layers.12'}
[rank1]:Stage 1: Modules to keep: {'layers.6', 'layers.5', 'layers.7', 'layers.4', 'layers.8'}
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.2', 'layers.0', 'tok_embeddings', 'layers.1'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.15', 'norm', 'output', 'layers.14'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreezeâ° Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml
[rank3]:		- dump_folder: /data2/shcho/torchtitan
[rank3]:		- description: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1108_GPipe_nofreeze_seed55_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/profile_trace/1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /data2/shcho/torchtitan/memory_snapshot/1108_GPipe_nofreeze_seed55_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 50
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /data2/shcho/torchtitan/images/1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- pplog_freq: 200
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 1B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.2-1B
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 5e-06
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca_cleaned
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- global_batch_size: 128
[rank3]:		- seq_len: 512
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 800
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: 55
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 4
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- interval: 1200
[rank3]:		- initial_load_path: /data2/shcho/torchtitan/base_model/Llama-3.2-1B/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /data2/shcho/torchtitan/comm_traces/1108_GPipe_nofreeze_seed55_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 32
[rank3]:		- seq_len: 512
[rank3]:		- freq: 200
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 50
[rank3]:		- stability_check_freq: 10
[rank3]:		- max_freeze_ratio: 0.7
[rank3]:		- adjustment: False
[rank3]:		- threshold: 0.05
[rank3]:		- percentile: 70
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: 2025. 11. 09. (ì¼) 21:31:14 KST
âœ”ï¸SERVER: dmserver1 (143.248.135.95),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/run.sh
âœ”ï¸OUTPUT: /home/shcho/torchtitan/logs/dmserver1/1104_llama1b/1108_GPipe_nofreeze_seed55.log
âœ”ï¸Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6

âœ”ï¸Running with nofreeze x GPipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1104_llama1b/config_1108.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
[rank3]:2025-11-09 21:31:20,864 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank3]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank3]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank3]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank3]:"
[rank2]:2025-11-09 21:31:20,864 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank2]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank2]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank2]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank2]:"
[rank0]:2025-11-09 21:31:20,924 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank0]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank0]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank0]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank0]:"
[rank1]:2025-11-09 21:31:20,970 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, with bf16 autocast
[rank1]:1107: for timelyapf: clone().cpu() removed to fix freezing issue.!!! yay~~~
[rank1]:1108: steps 1200 (3 epochs) -> 800 (2 epochs) / alpaca dataset
[rank1]:    - learning rate : tried lr=3e-6 (->3e-9), 5e-6(->0), 1e-5(->0). BEST: lr=5e-6
[rank1]:"
[rank3]:2025-11-09 21:31:21,131 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-09 21:31:21,133 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-09 21:31:21,115 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-09 21:31:21,118 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 21:31:21,232 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-09 21:31:21,235 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 21:31:21,239 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-11-09 21:31:21,240 - INFO - Loading tokenizer from tokenizer.json
[rank1]:2025-11-09 21:31:21,257 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-09 21:31:21,260 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-09 21:31:21,633 - INFO - Preparing alpaca_cleaned dataset from yahma/alpaca-cleaned
[rank0]:2025-11-09 21:31:24,684 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-11-09 21:31:24,829 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 21:31:24,867 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 21:31:24,868 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank2]:2025-11-09 21:31:24,891 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-09 21:31:24,894 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-09 21:31:24,895 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 21:31:24,929 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 21:31:24,954 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-09 21:31:24,955 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-09 21:31:25,089 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-09 21:31:25,089 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-09 21:31:25,089 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank1]:2025-11-09 21:31:24,995 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-09 21:31:25,035 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 21:31:25,062 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-09 21:31:25,062 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-11-09 21:31:25,141 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-09 21:31:25,141 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-09 21:31:25,142 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-09 21:31:25,252 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-09 21:31:25,252 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-09 21:31:25,252 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run wytxm2qo
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1108_GPipe_nofreeze_seed55_dm1/20251109-2131/wandb/run-20251109_213125-wytxm2qo
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1108_GPipe_nofreeze_seed55_dm1
[rank3]:wandb: â­ï¸ View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: ðŸš€ View run at https://wandb.ai/orangingq/torchtitan/runs/wytxm2qo
[rank3]:2025-11-09 21:31:26,446 - INFO - WandB logging enabled
[rank3]:2025-11-09 21:31:26,447 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-09 21:31:26,485 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 21:31:26,514 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-09 21:31:26,514 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-11-09 21:31:26,696 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-09 21:31:26,697 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-09 21:31:26,698 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-09 21:31:26,715 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-09 21:31:26,716 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1.
[rank0]:2025-11-09 21:31:26,714 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1
[rank0]:2025-11-09 21:31:26,715 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-09 21:31:26,715 - INFO - Trainer is initialized with local batch size 32, global batch size 128, gradient accumulation steps 4, sequence length 512, total steps 800 (warmup 100)
[rank0]:2025-11-09 21:31:26,715 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1.
[rank0]:2025-11-09 21:31:26,716 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1/step-800.
[rank1]:2025-11-09 21:31:26,714 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-09 21:31:26,715 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1.
[rank2]:2025-11-09 21:31:26,714 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank2]:2025-11-09 21:31:26,714 - WARNING - checkpoint.initial_load_path is provided but the checkpoint.folder exists. Checkpointer will use the checkpoints from the checkpoint.folder /data2/shcho/torchtitan/checkpoint/1108_GPipe_nofreeze_seed55_dm1.
[rank0]:2025-11-09 21:31:29,289 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-09 21:31:29,289 - INFO - Finished loading the checkpoint in 2.57 seconds.
[rank0]:2025-11-09 21:31:29,289 - INFO - Training starts at step 1
[rank3]:2025-11-09 21:31:32,384 - INFO -  step:  1  loss:  0.4208  grad_norm:  0.3536  memory: 24.19GiB(50.91%)  tps: 2,779  tflops: 21.17  mfu: 6.78%
[rank3]:2025-11-09 21:31:32,384 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-09 21:31:32,377 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3536  memory: 12.38GiB(26.05%)  tps: 2,232  tflops: 17.00  mfu: 5.45%
[rank1]:2025-11-09 21:31:32,377 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-11-09 21:31:32,374 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3536  memory:  9.99GiB(21.03%)  tps: 2,201  tflops: 16.76  mfu: 5.37%
[rank2]:2025-11-09 21:31:32,374 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-09 21:31:32,413 - INFO -  step:  1  loss: -4.0000  grad_norm:  0.3536  memory: 12.80GiB(26.95%)  tps: 2,171  tflops: 16.54  mfu: 5.30%
[rank0]:2025-11-09 21:31:32,414 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
