
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 11. 03. (Ïõî) 18:54:52 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1029_llama1b/run_1031.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1029_llama1b/1031_GPipe_nofreeze.log
‚úîÔ∏èMain Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
+ more freezing in autofreeze mode. 
+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.

‚úîÔ∏èRunning with nofreeze x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1029_llama1b/config_1031.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
+ more freezing in autofreeze mode. 
+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
[rank3]:2025-11-03 18:54:58,398 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank3]:+ more freezing in autofreeze mode. 
[rank3]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank3]:"
[rank1]:2025-11-03 18:54:58,399 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank1]:+ more freezing in autofreeze mode. 
[rank1]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank1]:"
[rank1]:2025-11-03 18:54:58,588 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-03 18:54:58,604 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-11-03 18:54:58,606 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-03 18:54:58,602 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank0]:+ more freezing in autofreeze mode. 
[rank0]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank0]:"
[rank1]:2025-11-03 18:54:58,590 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-11-03 18:54:58,601 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank2]:+ more freezing in autofreeze mode. 
[rank2]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank2]:"
[rank0]:2025-11-03 18:54:58,807 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-03 18:54:58,809 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-03 18:54:58,814 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-11-03 18:54:58,795 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-03 18:54:58,797 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-03 18:54:59,260 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-03 18:54:59,708 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-11-03 18:55:02,930 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-03 18:55:02,966 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-03 18:55:02,994 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-11-03 18:55:02,994 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-03 18:55:03,144 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-03 18:55:03,165 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-03 18:55:03,166 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-03 18:55:03,166 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank0]:2025-11-03 18:55:03,292 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-03 18:55:03,329 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-03 18:55:03,330 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-03 18:55:03,354 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-11-03 18:55:03,354 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-11-03 18:55:03,531 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-03 18:55:03,531 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-03 18:55:03,540 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run a9e8y6ic
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1031_GPipe_nofreeze_dm1/20251103-1855/wandb/run-20251103_185503-a9e8y6ic
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1031_GPipe_nofreeze_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/a9e8y6ic
[rank1]:2025-11-03 18:55:04,640 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-03 18:55:04,677 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-03 18:55:04,713 - INFO - WandB logging enabled
[rank3]:2025-11-03 18:55:04,714 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-03 18:55:04,756 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-03 18:55:04,789 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'norm', 'output']
[rank3]:2025-11-03 18:55:04,790 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-03 18:55:04,704 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-11-03 18:55:04,704 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-11-03 18:55:04,878 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-03 18:55:04,878 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-03 18:55:04,879 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank3]:2025-11-03 18:55:04,988 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-03 18:55:04,989 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-03 18:55:04,990 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-03 18:55:05,006 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-03 18:55:05,006 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1031_GPipe_nofreeze_dm1
[rank0]:2025-11-03 18:55:05,006 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-03 18:55:05,006 - INFO - Preparing c4_validation dataset from allenai/c4
[rank2]:2025-11-03 18:55:05,006 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank1]:2025-11-03 18:55:05,006 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-03 18:55:12,188 - INFO - Trainer is initialized with local batch size 32, global batch size 64, gradient accumulation steps 2, sequence length 512, total steps 2400 (warmup 300)
[rank0]:2025-11-03 18:55:12,188 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B-Instruct/original_dcp.
[rank0]:2025-11-03 18:55:17,368 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-11-03 18:55:17,368 - INFO - Finished loading the checkpoint in 5.18 seconds.
[rank0]:2025-11-03 18:55:17,376 - INFO - Training starts at step 1
