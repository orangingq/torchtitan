
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 11. 04. (Ìôî) 01:38:16 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1029_llama1b/run_1031.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1029_llama1b/1031_Interleaved1F1B_timelyapf.log
‚úîÔ∏èMain Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
+ more freezing in autofreeze mode. 
+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.

‚úîÔ∏èRunning with timelyapf x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1029_llama1b/config_1031.toml --job.description="Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
+ more freezing in autofreeze mode. 
+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyapf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
[rank0]:2025-11-04 01:38:22,680 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank0]:+ more freezing in autofreeze mode. 
[rank0]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank0]:"
[rank1]:2025-11-04 01:38:22,680 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank1]:+ more freezing in autofreeze mode. 
[rank1]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank1]:"
[rank3]:2025-11-04 01:38:22,858 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank3]:+ more freezing in autofreeze mode. 
[rank3]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank3]:"
[rank2]:2025-11-04 01:38:22,799 - INFO - Starting job: "Main Table Experiment, without streaming mode, sample-level with truncation, 2 epochs, with bf16 autocast
[rank2]:+ more freezing in autofreeze mode. 
[rank2]:+ lr_scheduler min_lr = 0 -> 1e-6, cosine decay.
[rank2]:"
[rank0]:2025-11-04 01:38:22,896 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-11-04 01:38:22,899 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-04 01:38:22,903 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-11-04 01:38:22,906 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-11-04 01:38:22,908 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-04 01:38:23,072 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-04 01:38:23,032 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-11-04 01:38:23,035 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-11-04 01:38:23,075 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-11-04 01:38:23,376 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-11-04 01:38:23,752 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank1]:2025-11-04 01:38:26,702 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank1]:2025-11-04 01:38:26,743 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-04 01:38:26,770 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
[rank2]:2025-11-04 01:38:26,710 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank2]:2025-11-04 01:38:26,750 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-04 01:38:26,779 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
[rank0]:2025-11-04 01:38:26,841 - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=256, ffn_dim_multiplier=1.5, norm_eps=1e-05, rope_theta=500000, max_seq_len=512, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-11-04 01:38:26,793 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
[rank2]:2025-11-04 01:38:26,794 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-11-04 01:38:26,783 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
[rank1]:2025-11-04 01:38:26,784 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-11-04 01:38:26,968 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank2]:2025-11-04 01:38:26,968 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank2]:2025-11-04 01:38:26,969 - INFO - CUDA memory usage for model: 0.92GiB(1.94%)
[rank1]:2025-11-04 01:38:26,968 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank1]:2025-11-04 01:38:26,968 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank1]:2025-11-04 01:38:26,969 - INFO - CUDA memory usage for model: 1.13GiB(2.39%)
[rank0]:2025-11-04 01:38:26,993 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:2025-11-04 01:38:27,032 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-04 01:38:27,033 - INFO - Model llama3 1B size: 1,498,482,688 total parameters
[rank0]:2025-11-04 01:38:27,059 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:2025-11-04 01:38:27,072 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:2025-11-04 01:38:27,073 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-11-04 01:38:27,241 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:2025-11-04 01:38:27,241 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:2025-11-04 01:38:27,241 - INFO - CUDA memory usage for model: 1.90GiB(3.99%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 9dxw4ra0
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1031_Interleaved1F1B_timelyapf_dm1/20251104-0138/wandb/run-20251104_013827-9dxw4ra0
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1031_Interleaved1F1B_timelyapf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/9dxw4ra0
[rank3]:2025-11-04 01:38:28,897 - INFO - WandB logging enabled
[rank3]:2025-11-04 01:38:28,898 - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:2025-11-04 01:38:28,937 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-04 01:38:28,965 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:2025-11-04 01:38:28,978 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:2025-11-04 01:38:28,979 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-11-04 01:38:29,176 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1031_Interleaved1F1B_timelyapf_dm1
[rank2]:2025-11-04 01:38:29,178 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-04 01:38:29,176 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-04 01:38:29,176 - INFO - Preparing c4_validation dataset from allenai/c4
[rank1]:2025-11-04 01:38:29,176 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank3]:2025-11-04 01:38:29,157 - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:2025-11-04 01:38:29,158 - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:2025-11-04 01:38:29,159 - INFO - CUDA memory usage for model: 1.66GiB(3.50%)
[rank3]:2025-11-04 01:38:29,175 - INFO - Mixed precision training with TP or PP is handled by autocast
[rank0]:2025-11-04 01:38:34,649 - INFO - Trainer is initialized with local batch size 32, global batch size 64, gradient accumulation steps 2, sequence length 512, total steps 2400 (warmup 300)
[rank0]:2025-11-04 01:38:34,649 - INFO - Loading the checkpoint from /data2/shcho/torchtitan/base_model/Llama-3.2-1B-Instruct/original_dcp.
[rank0]:2025-11-04 01:38:37,561 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-11-04 01:38:37,561 - INFO - Finished loading the checkpoint in 2.91 seconds.
[rank0]:2025-11-04 01:38:37,561 - INFO - Training starts at step 1
[rank2]:2025-11-04 01:38:39,475 - INFO -  step:  1  loss: -2.0000  grad_norm: 85.2027  memory:  6.24GiB(13.13%)  tps: 644  tflops: 4.90  mfu: 1.57%
[rank2]:2025-11-04 01:38:39,476 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-11-04 01:38:39,483 - INFO -  step:  1  loss:  9.0164  grad_norm: 85.2027  memory: 14.83GiB(31.21%)  tps: 777  tflops: 5.92  mfu: 1.90%
[rank3]:2025-11-04 01:38:39,484 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-11-04 01:38:39,484 - INFO -  step:  1  loss: -2.0000  grad_norm: 85.2027  memory:  9.23GiB(19.43%)  tps: 643  tflops: 4.90  mfu: 1.57%
[rank1]:2025-11-04 01:38:39,484 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-04 01:38:39,506 - INFO -  step:  1  loss: -2.0000  grad_norm: 85.2027  memory: 10.87GiB(22.87%)  tps: 657  tflops: 5.00  mfu: 1.60%
[rank0]:2025-11-04 01:38:39,506 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-11-04 01:39:42,441 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-04 01:39:43,770 - INFO -  step: 50  loss: -2.0000  grad_norm: 49.1223  memory:  8.13GiB(17.12%)  tps: 6,243  tflops: 47.55  mfu: 15.24%
[rank1]:2025-11-04 01:39:43,773 - INFO -  step: 50  loss: -2.0000  grad_norm: 49.1223  memory: 11.85GiB(24.93%)  tps: 6,244  tflops: 47.55  mfu: 15.24%
[rank0]:2025-11-04 01:39:43,784 - INFO -  step: 50  loss: -2.0000  grad_norm: 49.1223  memory: 14.71GiB(30.97%)  tps: 6,245  tflops: 47.56  mfu: 15.24%
[rank3]:2025-11-04 01:39:43,782 - INFO -  step: 50  loss:  9.7996  grad_norm: 49.1223  memory: 18.19GiB(38.28%)  tps: 6,243  tflops: 47.55  mfu: 15.24%
[rank0]:2025-11-04 01:40:48,216 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-04 01:40:49,219 - INFO - Avg. fwd time: 6.3289 / Avg. bwd time: 20.9826 / Avg. batch time: 544.3070 (ms) / GPU bubble ratio: 19.72%
[rank2]:2025-11-04 01:40:49,274 - INFO - Avg. fwd time: 3.8193 / Avg. bwd time: 9.6786 / Avg. batch time: 577.7436 (ms) / GPU bubble ratio: 62.62%
[rank0]:2025-11-04 01:40:49,294 - INFO - Avg. fwd time: 4.1432 / Avg. bwd time: 12.0889 / Avg. batch time: 629.6843 (ms) / GPU bubble ratio: 58.76%
[rank1]:2025-11-04 01:40:49,371 - INFO - Avg. fwd time: 4.7608 / Avg. bwd time: 12.0202 / Avg. batch time: 606.6321 (ms) / GPU bubble ratio: 55.74%
[rank2]:2025-11-04 01:40:49,532 - INFO -  step: 100  loss: -2.0000  grad_norm: 31.6704  memory:  8.13GiB(17.12%)  tps: 6,229  tflops: 47.44  mfu: 15.20%
[rank2]:2025-11-04 01:40:49,532 - WARNING - Nothing to draw before the end of warmup.
[rank1]:2025-11-04 01:40:49,536 - INFO -  step: 100  loss: -2.0000  grad_norm: 31.6704  memory: 11.85GiB(24.93%)  tps: 6,229  tflops: 47.44  mfu: 15.20%
[rank1]:2025-11-04 01:40:49,536 - WARNING - Nothing to draw before the end of warmup.
[rank0]:2025-11-04 01:40:49,547 - INFO -  step: 100  loss: -2.0000  grad_norm: 31.6704  memory: 14.71GiB(30.97%)  tps: 6,228  tflops: 47.44  mfu: 15.20%
[rank0]:2025-11-04 01:40:49,547 - WARNING - Nothing to draw before the end of warmup.
[rank3]:2025-11-04 01:40:49,544 - INFO -  step: 100  loss:  8.5679  grad_norm: 31.6704  memory: 18.19GiB(38.28%)  tps: 6,229  tflops: 47.44  mfu: 15.20%
[rank3]:2025-11-04 01:40:49,544 - WARNING - Nothing to draw before the end of warmup.
[rank0]:2025-11-04 01:41:54,053 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-11-04 01:41:55,401 - INFO -  step: 150  loss: -2.0000  grad_norm: 47.7300  memory:  8.13GiB(17.12%)  tps: 6,218  tflops: 47.36  mfu: 15.18%
[rank1]:2025-11-04 01:41:55,404 - INFO -  step: 150  loss: -2.0000  grad_norm: 47.7300  memory: 11.85GiB(24.93%)  tps: 6,218  tflops: 47.36  mfu: 15.18%
[rank3]:2025-11-04 01:41:55,412 - INFO -  step: 150  loss:  5.0414  grad_norm: 47.7300  memory: 18.19GiB(38.28%)  tps: 6,219  tflops: 47.36  mfu: 15.18%
[rank0]:2025-11-04 01:41:55,415 - INFO -  step: 150  loss: -2.0000  grad_norm: 47.7300  memory: 14.71GiB(30.97%)  tps: 6,219  tflops: 47.36  mfu: 15.18%
[rank0]:2025-11-04 01:42:59,923 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-11-04 01:43:00,941 - INFO - Avg. fwd time: 6.3349 / Avg. bwd time: 21.0487 / Avg. batch time: 545.4051 (ms) / GPU bubble ratio: 19.67%
[rank2]:2025-11-04 01:43:00,997 - INFO - Avg. fwd time: 3.8190 / Avg. bwd time: 9.7073 / Avg. batch time: 578.7706 (ms) / GPU bubble ratio: 62.61%
[rank0]:2025-11-04 01:43:01,017 - INFO - Avg. fwd time: 4.1426 / Avg. bwd time: 12.1221 / Avg. batch time: 630.8334 (ms) / GPU bubble ratio: 58.75%
[rank1]:2025-11-04 01:43:01,095 - INFO - Avg. fwd time: 4.7669 / Avg. bwd time: 12.0613 / Avg. batch time: 607.7370 (ms) / GPU bubble ratio: 55.70%
[rank1]:2025-11-04 01:43:01,260 - INFO -  step: 200  loss: -2.0000  grad_norm: 38.9563  memory: 11.85GiB(24.93%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank1]:2025-11-04 01:43:01,260 - INFO - üîé  Running validation at step 200
[rank2]:2025-11-04 01:43:01,257 - INFO -  step: 200  loss: -2.0000  grad_norm: 38.9563  memory:  8.13GiB(17.12%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank2]:2025-11-04 01:43:01,257 - INFO - üîé  Running validation at step 200
[rank3]:2025-11-04 01:43:01,269 - INFO -  step: 200  loss:  1.7715  grad_norm: 38.9563  memory: 18.19GiB(38.28%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank3]:2025-11-04 01:43:01,269 - INFO - üîé  Running validation at step 200
[rank0]:2025-11-04 01:43:01,271 - INFO -  step: 200  loss: -2.0000  grad_norm: 38.9563  memory: 14.71GiB(30.97%)  tps: 6,220  tflops: 47.37  mfu: 15.18%
[rank0]:2025-11-04 01:43:01,271 - INFO - üîé  Running validation at step 200
[rank0]:2025-11-04 01:43:28,390 - INFO - validate step: 200  loss: -1.0000  memory: 14.71GiB(30.97%)  tps: 15,104
[rank0]:2025-11-04 01:43:28,391 - WARNING - Nothing to draw before the end of warmup.
[rank1]:2025-11-04 01:43:28,396 - INFO - validate step: 200  loss: -1.0000  memory: 11.85GiB(24.93%)  tps: 15,095
[rank1]:2025-11-04 01:43:28,397 - WARNING - Nothing to draw before the end of warmup.
[rank2]:2025-11-04 01:43:28,409 - INFO - validate step: 200  loss: -1.0000  memory:  8.13GiB(17.12%)  tps: 15,085
[rank2]:2025-11-04 01:43:28,410 - WARNING - Nothing to draw before the end of warmup.
[rank3]:2025-11-04 01:43:28,411 - INFO - validate step: 200  loss:  1.5306  memory: 18.19GiB(38.28%)  tps: 15,092
[rank3]:2025-11-04 01:43:28,412 - WARNING - Nothing to draw before the end of warmup.
[rank0]:2025-11-04 01:44:32,820 - INFO - [GC] Peforming periodical GC collection 0.01 seconds
[rank1]:2025-11-04 01:44:34,138 - INFO -  step: 250  loss: -2.0000  grad_norm: 53.6222  memory: 11.85GiB(24.93%)  tps: 6,230  tflops: 47.45  mfu: 15.21%
[rank2]:2025-11-04 01:44:34,135 - INFO -  step: 250  loss: -2.0000  grad_norm: 53.6222  memory:  8.13GiB(17.12%)  tps: 6,232  tflops: 47.46  mfu: 15.21%
[rank3]:2025-11-04 01:44:34,147 - INFO -  step: 250  loss:  0.6771  grad_norm: 53.6222  memory: 18.19GiB(38.28%)  tps: 6,231  tflops: 47.46  mfu: 15.21%
[rank0]:2025-11-04 01:44:34,149 - INFO -  step: 250  loss: -2.0000  grad_norm: 53.6222  memory: 14.71GiB(30.97%)  tps: 6,229  tflops: 47.44  mfu: 15.21%
