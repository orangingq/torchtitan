
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 10. 21. (Ìôî) 03:10:20 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1020_main/1020_interleaved1f1b_nofreeze.log
‚úîÔ∏èMain Table Experiment
‚úîÔ∏èRunning with nofreeze x interleaved1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1020_main/config.toml --job.description="Main Table Experiment" --training.global_batch_size=512 --training.local_batch_size=16 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
Traceback (most recent call last):
  File "/data2/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    from torch.distributed.run import main
  File "/data2/shcho/miniforge3/envs/llm/lib/python3.11/site-packages/torch/__init__.py", line 405, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: /data2/shcho/miniforge3/envs/llm/lib/python3.11/site-packages/torch/../../../././libcusparse.so.12: undefined symbol: __nvJitLinkGetErrorLogSize_12_9, version libnvJitLink.so.12

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 10. 21. (Ìôî) 03:12:36 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1020_main/run.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1020_main/1020_interleaved1f1b_nofreeze.log
‚úîÔ∏èMain Table Experiment
‚úîÔ∏èRunning with nofreeze x interleaved1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1020_main/config.toml --job.description="Main Table Experiment" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --metrics.log_freq=50 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
Traceback (most recent call last):
  File "/data2/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    from torch.distributed.run import main
  File "/data2/shcho/miniforge3/envs/llm/lib/python3.11/site-packages/torch/__init__.py", line 405, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: /data2/shcho/miniforge3/envs/llm/lib/python3.11/site-packages/torch/../../../././libcusparse.so.12: undefined symbol: __nvJitLinkGetErrorLogSize_12_9, version libnvJitLink.so.12

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: 2025. 10. 21. (Ìôî) 14:56:43 KST
‚úîÔ∏èSERVER: dmserver1 (143.248.135.95),  GPUs: 3,4,5,6
‚úîÔ∏èSCRIPT: /home/shcho/torchtitan/logs/dmserver1/1020_main/run.sh
‚úîÔ∏èOUTPUT: /home/shcho/torchtitan/logs/dmserver1/1020_main/1020_interleaved1f1b_nofreeze.log
‚úîÔ∏èMain Table Experiment
‚úîÔ∏èRunning with nofreeze x interleaved1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/home/shcho/torchtitan/logs/dmserver1/1020_main/config.toml --job.description="Main Table Experiment" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --metrics.log_freq=50 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
2025-10-21 14:56:49,310 - Starting job: "Main Table Experiment"
2025-10-21 14:56:49,316 - Starting job: "Main Table Experiment"
2025-10-21 14:56:49,316 - Starting job: "Main Table Experiment"
[rank0]:[titan] 2025-10-21 14:56:49,316 - root - INFO - Starting job: "Main Table Experiment"
2025-10-21 14:56:49,421 - Starting job: "Main Table Experiment"
[rank3]:[titan] 2025-10-21 14:56:49,421 - root - INFO - Starting job: "Main Table Experiment"
2025-10-21 14:56:49,932 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-21 14:56:49,934 - Building 1-D device mesh with ['pp'], [4]
2025-10-21 14:56:49,940 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-21 14:56:49,943 - Building 1-D device mesh with ['pp'], [4]
2025-10-21 14:56:49,954 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-21 14:56:49,957 - Building 1-D device mesh with ['pp'], [4]
2025-10-21 14:56:49,960 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-21 14:56:49,964 - Building 1-D device mesh with ['pp'], [4]
2025-10-21 14:56:49,968 - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-10-21 14:56:49,960 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-10-21 14:56:49,964 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-10-21 14:56:49,968 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-10-21 14:56:49,932 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-10-21 14:56:49,934 - root - INFO - Building 1-D device mesh with ['pp'], [4]
2025-10-21 14:56:50,421 - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-10-21 14:56:50,421 - root - INFO - Loading tokenizer from tokenizer.json
2025-10-21 14:56:50,864 - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-10-21 14:56:50,864 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-10-21 14:56:53,787 - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-10-21 14:56:53,787 - root - INFO - Building llama3 1B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=2048, n_layers=16, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-10-21 14:56:53,938 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-21 14:56:53,967 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-21 14:56:53,986 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:53,987 - [34mModel llama3 1B [31msize: 1,397,819,392 total parameters[39m
[rank0]:[titan] 2025-10-21 14:56:53,938 - root - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank0]:[titan] 2025-10-21 14:56:53,986 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:[titan] 2025-10-21 14:56:53,987 - root - INFO - [34mModel llama3 1B [31msize: 1,397,819,392 total parameters[39m
2025-10-21 14:56:54,007 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:54,011 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
2025-10-21 14:56:54,024 - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
2025-10-21 14:56:54,025 - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
2025-10-21 14:56:54,036 - PP rank 2 is building stage_idx 2 with modules ['layers.5', 'layers.6']
2025-10-21 14:56:54,049 - PP rank 2 is building stage_idx 6 with modules ['layers.13', 'layers.14']
2025-10-21 14:56:54,050 - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
[rank0]:[titan] 2025-10-21 14:56:54,011 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1']
[rank0]:[titan] 2025-10-21 14:56:54,024 - root - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.9', 'layers.10']
[rank0]:[titan] 2025-10-21 14:56:54,025 - root - INFO - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
2025-10-21 14:56:54,203 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:54,203 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-21 14:56:54,204 - CUDA memory usage for model: 1.80GiB(3.79%)
2025-10-21 14:56:54,205 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-10-21 14:56:54,251 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:54,251 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-21 14:56:54,252 - CUDA memory usage for model: 0.82GiB(1.73%)
2025-10-21 14:56:54,252 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank0]:[titan] 2025-10-21 14:56:54,203 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank0]:[titan] 2025-10-21 14:56:54,203 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-10-21 14:56:54,204 - root - INFO - CUDA memory usage for model: 1.80GiB(3.79%)
[rank0]:[titan] 2025-10-21 14:56:54,205 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
2025-10-21 14:56:54,923 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-21 14:56:54,961 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:54,988 - PP rank 1 is building stage_idx 1 with modules ['layers.2', 'layers.3', 'layers.4']
2025-10-21 14:56:55,001 - PP rank 1 is building stage_idx 5 with modules ['layers.11', 'layers.12']
2025-10-21 14:56:55,002 - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-21 14:56:55,197 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:55,197 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-21 14:56:55,197 - CUDA memory usage for model: 1.02GiB(2.14%)
2025-10-21 14:56:55,198 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: setting up run avdaqoae
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /data2/shcho/torchtitan/tb/1020_interleaved1f1b_nofreeze_dm1/20251021-1456/wandb/run-20251021_145654-avdaqoae
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1020_interleaved1f1b_nofreeze_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/avdaqoae
2025-10-21 14:56:56,070 - WandB logging enabled
2025-10-21 14:56:56,072 - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
[rank3]:[titan] 2025-10-21 14:56:56,070 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-10-21 14:56:56,072 - root - INFO - CUDA capacity: NVIDIA RTX 6000 Ada Generation with 47.51GiB memory
2025-10-21 14:56:56,109 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:56,137 - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
2025-10-21 14:56:56,150 - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
2025-10-21 14:56:56,152 - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
[rank3]:[titan] 2025-10-21 14:56:56,109 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:[titan] 2025-10-21 14:56:56,137 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.7', 'layers.8']
[rank3]:[titan] 2025-10-21 14:56:56,150 - root - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.15', 'norm', 'output']
[rank3]:[titan] 2025-10-21 14:56:56,152 - root - INFO - Using pipeline schedule interleaved1f1b with 8 microbatches and 8 stages.
2025-10-21 14:56:57,020 - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
2025-10-21 14:56:57,021 - Peak FLOPS used for computing MFU: 3.120e+14
2025-10-21 14:56:57,022 - CUDA memory usage for model: 1.60GiB(3.38%)
2025-10-21 14:56:57,024 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-10-21 14:56:57,039 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-21 14:56:57,039 - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1020_interleaved1f1b_nofreeze_dm1
2025-10-21 14:56:57,039 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-21 14:56:57,039 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-21 14:56:57,040 - Mixed precision training is disabled
2025-10-21 14:56:57,039 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-10-21 14:56:57,040 - Mixed precision training is disabled
2025-10-21 14:56:57,040 - Mixed precision training is disabled
2025-10-21 14:56:57,040 - Mixed precision training is disabled
2025-10-21 14:56:57,042 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-10-21 14:56:57,042 - Training starts at step 1
[rank0]:[titan] 2025-10-21 14:56:57,039 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /data2/shcho/torchtitan/checkpoint/1020_interleaved1f1b_nofreeze_dm1
[rank0]:[titan] 2025-10-21 14:56:57,039 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-10-21 14:56:57,040 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-10-21 14:56:57,042 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-10-21 14:56:57,042 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-10-21 14:56:57,020 - root - WARNING - Peak flops undefined for: NVIDIA RTX 6000 Ada Generation, fallback to A100
[rank3]:[titan] 2025-10-21 14:56:57,021 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-10-21 14:56:57,022 - root - INFO - CUDA memory usage for model: 1.60GiB(3.38%)
[rank3]:[titan] 2025-10-21 14:56:57,024 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-10-21 14:56:57,039 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-10-21 14:56:57,040 - root - INFO - Mixed precision training is disabled
[rank0]:/data2/shcho/miniforge3/envs/llm_eval/lib/python3.11/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-10-21 14:57:15,078 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory:  4.57GiB(9.63%) [34m tps: 1,555 [36m tflops: 11.22 [35m mfu: 3.60%[39m
2025-10-21 14:57:15,078 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-21 14:57:15,087 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory:  7.00GiB(14.73%) [34m tps: 1,628 [36m tflops: 11.74 [35m mfu: 3.76%[39m
2025-10-21 14:57:15,088 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-21 14:57:15,090 - [31m step:  1 [32m loss: 12.2559 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory: 13.30GiB(28.01%) [34m tps: 1,727 [36m tflops: 12.46 [35m mfu: 3.99%[39m
2025-10-21 14:57:15,091 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-21 14:57:15,115 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory:  8.77GiB(18.47%) [34m tps: 1,551 [36m tflops: 11.19 [35m mfu: 3.59%[39m
2025-10-21 14:57:15,115 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-10-21 14:57:15,115 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory:  8.77GiB(18.47%) [34m tps: 1,551 [36m tflops: 11.19 [35m mfu: 3.59%[39m
[rank0]:[titan] 2025-10-21 14:57:15,115 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-10-21 14:57:15,090 - root - INFO - [31m step:  1 [32m loss: 12.2559 [38;2;180;60;0m grad_norm:  0.1599 [38;2;54;234;195m memory: 13.30GiB(28.01%) [34m tps: 1,727 [36m tflops: 12.46 [35m mfu: 3.99%[39m
[rank3]:[titan] 2025-10-21 14:57:15,091 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-10-21 15:10:45,002 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 15:10:45,002 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 15:11:01,936 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
2025-10-21 15:11:01,940 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
2025-10-21 15:11:01,949 - [31m step: 50 [32m loss:  7.0076 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
2025-10-21 15:11:01,951 - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
[rank3]:[titan] 2025-10-21 15:11:01,949 - root - INFO - [31m step: 50 [32m loss:  7.0076 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
[rank0]:[titan] 2025-10-21 15:11:01,951 - root - INFO - [31m step: 50 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0997 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,942 [36m tflops: 14.01 [35m mfu: 4.49%[39m
2025-10-21 15:24:50,337 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 15:24:50,337 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 15:25:07,367 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
2025-10-21 15:25:07,370 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
2025-10-21 15:25:07,379 - [31m step: 100 [32m loss:  6.1230 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
2025-10-21 15:25:07,381 - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
[rank0]:[titan] 2025-10-21 15:25:07,381 - root - INFO - [31m step: 100 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
[rank3]:[titan] 2025-10-21 15:25:07,379 - root - INFO - [31m step: 100 [32m loss:  6.1230 [38;2;180;60;0m grad_norm:  0.1528 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,938 [36m tflops: 13.98 [35m mfu: 4.48%[39m
2025-10-21 15:38:56,603 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 15:38:56,603 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 15:39:13,618 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
2025-10-21 15:39:13,623 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
2025-10-21 15:39:13,632 - [31m step: 150 [32m loss:  5.7129 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
2025-10-21 15:39:13,632 - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
[rank0]:[titan] 2025-10-21 15:39:13,632 - root - INFO - [31m step: 150 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
[rank3]:[titan] 2025-10-21 15:39:13,632 - root - INFO - [31m step: 150 [32m loss:  5.7129 [38;2;180;60;0m grad_norm:  0.0755 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,936 [36m tflops: 13.97 [35m mfu: 4.48%[39m
2025-10-21 15:53:04,360 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 15:53:04,360 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 15:53:21,335 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
2025-10-21 15:53:21,342 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
2025-10-21 15:53:21,348 - [31m step: 200 [32m loss:  5.3707 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
2025-10-21 15:53:21,350 - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
[rank0]:[titan] 2025-10-21 15:53:21,350 - root - INFO - [31m step: 200 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
[rank3]:[titan] 2025-10-21 15:53:21,348 - root - INFO - [31m step: 200 [32m loss:  5.3707 [38;2;180;60;0m grad_norm:  0.0770 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,933 [36m tflops: 13.94 [35m mfu: 4.47%[39m
2025-10-21 16:07:14,971 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 16:07:14,971 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 16:07:31,913 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:07:31,917 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:07:31,926 - [31m step: 250 [32m loss:  5.2798 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:07:31,928 - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
[rank3]:[titan] 2025-10-21 16:07:31,926 - root - INFO - [31m step: 250 [32m loss:  5.2798 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
[rank0]:[titan] 2025-10-21 16:07:31,928 - root - INFO - [31m step: 250 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0616 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:21:22,107 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 16:21:22,107 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 16:21:38,969 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
2025-10-21 16:21:38,972 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
2025-10-21 16:21:38,981 - [31m step: 300 [32m loss:  5.0670 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
2025-10-21 16:21:38,983 - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
[rank0]:[titan] 2025-10-21 16:21:38,983 - root - INFO - [31m step: 300 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
[rank3]:[titan] 2025-10-21 16:21:38,981 - root - INFO - [31m step: 300 [32m loss:  5.0670 [38;2;180;60;0m grad_norm:  0.0613 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,934 [36m tflops: 13.95 [35m mfu: 4.47%[39m
2025-10-21 16:35:32,503 - [GC] Peforming periodical GC collection 0.01 seconds
[rank0]:[titan] 2025-10-21 16:35:32,503 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds
2025-10-21 16:35:49,589 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory:  6.23GiB(13.12%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:35:49,593 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory:  9.05GiB(19.05%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:35:49,602 - [31m step: 350 [32m loss:  4.9157 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
2025-10-21 16:35:49,604 - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
[rank3]:[titan] 2025-10-21 16:35:49,602 - root - INFO - [31m step: 350 [32m loss:  4.9157 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory: 16.72GiB(35.20%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
[rank0]:[titan] 2025-10-21 16:35:49,604 - root - INFO - [31m step: 350 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.0573 [38;2;54;234;195m memory: 12.33GiB(25.95%) [34m tps: 1,926 [36m tflops: 13.89 [35m mfu: 4.45%[39m
