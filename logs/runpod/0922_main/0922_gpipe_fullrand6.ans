
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Sep 22 06:47:04 UTC 2025
âœ”ï¸SERVER: 521c56b8c386 (172.19.0.2),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_fullrand6.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod
âœ”ï¸Running with fullrand6 x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] 
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] *****************************************
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] *****************************************
2025-09-22 06:47:11,247 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:47:11,448 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:47:11,461 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:47:11,463 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:47:11,468 - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:47:11,574 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:47:11,574 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:47:11,708 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:47:11,710 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:47:11,715 - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:47:11,822 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:47:11,827 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:47:11,832 - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:47:11,867 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:47:11,867 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:47:11,822 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:47:11,827 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:47:11,832 - root - INFO - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:47:12,083 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:47:12,087 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:47:12,092 - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:47:12,083 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:47:12,087 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:47:12,092 - root - INFO - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:47:12,385 - Loading tokenizer from tokenizer.json
2025-09-22 06:47:12,387 - Loading tokenizer from tokenizer.json
2025-09-22 06:47:12,387 - Loading tokenizer from tokenizer.json
2025-09-22 06:47:12,387 - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:47:12,385 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:47:12,387 - root - INFO - Loading tokenizer from tokenizer.json
2025-09-22 06:47:12,702 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:47:12,704 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:47:12,713 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:47:12,715 - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:47:12,702 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:47:12,704 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:47:14,619 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:47:14,619 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:47:14,853 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:47:14,873 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:47:14,877 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:47:14,900 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:47:14,877 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:47:14,900 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:47:14,853 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:47:14,921 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
2025-09-22 06:47:14,922 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:47:14,921 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:47:14,922 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:47:15,099 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:47:15,099 - CUDA memory usage for model: 8.46GiB(10.67%)
2025-09-22 06:47:15,100 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank0]:[titan] 2025-09-22 06:47:15,099 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:47:15,099 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:47:15,100 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:47:15,126 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:47:15,150 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:47:15,172 - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
2025-09-22 06:47:15,172 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:47:15,350 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:47:15,350 - CUDA memory usage for model: 7.33GiB(9.24%)
2025-09-22 06:47:15,351 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:47:15,467 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:47:15,722 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:47:15,748 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:47:15,770 - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
2025-09-22 06:47:15,770 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:47:15,792 - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
2025-09-22 06:47:15,794 - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250922-0647
2025-09-22 06:47:15,795 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:47:15,792 - root - ERROR - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank3]:[titan] 2025-09-22 06:47:15,794 - root - INFO - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250922-0647
[rank3]:[titan] 2025-09-22 06:47:15,795 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:47:15,816 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:47:15,838 - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
2025-09-22 06:47:15,838 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:47:15,816 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:47:15,838 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:47:15,838 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:47:15,937 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:47:15,937 - CUDA memory usage for model: 6.51GiB(8.21%)
2025-09-22 06:47:15,938 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:47:16,015 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:47:16,015 - CUDA memory usage for model: 7.66GiB(9.66%)
2025-09-22 06:47:16,016 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:47:16,024 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
2025-09-22 06:47:16,024 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:47:16,024 - Mixed precision training is disabled
2025-09-22 06:47:16,024 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
2025-09-22 06:47:16,024 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:47:16,024 - Mixed precision training is disabled
2025-09-22 06:47:16,024 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
2025-09-22 06:47:16,025 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:47:16,024 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
2025-09-22 06:47:16,025 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:47:16,025 - Mixed precision training is disabled
2025-09-22 06:47:16,025 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:47:16,025 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:47:16,025 - Mixed precision training is disabled
2025-09-22 06:47:16,025 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:47:16,025 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:47:16,025 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:47:16,025 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:47:16,025 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:47:16,025 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:47:16,015 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:47:16,015 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:47:16,016 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:47:24,401 - [GC] GC collection for checkpoint loading. 0.00 seconds
2025-09-22 06:47:24,401 - Finished loading the checkpoint in 8.38 seconds.
2025-09-22 06:47:24,401 - Training starts at step 1
2025-09-22 06:47:24,404 - [GC] GC collection for checkpoint loading. 0.01 seconds
2025-09-22 06:47:24,404 - Finished loading the checkpoint in 8.38 seconds.
2025-09-22 06:47:24,404 - Training starts at step 1
2025-09-22 06:47:24,404 - [GC] GC collection for checkpoint loading. 0.01 seconds
2025-09-22 06:47:24,405 - Finished loading the checkpoint in 8.38 seconds.
2025-09-22 06:47:24,405 - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:47:24,404 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:47:24,405 - root - INFO - Finished loading the checkpoint in 8.38 seconds.
[rank0]:[titan] 2025-09-22 06:47:24,405 - root - INFO - Training starts at step 1
2025-09-22 06:47:24,456 - [GC] GC collection for checkpoint loading. 0.06 seconds
2025-09-22 06:47:24,456 - Finished loading the checkpoint in 8.43 seconds.
2025-09-22 06:47:24,456 - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - [GC] GC collection for checkpoint loading. 0.06 seconds
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - Finished loading the checkpoint in 8.43 seconds.
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-09-22 06:49:36,474 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 35.64GiB(44.97%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
2025-09-22 06:49:36,474 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:49:36,480 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 40.01GiB(50.49%) [34m tps: 232 [36m tflops: 10.81 [35m mfu: 3.47%[39m
2025-09-22 06:49:36,480 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:49:36,483 - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
2025-09-22 06:49:36,484 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:49:36,504 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.79 [35m mfu: 3.46%[39m
2025-09-22 06:49:36,504 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 06:49:36,504 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.79 [35m mfu: 3.46%[39m
[rank0]:[titan] 2025-09-22 06:49:36,504 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 06:49:36,483 - root - INFO - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
[rank3]:[titan] 2025-09-22 06:49:36,484 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
W0922 06:53:06.949000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13738 closing signal SIGTERM
W0922 06:53:06.951000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13739 closing signal SIGTERM
W0922 06:53:06.952000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13740 closing signal SIGTERM
E0922 06:53:08.206000 13644 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: -9) local_rank: 3 (pid: 13741) of binary: /usr/bin/python
[rank0]:Stage 0: Modules to keep: {'layers.6', 'tok_embeddings', 'layers.2', 'layers.3', 'layers.7', 'layers.1', 'layers.0', 'layers.4', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.31', 'layers.28', 'layers.25', 'layers.29', 'layers.26', 'output', 'norm', 'layers.30', 'layers.27'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
timelyfreeze.train FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 13738)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13738
[2]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 13739)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13739
[3]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 13740)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13740
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:53:06
  host      : 521c56b8c386
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 13741)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13741
======================================================
[W922 06:53:08.692571155 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
