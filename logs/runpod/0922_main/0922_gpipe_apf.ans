
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Sep 22 06:53:08 UTC 2025
âœ”ï¸SERVER: 521c56b8c386 (172.19.0.2),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_apf.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod
âœ”ï¸Running with apf x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] 
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] *****************************************
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] *****************************************
2025-09-22 06:53:15,015 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:53:15,155 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:53:15,233 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:53:15,236 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:53:15,240 - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:53:15,155 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:53:15,305 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:53:15,305 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:53:15,414 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:53:15,417 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:53:15,421 - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:53:15,414 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:53:15,417 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:53:15,421 - root - INFO - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:53:15,562 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:53:15,567 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:53:15,572 - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:53:15,562 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:53:15,567 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:53:15,572 - root - INFO - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:53:15,652 - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
2025-09-22 06:53:15,883 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-09-22 06:53:15,888 - Building 1-D device mesh with ['pp'], [4]
2025-09-22 06:53:15,893 - [GC] Initial GC collection 0.00 seconds
2025-09-22 06:53:16,162 - Loading tokenizer from tokenizer.json
2025-09-22 06:53:16,164 - Loading tokenizer from tokenizer.json
2025-09-22 06:53:16,164 - Loading tokenizer from tokenizer.json
2025-09-22 06:53:16,164 - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:53:16,164 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:53:16,164 - root - INFO - Loading tokenizer from tokenizer.json
2025-09-22 06:53:16,455 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:53:16,484 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:53:16,486 - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:53:16,521 - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:53:16,521 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:53:16,484 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
2025-09-22 06:53:19,268 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:53:19,308 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:53:19,308 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:53:19,523 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:53:19,544 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:53:19,566 - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
2025-09-22 06:53:19,566 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:53:19,582 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:53:19,606 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:53:19,630 - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
2025-09-22 06:53:19,631 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:53:19,582 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:53:19,606 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:53:19,630 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:53:19,631 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:53:19,764 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:53:19,765 - CUDA memory usage for model: 7.33GiB(9.24%)
2025-09-22 06:53:19,766 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:53:19,833 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:[titan] 2025-09-22 06:53:19,833 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:53:19,876 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:53:19,876 - CUDA memory usage for model: 8.46GiB(10.67%)
2025-09-22 06:53:19,877 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank0]:[titan] 2025-09-22 06:53:19,876 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:53:19,876 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:53:19,877 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:53:20,135 - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
2025-09-22 06:53:20,399 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:53:20,424 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:53:20,448 - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
2025-09-22 06:53:20,448 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:53:20,664 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:53:20,664 - CUDA memory usage for model: 6.51GiB(8.21%)
2025-09-22 06:53:20,665 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:53:20,793 - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
2025-09-22 06:53:20,796 - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_apf_dm4/20250922-0653
2025-09-22 06:53:20,797 - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
2025-09-22 06:53:20,818 - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
2025-09-22 06:53:20,840 - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
2025-09-22 06:53:20,840 - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:53:20,793 - root - ERROR - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank3]:[titan] 2025-09-22 06:53:20,796 - root - INFO - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_apf_dm4/20250922-0653
[rank3]:[titan] 2025-09-22 06:53:20,797 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:53:20,818 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:53:20,840 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:53:20,840 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
2025-09-22 06:53:21,049 - Peak FLOPS used for computing MFU: 3.120e+14
2025-09-22 06:53:21,049 - CUDA memory usage for model: 7.66GiB(9.66%)
2025-09-22 06:53:21,050 - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:53:21,049 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:53:21,049 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:53:21,050 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
2025-09-22 06:53:21,059 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
2025-09-22 06:53:21,059 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:53:21,059 - Mixed precision training is disabled
2025-09-22 06:53:21,059 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
2025-09-22 06:53:21,059 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:53:21,059 - Mixed precision training is disabled
2025-09-22 06:53:21,059 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
2025-09-22 06:53:21,059 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:53:21,059 - Mixed precision training is disabled
2025-09-22 06:53:21,059 - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
2025-09-22 06:53:21,060 - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
2025-09-22 06:53:21,060 - Mixed precision training is disabled
2025-09-22 06:53:21,060 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:53:21,060 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:53:21,060 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:53:21,060 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:53:21,061 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:53:21,061 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:53:21,061 - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
2025-09-22 06:53:21,061 - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
2025-09-22 06:53:29,431 - [GC] GC collection for checkpoint loading. 0.00 seconds
2025-09-22 06:53:29,431 - [GC] GC collection for checkpoint loading. 0.00 seconds
2025-09-22 06:53:29,431 - Finished loading the checkpoint in 8.37 seconds.
2025-09-22 06:53:29,431 - Finished loading the checkpoint in 8.37 seconds.
2025-09-22 06:53:29,431 - Training starts at step 1
2025-09-22 06:53:29,431 - Training starts at step 1
2025-09-22 06:53:29,431 - Step [1]
2025-09-22 06:53:29,431 - Step [1]
2025-09-22 06:53:29,434 - [GC] GC collection for checkpoint loading. 0.01 seconds
2025-09-22 06:53:29,435 - Finished loading the checkpoint in 8.37 seconds.
2025-09-22 06:53:29,435 - Training starts at step 1
2025-09-22 06:53:29,435 - Step [1]
2025-09-22 06:53:29,459 - [GC] GC collection for checkpoint loading. 0.03 seconds
2025-09-22 06:53:29,460 - Finished loading the checkpoint in 8.40 seconds.
2025-09-22 06:53:29,460 - Training starts at step 1
2025-09-22 06:53:29,460 - Step [1]
[rank0]:[titan] 2025-09-22 06:53:29,434 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Finished loading the checkpoint in 8.37 seconds.
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Step [1]
[rank3]:[titan] 2025-09-22 06:53:29,459 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Finished loading the checkpoint in 8.40 seconds.
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Step [1]
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-09-22 06:55:41,547 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 35.64GiB(44.97%) [34m tps: 232 [36m tflops: 10.83 [35m mfu: 3.47%[39m
2025-09-22 06:55:41,547 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:55:41,553 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 40.01GiB(50.49%) [34m tps: 231 [36m tflops: 10.76 [35m mfu: 3.45%[39m
2025-09-22 06:55:41,554 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:55:41,557 - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
2025-09-22 06:55:41,558 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:55:41,578 - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.76 [35m mfu: 3.45%[39m
2025-09-22 06:55:41,578 - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
2025-09-22 06:55:41,579 - Step [2]
2025-09-22 06:55:41,579 - Step [2]
2025-09-22 06:55:41,579 - Step [2]
2025-09-22 06:55:41,579 - Step [2]
[rank3]:[titan] 2025-09-22 06:55:41,557 - root - INFO - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
[rank3]:[titan] 2025-09-22 06:55:41,558 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 06:55:41,579 - root - INFO - Step [2]
[rank0]:[titan] 2025-09-22 06:55:41,578 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.76 [35m mfu: 3.45%[39m
[rank0]:[titan] 2025-09-22 06:55:41,578 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 06:55:41,579 - root - INFO - Step [2]
2025-09-22 06:57:52,000 - Step [3]
[rank3]:[titan] 2025-09-22 06:57:52,000 - root - INFO - Step [3]
2025-09-22 06:57:53,239 - Step [3]
2025-09-22 06:57:53,748 - Step [3]
2025-09-22 06:57:54,189 - Step [3]
[rank0]:[titan] 2025-09-22 06:57:54,189 - root - INFO - Step [3]
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15490 closing signal SIGTERM
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15491 closing signal SIGTERM
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15493 closing signal SIGTERM
E0922 06:58:42.127000 15410 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: -9) local_rank: 2 (pid: 15492) of binary: /usr/bin/python
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.2', 'layers.4', 'layers.7', 'layers.5', 'layers.6', 'layers.0', 'tok_embeddings', 'layers.1'}
[rank3]:Stage 3: Modules to keep: {'layers.26', 'layers.30', 'layers.28', 'norm', 'layers.29', 'layers.31', 'output', 'layers.27', 'layers.25'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
timelyfreeze.train FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 15490)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15490
[2]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 15491)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15491
[3]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 15493)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15493
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:58:41
  host      : 521c56b8c386
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 15492)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15492
======================================================
[W922 06:58:42.457161782 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
