nohup: ignoring input

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Sep 22 06:47:04 UTC 2025
âœ”ï¸SERVER: 521c56b8c386 (172.19.0.2),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_fullrand6.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod
âœ”ï¸Running with fullrand6 x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] 
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] *****************************************
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:47:05.641000 13644 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:47:11,574 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:47:11,867 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:47:11,822 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:47:11,827 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:47:11,832 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:47:12,083 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:47:12,087 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:47:12,092 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:47:12,385 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:47:12,387 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:47:12,702 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:47:12,704 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:47:14,619 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:47:14,877 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:47:14,900 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:47:14,853 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:47:14,921 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:47:14,922 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:47:15,099 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:47:15,099 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:47:15,100 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:47:15,792 - root - ERROR - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank3]:[titan] 2025-09-22 06:47:15,794 - root - INFO - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250922-0647
[rank3]:[titan] 2025-09-22 06:47:15,795 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:47:15,816 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:47:15,838 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:47:15,838 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:47:16,015 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:47:16,015 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:47:16,016 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:47:16,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:47:16,025 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:47:24,404 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:47:24,405 - root - INFO - Finished loading the checkpoint in 8.38 seconds.
[rank0]:[titan] 2025-09-22 06:47:24,405 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - [GC] GC collection for checkpoint loading. 0.06 seconds
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - Finished loading the checkpoint in 8.43 seconds.
[rank3]:[titan] 2025-09-22 06:47:24,456 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:[titan] 2025-09-22 06:49:36,504 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.79 [35m mfu: 3.46%[39m
[rank0]:[titan] 2025-09-22 06:49:36,504 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 06:49:36,483 - root - INFO - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
[rank3]:[titan] 2025-09-22 06:49:36,484 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
W0922 06:53:06.949000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13738 closing signal SIGTERM
W0922 06:53:06.951000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13739 closing signal SIGTERM
W0922 06:53:06.952000 13644 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 13740 closing signal SIGTERM
E0922 06:53:08.206000 13644 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: -9) local_rank: 3 (pid: 13741) of binary: /usr/bin/python
[rank0]:Stage 0: Modules to keep: {'layers.6', 'tok_embeddings', 'layers.2', 'layers.3', 'layers.7', 'layers.1', 'layers.0', 'layers.4', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.31', 'layers.28', 'layers.25', 'layers.29', 'layers.26', 'output', 'norm', 'layers.30', 'layers.27'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
timelyfreeze.train FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 13738)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13738
[2]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 13739)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13739
[3]:
  time      : 2025-09-22_06:53:08
  host      : 521c56b8c386
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 13740)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13740
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:53:06
  host      : 521c56b8c386
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 13741)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 13741
======================================================
[W922 06:53:08.692571155 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Sep 22 06:53:08 UTC 2025
âœ”ï¸SERVER: 521c56b8c386 (172.19.0.2),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_apf.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod
âœ”ï¸Running with apf x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] 
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] *****************************************
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:53:10.100000 15410 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:53:15,155 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:53:15,305 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:53:15,414 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:53:15,417 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:53:15,421 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:53:15,562 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:53:15,567 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:53:15,572 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:53:16,164 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:53:16,164 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:53:16,521 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:53:16,484 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:53:19,308 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:53:19,582 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:53:19,606 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:53:19,630 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:53:19,631 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:53:19,833 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:53:19,876 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:53:19,876 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:53:19,877 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:53:20,793 - root - ERROR - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank3]:[titan] 2025-09-22 06:53:20,796 - root - INFO - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_apf_dm4/20250922-0653
[rank3]:[titan] 2025-09-22 06:53:20,797 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:53:20,818 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:53:20,840 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:53:20,840 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:53:21,049 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:53:21,049 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:53:21,050 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:53:21,059 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:53:21,060 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:53:29,434 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Finished loading the checkpoint in 8.37 seconds.
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:53:29,435 - root - INFO - Step [1]
[rank3]:[titan] 2025-09-22 06:53:29,459 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Finished loading the checkpoint in 8.40 seconds.
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:53:29,460 - root - INFO - Step [1]
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[titan] 2025-09-22 06:55:41,557 - root - INFO - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 233 [36m tflops: 10.86 [35m mfu: 3.48%[39m
[rank3]:[titan] 2025-09-22 06:55:41,558 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 06:55:41,579 - root - INFO - Step [2]
[rank0]:[titan] 2025-09-22 06:55:41,578 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.76 [35m mfu: 3.45%[39m
[rank0]:[titan] 2025-09-22 06:55:41,578 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 06:55:41,579 - root - INFO - Step [2]
[rank3]:[titan] 2025-09-22 06:57:52,000 - root - INFO - Step [3]
[rank0]:[titan] 2025-09-22 06:57:54,189 - root - INFO - Step [3]
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15490 closing signal SIGTERM
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15491 closing signal SIGTERM
W0922 06:58:41.078000 15410 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 15493 closing signal SIGTERM
E0922 06:58:42.127000 15410 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: -9) local_rank: 2 (pid: 15492) of binary: /usr/bin/python
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.2', 'layers.4', 'layers.7', 'layers.5', 'layers.6', 'layers.0', 'tok_embeddings', 'layers.1'}
[rank3]:Stage 3: Modules to keep: {'layers.26', 'layers.30', 'layers.28', 'norm', 'layers.29', 'layers.31', 'output', 'layers.27', 'layers.25'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
timelyfreeze.train FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 15490)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15490
[2]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 15491)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15491
[3]:
  time      : 2025-09-22_06:58:42
  host      : 521c56b8c386
  rank      : 3 (local_rank: 3)
  exitcode  : -9 (pid: 15493)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15493
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:58:41
  host      : 521c56b8c386
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 15492)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 15492
======================================================
[W922 06:58:42.457161782 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Sep 22 06:58:42 UTC 2025
âœ”ï¸SERVER: 521c56b8c386 (172.19.0.2),  GPUs: 0,1,2,3
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_nofreeze.ans
âœ”ï¸Main Table Experiment for Llama 3.1 8B on Runpod
âœ”ï¸Running with nofreeze x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
W0922 06:58:43.790000 16839 torch/distributed/run.py:815] 
W0922 06:58:43.790000 16839 torch/distributed/run.py:815] *****************************************
W0922 06:58:43.790000 16839 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:58:43.790000 16839 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:58:48,537 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:58:48,828 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:58:48,833 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:58:48,838 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:58:48,972 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:58:49,193 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:58:49,198 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:58:49,203 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:58:49,476 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:58:49,475 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:58:49,788 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:58:49,814 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:58:50,564 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:58:50,692 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:58:50,942 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:58:50,963 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:58:50,984 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:58:50,984 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:58:51,167 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:58:51,167 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:58:51,168 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:58:51,507 - root - ERROR - Failed to create WandB logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
[rank3]:[titan] 2025-09-22 06:58:51,509 - root - INFO - TensorBoard logging enabled. Logs will be saved at /workspace/torchtitan_data/tb/0922_gpipe_nofreeze_dm4/20250922-0658
[rank3]:[titan] 2025-09-22 06:58:51,510 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:58:51,533 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:58:51,555 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:58:51,555 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:58:51,761 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_nofreeze_dm4
[rank0]:[titan] 2025-09-22 06:58:51,761 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:58:51,761 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:58:51,763 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:58:51,763 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:58:51,752 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:58:51,753 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:58:51,753 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:58:51,763 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_nofreeze_dm4
[rank3]:[titan] 2025-09-22 06:58:51,763 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:58:51,763 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:58:51,765 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:58:51,765 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:59:00,657 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:59:00,657 - root - INFO - Finished loading the checkpoint in 8.89 seconds.
[rank0]:[titan] 2025-09-22 06:59:00,657 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:59:00,657 - root - INFO - Step [1]
[rank3]:[titan] 2025-09-22 06:59:00,704 - root - INFO - [GC] GC collection for checkpoint loading. 0.06 seconds
[rank3]:[titan] 2025-09-22 06:59:00,704 - root - INFO - Finished loading the checkpoint in 8.94 seconds.
[rank3]:[titan] 2025-09-22 06:59:00,704 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:59:00,704 - root - INFO - Step [1]
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[titan] 2025-09-22 07:01:12,899 - root - INFO - [31m step:  1 [32m loss:  1.9227 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 232 [36m tflops: 10.81 [35m mfu: 3.47%[39m
[rank3]:[titan] 2025-09-22 07:01:12,900 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 07:01:12,920 - root - INFO - Step [2]
[rank0]:[titan] 2025-09-22 07:01:12,918 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.5314 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 231 [36m tflops: 10.77 [35m mfu: 3.45%[39m
[rank0]:[titan] 2025-09-22 07:01:12,918 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 07:01:12,919 - root - INFO - Step [2]
[rank3]:[titan] 2025-09-22 07:03:23,255 - root - INFO - Step [3]
[rank0]:[titan] 2025-09-22 07:03:25,444 - root - INFO - Step [3]
[rank3]:[titan] 2025-09-22 07:05:35,864 - root - INFO - Step [4]
[rank0]:[titan] 2025-09-22 07:05:38,052 - root - INFO - Step [4]
[rank3]:[titan] 2025-09-22 07:07:48,468 - root - INFO - Step [5]
[rank0]:[titan] 2025-09-22 07:07:50,658 - root - INFO - Step [5]
[rank3]:[titan] 2025-09-22 07:10:01,110 - root - INFO - Step [6]
[rank0]:[titan] 2025-09-22 07:10:03,299 - root - INFO - Step [6]
[rank3]:[titan] 2025-09-22 07:12:13,788 - root - INFO - Step [7]
[rank0]:[titan] 2025-09-22 07:12:15,977 - root - INFO - Step [7]
[rank3]:[titan] 2025-09-22 07:14:26,451 - root - INFO - Step [8]
[rank0]:[titan] 2025-09-22 07:14:28,641 - root - INFO - Step [8]
[rank3]:[titan] 2025-09-22 07:16:39,116 - root - INFO - Step [9]
[rank0]:[titan] 2025-09-22 07:16:41,308 - root - INFO - Step [9]
[rank3]:[titan] 2025-09-22 07:18:51,792 - root - INFO - Step [10]
[rank0]:[titan] 2025-09-22 07:18:53,980 - root - INFO - Step [10]
[rank3]:[titan] 2025-09-22 07:21:04,422 - root - INFO - Step [11]
[rank0]:[titan] 2025-09-22 07:21:06,610 - root - INFO - Step [11]
[rank3]:[titan] 2025-09-22 07:23:17,051 - root - INFO - Step [12]
[rank0]:[titan] 2025-09-22 07:23:19,242 - root - INFO - Step [12]
[rank3]:[titan] 2025-09-22 07:25:29,714 - root - INFO - Step [13]
[rank0]:[titan] 2025-09-22 07:25:31,907 - root - INFO - Step [13]
[rank3]:[titan] 2025-09-22 07:27:42,339 - root - INFO - Step [14]
[rank0]:[titan] 2025-09-22 07:27:44,533 - root - INFO - Step [14]
[rank3]:[titan] 2025-09-22 07:29:55,044 - root - INFO - Step [15]
[rank0]:[titan] 2025-09-22 07:29:57,233 - root - INFO - Step [15]
[rank3]:[titan] 2025-09-22 07:32:07,690 - root - INFO - Step [16]
[rank0]:[titan] 2025-09-22 07:32:09,877 - root - INFO - Step [16]
[rank3]:[titan] 2025-09-22 07:34:20,320 - root - INFO - Step [17]
[rank0]:[titan] 2025-09-22 07:34:22,509 - root - INFO - Step [17]
[rank3]:[titan] 2025-09-22 07:36:32,930 - root - INFO - Step [18]
[rank0]:[titan] 2025-09-22 07:36:35,120 - root - INFO - Step [18]
