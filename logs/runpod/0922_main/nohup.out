nohup: ignoring input

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 02:52:38 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_fullrand6.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with fullrand6 x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 02:52:40.056000 10257 torch/distributed/run.py:815] 
W0922 02:52:40.056000 10257 torch/distributed/run.py:815] *****************************************
W0922 02:52:40.056000 10257 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 02:52:40.056000 10257 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 02:52:45,055 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 02:52:45,478 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 02:52:45,481 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 02:52:45,485 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 02:52:45,624 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 02:52:45,844 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 02:52:45,848 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 02:52:45,853 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 02:52:46,159 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 02:52:46,162 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 02:52:46,473 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 02:52:46,478 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 02:52:48,751 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 02:52:49,018 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 02:52:49,019 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 02:52:49,020 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 02:52:49,041 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 02:52:49,041 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 02:52:49,222 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 02:52:49,222 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 02:52:49,223 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 02:52:49,223 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 02:52:49,326 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_fullrand6_dm4/20250922-0252/wandb/run-20250922_025250-v382wgt9
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_gpipe_fullrand6_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/v382wgt9
[rank3]:[titan] 2025-09-22 02:52:52,163 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 02:52:52,164 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 02:52:52,165 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 02:52:52,168 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 02:52:52,193 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 02:52:52,193 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 02:52:52,402 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank0]:[titan] 2025-09-22 02:52:52,402 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 02:52:52,402 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 02:52:52,403 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 02:52:52,403 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 02:52:52,390 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 02:52:52,390 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 02:52:52,391 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 02:52:52,392 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 02:52:52,402 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_fullrand6_dm4
[rank3]:[titan] 2025-09-22 02:52:52,402 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 02:52:52,402 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 02:52:52,402 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 02:52:52,403 - root - INFO - Training starts at step 1
[rank0]:/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[titan] 2025-09-22 02:55:05,674 - root - INFO - [31m step:  1 [32m loss: 12.2167 [38;2;180;60;0m grad_norm:  0.3278 [38;2;54;234;195m memory: 46.16GiB(58.25%) [34m tps: 245 [36m tflops: 11.45 [35m mfu: 3.67%[39m
[rank3]:[titan] 2025-09-22 02:55:05,675 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:[titan] 2025-09-22 02:55:05,696 - root - INFO - [31m step:  1 [32m loss: -16.0000 [38;2;180;60;0m grad_norm:  0.3278 [38;2;54;234;195m memory: 39.56GiB(49.92%) [34m tps: 240 [36m tflops: 11.18 [35m mfu: 3.58%[39m
[rank0]:[titan] 2025-09-22 02:55:05,697 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[titan] 2025-09-22 04:41:09,082 - root - INFO - [GC] Peforming periodical GC collection 0.08 seconds
[rank0]:[titan] 2025-09-22 04:41:11,197 - root - INFO - [GC] Peforming periodical GC collection 0.01 seconds

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 06:03:27 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_apf.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with apf x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 06:03:29.221000 49346 torch/distributed/run.py:815] 
W0922 06:03:29.221000 49346 torch/distributed/run.py:815] *****************************************
W0922 06:03:29.221000 49346 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:03:29.221000 49346 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:03:34,088 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:03:34,319 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:03:34,606 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:03:34,612 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:03:34,618 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:03:35,058 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:03:35,062 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:03:35,066 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:03:35,932 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:03:35,927 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:03:36,250 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:03:36,247 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:03:37,902 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:[titan] 2025-09-22 06:03:38,021 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:03:38,166 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:03:38,167 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:03:38,168 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:03:38,189 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:03:38,190 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:03:38,428 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:03:38,428 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:03:38,429 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:03:38,429 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_apf_dm4/20250922-0603/wandb/run-20250922_060339-p46ue552
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_gpipe_apf_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/p46ue552
[rank3]:[titan] 2025-09-22 06:03:40,163 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 06:03:40,165 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:03:40,166 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:03:40,167 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:03:40,190 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:03:40,190 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:03:40,371 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:03:40,372 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:03:40,372 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:03:40,373 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:03:40,383 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank3]:[titan] 2025-09-22 06:03:40,384 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:03:40,384 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:03:40,385 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:03:40,385 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:03:40,382 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_apf_dm4
[rank0]:[titan] 2025-09-22 06:03:40,382 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:03:40,382 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:03:40,383 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:03:40,383 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:03:48,928 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:03:48,928 - root - INFO - Finished loading the checkpoint in 8.54 seconds.
[rank3]:[titan] 2025-09-22 06:03:48,928 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:03:48,906 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:03:48,906 - root - INFO - Finished loading the checkpoint in 8.52 seconds.
[rank0]:[titan] 2025-09-22 06:03:48,906 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:03:49,842 - root - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
[rank0]:[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
[rank0]:[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
[rank0]:[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:[rank0]:                                         ^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 941352 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:
[rank0]:[rank0]: The above exception was the direct cause of the following exception:
[rank0]:
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 723, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
[rank0]:[rank0]:     self.train_step(data_iterator)
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
[rank0]:[rank0]:     loss = self.forward_backward_step(input_dict, labels)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
[rank0]:[rank0]:     self.pp_schedule.step(
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
[rank0]:[rank0]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
[rank0]:[rank0]:     output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
[rank0]:[rank0]:     raise RuntimeError(exc_msg) from e
[rank0]:[rank0]: RuntimeError: 
[rank0]:[rank0]:             [Stage 0] failed to run forward:
[rank0]:[rank0]:             args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
[rank0]:[rank0]:             kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
[rank0]:[rank0]:             
[rank0]:[rank0]:[W922 06:03:50.608119950 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[rank0]:[W922 06:03:51.967452842 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[titan] 2025-09-22 06:03:52,371 - root - INFO - Destroying the purge thread.
W0922 06:03:52.403000 49346 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49444 closing signal SIGTERM
W0922 06:03:52.404000 49346 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49445 closing signal SIGTERM
W0922 06:03:52.405000 49346 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49446 closing signal SIGTERM
E0922 06:03:53.578000 49346 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: 1) local_rank: 0 (pid: 49443) of binary: /usr/bin/python
E0922 06:03:53.589000 49346 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_4b5871w8/27a10e9e-204f-4aff-ac94-f8c3766e5b0e_pxpors43/attempt_0/0/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.4', 'layers.2', 'layers.0', 'layers.5', 'layers.6', 'tok_embeddings', 'layers.1', 'layers.7', 'layers.3'}
[rank3]:Stage 3: Modules to keep: {'output', 'layers.31', 'layers.25', 'layers.26', 'layers.28', 'layers.29', 'norm', 'layers.27', 'layers.30'}
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:03:50
  host      : a3bd8adfcbcd
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 49444)
  error_file: /tmp/torchelastic_4b5871w8/27a10e9e-204f-4aff-ac94-f8c3766e5b0e_pxpors43/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 300, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 198, in forward
      return self.wo(output)
             ^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 6.25 MiB is free. Process 789033 has 55.37 GiB memory in use. Process 941353 has 23.87 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 24.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 1] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
[2]:
  time      : 2025-09-22_06:03:51
  host      : a3bd8adfcbcd
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 49445)
  error_file: /tmp/torchelastic_4b5871w8/27a10e9e-204f-4aff-ac94-f8c3766e5b0e_pxpors43/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 359, in stage_backward
      torch.autograd.backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 354, in backward
      _engine_run_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.25 GiB of which 52.25 MiB is free. Process 789034 has 49.39 GiB memory in use. Process 941354 has 29.80 GiB memory in use. Of the allocated memory 29.04 GiB is allocated by PyTorch, and 32.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 761, in _step_microbatches
      self._stage.backward_one_chunk(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 812, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 670, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 604, in <lambda>
      stage_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 393, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
          Output gradient: ('Tensor(torch.Size([1, 1024, 4096]), grad=False, dtype=torch.float32)',)
          Input: ['Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)']
          
  
[3]:
  time      : 2025-09-22_06:03:52
  host      : a3bd8adfcbcd
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 49446)
  error_file: /tmp/torchelastic_4b5871w8/27a10e9e-204f-4aff-ac94-f8c3766e5b0e_pxpors43/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacity of 79.25 GiB of which 12.25 MiB is free. Process 789035 has 62.20 GiB memory in use. Process 941355 has 17.03 GiB memory in use. Of the allocated memory 16.31 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 3] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:03:49
  host      : a3bd8adfcbcd
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 49443)
  error_file: /tmp/torchelastic_4b5871w8/27a10e9e-204f-4aff-ac94-f8c3766e5b0e_pxpors43/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
                                          ^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 941352 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 0] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
============================================================
[W922 06:03:53.981119067 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 06:03:54 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_nofreeze.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with nofreeze x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 06:03:55.203000 49881 torch/distributed/run.py:815] 
W0922 06:03:55.203000 49881 torch/distributed/run.py:815] *****************************************
W0922 06:03:55.203000 49881 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:03:55.203000 49881 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:04:00,683 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:04:00,669 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:04:01,247 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:04:01,251 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:04:01,256 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:04:01,289 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:04:01,296 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:04:01,302 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:04:02,148 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:04:02,167 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:04:02,452 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:04:02,467 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:04:03,890 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:[titan] 2025-09-22 06:04:04,084 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:04:04,143 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:04:04,145 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:04,146 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:04:04,167 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:04:04,167 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:04:05,093 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:05,093 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:04:05,094 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:04:05,095 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_nofreeze_dm4/20250922-0604/wandb/run-20250922_060405-2z6owvd3
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_gpipe_nofreeze_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/2z6owvd3
[rank3]:[titan] 2025-09-22 06:04:06,426 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 06:04:06,776 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:04:06,777 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:04:06,780 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:04:06,827 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:04:06,828 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:04:07,012 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:04:07,013 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:04:07,013 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:04:07,014 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:04:07,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_nofreeze_dm4
[rank3]:[titan] 2025-09-22 06:04:07,024 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:04:07,024 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:04:07,026 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:04:07,026 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:04:07,024 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_nofreeze_dm4
[rank0]:[titan] 2025-09-22 06:04:07,025 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:04:07,025 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:04:07,026 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:04:07,026 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:04:15,668 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:04:15,668 - root - INFO - Finished loading the checkpoint in 8.64 seconds.
[rank0]:[titan] 2025-09-22 06:04:15,668 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:04:15,684 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:04:15,684 - root - INFO - Finished loading the checkpoint in 8.66 seconds.
[rank3]:[titan] 2025-09-22 06:04:15,685 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:04:16,530 - root - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
[rank0]:[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
[rank0]:[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
[rank0]:[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:[rank0]:                                         ^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 942057 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:
[rank0]:[rank0]: The above exception was the direct cause of the following exception:
[rank0]:
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 723, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
[rank0]:[rank0]:     self.train_step(data_iterator)
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
[rank0]:[rank0]:     loss = self.forward_backward_step(input_dict, labels)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
[rank0]:[rank0]:     self.pp_schedule.step(
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
[rank0]:[rank0]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
[rank0]:[rank0]:     output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
[rank0]:[rank0]:     raise RuntimeError(exc_msg) from e
[rank0]:[rank0]: RuntimeError: 
[rank0]:[rank0]:             [Stage 0] failed to run forward:
[rank0]:[rank0]:             args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
[rank0]:[rank0]:             kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
[rank0]:[rank0]:             
[rank0]:[rank0]:[W922 06:04:17.450536573 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[rank0]:[W922 06:04:18.580873412 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0922 06:04:18.908000 49881 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49952 closing signal SIGTERM
W0922 06:04:18.908000 49881 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49953 closing signal SIGTERM
W0922 06:04:18.909000 49881 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 49954 closing signal SIGTERM
E0922 06:04:20.090000 49881 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: 1) local_rank: 0 (pid: 49951) of binary: /usr/bin/python
E0922 06:04:20.100000 49881 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_hxawtt09/612607a0-bf73-4199-98df-bf30cede4ca9_zusxzhvk/attempt_0/0/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.2', 'layers.1', 'layers.6', 'layers.3', 'tok_embeddings', 'layers.0', 'layers.4', 'layers.7', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.25', 'layers.28', 'norm', 'layers.26', 'layers.29', 'layers.27', 'layers.31', 'output', 'layers.30'}
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:04:17
  host      : a3bd8adfcbcd
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 49952)
  error_file: /tmp/torchelastic_hxawtt09/612607a0-bf73-4199-98df-bf30cede4ca9_zusxzhvk/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 300, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 198, in forward
      return self.wo(output)
             ^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 6.25 MiB is free. Process 789033 has 55.37 GiB memory in use. Process 942058 has 23.87 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 24.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 1] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
[2]:
  time      : 2025-09-22_06:04:18
  host      : a3bd8adfcbcd
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 49953)
  error_file: /tmp/torchelastic_hxawtt09/612607a0-bf73-4199-98df-bf30cede4ca9_zusxzhvk/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 359, in stage_backward
      torch.autograd.backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 354, in backward
      _engine_run_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.25 GiB of which 52.25 MiB is free. Process 789034 has 49.39 GiB memory in use. Process 942059 has 29.80 GiB memory in use. Of the allocated memory 29.04 GiB is allocated by PyTorch, and 32.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 761, in _step_microbatches
      self._stage.backward_one_chunk(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 812, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 670, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 604, in <lambda>
      stage_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 393, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
          Output gradient: ('Tensor(torch.Size([1, 1024, 4096]), grad=False, dtype=torch.float32)',)
          Input: ['Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)']
          
  
[3]:
  time      : 2025-09-22_06:04:20
  host      : a3bd8adfcbcd
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 49954)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 49954
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:04:16
  host      : a3bd8adfcbcd
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 49951)
  error_file: /tmp/torchelastic_hxawtt09/612607a0-bf73-4199-98df-bf30cede4ca9_zusxzhvk/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
                                          ^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 942057 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 0] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
============================================================
[W922 06:04:20.424888408 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 06:04:20 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_gpipe_timelyapf.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with timelyapf x gpipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyapf --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 06:04:21.668000 50292 torch/distributed/run.py:815] 
W0922 06:04:21.668000 50292 torch/distributed/run.py:815] *****************************************
W0922 06:04:21.668000 50292 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:04:21.668000 50292 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:04:26,343 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:04:26,525 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:04:26,796 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:04:26,799 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:04:26,803 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:04:27,127 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:04:27,130 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:04:27,134 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:04:28,289 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:04:28,287 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:04:28,583 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:04:28,584 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:04:29,894 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:04:30,410 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:04:30,672 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:04:30,674 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:30,675 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:04:30,697 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:04:30,698 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:04:30,890 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:30,890 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:04:30,891 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:04:30,892 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_gpipe_timelyapf_dm4/20250922-0604/wandb/run-20250922_060431-7padsnk3
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_gpipe_timelyapf_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/7padsnk3
[rank3]:[titan] 2025-09-22 06:04:32,100 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 06:04:32,101 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:04:32,102 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:04:32,103 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:04:32,126 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:04:32,126 - root - INFO - Using pipeline schedule gpipe with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:04:32,301 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:04:32,301 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:04:32,302 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:04:32,302 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank0]:[titan] 2025-09-22 06:04:32,311 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_timelyapf_dm4
[rank0]:[titan] 2025-09-22 06:04:32,311 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:04:32,311 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:04:32,311 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_gpipe_timelyapf_dm4
[rank3]:[titan] 2025-09-22 06:04:32,311 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:04:32,311 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:04:32,312 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:04:32,313 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:04:32,313 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:04:32,313 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:04:40,803 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:04:40,803 - root - INFO - Finished loading the checkpoint in 8.49 seconds.
[rank3]:[titan] 2025-09-22 06:04:40,804 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:04:40,779 - root - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:[titan] 2025-09-22 06:04:40,779 - root - INFO - Finished loading the checkpoint in 8.47 seconds.
[rank0]:[titan] 2025-09-22 06:04:40,780 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:04:41,611 - root - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
[rank0]:[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
[rank0]:[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
[rank0]:[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:[rank0]:                                         ^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 942728 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:
[rank0]:[rank0]: The above exception was the direct cause of the following exception:
[rank0]:
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 723, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
[rank0]:[rank0]:     self.train_step(data_iterator)
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
[rank0]:[rank0]:     loss = self.forward_backward_step(input_dict, labels)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
[rank0]:[rank0]:     self.pp_schedule.step(
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
[rank0]:[rank0]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
[rank0]:[rank0]:     output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
[rank0]:[rank0]:     raise RuntimeError(exc_msg) from e
[rank0]:[rank0]: RuntimeError: 
[rank0]:[rank0]:             [Stage 0] failed to run forward:
[rank0]:[rank0]:             args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
[rank0]:[rank0]:             kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
[rank0]:[rank0]:             
[rank0]:[rank0]:[W922 06:04:42.344029386 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[rank0]:[W922 06:04:43.825671458 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[rank3]:[titan] 2025-09-22 06:04:44,033 - root - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
W0922 06:04:44.245000 50292 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50363 closing signal SIGTERM
W0922 06:04:44.245000 50292 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50364 closing signal SIGTERM
W0922 06:04:44.246000 50292 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50365 closing signal SIGTERM
E0922 06:04:45.425000 50292 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: 1) local_rank: 0 (pid: 50362) of binary: /usr/bin/python
E0922 06:04:45.431000 50292 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_vcjfj4r2/08aece19-7a3e-43a4-bd5c-826c3e66840b_7_9elihq/attempt_0/0/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.4', 'layers.7', 'layers.2', 'layers.1', 'layers.0', 'tok_embeddings', 'layers.6', 'layers.5'}
[rank3]:Stage 3: Modules to keep: {'layers.26', 'output', 'layers.25', 'layers.28', 'layers.31', 'layers.27', 'layers.29', 'layers.30', 'norm'}
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:04:42
  host      : a3bd8adfcbcd
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 50363)
  error_file: /tmp/torchelastic_vcjfj4r2/08aece19-7a3e-43a4-bd5c-826c3e66840b_7_9elihq/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 300, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 198, in forward
      return self.wo(output)
             ^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 6.25 MiB is free. Process 789033 has 55.37 GiB memory in use. Process 942729 has 23.87 GiB memory in use. Of the allocated memory 23.15 GiB is allocated by PyTorch, and 24.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 1] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
[2]:
  time      : 2025-09-22_06:04:43
  host      : a3bd8adfcbcd
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 50364)
  error_file: /tmp/torchelastic_vcjfj4r2/08aece19-7a3e-43a4-bd5c-826c3e66840b_7_9elihq/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 359, in stage_backward
      torch.autograd.backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py", line 354, in backward
      _engine_run_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.25 GiB of which 52.25 MiB is free. Process 789034 has 49.39 GiB memory in use. Process 942730 has 29.80 GiB memory in use. Of the allocated memory 29.04 GiB is allocated by PyTorch, and 32.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 761, in _step_microbatches
      self._stage.backward_one_chunk(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 812, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 670, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 604, in <lambda>
      stage_backward(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/_backward.py", line 393, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
          Output gradient: ('Tensor(torch.Size([1, 1024, 4096]), grad=False, dtype=torch.float32)',)
          Input: ['Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)']
          
  
[3]:
  time      : 2025-09-22_06:04:44
  host      : a3bd8adfcbcd
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 50365)
  error_file: /tmp/torchelastic_vcjfj4r2/08aece19-7a3e-43a4-bd5c-826c3e66840b_7_9elihq/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 3 has a total capacity of 79.25 GiB of which 12.25 MiB is free. Process 789035 has 62.20 GiB memory in use. Process 942731 has 17.03 GiB memory in use. Of the allocated memory 16.31 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 443, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 3] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024, 4096]), grad=True, dtype=torch.float32)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:04:41
  host      : a3bd8adfcbcd
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 50362)
  error_file: /tmp/torchelastic_vcjfj4r2/08aece19-7a3e-43a4-bd5c-826c3e66840b_7_9elihq/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
                                          ^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 942728 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 734, in _step_microbatches
      output = self._stage.forward_one_chunk(i, arg_mbs[i], kwarg_mbs[i])  # type: ignore[index]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 0] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
============================================================
[W922 06:04:45.830779924 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 06:04:45 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_1f1b_fullrand6.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with fullrand6 x 1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand6 --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 06:04:47.149000 50736 torch/distributed/run.py:815] 
W0922 06:04:47.149000 50736 torch/distributed/run.py:815] *****************************************
W0922 06:04:47.149000 50736 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:04:47.149000 50736 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:04:52,214 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:04:52,740 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:04:53,046 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:04:53,049 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:04:53,054 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:04:53,251 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:04:53,254 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:04:53,259 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:04:54,111 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:04:54,107 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:04:54,412 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:04:54,418 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:04:55,917 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:04:56,437 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:04:56,705 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:04:56,706 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:56,706 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:04:56,727 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:04:56,728 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:04:56,936 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:56,936 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:04:56,936 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:04:56,937 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_1f1b_fullrand6_dm4/20250922-0604/wandb/run-20250922_060457-9pm2f33z
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_1f1b_fullrand6_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/9pm2f33z
[rank3]:[titan] 2025-09-22 06:04:58,217 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 06:04:58,218 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:04:58,219 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:04:58,221 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:04:58,243 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:04:58,243 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:04:58,434 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_1f1b_fullrand6_dm4
[rank0]:[titan] 2025-09-22 06:04:58,434 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:04:58,423 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:04:58,434 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:04:58,423 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:04:58,424 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank0]:[titan] 2025-09-22 06:04:58,435 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:04:58,424 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:04:58,434 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_1f1b_fullrand6_dm4
[rank0]:[titan] 2025-09-22 06:04:58,435 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank3]:[titan] 2025-09-22 06:04:58,434 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:04:58,434 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:04:58,434 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:04:58,435 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:05:06,787 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:05:06,787 - root - INFO - Finished loading the checkpoint in 8.35 seconds.
[rank0]:[titan] 2025-09-22 06:05:06,787 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:05:06,808 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:05:06,809 - root - INFO - Finished loading the checkpoint in 8.37 seconds.
[rank3]:[titan] 2025-09-22 06:05:06,809 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:05:07,695 - root - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
[rank0]:[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
[rank0]:[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
[rank0]:[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:[rank0]:                                         ^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 943388 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:
[rank0]:[rank0]: The above exception was the direct cause of the following exception:
[rank0]:
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 723, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
[rank0]:[rank0]:     self.train_step(data_iterator)
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
[rank0]:[rank0]:     loss = self.forward_backward_step(input_dict, labels)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
[rank0]:[rank0]:     self.pp_schedule.step(
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
[rank0]:[rank0]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 862, in _step_microbatches
[rank0]:[rank0]:     output = self._stage.forward_one_chunk(
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
[rank0]:[rank0]:     raise RuntimeError(exc_msg) from e
[rank0]:[rank0]: RuntimeError: 
[rank0]:[rank0]:             [Stage 0] failed to run forward:
[rank0]:[rank0]:             args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
[rank0]:[rank0]:             kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
[rank0]:[rank0]:             
[rank0]:[rank0]:[W922 06:05:08.548781227 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[rank0]:[W922 06:05:09.856321511 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0922 06:05:10.204000 50736 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50816 closing signal SIGTERM
W0922 06:05:10.205000 50736 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50817 closing signal SIGTERM
W0922 06:05:10.206000 50736 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 50818 closing signal SIGTERM
E0922 06:05:11.289000 50736 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: 1) local_rank: 0 (pid: 50815) of binary: /usr/bin/python
E0922 06:05:11.299000 50736 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_6__5eb_z/a113ebed-189a-488c-b6ff-ecbddf81ef1c_4ybg3as6/attempt_0/0/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.3', 'layers.2', 'layers.5', 'tok_embeddings', 'layers.1', 'layers.4', 'layers.7', 'layers.6', 'layers.0'}
[rank3]:Stage 3: Modules to keep: {'layers.26', 'layers.27', 'output', 'layers.25', 'norm', 'layers.31', 'layers.28', 'layers.29', 'layers.30'}
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:05:11
  host      : a3bd8adfcbcd
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 50816)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 50816
[2]:
  time      : 2025-09-22_06:05:11
  host      : a3bd8adfcbcd
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 50817)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 50817
[3]:
  time      : 2025-09-22_06:05:11
  host      : a3bd8adfcbcd
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 50818)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 50818
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:05:07
  host      : a3bd8adfcbcd
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 50815)
  error_file: /tmp/torchelastic_6__5eb_z/a113ebed-189a-488c-b6ff-ecbddf81ef1c_4ybg3as6/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
                                          ^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 943388 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 862, in _step_microbatches
      output = self._stage.forward_one_chunk(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 0] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
============================================================
[W922 06:05:11.692129046 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Sep 22 06:05:11 UTC 2025
‚úîÔ∏èSERVER: a3bd8adfcbcd (172.19.0.2),  GPUs: 0,1,2,3
‚úîÔ∏èSCRIPT: 
‚úîÔ∏èOUTPUT: /workspace/torchtitan/logs/runpod/0922_main/0922_1f1b_apf.log
‚úîÔ∏èMain Table Experiment for Llama 3.1 8B on Runpod
‚úîÔ∏èRunning with apf x 1f1b ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local-ranks-filter=0,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/workspace/torchtitan/logs/runpod/0922_main/config.toml --job.description="Main Table Experiment for Llama 3.1 8B on Runpod" --training.global_batch_size=128 --training.local_batch_size=8 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf --freezing.stability_check_freq=50 --freezing.aggressiveness=0.05
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W0922 06:05:13.028000 51217 torch/distributed/run.py:815] 
W0922 06:05:13.028000 51217 torch/distributed/run.py:815] *****************************************
W0922 06:05:13.028000 51217 torch/distributed/run.py:815] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0922 06:05:13.028000 51217 torch/distributed/run.py:815] *****************************************
[rank3]:[titan] 2025-09-22 06:05:18,138 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank0]:[titan] 2025-09-22 06:05:18,162 - root - INFO - Starting job: "Main Table Experiment for Llama 3.1 8B on Runpod"
[rank3]:[titan] 2025-09-22 06:05:18,936 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-09-22 06:05:18,939 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:[titan] 2025-09-22 06:05:18,944 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:[titan] 2025-09-22 06:05:18,931 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-09-22 06:05:18,937 - root - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:[titan] 2025-09-22 06:05:18,942 - root - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[titan] 2025-09-22 06:05:19,938 - root - INFO - Loading tokenizer from tokenizer.json
[rank0]:[titan] 2025-09-22 06:05:19,938 - root - INFO - Loading tokenizer from tokenizer.json
[rank3]:[titan] 2025-09-22 06:05:20,235 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank0]:[titan] 2025-09-22 06:05:20,230 - root - INFO - Preparing slimorca dataset from Open-Orca/SlimOrca
[rank3]:[titan] 2025-09-22 06:05:21,060 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:05:21,132 - root - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:[titan] 2025-09-22 06:05:21,385 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank0]:[titan] 2025-09-22 06:05:21,386 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:05:21,387 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank0]:[titan] 2025-09-22 06:05:21,409 - root - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:[titan] 2025-09-22 06:05:21,409 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank0]:[titan] 2025-09-22 06:05:21,610 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank0]:[titan] 2025-09-22 06:05:21,610 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank0]:[titan] 2025-09-22 06:05:21,610 - root - INFO - CUDA memory usage for model: 8.46GiB(10.67%)
[rank0]:[titan] 2025-09-22 06:05:21,611 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: creating run
[rank3]:wandb: Tracking run with wandb version 0.22.0
[rank3]:wandb: Run data is saved locally in /workspace/torchtitan_data/tb/0922_1f1b_apf_dm4/20250922-0605/wandb/run-20250922_060522-5s6jrw3x
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 0922_1f1b_apf_dm4
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/5s6jrw3x
[rank3]:[titan] 2025-09-22 06:05:23,450 - root - INFO - WandB logging enabled
[rank3]:[titan] 2025-09-22 06:05:23,451 - root - INFO - CUDA capacity: NVIDIA A100 80GB PCIe with 79.25GiB memory
[rank3]:[titan] 2025-09-22 06:05:23,452 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:05:23,453 - root - INFO - [34mModel llama3 8B [31msize: 8,030,261,248 total parameters[39m
[rank3]:[titan] 2025-09-22 06:05:23,475 - root - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:[titan] 2025-09-22 06:05:23,475 - root - INFO - Using pipeline schedule 1f1b with 8 microbatches and 4 stages.
[rank3]:[titan] 2025-09-22 06:05:23,649 - root - WARNING - Error running lspci: [Errno 2] No such file or directory: 'lspci', fallback to use device_name
[rank3]:[titan] 2025-09-22 06:05:23,650 - root - INFO - Peak FLOPS used for computing MFU: 3.120e+14
[rank3]:[titan] 2025-09-22 06:05:23,650 - root - INFO - CUDA memory usage for model: 7.66GiB(9.66%)
[rank3]:[titan] 2025-09-22 06:05:23,651 - root - WARNING - Warmup (200) + decay (400) steps exceed total training steps (500). Adjusting decay steps to 300.
[rank3]:[titan] 2025-09-22 06:05:23,659 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_1f1b_apf_dm4
[rank3]:[titan] 2025-09-22 06:05:23,660 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank3]:[titan] 2025-09-22 06:05:23,660 - root - INFO - Mixed precision training is disabled
[rank3]:[titan] 2025-09-22 06:05:23,661 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank3]:[titan] 2025-09-22 06:05:23,661 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:05:23,660 - root - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /workspace/torchtitan_data/checkpoint/0922_1f1b_apf_dm4
[rank0]:[titan] 2025-09-22 06:05:23,660 - root - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:[titan] 2025-09-22 06:05:23,660 - root - INFO - Mixed precision training is disabled
[rank0]:[titan] 2025-09-22 06:05:23,661 - root - INFO - Trainer is initialized with local batch size 8, global batch size 128, gradient accumulation steps 16, sequence length 1024, total steps 500 (warmup 200)
[rank0]:[titan] 2025-09-22 06:05:23,661 - root - INFO - Loading the checkpoint from /workspace/torchtitan_data/base_model/Llama-3.1-8B/original_dcp.
[rank0]:[titan] 2025-09-22 06:05:32,788 - root - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:[titan] 2025-09-22 06:05:32,788 - root - INFO - Finished loading the checkpoint in 9.13 seconds.
[rank0]:[titan] 2025-09-22 06:05:32,788 - root - INFO - Training starts at step 1
[rank3]:[titan] 2025-09-22 06:05:32,805 - root - INFO - [GC] GC collection for checkpoint loading. 0.03 seconds
[rank3]:[titan] 2025-09-22 06:05:32,805 - root - INFO - Finished loading the checkpoint in 9.14 seconds.
[rank3]:[titan] 2025-09-22 06:05:32,805 - root - INFO - Training starts at step 1
[rank0]:[titan] 2025-09-22 06:05:33,660 - root - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
[rank0]:[rank0]:     output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
[rank0]:[rank0]:     out_val = self.submod(*args, **kwargs)
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
[rank0]:[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
[rank0]:[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
[rank0]:[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:[rank0]:                                         ^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:[rank0]:     return forward_call(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 944127 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:
[rank0]:[rank0]: The above exception was the direct cause of the following exception:
[rank0]:
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 723, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
[rank0]:[rank0]:     self.train_step(data_iterator)
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
[rank0]:[rank0]:     loss = self.forward_backward_step(input_dict, labels)
[rank0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
[rank0]:[rank0]:     self.pp_schedule.step(
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
[rank0]:[rank0]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 862, in _step_microbatches
[rank0]:[rank0]:     output = self._stage.forward_one_chunk(
[rank0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
[rank0]:[rank0]:     raise RuntimeError(exc_msg) from e
[rank0]:[rank0]: RuntimeError: 
[rank0]:[rank0]:             [Stage 0] failed to run forward:
[rank0]:[rank0]:             args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
[rank0]:[rank0]:             kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
[rank0]:[rank0]:             
[rank0]:[rank0]:[W922 06:05:34.655846822 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[rank0]:[W922 06:05:35.798672522 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0922 06:05:36.128000 51217 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 51303 closing signal SIGTERM
W0922 06:05:36.129000 51217 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 51304 closing signal SIGTERM
W0922 06:05:36.130000 51217 torch/distributed/elastic/multiprocessing/api.py:939] Sending process 51305 closing signal SIGTERM
E0922 06:05:37.206000 51217 torch/distributed/elastic/multiprocessing/api.py:913] failed (exitcode: 1) local_rank: 0 (pid: 51302) of binary: /usr/bin/python
E0922 06:05:37.212000 51217 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_3wja76q4/204e7aa7-5fcd-438a-b4ed-6e7181258781_pqrweusw/attempt_0/0/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.2', 'tok_embeddings', 'layers.0', 'layers.5', 'layers.3', 'layers.1', 'layers.7', 'layers.6', 'layers.4'}
[rank3]:Stage 3: Modules to keep: {'layers.29', 'layers.27', 'output', 'layers.28', 'layers.25', 'layers.26', 'layers.31', 'layers.30', 'norm'}
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 949, in main
    run(args)
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py", line 940, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-09-22_06:05:37
  host      : a3bd8adfcbcd
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 51303)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 51303
[2]:
  time      : 2025-09-22_06:05:37
  host      : a3bd8adfcbcd
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 51304)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 51304
[3]:
  time      : 2025-09-22_06:05:37
  host      : a3bd8adfcbcd
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 51305)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 51305
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-22_06:05:33
  host      : a3bd8adfcbcd
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 51302)
  error_file: /tmp/torchelastic_3wja76q4/204e7aa7-5fcd-438a-b4ed-6e7181258781_pqrweusw/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 704, in forward_one_chunk
      output = self.forward_maybe_with_nosync(*composite_args, **composite_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 564, in forward_maybe_with_nosync
      out_val = self.submod(*args, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 432, in forward
      h = layer(h, self.freqs_cis)
          ^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 301, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/torchtitan/models/llama3/model/model.py", line 237, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
                                          ^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
      return forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py", line 134, in forward
      return F.linear(input, self.weight, self.bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 48.25 MiB is free. Process 789032 has 58.92 GiB memory in use. Process 944127 has 20.27 GiB memory in use. Of the allocated memory 19.56 GiB is allocated by PyTorch, and 20.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
      return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 571, in train
      self.train_step(data_iterator)
    File "/workspace/torchtitan/timelyfreeze/train.py", line 483, in train_step
      loss = self.forward_backward_step(input_dict, labels)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/workspace/torchtitan/timelyfreeze/train.py", line 439, in forward_backward_step
      self.pp_schedule.step(
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 620, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/schedules.py", line 862, in _step_microbatches
      output = self._stage.forward_one_chunk(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/usr/local/lib/python3.11/dist-packages/torch/distributed/pipelining/stage.py", line 712, in forward_one_chunk
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
              [Stage 0] failed to run forward:
              args: ('Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)',)
              kwargs: {'input_batch': 'Tensor(torch.Size([1, 1024]), grad=False, dtype=torch.int64)'}
              
  
============================================================
[W922 06:05:37.597182774 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
