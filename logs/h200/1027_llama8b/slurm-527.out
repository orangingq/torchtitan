‚úîÔ∏è SLURM JOB GPUS: 0,1,6,7
‚úîÔ∏è Using Slurm-assigned GPU(s): 0,1,6,7

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 18:56:38 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_nofreeze.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with nofreeze x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 18:56:40.087000 2480963 site-packages/torch/distributed/run.py:811] 
W1026 18:56:40.087000 2480963 site-packages/torch/distributed/run.py:811] *****************************************
W1026 18:56:40.087000 2480963 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 18:56:40.087000 2480963 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-26 18:56:45,997 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 18:56:45,978 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 18:56:46,095 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 18:56:46,079 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 18:56:47,066 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 18:56:47,069 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 18:56:47,088 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 18:56:47,090 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:56:47,243 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 18:56:47,245 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:56:47,252 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-26 18:56:47,346 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 18:56:47,348 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:56:49,701 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 18:56:49,996 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 18:56:51,742 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-26 18:56:51,895 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 18:56:52,001 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 18:56:52,004 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 18:56:51,966 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 18:56:51,966 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 18:56:52,101 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 18:56:52,101 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 18:56:52,080 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 18:56:52,104 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 18:56:52,104 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 18:56:52,216 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 18:56:52,216 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-26 18:56:52,755 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 18:56:52,756 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank0]:2025-10-26 18:56:52,838 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 18:56:52,839 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: setting up run k2yoqu8y
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_nofreeze_dm1/20251026-1856/wandb/run-20251026_185652-k2yoqu8y
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_nofreeze_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/k2yoqu8y
[rank3]:2025-10-26 18:56:53,714 - INFO - WandB logging enabled
[rank3]:2025-10-26 18:56:53,727 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 18:56:53,801 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 18:56:53,801 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-10-26 18:56:54,409 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 18:56:54,409 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 18:56:54,440 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_nofreeze_dm1
[rank0]:2025-10-26 18:56:54,440 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 18:56:54,440 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 18:56:54,440 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 18:56:57,912 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 18:56:57,912 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 18:57:07,859 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 18:57:07,859 - INFO - Finished loading the checkpoint in 9.95 seconds.
[rank0]:2025-10-26 18:57:07,859 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 18:57:55,614 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 257  tflops: 12.00  mfu: 1.21%
[rank1]:2025-10-26 18:57:55,614 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 18:57:55,633 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 258  tflops: 12.02  mfu: 1.22%
[rank0]:2025-10-26 18:57:55,634 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 18:57:55,812 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 257  tflops: 11.99  mfu: 1.21%
[rank2]:2025-10-26 18:57:55,812 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 18:57:55,854 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 264  tflops: 12.31  mfu: 1.24%
[rank3]:2025-10-26 18:57:55,854 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 18:58:01,545 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_nofreeze_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/k2yoqu8y
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_nofreeze_dm1/20251026-1856/wandb/run-20251026_185652-k2yoqu8y/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 18:58:05.939000 2480963 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2481044 closing signal SIGTERM
W1026 18:58:05.940000 2480963 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2481045 closing signal SIGTERM
W1026 18:58:05.941000 2480963 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2481046 closing signal SIGTERM
E1026 18:58:08.897000 2480963 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2481047) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 18:58:08.915000 2480963 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_mvsi7pj0/8c9c334b-b086-428d-8446-bbefd1686703_m3npuqr7/attempt_0/3/error.json)
[rank1]:Stage 1: Modules to keep: {'layers.15', 'layers.12', 'layers.11', 'layers.9', 'layers.8', 'layers.16', 'layers.13', 'layers.14', 'layers.10'}
[rank2]:Stage 2: Modules to keep: {'layers.18', 'layers.24', 'layers.17', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.19'}
[rank0]:Stage 0: Modules to keep: {'layers.4', 'layers.6', 'layers.2', 'layers.3', 'tok_embeddings', 'layers.5', 'layers.7', 'layers.0', 'layers.1'}
[rank3]:Stage 3: Modules to keep: {'layers.27', 'layers.30', 'layers.25', 'layers.31', 'norm', 'layers.29', 'layers.26', 'output', 'layers.28'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_nofreeze_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_nofreeze_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_nofreeze_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_nofreeze_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_nofreeze_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_nofreeze_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_nofreeze_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_nofreeze_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_18:58:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2481044)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2481044
[2]:
  time      : 2025-10-26_18:58:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2481045)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2481045
[3]:
  time      : 2025-10-26_18:58:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2481046)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2481046
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_18:58:01
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2481047)
  error_file: /tmp/torchelastic_mvsi7pj0/8c9c334b-b086-428d-8446-bbefd1686703_m3npuqr7/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 18:58:10 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_apf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with apf x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 18:58:11.656000 2483068 site-packages/torch/distributed/run.py:811] 
W1026 18:58:11.656000 2483068 site-packages/torch/distributed/run.py:811] *****************************************
W1026 18:58:11.656000 2483068 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 18:58:11.656000 2483068 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 18:58:17,283 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 18:58:17,456 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 18:58:17,410 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 18:58:17,580 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 18:58:18,503 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 18:58:18,505 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 18:58:18,809 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 18:58:18,811 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-26 18:58:18,964 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 18:58:18,967 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:58:18,981 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 18:58:18,983 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:58:18,988 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-10-26 18:58:21,708 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 18:58:21,987 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 18:58:23,653 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-26 18:58:23,978 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 18:58:23,916 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 18:58:23,964 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank1]:2025-10-26 18:58:24,051 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 18:58:24,051 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 18:58:23,987 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 18:58:23,988 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 18:58:24,167 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-26 18:58:24,239 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 18:58:24,239 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 18:58:24,661 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 18:58:24,662 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 18:58:24,624 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 18:58:24,625 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank2]:2025-10-26 18:58:24,682 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 18:58:24,682 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run w2204owm
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_apf_dm1/20251026-1858/wandb/run-20251026_185824-w2204owm
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_apf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/w2204owm
[rank3]:2025-10-26 18:58:25,718 - INFO - WandB logging enabled
[rank3]:2025-10-26 18:58:25,745 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 18:58:25,820 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 18:58:25,820 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 18:58:26,581 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_apf_dm1
[rank0]:2025-10-26 18:58:26,581 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 18:58:26,581 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 18:58:26,581 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 18:58:26,554 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 18:58:26,554 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 18:58:30,041 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 18:58:30,041 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 18:58:41,739 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 18:58:41,739 - INFO - Finished loading the checkpoint in 11.70 seconds.
[rank0]:2025-10-26 18:58:41,739 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 18:59:29,713 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 249  tflops: 11.63  mfu: 1.18%
[rank1]:2025-10-26 18:59:29,714 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 18:59:29,731 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 249  tflops: 11.62  mfu: 1.17%
[rank0]:2025-10-26 18:59:29,732 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 18:59:29,757 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 250  tflops: 11.66  mfu: 1.18%
[rank2]:2025-10-26 18:59:29,757 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 18:59:30,117 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 255  tflops: 11.88  mfu: 1.20%
[rank3]:2025-10-26 18:59:30,117 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 18:59:35,768 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_apf_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/w2204owm
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_apf_dm1/20251026-1858/wandb/run-20251026_185824-w2204owm/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 18:59:39.887000 2483068 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2483148 closing signal SIGTERM
W1026 18:59:39.888000 2483068 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2483149 closing signal SIGTERM
W1026 18:59:39.889000 2483068 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2483150 closing signal SIGTERM
E1026 18:59:42.753000 2483068 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2483151) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 18:59:42.768000 2483068 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_wses08d6/cd32ed07-58cc-418a-adf4-7f90234ba24f_zuiujldk/attempt_0/3/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.6', 'layers.1', 'tok_embeddings', 'layers.2', 'layers.5', 'layers.3', 'layers.4', 'layers.0', 'layers.7'}
[rank1]:Stage 1: Modules to keep: {'layers.10', 'layers.16', 'layers.8', 'layers.11', 'layers.12', 'layers.13', 'layers.9', 'layers.14', 'layers.15'}
[rank2]:Stage 2: Modules to keep: {'layers.24', 'layers.21', 'layers.20', 'layers.22', 'layers.23', 'layers.17', 'layers.19', 'layers.18'}
[rank3]:Stage 3: Modules to keep: {'layers.30', 'output', 'norm', 'layers.27', 'layers.26', 'layers.29', 'layers.31', 'layers.25', 'layers.28'}
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_apf_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_apf_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_apf_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_apf_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_apf_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_apf_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_apf_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_apf_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_18:59:42
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2483148)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2483148
[2]:
  time      : 2025-10-26_18:59:42
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2483149)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2483149
[3]:
  time      : 2025-10-26_18:59:42
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2483150)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2483150
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_18:59:35
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2483151)
  error_file: /tmp/torchelastic_wses08d6/cd32ed07-58cc-418a-adf4-7f90234ba24f_zuiujldk/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 18:59:44 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_auto.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with auto x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=auto
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 18:59:45.420000 2485302 site-packages/torch/distributed/run.py:811] 
W1026 18:59:45.420000 2485302 site-packages/torch/distributed/run.py:811] *****************************************
W1026 18:59:45.420000 2485302 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 18:59:45.420000 2485302 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 18:59:51,738 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 18:59:51,932 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 18:59:52,000 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 18:59:52,015 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 18:59:52,373 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 18:59:52,376 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:59:52,869 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 18:59:52,871 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:59:52,876 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-10-26 18:59:52,892 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 18:59:52,895 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 18:59:52,945 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 18:59:52,947 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 18:59:56,587 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 18:59:56,889 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 18:59:58,586 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-10-26 18:59:58,797 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 18:59:58,870 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 18:59:58,871 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 18:59:58,846 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 18:59:58,855 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 18:59:58,916 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 18:59:58,941 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 18:59:58,941 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 18:59:58,946 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 18:59:58,947 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 18:59:59,103 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 18:59:59,103 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank1]:2025-10-26 18:59:59,233 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 18:59:59,233 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 18:59:59,237 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 18:59:59,237 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run n71rxpsx
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_auto_dm1/20251026-1859/wandb/run-20251026_185959-n71rxpsx
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_auto_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/n71rxpsx
[rank3]:2025-10-26 19:00:00,921 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:00:00,926 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:00:01,002 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:00:01,003 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-10-26 19:00:01,434 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:00:01,434 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:00:01,456 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_auto_dm1
[rank0]:2025-10-26 19:00:01,456 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:00:01,456 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:00:01,456 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 19:00:04,956 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:00:04,956 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:00:15,977 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:00:15,977 - INFO - Finished loading the checkpoint in 11.02 seconds.
[rank0]:2025-10-26 19:00:15,977 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:01:03,138 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 255  tflops: 11.90  mfu: 1.20%
[rank1]:2025-10-26 19:01:03,138 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:01:03,159 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 255  tflops: 11.89  mfu: 1.20%
[rank0]:2025-10-26 19:01:03,159 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:01:03,249 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 254  tflops: 11.87  mfu: 1.20%
[rank2]:2025-10-26 19:01:03,250 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:01:03,262 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 263  tflops: 12.27  mfu: 1.24%
[rank3]:2025-10-26 19:01:03,263 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:01:08,949 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_auto_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/n71rxpsx
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_auto_dm1/20251026-1859/wandb/run-20251026_185959-n71rxpsx/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:01:13.403000 2485302 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485398 closing signal SIGTERM
W1026 19:01:13.404000 2485302 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485399 closing signal SIGTERM
W1026 19:01:13.405000 2485302 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485400 closing signal SIGTERM
E1026 19:01:16.563000 2485302 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2485401) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:01:16.580000 2485302 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_g7xy3pnv/e0189707-299d-4350-b0c2-d2e684d2f4dd_hs93o2ky/attempt_0/3/error.json)
[rank2]:Stage 2: Modules to keep: {'layers.21', 'layers.19', 'layers.23', 'layers.20', 'layers.18', 'layers.22', 'layers.24', 'layers.17'}
[rank0]:Stage 0: Modules to keep: {'layers.0', 'layers.7', 'layers.2', 'layers.3', 'layers.6', 'tok_embeddings', 'layers.4', 'layers.5', 'layers.1'}
[rank1]:Stage 1: Modules to keep: {'layers.14', 'layers.13', 'layers.11', 'layers.15', 'layers.12', 'layers.9', 'layers.10', 'layers.8', 'layers.16'}
[rank3]:Stage 3: Modules to keep: {'layers.31', 'layers.30', 'layers.28', 'layers.26', 'layers.25', 'norm', 'output', 'layers.29', 'layers.27'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_auto_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_auto_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_auto_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_auto_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_auto_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_auto_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_auto_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_auto_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: auto
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:01:16
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2485398)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485398
[2]:
  time      : 2025-10-26_19:01:16
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2485399)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485399
[3]:
  time      : 2025-10-26_19:01:16
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2485400)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485400
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:01:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2485401)
  error_file: /tmp/torchelastic_g7xy3pnv/e0189707-299d-4350-b0c2-d2e684d2f4dd_hs93o2ky/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:01:18 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_fullrand7.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with fullrand7 x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:01:19.348000 2487049 site-packages/torch/distributed/run.py:811] 
W1026 19:01:19.348000 2487049 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:01:19.348000 2487049 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:01:19.348000 2487049 site-packages/torch/distributed/run.py:811] *****************************************
[rank0]:2025-10-26 19:01:24,918 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:01:25,239 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:01:25,259 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:01:25,262 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:01:25,267 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-26 19:01:25,261 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:01:25,426 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:01:25,864 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:01:25,867 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-26 19:01:26,069 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:01:26,072 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 19:01:26,319 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:01:26,322 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:01:28,908 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:01:29,196 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:01:30,828 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-10-26 19:01:31,091 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 19:01:31,081 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:01:31,164 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:01:31,187 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 19:01:31,187 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 19:01:31,177 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 19:01:31,177 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 19:01:31,193 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:01:31,266 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 19:01:31,266 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank0]:2025-10-26 19:01:32,231 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:01:32,232 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank1]:2025-10-26 19:01:32,232 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:01:32,232 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-26 19:01:32,239 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:01:32,239 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: setting up run sv0d0yu8
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_fullrand7_dm1/20251026-1901/wandb/run-20251026_190132-sv0d0yu8
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_fullrand7_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/sv0d0yu8
[rank3]:2025-10-26 19:01:33,030 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:01:33,065 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:01:33,139 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:01:33,139 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 19:01:35,893 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_fullrand7_dm1
[rank0]:2025-10-26 19:01:35,893 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:01:35,893 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:01:35,893 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 19:01:35,870 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:01:35,872 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:01:39,763 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:01:39,763 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:01:50,840 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 19:01:50,840 - INFO - Finished loading the checkpoint in 11.08 seconds.
[rank0]:2025-10-26 19:01:50,840 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-26 19:02:38,371 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 244  tflops: 11.37  mfu: 1.15%
[rank0]:2025-10-26 19:02:38,371 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-26 19:02:38,353 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 244  tflops: 11.37  mfu: 1.15%
[rank1]:2025-10-26 19:02:38,353 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:02:38,488 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 244  tflops: 11.36  mfu: 1.15%
[rank2]:2025-10-26 19:02:38,489 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:02:38,503 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 251  tflops: 11.69  mfu: 1.18%
[rank3]:2025-10-26 19:02:38,503 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:02:44,185 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_fullrand7_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/sv0d0yu8
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_fullrand7_dm1/20251026-1901/wandb/run-20251026_190132-sv0d0yu8/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:02:48.648000 2487049 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487125 closing signal SIGTERM
W1026 19:02:48.649000 2487049 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487126 closing signal SIGTERM
W1026 19:02:48.649000 2487049 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487127 closing signal SIGTERM
E1026 19:02:51.606000 2487049 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2487128) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:02:51.627000 2487049 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_m9by0xwc/3be12f87-0b30-44aa-a66c-b7114fa50bd6_mc10iri1/attempt_0/3/error.json)
[rank1]:Stage 1: Modules to keep: {'layers.15', 'layers.10', 'layers.8', 'layers.12', 'layers.9', 'layers.16', 'layers.13', 'layers.11', 'layers.14'}
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.4', 'layers.7', 'layers.2', 'layers.3', 'layers.6', 'layers.0', 'tok_embeddings', 'layers.5'}
[rank2]:Stage 2: Modules to keep: {'layers.21', 'layers.22', 'layers.17', 'layers.19', 'layers.18', 'layers.24', 'layers.23', 'layers.20'}
[rank3]:Stage 3: Modules to keep: {'layers.25', 'layers.26', 'layers.29', 'layers.31', 'layers.30', 'layers.27', 'norm', 'output', 'layers.28'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_fullrand7_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_fullrand7_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_fullrand7_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_fullrand7_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_fullrand7_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_fullrand7_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_fullrand7_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_fullrand7_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: fullrand7
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:02:51
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2487125)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487125
[2]:
  time      : 2025-10-26_19:02:51
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2487126)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487126
[3]:
  time      : 2025-10-26_19:02:51
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2487127)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487127
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:02:44
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2487128)
  error_file: /tmp/torchelastic_m9by0xwc/3be12f87-0b30-44aa-a66c-b7114fa50bd6_mc10iri1/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:02:53 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_timelyapf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with timelyapf x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyapf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:02:54.269000 2488871 site-packages/torch/distributed/run.py:811] 
W1026 19:02:54.269000 2488871 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:02:54.269000 2488871 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:02:54.269000 2488871 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 19:03:00,230 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:03:00,316 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:03:00,432 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:03:00,423 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:03:01,195 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:03:01,199 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:01,370 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:03:01,375 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:01,382 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-10-26 19:03:01,504 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:03:01,507 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-26 19:03:01,435 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:03:01,437 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:04,371 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:03:04,649 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:03:06,330 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-10-26 19:03:06,597 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:03:06,648 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:03:06,675 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 19:03:06,675 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 19:03:06,800 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank1]:2025-10-26 19:03:06,795 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:03:06,905 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:03:06,905 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank2]:2025-10-26 19:03:06,896 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 19:03:06,896 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 19:03:06,888 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 19:03:06,888 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank1]:2025-10-26 19:03:07,506 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:03:07,507 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-26 19:03:07,570 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:03:07,570 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: setting up run t05j35ft
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_timelyapf_dm1/20251026-1903/wandb/run-20251026_190307-t05j35ft
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_timelyapf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/t05j35ft
[rank3]:2025-10-26 19:03:08,360 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:03:08,362 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:03:08,435 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:03:08,435 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 19:03:08,879 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_timelyapf_dm1
[rank0]:2025-10-26 19:03:08,879 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:03:08,879 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:03:08,880 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 19:03:08,852 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:03:08,853 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:03:12,554 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:03:12,554 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:03:21,863 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:03:21,863 - INFO - Finished loading the checkpoint in 9.31 seconds.
[rank0]:2025-10-26 19:03:21,863 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:04:09,288 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 262  tflops: 12.24  mfu: 1.24%
[rank1]:2025-10-26 19:04:09,288 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:04:09,306 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 261  tflops: 12.20  mfu: 1.23%
[rank0]:2025-10-26 19:04:09,306 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:04:09,434 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 268  tflops: 12.52  mfu: 1.27%
[rank3]:2025-10-26 19:04:09,434 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:04:09,415 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 262  tflops: 12.22  mfu: 1.24%
[rank2]:2025-10-26 19:04:09,416 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:04:15,056 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading wandb-summary.json; uploading config.yaml
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_timelyapf_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/t05j35ft
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_timelyapf_dm1/20251026-1903/wandb/run-20251026_190307-t05j35ft/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:04:19.623000 2488871 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2488950 closing signal SIGTERM
W1026 19:04:19.624000 2488871 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2488951 closing signal SIGTERM
W1026 19:04:19.625000 2488871 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2488952 closing signal SIGTERM
E1026 19:04:22.789000 2488871 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2488953) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:04:22.805000 2488871 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_tsrqksfs/01849bbf-acca-4038-8e4e-b931aac31bd1_yle7lcs0/attempt_0/3/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.6', 'tok_embeddings', 'layers.7', 'layers.0', 'layers.5'}
[rank1]:Stage 1: Modules to keep: {'layers.11', 'layers.10', 'layers.15', 'layers.8', 'layers.16', 'layers.13', 'layers.14', 'layers.9', 'layers.12'}
[rank2]:Stage 2: Modules to keep: {'layers.17', 'layers.22', 'layers.18', 'layers.23', 'layers.20', 'layers.21', 'layers.19', 'layers.24'}
[rank3]:Stage 3: Modules to keep: {'layers.30', 'layers.26', 'layers.29', 'output', 'layers.31', 'layers.25', 'layers.28', 'layers.27', 'norm'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_timelyapf_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_timelyapf_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_timelyapf_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_timelyapf_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_timelyapf_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_timelyapf_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_timelyapf_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_timelyapf_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: timelyapf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:04:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2488950)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2488950
[2]:
  time      : 2025-10-26_19:04:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2488951)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2488951
[3]:
  time      : 2025-10-26_19:04:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2488952)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2488952
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:04:15
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2488953)
  error_file: /tmp/torchelastic_tsrqksfs/01849bbf-acca-4038-8e4e-b931aac31bd1_yle7lcs0/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:04:24 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run2.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_1F1B_timelyauto.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with timelyauto x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyauto
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:04:25.509000 2490548 site-packages/torch/distributed/run.py:811] 
W1026 19:04:25.509000 2490548 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:04:25.509000 2490548 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:04:25.509000 2490548 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 19:04:31,294 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:04:31,390 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:04:31,548 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:04:31,457 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:04:32,659 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:04:32,662 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 19:04:32,847 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:04:32,850 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:04:33,124 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:04:33,127 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:04:33,132 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-10-26 19:04:33,060 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:04:33,062 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:04:35,846 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:04:36,132 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:04:37,799 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-26 19:04:38,094 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:04:38,079 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:04:38,058 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:04:38,120 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:04:38,143 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-26 19:04:38,143 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-26 19:04:38,191 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-26 19:04:38,191 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank2]:2025-10-26 19:04:38,183 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-26 19:04:38,183 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-10-26 19:04:38,428 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:04:38,428 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank1]:2025-10-26 19:04:38,489 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:04:38,490 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-26 19:04:38,505 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:04:38,506 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run u3nhibhf
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_timelyauto_dm1/20251026-1904/wandb/run-20251026_190438-u3nhibhf
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_1F1B_timelyauto_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/u3nhibhf
[rank3]:2025-10-26 19:04:40,292 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:04:40,304 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:04:40,375 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:04:40,376 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-26 19:04:40,899 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_timelyauto_dm1
[rank0]:2025-10-26 19:04:40,899 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:04:40,899 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:04:40,899 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 19:04:40,868 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:04:40,868 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:04:44,600 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:04:44,601 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:04:55,977 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:04:55,977 - INFO - Finished loading the checkpoint in 11.38 seconds.
[rank0]:2025-10-26 19:04:55,977 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:05:44,612 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.42GiB(24.62%)  tps: 247  tflops: 11.50  mfu: 1.16%
[rank1]:2025-10-26 19:05:44,612 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:05:44,656 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 26.61GiB(19.03%)  tps: 246  tflops: 11.49  mfu: 1.16%
[rank2]:2025-10-26 19:05:44,656 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:05:44,677 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 38.77GiB(27.73%)  tps: 255  tflops: 11.88  mfu: 1.20%
[rank3]:2025-10-26 19:05:44,677 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:05:44,633 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 39.86GiB(28.51%)  tps: 246  tflops: 11.49  mfu: 1.16%
[rank0]:2025-10-26 19:05:44,633 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:05:50,462 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading output.log; uploading config.yaml
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 27.69477
[rank3]:wandb:       memory/max_active(GiB) 38.72043
[rank3]:wandb:       memory/max_reserved(%) 27.73268
[rank3]:wandb:     memory/max_reserved(GiB) 38.77344
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_1F1B_timelyauto_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/u3nhibhf
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_1F1B_timelyauto_dm1/20251026-1904/wandb/run-20251026_190438-u3nhibhf/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:    self._stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        bwd_mb_index,
[rank3]:        ^^^^^^^^^^^^^
[rank3]:        loss=loss,
[rank3]:        ^^^^^^^^^^
[rank3]:        last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
[rank3]:[rank3]:     self._stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_mb_index,
[rank3]:[rank3]:         ^^^^^^^^^^^^^
[rank3]:[rank3]:         loss=loss,
[rank3]:[rank3]:         ^^^^^^^^^^
[rank3]:[rank3]:         last_backward=bwd_mb_index == self._n_microbatches - 1,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:05:55.275000 2490548 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2490630 closing signal SIGTERM
W1026 19:05:55.276000 2490548 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2490631 closing signal SIGTERM
W1026 19:05:55.277000 2490548 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2490632 closing signal SIGTERM
E1026 19:05:58.326000 2490548 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2490633) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:05:58.349000 2490548 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_dkhp67ju/34578357-8a38-430c-a42f-d12603d9df79_8rhsicst/attempt_0/3/error.json)
[rank0]:Stage 0: Modules to keep: {'layers.6', 'layers.2', 'layers.0', 'tok_embeddings', 'layers.4', 'layers.7', 'layers.3', 'layers.1', 'layers.5'}
[rank2]:Stage 2: Modules to keep: {'layers.19', 'layers.18', 'layers.20', 'layers.17', 'layers.22', 'layers.23', 'layers.21', 'layers.24'}
[rank1]:Stage 1: Modules to keep: {'layers.14', 'layers.9', 'layers.12', 'layers.11', 'layers.16', 'layers.10', 'layers.8', 'layers.13', 'layers.15'}
[rank3]:Stage 3: Modules to keep: {'layers.29', 'layers.27', 'output', 'norm', 'layers.31', 'layers.28', 'layers.25', 'layers.30', 'layers.26'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_1F1B_timelyauto_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_1F1B_timelyauto_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_1F1B_timelyauto_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_1F1B_timelyauto_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_1F1B_timelyauto_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_1F1B_timelyauto_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: 1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_1F1B_timelyauto_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_1F1B_timelyauto_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: timelyauto
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:05:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2490630)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2490630
[2]:
  time      : 2025-10-26_19:05:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2490631)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2490631
[3]:
  time      : 2025-10-26_19:05:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2490632)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2490632
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:05:50
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2490633)
  error_file: /tmp/torchelastic_dkhp67ju/34578357-8a38-430c-a42f-d12603d9df79_8rhsicst/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 14.38 MiB is free. Process 2429367 has 94.62 GiB memory in use. Including non-PyTorch memory, this process has 45.16 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 202.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 626, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 897, in _step_microbatches
      self._stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          bwd_mb_index,
          ^^^^^^^^^^^^^
          loss=loss,
          ^^^^^^^^^^
          last_backward=bwd_mb_index == self._n_microbatches - 1,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================
‚úÖ All runs completed. Logs saved in /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b.
