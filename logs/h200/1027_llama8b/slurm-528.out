‚úîÔ∏è SLURM JOB GPUS: 2,3,4,5
‚úîÔ∏è Using Slurm-assigned GPU(s): 2,3,4,5

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 18:59:59 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_nofreeze.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with nofreeze x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:00:00.899000 2485610 site-packages/torch/distributed/run.py:811] 
W1026 19:00:00.899000 2485610 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:00:00.899000 2485610 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:00:00.899000 2485610 site-packages/torch/distributed/run.py:811] *****************************************
[rank2]:2025-10-26 19:00:06,371 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:00:06,544 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:00:06,622 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:00:06,723 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:00:07,438 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:00:07,441 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:00:07,660 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:00:07,663 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:00:07,670 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-26 19:00:07,667 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:00:07,670 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-26 19:00:07,681 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:00:07,683 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:00:10,187 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:00:10,527 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:00:12,205 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-10-26 19:00:12,431 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:00:12,465 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:00:12,502 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-10-26 19:00:12,522 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:00:12,524 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:00:12,511 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 19:00:12,585 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank0]:2025-10-26 19:00:12,516 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:00:12,539 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-10-26 19:00:12,560 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:00:12,562 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank1]:2025-10-26 19:00:12,606 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank3]:  warnings.warn(
[rank1]:2025-10-26 19:00:12,607 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-10-26 19:00:13,087 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:00:13,087 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-26 19:00:13,092 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:00:13,092 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank1]:2025-10-26 19:00:13,092 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:00:13,092 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run er7890cu
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_nofreeze_dm1/20251026-1900/wandb/run-20251026_190013-er7890cu
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_nofreeze_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/er7890cu
[rank3]:2025-10-26 19:00:14,274 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:00:14,279 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:00:14,352 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:00:14,374 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:00:14,375 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:00:14,691 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_nofreeze_dm1
[rank0]:2025-10-26 19:00:14,691 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:00:14,691 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:00:14,691 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 19:00:14,645 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:00:14,645 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:00:18,218 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:00:18,218 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:00:28,715 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 19:00:28,715 - INFO - Finished loading the checkpoint in 10.50 seconds.
[rank0]:2025-10-26 19:00:28,715 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:01:18,029 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.66GiB(33.37%)  tps: 250  tflops: 11.67  mfu: 1.18%
[rank1]:2025-10-26 19:01:18,029 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:01:18,068 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 250  tflops: 11.65  mfu: 1.18%
[rank2]:2025-10-26 19:01:18,068 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:01:18,077 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 45.96GiB(32.88%)  tps: 257  tflops: 11.99  mfu: 1.21%
[rank3]:2025-10-26 19:01:18,077 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:01:18,048 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 250  tflops: 11.66  mfu: 1.18%
[rank0]:2025-10-26 19:01:18,048 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[rank3]:E1026 19:01:27.053000 2485921 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank3]:2025-10-26 19:01:27,062 - INFO - Destroying the purge thread.
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.3', 'layers.2', 'tok_embeddings', 'layers.0'}
[rank0]:Stage 4: Modules to keep: {'layers.20', 'layers.18', 'layers.19', 'layers.17'}
[rank1]:Stage 1: Modules to keep: {'layers.7', 'layers.6', 'layers.8', 'layers.4', 'layers.5'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.12', 'layers.11', 'layers.9'}
[rank2]:Stage 6: Modules to keep: {'layers.25', 'layers.26', 'layers.27', 'layers.28'}
[rank1]:Stage 5: Modules to keep: {'layers.22', 'layers.21', 'layers.23', 'layers.24'}
[rank3]:Stage 3: Modules to keep: {'layers.14', 'layers.13', 'layers.15', 'layers.16'}
[rank3]:Stage 7: Modules to keep: {'output', 'layers.29', 'layers.31', 'layers.30', 'norm'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_nofreeze_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_nofreeze_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.87633
[rank3]:wandb:     memory/max_reserved(GiB) 45.96484
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_nofreeze_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/er7890cu
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_nofreeze_dm1/20251026-1900/wandb/run-20251026_190013-er7890cu/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:01:31.622000 2485610 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485918 closing signal SIGTERM
W1026 19:01:31.623000 2485610 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485919 closing signal SIGTERM
W1026 19:01:31.624000 2485610 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2485920 closing signal SIGTERM
E1026 19:01:35.561000 2485610 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2485921) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:01:35.577000 2485610 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_7368ljek/57cbe458-cebd-4e18-93fc-5b013798c990_vj_3wgez/attempt_0/3/error.json)
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:01:35
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2485918)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485918
[2]:
  time      : 2025-10-26_19:01:35
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2485919)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485919
[3]:
  time      : 2025-10-26_19:01:35
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2485920)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2485920
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:01:27
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2485921)
  error_file: /tmp/torchelastic_7368ljek/57cbe458-cebd-4e18-93fc-5b013798c990_vj_3wgez/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:01:36 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_apf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with apf x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:01:38.185000 2487701 site-packages/torch/distributed/run.py:811] 
W1026 19:01:38.185000 2487701 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:01:38.185000 2487701 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:01:38.185000 2487701 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 19:01:44,005 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:01:43,995 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:01:44,070 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:01:44,238 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:01:44,818 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:01:44,820 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:01:44,825 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-26 19:01:44,924 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:01:44,927 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 19:01:44,957 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:01:44,959 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-26 19:01:45,052 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:01:45,054 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:01:47,405 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:01:47,724 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:01:49,364 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-10-26 19:01:49,590 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 19:01:49,594 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:01:49,616 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:01:49,698 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-10-26 19:01:49,718 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:01:49,719 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:01:49,693 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:01:49,715 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank1]:2025-10-26 19:01:49,703 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank0]:2025-10-26 19:01:49,736 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:01:49,737 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:01:49,725 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank1]:2025-10-26 19:01:49,726 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-26 19:01:50,041 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:01:50,042 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank1]:2025-10-26 19:01:50,052 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:01:50,052 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 19:01:50,053 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:01:50,054 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run a4iyyl6t
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_apf_dm1/20251026-1901/wandb/run-20251026_190150-a4iyyl6t
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_apf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/a4iyyl6t
[rank3]:2025-10-26 19:01:51,479 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:01:51,489 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:01:51,562 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:01:51,585 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:01:51,586 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:2025-10-26 19:01:52,648 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:01:52,649 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:01:52,704 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_apf_dm1
[rank0]:2025-10-26 19:01:52,704 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:01:52,704 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:01:52,704 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 19:01:56,072 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:01:56,072 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:02:05,533 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:02:05,533 - INFO - Finished loading the checkpoint in 9.46 seconds.
[rank0]:2025-10-26 19:02:05,533 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-26 19:02:54,090 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 254  tflops: 11.87  mfu: 1.20%
[rank0]:2025-10-26 19:02:54,091 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-26 19:02:54,071 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.70GiB(33.40%)  tps: 254  tflops: 11.87  mfu: 1.20%
[rank1]:2025-10-26 19:02:54,071 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:02:54,115 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 46.00GiB(32.90%)  tps: 262  tflops: 12.21  mfu: 1.23%
[rank3]:2025-10-26 19:02:54,115 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:02:54,109 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 254  tflops: 11.86  mfu: 1.20%
[rank2]:2025-10-26 19:02:54,110 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.11', 'layers.10', 'layers.12'}
[rank2]:Stage 6: Modules to keep: {'layers.28', 'layers.26', 'layers.25', 'layers.27'}
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.3', 'tok_embeddings', 'layers.2', 'layers.0'}
[rank0]:Stage 4: Modules to keep: {'layers.20', 'layers.18', 'layers.17', 'layers.19'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.5', 'layers.8', 'layers.7', 'layers.6'}
[rank1]:Stage 5: Modules to keep: {'layers.22', 'layers.23', 'layers.24', 'layers.21'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.15', 'layers.14', 'layers.16'}
[rank3]:Stage 7: Modules to keep: {'layers.29', 'output', 'layers.30', 'layers.31', 'norm'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_apf_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_apf_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_apf_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_apf_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_apf_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_apf_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_apf_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_apf_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:[rank3]:E1026 19:03:03.026000 2487793 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank3]:2025-10-26 19:03:03,035 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.90427
[rank3]:wandb:     memory/max_reserved(GiB) 46.00391
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_apf_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/a4iyyl6t
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_apf_dm1/20251026-1901/wandb/run-20251026_190150-a4iyyl6t/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 806.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.82 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 208.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 806.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.82 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 208.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:03:07.354000 2487701 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487790 closing signal SIGTERM
W1026 19:03:07.355000 2487701 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487791 closing signal SIGTERM
W1026 19:03:07.356000 2487701 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2487792 closing signal SIGTERM
E1026 19:03:10.516000 2487701 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2487793) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:03:10.530000 2487701 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_ujkp89lg/710e79ac-2a66-4caf-a5fc-8643f3a41559_zycz8o0q/attempt_0/3/error.json)
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:03:10
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2487790)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487790
[2]:
  time      : 2025-10-26_19:03:10
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2487791)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487791
[3]:
  time      : 2025-10-26_19:03:10
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2487792)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2487792
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:03:03
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2487793)
  error_file: /tmp/torchelastic_ujkp89lg/710e79ac-2a66-4caf-a5fc-8643f3a41559_zycz8o0q/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 806.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.82 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 208.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:03:11 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_auto.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with auto x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=auto
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:03:13.129000 2489411 site-packages/torch/distributed/run.py:811] 
W1026 19:03:13.129000 2489411 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:03:13.129000 2489411 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:03:13.129000 2489411 site-packages/torch/distributed/run.py:811] *****************************************
[rank2]:2025-10-26 19:03:19,205 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:03:19,392 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:03:19,378 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:03:19,369 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:03:19,896 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:03:19,898 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:20,247 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:03:20,250 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:20,256 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-10-26 19:03:20,251 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:03:20,253 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-26 19:03:20,298 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:03:20,301 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:03:22,645 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:03:22,950 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:03:24,808 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-10-26 19:03:24,843 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:03:24,914 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank1]:2025-10-26 19:03:24,926 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 19:03:25,002 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank2]:2025-10-26 19:03:24,938 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:03:24,939 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:03:25,023 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank1]:2025-10-26 19:03:25,025 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:03:25,069 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:03:25,119 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank2]:2025-10-26 19:03:25,206 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:03:25,206 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-26 19:03:25,144 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-10-26 19:03:25,165 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:03:25,167 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank1]:2025-10-26 19:03:25,321 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:03:25,321 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 19:03:25,457 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:03:25,457 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run 9jirouf7
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_auto_dm1/20251026-1903/wandb/run-20251026_190325-9jirouf7
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_auto_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/9jirouf7
[rank3]:2025-10-26 19:03:26,824 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:03:26,843 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:03:26,915 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:03:26,936 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:03:26,937 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:2025-10-26 19:03:27,405 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:03:27,405 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:03:27,459 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_auto_dm1
[rank0]:2025-10-26 19:03:27,460 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:03:27,460 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:03:27,460 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 19:03:31,145 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:03:31,145 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:03:41,321 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 19:03:41,321 - INFO - Finished loading the checkpoint in 10.18 seconds.
[rank0]:2025-10-26 19:03:41,322 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:04:30,312 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.66GiB(33.37%)  tps: 251  tflops: 11.70  mfu: 1.18%
[rank1]:2025-10-26 19:04:30,313 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:04:30,357 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 45.96GiB(32.88%)  tps: 258  tflops: 12.04  mfu: 1.22%
[rank3]:2025-10-26 19:04:30,358 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:04:30,351 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 250  tflops: 11.67  mfu: 1.18%
[rank2]:2025-10-26 19:04:30,351 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:04:30,330 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 251  tflops: 11.72  mfu: 1.18%
[rank0]:2025-10-26 19:04:30,331 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.9', 'layers.12', 'layers.11'}
[rank2]:Stage 6: Modules to keep: {'layers.25', 'layers.28', 'layers.27', 'layers.26'}
[rank1]:Stage 1: Modules to keep: {'layers.7', 'layers.5', 'layers.6', 'layers.8', 'layers.4'}
[rank1]:Stage 5: Modules to keep: {'layers.23', 'layers.22', 'layers.21', 'layers.24'}
[rank0]:Stage 0: Modules to keep: {'layers.2', 'layers.1', 'layers.0', 'layers.3', 'tok_embeddings'}
[rank0]:Stage 4: Modules to keep: {'layers.17', 'layers.19', 'layers.20', 'layers.18'}
[rank3]:Stage 3: Modules to keep: {'layers.15', 'layers.14', 'layers.13', 'layers.16'}
[rank3]:Stage 7: Modules to keep: {'layers.29', 'output', 'layers.30', 'layers.31', 'norm'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_auto_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_auto_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_auto_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_auto_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_auto_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_auto_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_auto_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_auto_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: auto
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:[rank3]:E1026 19:04:39.399000 2489512 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank3]:2025-10-26 19:04:39,408 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading config.yaml
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.87633
[rank3]:wandb:     memory/max_reserved(GiB) 45.96484
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_auto_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/9jirouf7
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_auto_dm1/20251026-1903/wandb/run-20251026_190325-9jirouf7/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 786.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.84 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 228.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 786.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.84 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 228.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:04:44.247000 2489411 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2489509 closing signal SIGTERM
W1026 19:04:44.248000 2489411 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2489510 closing signal SIGTERM
W1026 19:04:44.249000 2489411 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2489511 closing signal SIGTERM
E1026 19:04:47.394000 2489411 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2489512) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:04:47.411000 2489411 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_ru3clw7m/57650733-81a8-4108-b51d-8938d43aca07_l5ugoc4x/attempt_0/3/error.json)
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:04:47
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2489509)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2489509
[2]:
  time      : 2025-10-26_19:04:47
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2489510)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2489510
[3]:
  time      : 2025-10-26_19:04:47
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2489511)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2489511
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:04:39
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2489512)
  error_file: /tmp/torchelastic_ru3clw7m/57650733-81a8-4108-b51d-8938d43aca07_l5ugoc4x/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 786.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.84 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 228.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:04:48 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_fullrand7.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with fullrand7 x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:04:50.038000 2491262 site-packages/torch/distributed/run.py:811] 
W1026 19:04:50.038000 2491262 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:04:50.038000 2491262 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:04:50.038000 2491262 site-packages/torch/distributed/run.py:811] *****************************************
[rank0]:2025-10-26 19:04:55,831 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:04:55,911 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:04:55,902 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:04:55,958 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:04:57,107 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:04:57,117 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:04:57,127 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-10-26 19:04:57,165 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:04:57,168 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-26 19:04:57,174 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:04:57,177 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-26 19:04:57,199 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:04:57,201 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:04:59,829 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:05:00,127 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:05:02,104 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-26 19:05:02,125 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-26 19:05:02,200 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-10-26 19:05:02,221 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank1]:2025-10-26 19:05:02,223 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:05:02,362 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:05:02,411 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-26 19:05:02,435 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-10-26 19:05:02,457 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:05:02,459 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-10-26 19:05:02,401 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:05:02,479 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-10-26 19:05:02,501 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:05:02,502 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:05:02,679 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:05:02,679 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 19:05:02,826 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:05:02,826 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-26 19:05:02,850 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:05:02,850 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run jfrnydce
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_fullrand7_dm1/20251026-1905/wandb/run-20251026_190503-jfrnydce
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_fullrand7_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/jfrnydce
[rank3]:2025-10-26 19:05:04,414 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:05:04,444 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:05:04,516 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:05:04,537 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:05:04,538 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:2025-10-26 19:05:05,040 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:05:05,040 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:05:05,067 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_fullrand7_dm1
[rank0]:2025-10-26 19:05:05,068 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:05:05,068 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:05:05,068 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 19:05:08,719 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:05:08,719 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:05:17,959 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:05:17,959 - INFO - Finished loading the checkpoint in 9.24 seconds.
[rank0]:2025-10-26 19:05:17,959 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-26 19:06:06,806 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 254  tflops: 11.87  mfu: 1.20%
[rank0]:2025-10-26 19:06:06,806 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-26 19:06:06,786 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.70GiB(33.40%)  tps: 254  tflops: 11.83  mfu: 1.20%
[rank1]:2025-10-26 19:06:06,786 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:06:06,832 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 45.96GiB(32.88%)  tps: 263  tflops: 12.26  mfu: 1.24%
[rank3]:2025-10-26 19:06:06,833 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:06:06,821 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 255  tflops: 11.87  mfu: 1.20%
[rank2]:2025-10-26 19:06:06,821 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[rank3]:E1026 19:06:15.771000 2491340 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank1]:Stage 1: Modules to keep: {'layers.5', 'layers.6', 'layers.8', 'layers.7', 'layers.4'}
[rank1]:Stage 5: Modules to keep: {'layers.22', 'layers.24', 'layers.23', 'layers.21'}
[rank0]:Stage 0: Modules to keep: {'layers.0', 'layers.3', 'layers.1', 'layers.2', 'tok_embeddings'}
[rank0]:Stage 4: Modules to keep: {'layers.19', 'layers.18', 'layers.17', 'layers.20'}
[rank2]:Stage 2: Modules to keep: {'layers.11', 'layers.12', 'layers.10', 'layers.9'}
[rank2]:Stage 6: Modules to keep: {'layers.27', 'layers.25', 'layers.26', 'layers.28'}
[rank3]:Stage 3: Modules to keep: {'layers.16', 'layers.14', 'layers.13', 'layers.15'}
[rank3]:Stage 7: Modules to keep: {'norm', 'output', 'layers.29', 'layers.31', 'layers.30'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_fullrand7_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_fullrand7_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: fullrand7
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:2025-10-26 19:06:15,779 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.87633
[rank3]:wandb:     memory/max_reserved(GiB) 45.96484
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_fullrand7_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/jfrnydce
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_fullrand7_dm1/20251026-1905/wandb/run-20251026_190503-jfrnydce/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:06:19.973000 2491262 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2491337 closing signal SIGTERM
W1026 19:06:19.973000 2491262 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2491338 closing signal SIGTERM
W1026 19:06:19.974000 2491262 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2491339 closing signal SIGTERM
E1026 19:06:22.929000 2491262 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2491340) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:06:22.945000 2491262 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_636n_b60/2a1f65d1-d080-4edb-9be8-29189d4322be_iiz1rtpy/attempt_0/3/error.json)
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:06:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2491337)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2491337
[2]:
  time      : 2025-10-26_19:06:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2491338)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2491338
[3]:
  time      : 2025-10-26_19:06:22
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2491339)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2491339
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:06:15
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2491340)
  error_file: /tmp/torchelastic_636n_b60/2a1f65d1-d080-4edb-9be8-29189d4322be_iiz1rtpy/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:06:24 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_timelyapf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with timelyapf x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyapf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:06:25.564000 2492586 site-packages/torch/distributed/run.py:811] 
W1026 19:06:25.564000 2492586 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:06:25.564000 2492586 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:06:25.564000 2492586 site-packages/torch/distributed/run.py:811] *****************************************
[rank2]:2025-10-26 19:06:31,302 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:06:31,454 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:06:31,499 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:06:31,560 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:06:32,322 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:06:32,325 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:06:32,626 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:06:32,628 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:06:32,633 - INFO - [GC] Initial GC collection 0.00 seconds
[rank1]:2025-10-26 19:06:32,723 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:06:32,726 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-26 19:06:32,730 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:06:32,732 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:06:35,238 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:06:35,529 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:06:37,174 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-26 19:06:37,425 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:06:37,436 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:06:37,515 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank1]:2025-10-26 19:06:37,522 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank2]:2025-10-26 19:06:37,461 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:06:37,538 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-10-26 19:06:37,559 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:06:37,561 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:06:37,544 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank1]:2025-10-26 19:06:37,545 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-10-26 19:06:37,552 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-10-26 19:06:37,572 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:06:37,573 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank1]:2025-10-26 19:06:37,800 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:06:37,800 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-26 19:06:37,877 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:06:37,878 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank2]:2025-10-26 19:06:37,884 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:06:37,884 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run ehylokop
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_timelyapf_dm1/20251026-1906/wandb/run-20251026_190638-ehylokop
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_timelyapf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/ehylokop
[rank3]:2025-10-26 19:06:39,196 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:06:39,199 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:06:39,273 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:06:39,295 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:06:39,296 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:06:39,611 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_timelyapf_dm1
[rank0]:2025-10-26 19:06:39,611 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:06:39,611 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:06:39,611 - INFO - Preparing c4_validation dataset from allenai/c4
[rank3]:2025-10-26 19:06:39,582 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:06:39,582 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:06:43,290 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:06:43,290 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:06:53,198 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-26 19:06:53,198 - INFO - Finished loading the checkpoint in 9.91 seconds.
[rank0]:2025-10-26 19:06:53,198 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:07:41,853 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.70GiB(33.40%)  tps: 255  tflops: 11.87  mfu: 1.20%
[rank1]:2025-10-26 19:07:41,853 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:07:41,874 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 255  tflops: 11.87  mfu: 1.20%
[rank0]:2025-10-26 19:07:41,874 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:07:41,895 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 255  tflops: 11.87  mfu: 1.20%
[rank2]:2025-10-26 19:07:41,896 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:07:41,907 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 45.95GiB(32.86%)  tps: 261  tflops: 12.20  mfu: 1.23%
[rank3]:2025-10-26 19:07:41,908 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:Stage 1: Modules to keep: {'layers.7', 'layers.4', 'layers.6', 'layers.8', 'layers.5'}
[rank2]:Stage 2: Modules to keep: {'layers.10', 'layers.9', 'layers.12', 'layers.11'}
[rank2]:Stage 6: Modules to keep: {'layers.26', 'layers.28', 'layers.25', 'layers.27'}
[rank1]:Stage 5: Modules to keep: {'layers.21', 'layers.23', 'layers.22', 'layers.24'}
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.0', 'layers.1', 'layers.3', 'layers.2'}
[rank0]:Stage 4: Modules to keep: {'layers.19', 'layers.17', 'layers.20', 'layers.18'}
[rank3]:Stage 3: Modules to keep: {'layers.13', 'layers.16', 'layers.14', 'layers.15'}
[rank3]:Stage 7: Modules to keep: {'output', 'layers.29', 'layers.31', 'norm', 'layers.30'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_timelyapf_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_timelyapf_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: timelyapf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:[rank3]:E1026 19:07:50.933000 2492671 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank3]:2025-10-26 19:07:50,941 - INFO - Destroying the purge thread.
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading wandb-summary.json; uploading config.yaml
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.86236
[rank3]:wandb:     memory/max_reserved(GiB) 45.94531
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_timelyapf_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/ehylokop
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_timelyapf_dm1/20251026-1906/wandb/run-20251026_190638-ehylokop/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:07:55.734000 2492586 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2492668 closing signal SIGTERM
W1026 19:07:55.735000 2492586 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2492669 closing signal SIGTERM
W1026 19:07:55.735000 2492586 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2492670 closing signal SIGTERM
E1026 19:07:58.903000 2492586 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2492671) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:07:58.924000 2492586 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_p_jmun3t/84a8e5bd-b5d5-4bcf-b25b-20d5f31cae75_4fbry77x/attempt_0/3/error.json)
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:07:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2492668)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2492668
[2]:
  time      : 2025-10-26_19:07:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2492669)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2492669
[3]:
  time      : 2025-10-26_19:07:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2492670)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2492670
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:07:50
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2492671)
  error_file: /tmp/torchelastic_p_jmun3t/84a8e5bd-b5d5-4bcf-b25b-20d5f31cae75_4fbry77x/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sun Oct 26 19:08:00 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 2,3,4,5
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run3.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_Interleaved1F1B_timelyauto.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with timelyauto x Interleaved1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=timelyauto
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1026 19:08:01.562000 2494292 site-packages/torch/distributed/run.py:811] 
W1026 19:08:01.562000 2494292 site-packages/torch/distributed/run.py:811] *****************************************
W1026 19:08:01.562000 2494292 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1026 19:08:01.562000 2494292 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-26 19:08:07,383 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-26 19:08:07,456 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-26 19:08:07,485 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-26 19:08:07,554 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-26 19:08:08,470 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-26 19:08:08,473 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-26 19:08:08,571 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-26 19:08:08,573 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:08:08,541 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-26 19:08:08,542 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:08:08,547 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-10-26 19:08:08,661 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-26 19:08:08,663 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-26 19:08:10,936 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-26 19:08:11,216 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-26 19:08:13,078 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank2]:2025-10-26 19:08:13,169 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-26 19:08:13,242 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.9', 'layers.10', 'layers.11', 'layers.12']
[rank2]:2025-10-26 19:08:13,262 - INFO - PP rank 2 is building stage_idx 6 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28']
[rank2]:2025-10-26 19:08:13,264 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank1]:2025-10-26 19:08:13,391 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-10-26 19:08:13,335 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-26 19:08:13,383 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank1]:2025-10-26 19:08:13,466 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8']
[rank1]:2025-10-26 19:08:13,487 - INFO - PP rank 1 is building stage_idx 5 with modules ['layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank1]:2025-10-26 19:08:13,489 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank0]:2025-10-26 19:08:13,405 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3']
[rank0]:2025-10-26 19:08:13,426 - INFO - PP rank 0 is building stage_idx 4 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20']
[rank0]:2025-10-26 19:08:13,427 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank2]:2025-10-26 19:08:13,514 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-26 19:08:13,514 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-26 19:08:13,666 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-26 19:08:13,667 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank1]:2025-10-26 19:08:13,712 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-26 19:08:13,712 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run f4xif0ds
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_timelyauto_dm1/20251026-1908/wandb/run-20251026_190813-f4xif0ds
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_Interleaved1F1B_timelyauto_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/f4xif0ds
[rank3]:2025-10-26 19:08:14,982 - INFO - WandB logging enabled
[rank3]:2025-10-26 19:08:14,988 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-26 19:08:15,062 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank3]:2025-10-26 19:08:15,083 - INFO - PP rank 3 is building stage_idx 7 with modules ['layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-26 19:08:15,085 - INFO - Using pipeline schedule Interleaved1F1B with 8 microbatches and 8 stages.
[rank3]:2025-10-26 19:08:15,337 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-26 19:08:15,337 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-26 19:08:15,364 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_timelyauto_dm1
[rank0]:2025-10-26 19:08:15,365 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-26 19:08:15,365 - INFO - Mixed precision training is disabled
[rank0]:2025-10-26 19:08:15,365 - INFO - Preparing c4_validation dataset from allenai/c4
[rank0]:2025-10-26 19:08:19,250 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-26 19:08:19,250 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-26 19:08:28,264 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-26 19:08:28,264 - INFO - Finished loading the checkpoint in 9.01 seconds.
[rank0]:2025-10-26 19:08:28,264 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:2025-10-26 19:09:16,636 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 46.70GiB(33.40%)  tps: 259  tflops: 12.09  mfu: 1.22%
[rank1]:2025-10-26 19:09:16,636 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-26 19:09:16,681 - INFO -  step:  1  loss:  4.0063  grad_norm: 613.1230  memory: 45.95GiB(32.86%)  tps: 266  tflops: 12.40  mfu: 1.25%
[rank3]:2025-10-26 19:09:16,682 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-26 19:09:16,654 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 50.58GiB(36.18%)  tps: 259  tflops: 12.08  mfu: 1.22%
[rank0]:2025-10-26 19:09:16,654 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-26 19:09:16,675 - INFO -  step:  1  loss: -4.0000  grad_norm: 613.1230  memory: 34.15GiB(24.43%)  tps: 258  tflops: 12.04  mfu: 1.22%
[rank2]:2025-10-26 19:09:16,675 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:[rank3]:E1026 19:09:25.577000 2494379 site-packages/torch/distributed/pipelining/schedules.py:2165] _PipelineScheduleRuntime caught exception at step 70 when running action 7B7.  Full Schedule:
[rank3]:2025-10-26 19:09:25,585 - INFO - Destroying the purge thread.
[rank2]:Stage 2: Modules to keep: {'layers.9', 'layers.11', 'layers.12', 'layers.10'}
[rank2]:Stage 6: Modules to keep: {'layers.27', 'layers.28', 'layers.26', 'layers.25'}
[rank0]:Stage 0: Modules to keep: {'layers.0', 'layers.1', 'tok_embeddings', 'layers.2', 'layers.3'}
[rank0]:Stage 4: Modules to keep: {'layers.19', 'layers.20', 'layers.17', 'layers.18'}
[rank1]:Stage 1: Modules to keep: {'layers.4', 'layers.7', 'layers.6', 'layers.5', 'layers.8'}
[rank1]:Stage 5: Modules to keep: {'layers.21', 'layers.23', 'layers.22', 'layers.24'}
[rank3]:Stage 3: Modules to keep: {'layers.16', 'layers.13', 'layers.15', 'layers.14'}
[rank3]:Stage 7: Modules to keep: {'layers.30', 'layers.31', 'layers.29', 'output', 'norm'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1027_Interleaved1F1B_timelyauto_dm1
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/profile_trace/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/memory_snapshot/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- image_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 2e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 1000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: Interleaved1F1B
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 2
[rank3]:		- stages_list: [3, 7]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- checkpoint_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/comm_traces/1027_Interleaved1F1B_timelyauto_dm1
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: True
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- seq_len: 1024
[rank3]:		- freq: 100
[rank3]:		- steps: 100
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: timelyauto
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:         Rank 0   Rank 1   Rank 2   Rank 3  
[rank3]:Step 00: 0UNSHARD 1UNSHARD 2UNSHARD 3UNSHARD
[rank3]:Step 01: 4UNSHARD 5UNSHARD 6UNSHARD 7UNSHARD
[rank3]:Step 02: 0F0      1RECV_F0 2RECV_F0 3RECV_F0
[rank3]:Step 03: 0SEND_F0 1F0      2F0      3F0     
[rank3]:Step 04: 4RECV_F0 1SEND_F0 2SEND_F0 3SEND_F0
[rank3]:Step 05: 0F1      1RECV_F1 2RECV_F1 3RECV_F1
[rank3]:Step 06: 0SEND_F1 1F1      2F1      3F1     
[rank3]:Step 07: 4RECV_F1 1SEND_F1 2SEND_F1 3SEND_F1
[rank3]:Step 08: 0F2      1RECV_F2 2RECV_F2 3RECV_F2
[rank3]:Step 09: 0SEND_F2 1F2      2F2      3F2     
[rank3]:Step 10: 4RECV_F2 1SEND_F2 2SEND_F2 3SEND_F2
[rank3]:Step 11: 0F3      1RECV_F3 2RECV_F3 3RECV_F3
[rank3]:Step 12: 0SEND_F3 1F3      2F3      3F3     
[rank3]:Step 13: 4RECV_F3 1SEND_F3 2SEND_F3 3SEND_F3
[rank3]:wandb: updating run metadata
[rank3]:wandb: uploading history steps 0-0, summary, console lines 172-277
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ
[rank3]:wandb:         memory/max_active(%) ‚ñÅ
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 613.12305
[rank3]:wandb: loss_metrics/global_avg_loss 4.00632
[rank3]:wandb: loss_metrics/global_max_loss 4.00632
[rank3]:wandb:                           lr 0.0
[rank3]:wandb:         memory/max_active(%) 28.74184
[rank3]:wandb:       memory/max_active(GiB) 40.18435
[rank3]:wandb:       memory/max_reserved(%) 32.86236
[rank3]:wandb:     memory/max_reserved(GiB) 45.94531
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1027_Interleaved1F1B_timelyauto_dm1 at: https://wandb.ai/orangingq/torchtitan/runs/f4xif0ds
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1027_Interleaved1F1B_timelyauto_dm1/20251026-1908/wandb/run-20251026_190813-f4xif0ds/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:    torch.autograd.backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        stage_output_tensors,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:        grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:    _engine_run_backward(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~^
[rank3]:        tensors,
[rank3]:        ^^^^^^^^
[rank3]:    ...<5 lines>...
[rank3]:        accumulate_grad=True,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        t_outputs, *args, **kwargs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )  # Calls into the C++ engine to run the backward pass
[rank3]:    ^
[rank3]:torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:    self.train_step(data_iterator)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:    loss = self.forward_backward_step(input_dict, labels)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:    self.pp_schedule.step(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        target=targets, losses=losses, input_batch=inputs
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:    self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:    raise e
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:    _perform_action(action)
[rank3]:    ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:    stage.backward_one_chunk(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        mb_index,
[rank3]:        ^^^^^^^^^
[rank3]:    ...<2 lines>...
[rank3]:        last_backward=last_backward,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:    grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        "full", bwd_kwargs, last_backward=last_backward
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:    result = perform_backward(backward_type)()
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:    stage_backward(
[rank3]:    ~~~~~~~~~~~~~~^
[rank3]:        bwd_kwargs["stage_output"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["output_grads"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        bwd_kwargs["input_values"],
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    ),
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:    raise RuntimeError(exc_msg) from e
[rank3]:RuntimeError: 
[rank3]:        Failed to run stage backward:
[rank3]:        Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:        Output gradient: None
[rank3]:        Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:        
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
[rank3]:[rank3]:     torch.autograd.backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         stage_output_tensors,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
[rank3]:[rank3]:     _engine_run_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         tensors,
[rank3]:[rank3]:         ^^^^^^^^
[rank3]:[rank3]:     ...<5 lines>...
[rank3]:[rank3]:         accumulate_grad=True,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
[rank3]:[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         t_outputs, *args, **kwargs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )  # Calls into the C++ engine to run the backward pass
[rank3]:[rank3]:     ^
[rank3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]:
[rank3]:[rank3]: The above exception was the direct cause of the following exception:
[rank3]:
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 725, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
[rank3]:[rank3]:     self.train_step(data_iterator)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
[rank3]:[rank3]:     loss = self.forward_backward_step(input_dict, labels)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
[rank3]:[rank3]:     self.pp_schedule.step(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         target=targets, losses=losses, input_batch=inputs
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
[rank3]:[rank3]:     self._step_microbatches(args_split, kwargs_split, targets_split, losses)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
[rank3]:[rank3]:     raise e
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
[rank3]:[rank3]:     _perform_action(action)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
[rank3]:[rank3]:     stage.backward_one_chunk(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         mb_index,
[rank3]:[rank3]:         ^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         last_backward=last_backward,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
[rank3]:[rank3]:     grads_input, _ = self.backward_maybe_with_nosync(
[rank3]:[rank3]:                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         "full", bwd_kwargs, last_backward=last_backward
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
[rank3]:[rank3]:     result = perform_backward(backward_type)()
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
[rank3]:[rank3]:     stage_backward(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~^
[rank3]:[rank3]:         bwd_kwargs["stage_output"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["output_grads"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:         bwd_kwargs["input_values"],
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     ),
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
[rank3]:[rank3]:     raise RuntimeError(exc_msg) from e
[rank3]:[rank3]: RuntimeError: 
[rank3]:[rank3]:         Failed to run stage backward:
[rank3]:[rank3]:         Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
[rank3]:[rank3]:         Output gradient: None
[rank3]:[rank3]:         Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
[rank3]:[rank3]:         
W1026 19:09:30.098000 2494292 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2494376 closing signal SIGTERM
W1026 19:09:30.098000 2494292 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2494377 closing signal SIGTERM
W1026 19:09:30.099000 2494292 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 2494378 closing signal SIGTERM
E1026 19:09:33.365000 2494292 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 2494379) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1026 19:09:33.382000 2494292 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_779j6o1g/b7d40c07-b237-47d9-a769-b39cf62ecd29_9b1sdk4w/attempt_0/3/error.json)
[rank3]:Step 14: 4F0      5RECV_F0 6RECV_F0 7RECV_F0
[rank3]:Step 15: 4SEND_F0 5F0      6F0      7F0     
[rank3]:Step 16: 4F1      5SEND_F0 6SEND_F0 7RECV_F1
[rank3]:Step 17: 4SEND_F1 5RECV_F1 6RECV_F1 7B0     
[rank3]:Step 18: 4F2      5F1      6F1      7SEND_B0
[rank3]:Step 19: 4SEND_F2 5SEND_F1 6SEND_F1 7RECV_F2
[rank3]:Step 20: 4F3      5RECV_F2 6RECV_B0 7F1     
[rank3]:Step 21: 4SEND_F3 5F2      6RECV_F2 7B1     
[rank3]:Step 22: 0F4      5SEND_F2 6F2      7SEND_B1
[rank3]:Step 23: 0SEND_F4 5RECV_F3 6SEND_F2 7RECV_F3
[rank3]:Step 24: 0F5      5F3      6RECV_F3 7F2     
[rank3]:Step 25: 0SEND_F5 5SEND_F3 6B0      7B2     
[rank3]:Step 26: 4RECV_B0 5RECV_B0 6SEND_B0 7SEND_B2
[rank3]:Step 27: 0F6      1RECV_F4 6RECV_B1 3RECV_F4
[rank3]:Step 28: 0SEND_F6 1F4      2RECV_F4 7F3     
[rank3]:Step 29: 4B0      1SEND_F4 6F3      3RECV_B0
[rank3]:Step 30: 4SEND_B0 1RECV_F5 6SEND_F3 7B3     
[rank3]:Step 31: 4RECV_B1 5B0      6B1      7SEND_B3
[rank3]:Step 32: 0F7      5SEND_B0 6SEND_B1 3RECV_F5
[rank3]:Step 33: 0SEND_F7 5RECV_B1 6RECV_B2 3F4     
[rank3]:Step 34: 4RECV_F4 1RECV_F6 2RECV_F5 3SEND_F4
[rank3]:Step 35: 4B1      1F5      2F4      3RECV_B1
[rank3]:Step 36: 4SEND_B1 1SEND_F5 2SEND_F4 3B0     
[rank3]:Step 37: 4RECV_B2 5B1      6B2      3SEND_B0
[rank3]:Step 38: 4F4      5SEND_B1 6SEND_B2 3RECV_F6
[rank3]:Step 39: 4SEND_F4 5RECV_B2 6RECV_B3 3F5     
[rank3]:Step 40: 4RECV_F5 1RECV_F7 2RECV_F6 3SEND_F5
[rank3]:Step 41: 4B2      1F6      2F5      3RECV_B2
[rank3]:Step 42: 4SEND_B2 1SEND_F6 2SEND_F5 3B1     
[rank3]:Step 43: 4RECV_B3 5B2      6B3      3SEND_B1
[rank3]:Step 44: 4F5      5SEND_B2 6SEND_B3 3RECV_F7
[rank3]:Step 45: 4SEND_F5 5RECV_B3 2RECV_B0 3F6     
[rank3]:Step 46: 4RECV_F6 5RECV_F4 2RECV_F7 3SEND_F6
[rank3]:Step 47: 4B3      1F7      2F6      3RECV_B3
[rank3]:Step 48: 4SEND_B3 1SEND_F7 2SEND_F6 3B2     
[rank3]:Step 49: 0RECV_B0 5B3      2B0      3SEND_B2
[rank3]:Step 50: 4F6      5SEND_B3 2SEND_B0 7RECV_F4
[rank3]:Step 51: 4SEND_F6 1RECV_B0 2RECV_B1 3F7     
[rank3]:Step 52: 4RECV_F7 5RECV_F5 6RECV_F4 3SEND_F7
[rank3]:Step 53: 0B0      5F4      2F7      3B3     
[rank3]:Step 54: 0RECV_B1 5SEND_F4 2SEND_F7 3SEND_B3
[rank3]:Step 55: 4F7      1B0      2B1      7RECV_F5
[rank3]:Step 56: 4SEND_F7 1SEND_B0 2SEND_B1 7F4     
[rank3]:Step 57: 0B1      1RECV_B1 2RECV_B2 7B4     
[rank3]:Step 58: 0RECV_B2 5RECV_F6 6RECV_F5 7SEND_B4
[rank3]:Step 59: 0B2      5F5      6F4      7RECV_F6
[rank3]:Step 60: 0RECV_B3 5SEND_F5 6SEND_F4 7F5     
[rank3]:Step 61: 0B3      1B1      2B2      7B5     
[rank3]:Step 62: 4RECV_B4 1SEND_B1 2SEND_B2 7SEND_B5
[rank3]:Step 63: 4B4      1RECV_B2 2RECV_B3 7RECV_F7
[rank3]:Step 64: 4SEND_B4 5RECV_F7 6RECV_F6 7F6     
[rank3]:Step 65: 4RECV_B5 5F6      6F5      3RECV_B4
[rank3]:Step 66: 4B5      5SEND_F6 6SEND_F5 7B6     
[rank3]:Step 67: 4SEND_B5 1B2      2B3      7SEND_B6
[rank3]:Step 68: 4RECV_B6 1SEND_B2 2SEND_B3 7F7     
[rank3]:Step 69: 4B6      1RECV_B3 6RECV_B4 3RECV_B5
[rank3]:Step 70: 4SEND_B6 5F7      6RECV_F7 7B7      <-- ERROR HERE
[rank3]:Step 71: 4RECV_B7 5SEND_F7 6F6      7SEND_B7
[rank3]:Step 72: 4B7      1B3      6SEND_F6 3RECV_B6
[rank3]:Step 73: 4SEND_B7 1SEND_B3 6B4      7RESHARD
[rank3]:Step 74: 4RESHARD 5RECV_B4 6SEND_B4 3B4     
[rank3]:Step 75: 0RECV_B4 5B4      6RECV_B5 3SEND_B4
[rank3]:Step 76: 0B4      5SEND_B4 6F7      3RECV_B7
[rank3]:Step 77: 0RECV_B5 5RECV_B5 6SEND_F7 3B5     
[rank3]:Step 78: 0B5      5B5      6B5      3SEND_B5
[rank3]:Step 79: 0RECV_B6 5SEND_B5 6SEND_B5 3B6     
[rank3]:Step 80: 0B6      5RECV_B6 6RECV_B6 3SEND_B6
[rank3]:Step 81: 0RECV_B7 5B6      6B6      3B7     
[rank3]:Step 82: 0B7      5SEND_B6 6SEND_B6 3SEND_B7
[rank3]:Step 83: 0RESHARD 5RECV_B7 6RECV_B7 3RESHARD
[rank3]:Step 84:          5B7      6B7              
[rank3]:Step 85:          5SEND_B7 6SEND_B7         
[rank3]:Step 86:          5RESHARD 6RESHARD         
[rank3]:Step 87:          1RECV_B4 2RECV_B4         
[rank3]:Step 88:          1B4      2B4              
[rank3]:Step 89:          1SEND_B4 2SEND_B4         
[rank3]:Step 90:          1RECV_B5 2RECV_B5         
[rank3]:Step 91:          1B5      2B5              
[rank3]:Step 92:          1SEND_B5 2SEND_B5         
[rank3]:Step 93:          1RECV_B6 2RECV_B6         
[rank3]:Step 94:          1B6      2B6              
[rank3]:Step 95:          1SEND_B6 2SEND_B6         
[rank3]:Step 96:          1RECV_B7 2RECV_B7         
[rank3]:Step 97:          1B7      2B7              
[rank3]:Step 98:          1SEND_B7 2SEND_B7         
[rank3]:Step 99:          1RESHARD 2RESHARD         
[rank3]:
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-26_19:09:33
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 2494376)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2494376
[2]:
  time      : 2025-10-26_19:09:33
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 2494377)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2494377
[3]:
  time      : 2025-10-26_19:09:33
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 2494378)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 2494378
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-26_19:09:25
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2494379)
  error_file: /tmp/torchelastic_779j6o1g/b7d40c07-b237-47d9-a769-b39cf62ecd29_9b1sdk4w/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 360, in stage_backward
      torch.autograd.backward(
      ~~~~~~~~~~~~~~~~~~~~~~~^
          stage_output_tensors,
          ^^^^^^^^^^^^^^^^^^^^^
          grad_tensors=output_grad_tensors,  # type: ignore[arg-type]
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/__init__.py", line 356, in backward
      _engine_run_backward(
      ~~~~~~~~~~~~~~~~~~~~^
          tensors,
          ^^^^^^^^
      ...<5 lines>...
          accumulate_grad=True,
          ^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py", line 849, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          t_outputs, *args, **kwargs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
      )  # Calls into the C++ engine to run the backward pass
      ^
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 139.81 GiB of which 766.38 MiB is free. Process 2429365 has 82.19 GiB memory in use. Including non-PyTorch memory, this process has 56.86 GiB memory in use. Of the allocated memory 54.28 GiB is allocated by PyTorch, and 248.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
  The above exception was the direct cause of the following exception:
  
  Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 584, in train
      self.train_step(data_iterator)
      ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 490, in train_step
      loss = self.forward_backward_step(input_dict, labels)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 447, in forward_backward_step
      self.pp_schedule.step(
      ~~~~~~~~~~~~~~~~~~~~~^
          target=targets, losses=losses, input_batch=inputs
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 1554, in step
      self._step_microbatches(args_split, kwargs_split, targets_split, losses)
      ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2178, in _step_microbatches
      raise e
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2163, in _step_microbatches
      _perform_action(action)
      ~~~~~~~~~~~~~~~^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/schedules.py", line 2095, in _perform_action
      stage.backward_one_chunk(
      ~~~~~~~~~~~~~~~~~~~~~~~~^
          mb_index,
          ^^^^^^^^^
      ...<2 lines>...
          last_backward=last_backward,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 803, in backward_one_chunk
      grads_input, _ = self.backward_maybe_with_nosync(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          "full", bwd_kwargs, last_backward=last_backward
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 657, in backward_maybe_with_nosync
      result = perform_backward(backward_type)()
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/stage.py", line 607, in <lambda>
      stage_backward(
      ~~~~~~~~~~~~~~^
          bwd_kwargs["stage_output"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["output_grads"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
          bwd_kwargs["input_values"],
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ),
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/pipelining/_backward.py", line 401, in stage_backward
      raise RuntimeError(exc_msg) from e
  RuntimeError: 
          Failed to run stage backward:
          Stage output: Tensor(torch.Size([]), grad=True, dtype=torch.float32)
          Output gradient: None
          Input: ['Tensor(torch.Size([2, 1024, 4096]), grad=True, dtype=torch.float32)', 'Tensor(torch.Size([2, 1024]), grad=False, dtype=torch.int64)']
          
  
============================================================
‚úÖ All runs completed. Logs saved in /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b.
