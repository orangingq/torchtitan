‚úîÔ∏è SLURM JOB GPUS: 0,1,2,7
‚úîÔ∏è Using Slurm-assigned GPU(s): 0,1,2,7

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Mon Oct 27 14:23:27 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 0,1,2,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/1027_GPipe_apf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation
‚úîÔ∏èRunning with apf x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1027_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1027 14:23:29.733000 3450001 site-packages/torch/distributed/run.py:811] 
W1027 14:23:29.733000 3450001 site-packages/torch/distributed/run.py:811] *****************************************
W1027 14:23:29.733000 3450001 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1027 14:23:29.733000 3450001 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-27 14:23:37,141 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank3]:2025-10-27 14:23:37,251 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank2]:2025-10-27 14:23:37,296 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank0]:2025-10-27 14:23:37,368 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment, without streaming mode, sample-level with truncation"
[rank1]:2025-10-27 14:23:37,935 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-27 14:23:37,937 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-27 14:23:38,241 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-27 14:23:38,243 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-27 14:23:38,259 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-27 14:23:38,261 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-27 14:23:38,278 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-27 14:23:38,281 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-27 14:23:38,287 - INFO - [GC] Initial GC collection 0.00 seconds
[rank0]:2025-10-27 14:23:40,232 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-27 14:23:40,513 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-27 14:23:42,172 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank0]:2025-10-27 14:23:42,429 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-27 14:23:42,459 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-27 14:23:42,536 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-27 14:23:42,536 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-27 14:23:42,480 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-27 14:23:42,503 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-27 14:23:42,503 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-27 14:23:42,780 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-27 14:23:42,780 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-27 14:23:42,777 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-27 14:23:42,777 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run enopdp8i
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1027_GPipe_apf_dm1/20251027-1423/wandb/run-20251027_142343-enopdp8i
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1027_GPipe_apf_dm1
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/enopdp8i
[rank3]:2025-10-27 14:23:44,268 - INFO - WandB logging enabled
[rank3]:2025-10-27 14:23:44,274 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-27 14:23:44,347 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-27 14:23:44,347 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank3]:2025-10-27 14:23:44,569 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-27 14:23:44,569 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank1]:2025-10-27 14:23:46,238 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-27 14:23:46,312 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-27 14:23:46,312 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-27 14:23:46,540 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1027_GPipe_apf_dm1
[rank0]:2025-10-27 14:23:46,540 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-27 14:23:46,540 - INFO - Mixed precision training is disabled
[rank0]:2025-10-27 14:23:46,540 - INFO - Preparing c4_validation dataset from allenai/c4
[rank1]:2025-10-27 14:23:46,517 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-27 14:23:46,518 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-27 14:23:49,562 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 1000 (warmup 100)
[rank0]:2025-10-27 14:23:49,562 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-27 14:23:59,522 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-27 14:23:59,522 - INFO - Finished loading the checkpoint in 9.96 seconds.
[rank0]:2025-10-27 14:23:59,522 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:2025-10-27 14:24:23,642 - INFO -  step:  1  loss: 15.8956  grad_norm:     inf  memory: 77.01GiB(55.08%)  tps: 417  tflops: 19.44  mfu: 1.97%
[rank3]:2025-10-27 14:24:23,642 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-27 14:24:23,658 - INFO -  step:  1  loss: -4.0000  grad_norm:     inf  memory: 61.95GiB(44.31%)  tps: 398  tflops: 18.56  mfu: 1.88%
[rank0]:2025-10-27 14:24:23,658 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-27 14:24:23,638 - INFO -  step:  1  loss: -4.0000  grad_norm:     inf  memory: 65.20GiB(46.63%)  tps: 439  tflops: 20.46  mfu: 2.07%
[rank1]:2025-10-27 14:24:23,639 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-27 14:24:23,636 - INFO -  step:  1  loss: -4.0000  grad_norm:     inf  memory: 58.03GiB(41.50%)  tps: 398  tflops: 18.58  mfu: 1.88%
[rank2]:2025-10-27 14:24:23,636 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-27 14:25:01,347 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,347 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,348 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,348 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,348 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,348 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,348 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,349 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,349 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,349 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,349 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,349 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,350 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,350 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,350 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank3]:2025-10-27 14:25:01,350 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,648 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:25:01,649 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,984 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank1]:2025-10-27 14:25:01,985 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,285 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,285 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,285 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,285 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,285 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank0]:2025-10-27 14:25:02,286 - WARNING - Batch index 1 is larger than log time length 0. Filling with zeros.
[rank2]:2025-10-27 14:31:23,181 - INFO -  step: 20  loss: -4.0000  grad_norm:     inf  memory: 71.04GiB(50.81%)  tps: 742  tflops: 34.61  mfu: 3.50%
[rank1]:2025-10-27 14:31:23,183 - INFO -  step: 20  loss: -4.0000  grad_norm:     inf  memory: 79.85GiB(57.11%)  tps: 742  tflops: 34.61  mfu: 3.50%
[rank0]:2025-10-27 14:31:23,187 - INFO -  step: 20  loss: -4.0000  grad_norm:     inf  memory: 78.87GiB(56.41%)  tps: 742  tflops: 34.61  mfu: 3.50%
[rank3]:2025-10-27 14:31:23,186 - INFO -  step: 20  loss: 15.1715  grad_norm:     inf  memory: 92.28GiB(66.01%)  tps: 742  tflops: 34.61  mfu: 3.50%
[rank3]:2025-10-27 14:31:27,763 - INFO - Avg. fwd time: 167.3459 / Avg. bwd time: 339.3317 / Avg. batch time: 4113.2589 (ms) / GPU bubble ratio: 1.45%
[rank2]:2025-10-27 14:31:28,062 - INFO - Avg. fwd time: 144.5795 / Avg. bwd time: 292.3296 / Avg. batch time: 4556.5712 (ms) / GPU bubble ratio: 23.29%
[rank1]:2025-10-27 14:31:28,398 - INFO - Avg. fwd time: 162.7800 / Avg. bwd time: 329.0697 / Avg. batch time: 5055.3051 (ms) / GPU bubble ratio: 22.16%
[rank0]:2025-10-27 14:31:28,698 - INFO - Avg. fwd time: 144.8684 / Avg. bwd time: 294.4934 / Avg. batch time: 5501.4851 (ms) / GPU bubble ratio: 36.11%
slurmstepd-localhost: error: *** JOB 551 ON localhost CANCELLED AT 2025-10-27T14:32:01 ***
