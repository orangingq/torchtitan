‚úîÔ∏è SLURM JOB GPUS: 4,5,6,7
‚úîÔ∏è Using Slurm-assigned GPU(s): 4,5,6,7

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sat Oct 25 06:44:32 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_1F1B_nofreeze.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with nofreeze x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] 
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] *****************************************
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-25 06:44:39,652 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-25 06:44:39,705 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-25 06:44:39,907 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-25 06:44:40,114 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-25 06:44:40,247 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-25 06:44:40,250 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-25 06:44:40,407 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-25 06:44:40,411 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-25 06:44:40,807 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-25 06:44:40,810 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:40,837 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-25 06:44:40,839 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:40,844 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 712, in <module>
[rank3]:[rank3]:     trainer = TrainerWithFreezer(config)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 131, in __init__
[rank3]:[rank3]:     dist_utils.set_determinism(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         world_mesh,
[rank3]:[rank3]:         ^^^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         job_config.training.deterministic,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 114, in set_determinism
[rank3]:[rank3]:     torch.distributed.broadcast(seed_tensor, src=0)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
[rank3]:[rank3]:     return func(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 2872, in broadcast
[rank3]:[rank3]:     work = group.broadcast([tensor], opts)
[rank3]:[rank3]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:94, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.27.5
[rank3]:[rank3]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank3]:[rank3]: Last error:
[rank3]:[rank3]: Failed to CUDA calloc async 2432 bytes
W1025 06:44:44.929000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378707 closing signal SIGTERM
W1025 06:44:44.930000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378708 closing signal SIGTERM
W1025 06:44:44.931000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378709 closing signal SIGTERM
E1025 06:44:46.817000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 1378710) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1025 06:44:46.834000 1378632 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_b7mo12vi/fe708f43-d2df-4964-ab47-10235736b207_613uwvd8/attempt_0/3/error.json)
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 1378707)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378707
[2]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 1378708)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378708
[3]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 1378709)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378709
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-25_06:44:42
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1378710)
  error_file: /tmp/torchelastic_b7mo12vi/fe708f43-d2df-4964-ab47-10235736b207_613uwvd8/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 131, in __init__
      dist_utils.set_determinism(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~^
          world_mesh,
          ^^^^^^^^^^^
      ...<2 lines>...
          job_config.training.deterministic,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 114, in set_determinism
      torch.distributed.broadcast(seed_tensor, src=0)
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
      return func(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 2872, in broadcast
      work = group.broadcast([tensor], opts)
  torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:94, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.27.5
  ncclUnhandledCudaError: Call to CUDA function failed.
  Last error:
  Failed to CUDA calloc async 2432 bytes
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sat Oct 25 06:44:48 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_1F1B_fullrand7.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with fullrand7 x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] 
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] *****************************************
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] *****************************************
[rank0]:2025-10-25 06:44:55,412 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-25 06:44:55,550 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-25 06:44:55,489 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-25 06:44:55,574 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-25 06:44:56,213 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-25 06:44:56,216 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:56,221 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-10-25 06:44:56,441 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-25 06:44:56,444 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-25 06:44:56,557 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-25 06:44:56,559 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-25 06:44:56,532 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-25 06:44:56,534 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:58,279 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-25 06:44:58,556 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-25 06:45:00,452 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-25 06:45:00,583 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-25 06:45:00,581 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-25 06:45:00,674 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-25 06:45:00,675 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-25 06:45:00,710 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-25 06:45:00,761 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank1]:2025-10-25 06:45:00,676 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-25 06:45:00,677 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-10-25 06:45:00,785 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-25 06:45:00,785 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-25 06:45:00,952 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-25 06:45:00,952 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-25 06:45:00,932 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-25 06:45:00,932 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-25 06:45:01,003 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-25 06:45:01,004 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run oesiiyl1
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1024_1F1B_fullrand7_h200/20251025-0645/wandb/run-20251025_064501-oesiiyl1
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1024_1F1B_fullrand7_h200
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/oesiiyl1
[rank3]:2025-10-25 06:45:02,504 - INFO - WandB logging enabled
[rank3]:2025-10-25 06:45:02,514 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-25 06:45:02,586 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-25 06:45:02,586 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-10-25 06:45:02,799 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-25 06:45:02,800 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-25 06:45:03,119 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1024_1F1B_fullrand7_h200
[rank0]:2025-10-25 06:45:03,119 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-25 06:45:03,119 - INFO - Mixed precision training is disabled
[rank0]:2025-10-25 06:45:03,119 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 2000 (warmup 100)
[rank0]:2025-10-25 06:45:03,119 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-25 06:45:11,954 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-25 06:45:11,954 - INFO - Finished loading the checkpoint in 8.83 seconds.
[rank0]:2025-10-25 06:45:11,954 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-25 06:45:35,344 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 39.86GiB(28.51%)  tps: 474  tflops: 22.10  mfu: 2.23%
[rank0]:2025-10-25 06:45:35,344 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-25 06:45:35,326 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 34.42GiB(24.62%)  tps: 473  tflops: 22.04  mfu: 2.23%
[rank1]:2025-10-25 06:45:35,326 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-25 06:45:35,398 - INFO -  step:  1  loss:  1.5658  grad_norm:  3.0754  memory: 38.77GiB(27.73%)  tps: 499  tflops: 23.27  mfu: 2.35%
[rank3]:2025-10-25 06:45:35,399 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-25 06:45:35,341 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 26.61GiB(19.03%)  tps: 472  tflops: 22.03  mfu: 2.23%
[rank2]:2025-10-25 06:45:35,341 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-25 06:52:36,504 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 06:52:36,500 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 06:52:36,502 - INFO -  step: 20  loss:  1.1858  grad_norm:  1.1848  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 06:52:36,497 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 06:59:59,789 - INFO -  step: 40  loss:  1.0795  grad_norm:  0.5194  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 06:59:59,784 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 06:59:59,791 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank1]:2025-10-25 06:59:59,787 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:03:19,245 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-25 07:07:23,047 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:07:23,052 - INFO -  step: 60  loss:  1.0643  grad_norm:  0.4492  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank1]:2025-10-25 07:07:23,049 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:07:23,053 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 07:14:46,372 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:14:46,378 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 07:14:46,374 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:14:46,376 - INFO -  step: 80  loss:  1.0453  grad_norm:  0.4305  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:15:34,282 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 07:15:35,617 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 07:15:35,951 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 07:15:36,250 - WARNING - Dataset alpaca is being re-looped
[rank3]:2025-10-25 07:20:17,185 - INFO - [Step 95] „Ä∞Ô∏è Monitoring Upperbound
[rank0]:2025-10-25 07:21:47,433 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-10-25 07:22:09,618 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:22:09,622 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 07:22:09,616 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:22:09,620 - INFO -  step: 100  loss:  1.0455  grad_norm:  0.4331  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:22:09,686 - INFO - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank3/251025_0722_stage3_step100.svg
[rank1]:2025-10-25 07:22:09,685 - INFO - Frozen Ratio History of Rank 1 (Stage 1)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank1/251025_0722_stage1_step100.svg
[rank0]:2025-10-25 07:22:09,686 - INFO - Frozen Ratio History of Rank 0 (Stage 0)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank0/251025_0722_stage0_step100.svg
[rank2]:2025-10-25 07:22:09,682 - INFO - Frozen Ratio History of Rank 2 (Stage 2)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank2/251025_0722_stage2_step100.svg
[rank1]:2025-10-25 07:29:33,001 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:29:33,003 - INFO -  step: 120  loss:  0.9929  grad_norm:  0.4404  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:29:33,005 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:29:32,999 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:36:56,344 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 07:36:56,346 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:36:56,350 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:36:56,348 - INFO -  step: 140  loss:  1.0202  grad_norm:  0.4142  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:40:15,859 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-10-25 07:44:19,713 - INFO -  step: 160  loss: -4.0000  grad_norm:  0.3963  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:44:19,717 - INFO -  step: 160  loss: -4.0000  grad_norm:  0.3963  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:44:19,715 - INFO -  step: 160  loss:  0.9803  grad_norm:  0.3963  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:44:19,710 - INFO -  step: 160  loss: -4.0000  grad_norm:  0.3963  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:45:57,514 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 07:45:58,846 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 07:45:59,181 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 07:45:59,479 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 07:51:43,081 - INFO -  step: 180  loss: -4.0000  grad_norm:  0.4069  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:51:43,085 - INFO -  step: 180  loss: -4.0000  grad_norm:  0.4069  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:51:43,079 - INFO -  step: 180  loss: -4.0000  grad_norm:  0.4069  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:51:43,083 - INFO -  step: 180  loss:  0.9551  grad_norm:  0.4069  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:58:44,264 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-25 07:59:06,456 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4214  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:59:06,460 - INFO -  step: 200  loss:  0.9449  grad_norm:  0.4214  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:59:06,462 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4214  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:59:06,527 - INFO - Frozen Ratio History of Rank 0 (Stage 0)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank0/251025_0759_stage0_step200.svg
[rank1]:2025-10-25 07:59:06,458 - INFO -  step: 200  loss: -4.0000  grad_norm:  0.4214  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 07:59:06,523 - INFO - Frozen Ratio History of Rank 1 (Stage 1)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank1/251025_0759_stage1_step200.svg
[rank2]:2025-10-25 07:59:06,519 - INFO - Frozen Ratio History of Rank 2 (Stage 2)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank2/251025_0759_stage2_step200.svg
[rank3]:2025-10-25 07:59:06,527 - INFO - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank3/251025_0759_stage3_step200.svg
[rank2]:2025-10-25 08:06:29,873 - INFO -  step: 220  loss: -4.0000  grad_norm:  0.3998  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank0]:2025-10-25 08:06:29,879 - INFO -  step: 220  loss: -4.0000  grad_norm:  0.3998  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank1]:2025-10-25 08:06:29,875 - INFO -  step: 220  loss: -4.0000  grad_norm:  0.3998  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank3]:2025-10-25 08:06:29,878 - INFO -  step: 220  loss:  0.9119  grad_norm:  0.3998  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank2]:2025-10-25 08:13:53,204 - INFO -  step: 240  loss: -4.0000  grad_norm:  0.4070  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 08:13:53,206 - INFO -  step: 240  loss: -4.0000  grad_norm:  0.4070  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:13:53,210 - INFO -  step: 240  loss: -4.0000  grad_norm:  0.4070  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:13:53,210 - INFO -  step: 240  loss:  0.8748  grad_norm:  0.4070  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:16:20,887 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 08:16:22,220 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 08:16:22,554 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 08:16:22,852 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 08:17:12,703 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-10-25 08:21:16,525 - INFO -  step: 260  loss: -4.0000  grad_norm:  0.4307  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 08:21:16,522 - INFO -  step: 260  loss: -4.0000  grad_norm:  0.4307  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:21:16,527 - INFO -  step: 260  loss:  0.8662  grad_norm:  0.4307  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:21:16,529 - INFO -  step: 260  loss: -4.0000  grad_norm:  0.4307  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:28:39,910 - INFO -  step: 280  loss:  0.8647  grad_norm:  0.4065  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 08:28:39,905 - INFO -  step: 280  loss: -4.0000  grad_norm:  0.4065  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:28:39,912 - INFO -  step: 280  loss: -4.0000  grad_norm:  0.4065  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 08:28:39,908 - INFO -  step: 280  loss: -4.0000  grad_norm:  0.4065  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:35:41,111 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank0]:2025-10-25 08:36:03,306 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4716  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 08:36:03,300 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4716  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 08:36:03,303 - INFO -  step: 300  loss: -4.0000  grad_norm:  0.4716  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:36:03,305 - INFO -  step: 300  loss:  0.8271  grad_norm:  0.4716  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:36:03,366 - INFO - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank3/251025_0836_stage3_step300.svg
[rank0]:2025-10-25 08:36:03,365 - INFO - Frozen Ratio History of Rank 0 (Stage 0)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank0/251025_0836_stage0_step300.svg
[rank2]:2025-10-25 08:36:03,359 - INFO - Frozen Ratio History of Rank 2 (Stage 2)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank2/251025_0836_stage2_step300.svg
[rank1]:2025-10-25 08:36:03,362 - INFO - Frozen Ratio History of Rank 1 (Stage 1)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank1/251025_0836_stage1_step300.svg
[rank0]:2025-10-25 08:43:26,764 - INFO -  step: 320  loss: -4.0000  grad_norm:  0.4791  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank1]:2025-10-25 08:43:26,760 - INFO -  step: 320  loss: -4.0000  grad_norm:  0.4791  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank3]:2025-10-25 08:43:26,763 - INFO -  step: 320  loss:  0.8316  grad_norm:  0.4791  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank2]:2025-10-25 08:43:26,758 - INFO -  step: 320  loss: -4.0000  grad_norm:  0.4791  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank3]:2025-10-25 08:46:49,849 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 08:46:51,183 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 08:46:51,517 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 08:46:51,816 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 08:50:50,118 - INFO -  step: 340  loss: -4.0000  grad_norm:  0.4572  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:50:50,123 - INFO -  step: 340  loss: -4.0000  grad_norm:  0.4572  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 08:50:50,116 - INFO -  step: 340  loss: -4.0000  grad_norm:  0.4572  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:50:50,121 - INFO -  step: 340  loss:  0.7773  grad_norm:  0.4572  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:54:09,646 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-10-25 08:58:13,498 - INFO -  step: 360  loss: -4.0000  grad_norm:  0.5002  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 08:58:13,501 - INFO -  step: 360  loss:  0.7983  grad_norm:  0.5002  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 08:58:13,496 - INFO -  step: 360  loss: -4.0000  grad_norm:  0.5002  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 08:58:13,502 - INFO -  step: 360  loss: -4.0000  grad_norm:  0.5002  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 09:05:36,844 - INFO -  step: 380  loss: -4.0000  grad_norm:  0.4350  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 09:05:36,851 - INFO -  step: 380  loss: -4.0000  grad_norm:  0.4350  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 09:05:36,847 - INFO -  step: 380  loss: -4.0000  grad_norm:  0.4350  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 09:05:36,850 - INFO -  step: 380  loss:  0.7296  grad_norm:  0.4350  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 09:12:38,016 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank0]:2025-10-25 09:13:00,208 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4652  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 09:13:00,204 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4652  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 09:13:00,202 - INFO -  step: 400  loss: -4.0000  grad_norm:  0.4652  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 09:13:00,265 - INFO - Frozen Ratio History of Rank 2 (Stage 2)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank2/251025_0913_stage2_step400.svg
[rank3]:2025-10-25 09:13:00,207 - INFO -  step: 400  loss:  0.7018  grad_norm:  0.4652  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 09:13:00,273 - INFO - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank3/251025_0913_stage3_step400.svg
[rank0]:2025-10-25 09:13:00,273 - INFO - Frozen Ratio History of Rank 0 (Stage 0)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank0/251025_0913_stage0_step400.svg
[rank1]:2025-10-25 09:13:00,269 - INFO - Frozen Ratio History of Rank 1 (Stage 1)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank1/251025_0913_stage1_step400.svg
[rank3]:2025-10-25 09:17:13,240 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 09:17:14,573 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 09:17:14,907 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 09:17:15,205 - WARNING - Dataset alpaca is being re-looped
[rank3]:2025-10-25 09:20:23,638 - INFO -  step: 420  loss:  0.6809  grad_norm:  0.4499  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank0]:2025-10-25 09:20:23,639 - INFO -  step: 420  loss: -4.0000  grad_norm:  0.4499  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank1]:2025-10-25 09:20:23,635 - INFO -  step: 420  loss: -4.0000  grad_norm:  0.4499  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank2]:2025-10-25 09:20:23,633 - INFO -  step: 420  loss: -4.0000  grad_norm:  0.4499  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.48%
[rank2]:2025-10-25 09:27:47,035 - INFO -  step: 440  loss: -4.0000  grad_norm:  0.4624  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 09:27:47,040 - INFO -  step: 440  loss:  0.6732  grad_norm:  0.4624  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 09:27:47,042 - INFO -  step: 440  loss: -4.0000  grad_norm:  0.4624  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 09:27:47,038 - INFO -  step: 440  loss: -4.0000  grad_norm:  0.4624  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 09:31:06,598 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank3]:2025-10-25 09:35:10,495 - INFO -  step: 460  loss:  0.7158  grad_norm:  0.5759  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank0]:2025-10-25 09:35:10,496 - INFO -  step: 460  loss: -4.0000  grad_norm:  0.5759  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank1]:2025-10-25 09:35:10,492 - INFO -  step: 460  loss: -4.0000  grad_norm:  0.5759  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank2]:2025-10-25 09:35:10,490 - INFO -  step: 460  loss: -4.0000  grad_norm:  0.5759  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.46  mfu: 3.48%
[rank2]:2025-10-25 09:42:33,850 - INFO -  step: 480  loss: -4.0000  grad_norm:  0.5644  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 09:42:33,855 - INFO -  step: 480  loss:  0.6666  grad_norm:  0.5644  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 09:42:33,852 - INFO -  step: 480  loss: -4.0000  grad_norm:  0.5644  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 09:42:33,856 - INFO -  step: 480  loss: -4.0000  grad_norm:  0.5644  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
