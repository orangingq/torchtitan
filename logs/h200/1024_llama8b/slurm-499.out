‚úîÔ∏è SLURM JOB GPUS: 4,5,6,7
‚úîÔ∏è Using Slurm-assigned GPU(s): 4,5,6,7

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sat Oct 25 06:44:32 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_1F1B_nofreeze.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with nofreeze x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] 
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] *****************************************
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1025 06:44:34.124000 1378632 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-25 06:44:39,652 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-25 06:44:39,705 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-25 06:44:39,907 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-25 06:44:40,114 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-25 06:44:40,247 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-25 06:44:40,250 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-25 06:44:40,407 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-25 06:44:40,411 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-25 06:44:40,807 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-25 06:44:40,810 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:40,837 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-25 06:44:40,839 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:40,844 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 712, in <module>
[rank3]:[rank3]:     trainer = TrainerWithFreezer(config)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 131, in __init__
[rank3]:[rank3]:     dist_utils.set_determinism(
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:[rank3]:         world_mesh,
[rank3]:[rank3]:         ^^^^^^^^^^^
[rank3]:[rank3]:     ...<2 lines>...
[rank3]:[rank3]:         job_config.training.deterministic,
[rank3]:[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:     )
[rank3]:[rank3]:     ^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 114, in set_determinism
[rank3]:[rank3]:     torch.distributed.broadcast(seed_tensor, src=0)
[rank3]:[rank3]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
[rank3]:[rank3]:     return func(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 2872, in broadcast
[rank3]:[rank3]:     work = group.broadcast([tensor], opts)
[rank3]:[rank3]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:94, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.27.5
[rank3]:[rank3]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank3]:[rank3]: Last error:
[rank3]:[rank3]: Failed to CUDA calloc async 2432 bytes
W1025 06:44:44.929000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378707 closing signal SIGTERM
W1025 06:44:44.930000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378708 closing signal SIGTERM
W1025 06:44:44.931000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 1378709 closing signal SIGTERM
E1025 06:44:46.817000 1378632 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 3 (pid: 1378710) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1025 06:44:46.834000 1378632 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_b7mo12vi/fe708f43-d2df-4964-ab47-10235736b207_613uwvd8/attempt_0/3/error.json)
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 1378707)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378707
[2]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 1378708)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378708
[3]:
  time      : 2025-10-25_06:44:46
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 1378709)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 1378709
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-25_06:44:42
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1378710)
  error_file: /tmp/torchelastic_b7mo12vi/fe708f43-d2df-4964-ab47-10235736b207_613uwvd8/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 131, in __init__
      dist_utils.set_determinism(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~^
          world_mesh,
          ^^^^^^^^^^^
      ...<2 lines>...
          job_config.training.deterministic,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 114, in set_determinism
      torch.distributed.broadcast(seed_tensor, src=0)
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 83, in wrapper
      return func(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 2872, in broadcast
      work = group.broadcast([tensor], opts)
  torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:94, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.27.5
  ncclUnhandledCudaError: Call to CUDA function failed.
  Last error:
  Failed to CUDA calloc async 2432 bytes
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Sat Oct 25 06:44:48 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_1F1B_fullrand7.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with fullrand7 x 1F1B ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] 
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] *****************************************
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1025 06:44:49.804000 1378838 site-packages/torch/distributed/run.py:811] *****************************************
[rank0]:2025-10-25 06:44:55,412 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-25 06:44:55,550 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-25 06:44:55,489 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-25 06:44:55,574 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-25 06:44:56,213 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-25 06:44:56,216 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:56,221 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-10-25 06:44:56,441 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-25 06:44:56,444 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-25 06:44:56,557 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-25 06:44:56,559 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank3]:2025-10-25 06:44:56,532 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-25 06:44:56,534 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-25 06:44:58,279 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-25 06:44:58,556 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-25 06:45:00,452 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-25 06:45:00,583 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-25 06:45:00,581 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-25 06:45:00,674 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-25 06:45:00,675 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank0]:2025-10-25 06:45:00,710 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-25 06:45:00,761 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank1]:2025-10-25 06:45:00,676 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-25 06:45:00,677 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-10-25 06:45:00,785 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-25 06:45:00,785 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank1]:2025-10-25 06:45:00,952 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-25 06:45:00,952 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-25 06:45:00,932 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-25 06:45:00,932 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank0]:2025-10-25 06:45:01,003 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-25 06:45:01,004 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run oesiiyl1
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1024_1F1B_fullrand7_h200/20251025-0645/wandb/run-20251025_064501-oesiiyl1
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1024_1F1B_fullrand7_h200
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/oesiiyl1
[rank3]:2025-10-25 06:45:02,504 - INFO - WandB logging enabled
[rank3]:2025-10-25 06:45:02,514 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-25 06:45:02,586 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-25 06:45:02,586 - INFO - Using pipeline schedule 1F1B with 8 microbatches and 4 stages.
[rank3]:2025-10-25 06:45:02,799 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-25 06:45:02,800 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-25 06:45:03,119 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1024_1F1B_fullrand7_h200
[rank0]:2025-10-25 06:45:03,119 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-25 06:45:03,119 - INFO - Mixed precision training is disabled
[rank0]:2025-10-25 06:45:03,119 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 2000 (warmup 100)
[rank0]:2025-10-25 06:45:03,119 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank0]:2025-10-25 06:45:11,954 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-25 06:45:11,954 - INFO - Finished loading the checkpoint in 8.83 seconds.
[rank0]:2025-10-25 06:45:11,954 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-25 06:45:35,344 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 39.86GiB(28.51%)  tps: 474  tflops: 22.10  mfu: 2.23%
[rank0]:2025-10-25 06:45:35,344 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-25 06:45:35,326 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 34.42GiB(24.62%)  tps: 473  tflops: 22.04  mfu: 2.23%
[rank1]:2025-10-25 06:45:35,326 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-25 06:45:35,398 - INFO -  step:  1  loss:  1.5658  grad_norm:  3.0754  memory: 38.77GiB(27.73%)  tps: 499  tflops: 23.27  mfu: 2.35%
[rank3]:2025-10-25 06:45:35,399 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-25 06:45:35,341 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 26.61GiB(19.03%)  tps: 472  tflops: 22.03  mfu: 2.23%
[rank2]:2025-10-25 06:45:35,341 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-25 06:52:36,504 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 06:52:36,500 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 06:52:36,502 - INFO -  step: 20  loss:  1.1858  grad_norm:  1.1848  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 06:52:36,497 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 06:59:59,789 - INFO -  step: 40  loss:  1.0795  grad_norm:  0.5194  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 06:59:59,784 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 06:59:59,791 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank1]:2025-10-25 06:59:59,787 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5194  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:03:19,245 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-25 07:07:23,047 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:07:23,052 - INFO -  step: 60  loss:  1.0643  grad_norm:  0.4492  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank1]:2025-10-25 07:07:23,049 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:07:23,053 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4492  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 07:14:46,372 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:14:46,378 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 07:14:46,374 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4305  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:14:46,376 - INFO -  step: 80  loss:  1.0453  grad_norm:  0.4305  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:15:34,282 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-25 07:15:35,617 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-25 07:15:35,951 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-25 07:15:36,250 - WARNING - Dataset alpaca is being re-looped
[rank3]:2025-10-25 07:20:17,185 - INFO - [Step 95] „Ä∞Ô∏è Monitoring Upperbound
[rank0]:2025-10-25 07:21:47,433 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank1]:2025-10-25 07:22:09,618 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank0]:2025-10-25 07:22:09,622 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank2]:2025-10-25 07:22:09,616 - INFO -  step: 100  loss: -4.0000  grad_norm:  0.4331  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:22:09,620 - INFO -  step: 100  loss:  1.0455  grad_norm:  0.4331  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.48  mfu: 3.49%
[rank3]:2025-10-25 07:22:09,686 - INFO - Frozen Ratio History of Rank 3 (Stage 3)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank3/251025_0722_stage3_step100.svg
[rank1]:2025-10-25 07:22:09,685 - INFO - Frozen Ratio History of Rank 1 (Stage 1)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank1/251025_0722_stage1_step100.svg
[rank0]:2025-10-25 07:22:09,686 - INFO - Frozen Ratio History of Rank 0 (Stage 0)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank0/251025_0722_stage0_step100.svg
[rank2]:2025-10-25 07:22:09,682 - INFO - Frozen Ratio History of Rank 2 (Stage 2)  is saved as: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/images/1024_1F1B_fullrand7_h200/freeze_ratio_history/rank2/251025_0722_stage2_step100.svg
[rank1]:2025-10-25 07:29:33,001 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:29:33,003 - INFO -  step: 120  loss:  0.9929  grad_norm:  0.4404  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:29:33,005 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:29:32,999 - INFO -  step: 120  loss: -4.0000  grad_norm:  0.4404  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank2]:2025-10-25 07:36:56,344 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 38.19GiB(27.32%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank1]:2025-10-25 07:36:56,346 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 48.97GiB(35.03%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:36:56,350 - INFO -  step: 140  loss: -4.0000  grad_norm:  0.4142  memory: 56.85GiB(40.66%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank3]:2025-10-25 07:36:56,348 - INFO -  step: 140  loss:  1.0202  grad_norm:  0.4142  memory: 52.47GiB(37.53%)  tps: 739  tflops: 34.47  mfu: 3.49%
[rank0]:2025-10-25 07:40:15,859 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
