‚úîÔ∏è SLURM JOB GPUS: 4,5,6,7
‚úîÔ∏è Using Slurm-assigned GPU(s): 4,5,6,7

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Fri Oct 24 17:02:25 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_GPipe_nofreeze.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with nofreeze x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.no-freeze
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1024 17:02:26.358000 962994 site-packages/torch/distributed/run.py:811] 
W1024 17:02:26.358000 962994 site-packages/torch/distributed/run.py:811] *****************************************
W1024 17:02:26.358000 962994 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1024 17:02:26.358000 962994 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-24 17:02:32,261 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-24 17:02:32,332 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-24 17:02:32,358 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-24 17:02:32,322 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-24 17:02:33,123 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-24 17:02:33,126 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:02:33,309 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-24 17:02:33,311 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:02:33,316 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-24 17:02:33,310 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-24 17:02:33,312 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-24 17:02:33,317 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-24 17:02:33,320 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:02:35,066 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-24 17:02:35,369 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-24 17:02:37,105 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-24 17:02:37,288 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-24 17:02:37,379 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-24 17:02:37,361 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-24 17:02:37,362 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 17:02:37,428 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-24 17:02:37,451 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-24 17:02:37,451 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-10-24 17:02:37,483 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank2]:2025-10-24 17:02:37,558 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-24 17:02:37,558 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 17:02:37,693 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-24 17:02:37,694 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank1]:2025-10-24 17:02:37,660 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-24 17:02:37,660 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-24 17:02:37,761 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-24 17:02:37,761 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run n0euiveq
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1024_GPipe_nofreeze_h200/20251024-1702/wandb/run-20251024_170238-n0euiveq
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1024_GPipe_nofreeze_h200
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/n0euiveq
[rank3]:2025-10-24 17:02:39,185 - INFO - WandB logging enabled
[rank3]:2025-10-24 17:02:39,186 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-24 17:02:39,260 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-24 17:02:39,260 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 17:02:39,502 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1024_GPipe_nofreeze_h200
[rank0]:2025-10-24 17:02:39,503 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-24 17:02:39,503 - INFO - Mixed precision training is disabled
[rank0]:2025-10-24 17:02:39,503 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 2000 (warmup 100)
[rank0]:2025-10-24 17:02:39,503 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank3]:2025-10-24 17:02:39,482 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-24 17:02:39,482 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-24 17:02:48,469 - INFO - [GC] GC collection for checkpoint loading. 0.01 seconds
[rank0]:2025-10-24 17:02:48,469 - INFO - Finished loading the checkpoint in 8.97 seconds.
[rank0]:2025-10-24 17:02:48,469 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-24 17:03:11,900 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 61.95GiB(44.31%)  tps: 475  tflops: 22.17  mfu: 2.24%
[rank0]:2025-10-24 17:03:11,901 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-24 17:03:11,887 - INFO -  step:  1  loss:  1.5658  grad_norm:  3.0754  memory: 77.01GiB(55.08%)  tps: 502  tflops: 23.40  mfu: 2.37%
[rank3]:2025-10-24 17:03:11,888 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-24 17:03:11,885 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 65.20GiB(46.63%)  tps: 474  tflops: 22.12  mfu: 2.24%
[rank1]:2025-10-24 17:03:11,886 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-24 17:03:11,883 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 58.03GiB(41.50%)  tps: 477  tflops: 22.25  mfu: 2.25%
[rank2]:2025-10-24 17:03:11,883 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-24 17:10:13,664 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank1]:2025-10-24 17:10:13,660 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank3]:2025-10-24 17:10:13,662 - INFO -  step: 20  loss:  1.1858  grad_norm:  1.1848  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank2]:2025-10-24 17:10:13,657 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank3]:2025-10-24 17:17:37,679 - INFO -  step: 40  loss:  1.0795  grad_norm:  0.5193  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank0]:2025-10-24 17:17:37,680 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank1]:2025-10-24 17:17:37,676 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank2]:2025-10-24 17:17:37,674 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank0]:2025-10-24 17:20:57,466 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-24 17:25:01,727 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4493  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank3]:2025-10-24 17:25:01,732 - INFO -  step: 60  loss:  1.0642  grad_norm:  0.4493  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank0]:2025-10-24 17:25:01,733 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4493  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank1]:2025-10-24 17:25:01,729 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4493  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank2]:2025-10-24 17:32:26,111 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 71.04GiB(50.81%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank0]:2025-10-24 17:32:26,118 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 78.87GiB(56.41%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank3]:2025-10-24 17:32:26,116 - INFO -  step: 80  loss:  1.0452  grad_norm:  0.4310  memory: 92.28GiB(66.01%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank1]:2025-10-24 17:32:26,114 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 79.85GiB(57.11%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank3]:2025-10-24 17:33:14,271 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-24 17:33:15,484 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-24 17:33:15,818 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-24 17:33:16,116 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-24 17:36:08,332 - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:     ~~~~~~~~~~~~~^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank0]:[rank0]:     draw_charts(self.freezer, self.step, job_config)
[rank0]:[rank0]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank0]:[rank0]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank0]:[rank0]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:2025-10-24 17:36:08,332 - INFO - Destroying the purge thread.
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank0]:[rank0]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank0]:[rank0]:                         ~~~~~~~~~~~~^^^
[rank0]:[rank0]: IndexError: list index out of range
[rank1]:2025-10-24 17:36:08,332 - INFO - Destroying the purge thread.
[rank1]:[rank1]: Traceback (most recent call last):
[rank1]:[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank1]:[rank1]:     trainer.train()
[rank1]:[rank1]:     ~~~~~~~~~~~~~^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank1]:[rank1]:     return f(*args, **kwargs)
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank1]:[rank1]:     draw_charts(self.freezer, self.step, job_config)
[rank1]:[rank1]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank1]:[rank1]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank1]:[rank1]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank1]:[rank1]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank1]:[rank1]:                         ~~~~~~~~~~~~^^^
[rank1]:[rank1]: IndexError: list index out of range
[rank2]:2025-10-24 17:36:08,332 - INFO - Destroying the purge thread.
[rank2]:[rank2]: Traceback (most recent call last):
[rank2]:[rank2]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank2]:[rank2]:   File "<frozen runpy>", line 88, in _run_code
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank2]:[rank2]:     trainer.train()
[rank2]:[rank2]:     ~~~~~~~~~~~~~^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank2]:[rank2]:     return f(*args, **kwargs)
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank2]:[rank2]:     draw_charts(self.freezer, self.step, job_config)
[rank2]:[rank2]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank2]:[rank2]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank2]:[rank2]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank2]:[rank2]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank2]:[rank2]:                         ~~~~~~~~~~~~^^^
[rank2]:[rank2]: IndexError: list index out of range
[rank3]:wandb: updating run metadata
[rank0]:[rank0]:[W1024 17:36:08.331376874 ProcessGroupNCCL.cpp:1552] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:wandb: uploading history steps 4-4, summary, console lines 176-176
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà
[rank3]:wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 0.43104
[rank3]:wandb: loss_metrics/global_avg_loss 1.04522
[rank3]:wandb: loss_metrics/global_max_loss 1.04522
[rank3]:wandb:                           lr 1e-05
[rank3]:wandb:         memory/max_active(%) 62.44231
[rank3]:wandb:       memory/max_active(GiB) 87.30143
[rank3]:wandb:       memory/max_reserved(%) 66.00552
[rank3]:wandb:     memory/max_reserved(GiB) 92.2832
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1024_GPipe_nofreeze_h200 at: https://wandb.ai/orangingq/torchtitan/runs/n0euiveq
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1024_GPipe_nofreeze_h200/20251024-1702/wandb/run-20251024_170238-n0euiveq/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:    draw_charts(self.freezer, self.step, job_config)
[rank3]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:    pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:                                                                      ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:    schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:                        ~~~~~~~~~~~~^^^
[rank3]:IndexError: list index out of range
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:[rank3]:     draw_charts(self.freezer, self.step, job_config)
[rank3]:[rank3]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:[rank3]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:[rank3]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:[rank3]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:[rank3]:                         ~~~~~~~~~~~~^^^
[rank3]:[rank3]: IndexError: list index out of range
W1024 17:36:11.479000 962994 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 963070 closing signal SIGTERM
W1024 17:36:11.480000 962994 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 963072 closing signal SIGTERM
W1024 17:36:11.480000 962994 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 963073 closing signal SIGTERM
E1024 17:36:13.069000 962994 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 1 (pid: 963071) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1024 17:36:13.087000 962994 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_kkw8pr1k/a15bcafd-c1a4-41ec-ba74-52b2432b6de3_5aicnwxl/attempt_0/1/error.json)
[rank1]:Stage 1: Modules to keep: {'layers.16', 'layers.10', 'layers.9', 'layers.13', 'layers.8', 'layers.11', 'layers.12', 'layers.14', 'layers.15'}
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.7', 'layers.5', 'layers.3', 'layers.0', 'layers.1', 'layers.6', 'layers.4', 'layers.2'}
[rank2]:Stage 2: Modules to keep: {'layers.22', 'layers.21', 'layers.17', 'layers.19', 'layers.20', 'layers.23', 'layers.24', 'layers.18'}
[rank3]:Stage 3: Modules to keep: {'layers.30', 'layers.31', 'norm', 'layers.29', 'layers.27', 'output', 'layers.25', 'layers.26', 'layers.28'}
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1024_GPipe_nofreeze_h200
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: profile_trace/1024_GPipe_nofreeze_h200
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: memory_snapshot/1024_GPipe_nofreeze_h200
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: tb/1024_GPipe_nofreeze_h200
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1024_GPipe_nofreeze_h200
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 1e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 2000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- folder: checkpoint/1024_GPipe_nofreeze_h200
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: comm_traces/1024_GPipe_nofreeze_h200
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 8
[rank3]:		- seq_len: 2048
[rank3]:		- freq: 10
[rank3]:		- steps: -1
[rank3]:	- freezing:
[rank3]:		- freeze: False
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-24_17:36:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 963070)
  error_file: /tmp/torchelastic_kkw8pr1k/a15bcafd-c1a4-41ec-ba74-52b2432b6de3_5aicnwxl/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[2]:
  time      : 2025-10-24_17:36:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 963072)
  error_file: /tmp/torchelastic_kkw8pr1k/a15bcafd-c1a4-41ec-ba74-52b2432b6de3_5aicnwxl/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[3]:
  time      : 2025-10-24_17:36:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 963073)
  error_file: /tmp/torchelastic_kkw8pr1k/a15bcafd-c1a4-41ec-ba74-52b2432b6de3_5aicnwxl/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-24_17:36:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 963071)
  error_file: /tmp/torchelastic_kkw8pr1k/a15bcafd-c1a4-41ec-ba74-52b2432b6de3_5aicnwxl/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Fri Oct 24 17:36:14 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_GPipe_fullrand7.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with fullrand7 x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1024 17:36:15.804000 977811 site-packages/torch/distributed/run.py:811] 
W1024 17:36:15.804000 977811 site-packages/torch/distributed/run.py:811] *****************************************
W1024 17:36:15.804000 977811 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1024 17:36:15.804000 977811 site-packages/torch/distributed/run.py:811] *****************************************
[rank1]:2025-10-24 17:36:21,840 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-24 17:36:22,029 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-24 17:36:21,960 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-24 17:36:21,981 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-24 17:36:22,512 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-24 17:36:22,514 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank2]:2025-10-24 17:36:22,910 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-24 17:36:22,913 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:36:22,892 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-24 17:36:22,895 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:36:22,900 - INFO - [GC] Initial GC collection 0.00 seconds
[rank3]:2025-10-24 17:36:22,885 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-24 17:36:22,888 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 17:36:24,640 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-24 17:36:24,924 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank2]:2025-10-24 17:36:26,837 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-24 17:36:26,910 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-24 17:36:26,910 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-10-24 17:36:26,925 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-24 17:36:26,999 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-24 17:36:26,999 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-10-24 17:36:27,118 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-24 17:36:27,119 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank1]:2025-10-24 17:36:27,209 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-24 17:36:27,209 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank0]:2025-10-24 17:36:27,707 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank0]:2025-10-24 17:36:27,971 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-24 17:36:28,022 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-24 17:36:28,054 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-24 17:36:28,054 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 17:36:28,283 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-24 17:36:28,283 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: setting up run qtdfqe8e
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1024_GPipe_fullrand7_h200/20251024-1736/wandb/run-20251024_173627-qtdfqe8e
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1024_GPipe_fullrand7_h200
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/qtdfqe8e
[rank3]:2025-10-24 17:36:28,766 - INFO - WandB logging enabled
[rank3]:2025-10-24 17:36:28,769 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-24 17:36:28,842 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-24 17:36:28,843 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 17:36:29,106 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1024_GPipe_fullrand7_h200
[rank0]:2025-10-24 17:36:29,106 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-24 17:36:29,106 - INFO - Mixed precision training is disabled
[rank0]:2025-10-24 17:36:29,106 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 2000 (warmup 100)
[rank0]:2025-10-24 17:36:29,106 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank3]:2025-10-24 17:36:29,083 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-24 17:36:29,084 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-24 17:36:38,112 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-24 17:36:38,112 - INFO - Finished loading the checkpoint in 9.01 seconds.
[rank0]:2025-10-24 17:36:38,112 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:2025-10-24 17:37:01,635 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 61.95GiB(44.31%)  tps: 487  tflops: 22.73  mfu: 2.30%
[rank2]:2025-10-24 17:37:01,614 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 58.03GiB(41.50%)  tps: 472  tflops: 22.00  mfu: 2.22%
[rank2]:2025-10-24 17:37:01,614 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-24 17:37:01,635 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-24 17:37:01,618 - INFO -  step:  1  loss:  1.5658  grad_norm:  3.0754  memory: 77.01GiB(55.08%)  tps: 500  tflops: 23.30  mfu: 2.36%
[rank3]:2025-10-24 17:37:01,618 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-24 17:37:01,615 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 65.20GiB(46.63%)  tps: 473  tflops: 22.06  mfu: 2.23%
[rank1]:2025-10-24 17:37:01,616 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-24 17:44:03,476 - INFO -  step: 20  loss:  1.1858  grad_norm:  1.1848  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank1]:2025-10-24 17:44:03,473 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank0]:2025-10-24 17:44:03,477 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank2]:2025-10-24 17:44:03,471 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank0]:2025-10-24 17:51:27,723 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.40  mfu: 3.48%
[rank2]:2025-10-24 17:51:27,716 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.40  mfu: 3.48%
[rank1]:2025-10-24 17:51:27,719 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5193  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.40  mfu: 3.48%
[rank3]:2025-10-24 17:51:27,721 - INFO -  step: 40  loss:  1.0795  grad_norm:  0.5193  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.40  mfu: 3.48%
[rank0]:2025-10-24 17:54:47,557 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-24 17:58:51,861 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4486  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank1]:2025-10-24 17:58:51,864 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4486  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank3]:2025-10-24 17:58:51,866 - INFO -  step: 60  loss:  1.0643  grad_norm:  0.4486  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank0]:2025-10-24 17:58:51,868 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4486  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank0]:2025-10-24 18:06:16,216 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 78.87GiB(56.41%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank2]:2025-10-24 18:06:16,210 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 71.04GiB(50.81%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank1]:2025-10-24 18:06:16,212 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 79.85GiB(57.11%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank3]:2025-10-24 18:06:16,215 - INFO -  step: 80  loss:  1.0454  grad_norm:  0.4310  memory: 92.28GiB(66.01%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank3]:2025-10-24 18:07:04,375 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-24 18:07:05,589 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-24 18:07:05,922 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-24 18:07:06,221 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-24 18:09:58,439 - INFO - Destroying the purge thread.
[rank2]:[rank2]: Traceback (most recent call last):
[rank2]:[rank2]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank2]:[rank2]:   File "<frozen runpy>", line 88, in _run_code
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank2]:[rank2]:     trainer.train()
[rank2]:[rank2]:     ~~~~~~~~~~~~~^^
[rank0]:2025-10-24 18:09:58,439 - INFO - Destroying the purge thread.
[rank0]:[rank0]: Traceback (most recent call last):
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank2]:[rank2]:     return f(*args, **kwargs)
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank2]:[rank2]:     draw_charts(self.freezer, self.step, job_config)
[rank2]:[rank2]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:     ~~~~~~~~~~~~~^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank2]:[rank2]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank2]:[rank2]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank2]:[rank2]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank2]:[rank2]:                         ~~~~~~~~~~~~^^^
[rank2]:[rank2]: IndexError: list index out of range
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank0]:[rank0]:     draw_charts(self.freezer, self.step, job_config)
[rank0]:[rank0]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank0]:[rank0]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank0]:[rank0]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank0]:[rank0]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank0]:[rank0]:                         ~~~~~~~~~~~~^^^
[rank0]:[rank0]: IndexError: list index out of range
[rank3]:2025-10-24 18:09:58,439 - INFO - Destroying the purge thread.
[rank1]:2025-10-24 18:09:58,439 - INFO - Destroying the purge thread.
[rank1]:[rank1]: Traceback (most recent call last):
[rank1]:[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank1]:[rank1]:     trainer.train()
[rank1]:[rank1]:     ~~~~~~~~~~~~~^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank1]:[rank1]:     return f(*args, **kwargs)
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank1]:[rank1]:     draw_charts(self.freezer, self.step, job_config)
[rank1]:[rank1]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank1]:[rank1]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank1]:[rank1]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank1]:[rank1]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank1]:[rank1]:                         ~~~~~~~~~~~~^^^
[rank1]:[rank1]: IndexError: list index out of range
[rank3]:wandb: updating run metadata
[rank0]:[rank0]:[W1024 18:09:59.431445687 ProcessGroupNCCL.cpp:1552] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà
[rank3]:wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 0.43098
[rank3]:wandb: loss_metrics/global_avg_loss 1.04544
[rank3]:wandb: loss_metrics/global_max_loss 1.04544
[rank3]:wandb:                           lr 1e-05
[rank3]:wandb:         memory/max_active(%) 62.44231
[rank3]:wandb:       memory/max_active(GiB) 87.30143
[rank3]:wandb:       memory/max_reserved(%) 66.00552
[rank3]:wandb:     memory/max_reserved(GiB) 92.2832
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1024_GPipe_fullrand7_h200 at: https://wandb.ai/orangingq/torchtitan/runs/qtdfqe8e
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1024_GPipe_fullrand7_h200/20251024-1736/wandb/run-20251024_173627-qtdfqe8e/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:    draw_charts(self.freezer, self.step, job_config)
[rank3]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:    pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:                                                                      ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:    schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:                        ~~~~~~~~~~~~^^^
[rank3]:IndexError: list index out of range
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:[rank3]:     draw_charts(self.freezer, self.step, job_config)
[rank3]:[rank3]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:[rank3]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:[rank3]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:[rank3]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:[rank3]:                         ~~~~~~~~~~~~^^^
[rank3]:[rank3]: IndexError: list index out of range
W1024 18:10:01.814000 977811 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 977887 closing signal SIGTERM
W1024 18:10:01.814000 977811 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 977888 closing signal SIGTERM
W1024 18:10:01.815000 977811 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 977889 closing signal SIGTERM
E1024 18:10:03.399000 977811 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 0 (pid: 977886) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1024 18:10:03.416000 977811 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_p6kt7uoa/3ab06b7c-3597-4a80-a841-943bcb280fb8_gpkcdopi/attempt_0/0/error.json)
[rank2]:Stage 2: Modules to keep: {'layers.21', 'layers.20', 'layers.23', 'layers.22', 'layers.24', 'layers.19', 'layers.18', 'layers.17'}
[rank1]:Stage 1: Modules to keep: {'layers.9', 'layers.11', 'layers.16', 'layers.15', 'layers.8', 'layers.13', 'layers.14', 'layers.10', 'layers.12'}
[rank0]:Stage 0: Modules to keep: {'tok_embeddings', 'layers.4', 'layers.0', 'layers.6', 'layers.1', 'layers.7', 'layers.3', 'layers.5', 'layers.2'}
[rank3]:Stage 3: Modules to keep: {'output', 'layers.27', 'norm', 'layers.29', 'layers.28', 'layers.30', 'layers.31', 'layers.25', 'layers.26'}
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1024_GPipe_fullrand7_h200
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: profile_trace/1024_GPipe_fullrand7_h200
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: memory_snapshot/1024_GPipe_fullrand7_h200
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: tb/1024_GPipe_fullrand7_h200
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1024_GPipe_fullrand7_h200
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 1e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 2000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- folder: checkpoint/1024_GPipe_fullrand7_h200
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: comm_traces/1024_GPipe_fullrand7_h200
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 8
[rank3]:		- seq_len: 2048
[rank3]:		- freq: 10
[rank3]:		- steps: -1
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: fullrand7
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-24_18:09:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 977887)
  error_file: /tmp/torchelastic_p6kt7uoa/3ab06b7c-3597-4a80-a841-943bcb280fb8_gpkcdopi/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[2]:
  time      : 2025-10-24_18:09:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 977888)
  error_file: /tmp/torchelastic_p6kt7uoa/3ab06b7c-3597-4a80-a841-943bcb280fb8_gpkcdopi/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[3]:
  time      : 2025-10-24_18:09:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 977889)
  error_file: /tmp/torchelastic_p6kt7uoa/3ab06b7c-3597-4a80-a841-943bcb280fb8_gpkcdopi/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-24_18:09:58
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 977886)
  error_file: /tmp/torchelastic_p6kt7uoa/3ab06b7c-3597-4a80-a841-943bcb280fb8_gpkcdopi/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
============================================================

üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
‚úîÔ∏èCurrent Timestamp: Fri Oct 24 18:10:04 UTC 2025
‚úîÔ∏èSERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
‚úîÔ∏èSCRIPT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/run.sh
‚úîÔ∏èOUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/1024_GPipe_apf.log
‚úîÔ∏èLlama 3.1 8B Instruct Experiment
‚úîÔ∏èRunning with apf x GPipe ... 
‚òëÔ∏è> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml --job.description="Llama 3.1 8B Instruct Experiment" --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=apf
üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•
W1024 18:10:06.005000 992521 site-packages/torch/distributed/run.py:811] 
W1024 18:10:06.005000 992521 site-packages/torch/distributed/run.py:811] *****************************************
W1024 18:10:06.005000 992521 site-packages/torch/distributed/run.py:811] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1024 18:10:06.005000 992521 site-packages/torch/distributed/run.py:811] *****************************************
[rank3]:2025-10-24 18:10:11,906 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank2]:2025-10-24 18:10:11,977 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank1]:2025-10-24 18:10:11,994 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank0]:2025-10-24 18:10:12,061 - INFO - Starting job: "Llama 3.1 8B Instruct Experiment"
[rank3]:2025-10-24 18:10:12,738 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:2025-10-24 18:10:12,741 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 18:10:12,898 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:2025-10-24 18:10:12,900 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 18:10:12,906 - INFO - [GC] Initial GC collection 0.00 seconds
[rank2]:2025-10-24 18:10:12,870 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:2025-10-24 18:10:12,873 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank1]:2025-10-24 18:10:12,862 - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:2025-10-24 18:10:12,864 - INFO - Building 1-D device mesh with ['pp'], [4]
[rank0]:2025-10-24 18:10:14,705 - INFO - Loading tokenizer from tokenizer.json
[rank0]:2025-10-24 18:10:15,003 - INFO - Preparing alpaca dataset from tatsu-lab/alpaca
[rank0]:2025-10-24 18:10:16,854 - INFO - Building llama3 8B with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_seq_len=1024, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)
[rank1]:2025-10-24 18:10:16,929 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank2]:2025-10-24 18:10:16,922 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank1]:2025-10-24 18:10:17,028 - INFO - PP rank 1 is building stage_idx 1 with modules ['layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16']
[rank1]:2025-10-24 18:10:17,028 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank2]:2025-10-24 18:10:17,031 - INFO - PP rank 2 is building stage_idx 2 with modules ['layers.17', 'layers.18', 'layers.19', 'layers.20', 'layers.21', 'layers.22', 'layers.23', 'layers.24']
[rank2]:2025-10-24 18:10:17,031 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 18:10:17,109 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank0]:2025-10-24 18:10:17,160 - INFO - Model llama3 8B size: 8,030,261,248 total parameters
[rank0]:2025-10-24 18:10:17,184 - INFO - PP rank 0 is building stage_idx 0 with modules ['tok_embeddings', 'layers.0', 'layers.1', 'layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7']
[rank0]:2025-10-24 18:10:17,185 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank1]:2025-10-24 18:10:17,308 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank1]:2025-10-24 18:10:17,308 - INFO - CUDA memory usage for model: 7.33GiB(5.24%)
[rank2]:2025-10-24 18:10:17,319 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank2]:2025-10-24 18:10:17,319 - INFO - CUDA memory usage for model: 6.51GiB(4.65%)
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank3]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
[rank3]:  warnings.warn(
[rank0]:2025-10-24 18:10:17,406 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank0]:2025-10-24 18:10:17,406 - INFO - CUDA memory usage for model: 8.46GiB(6.05%)
[rank3]:wandb: Currently logged in as: orangingq to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank3]:wandb: setting up run mtwtry8u
[rank3]:wandb: Tracking run with wandb version 0.22.2
[rank3]:wandb: Run data is saved locally in /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/tb/1024_GPipe_apf_h200/20251024-1810/wandb/run-20251024_181018-mtwtry8u
[rank3]:wandb: Run `wandb offline` to turn off syncing.
[rank3]:wandb: Syncing run 1024_GPipe_apf_h200
[rank3]:wandb: ‚≠êÔ∏è View project at https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: üöÄ View run at https://wandb.ai/orangingq/torchtitan/runs/mtwtry8u
[rank3]:2025-10-24 18:10:19,105 - INFO - WandB logging enabled
[rank3]:2025-10-24 18:10:19,106 - INFO - CUDA capacity: NVIDIA H200 with 139.81GiB memory
[rank3]:2025-10-24 18:10:19,180 - INFO - PP rank 3 is building stage_idx 3 with modules ['layers.25', 'layers.26', 'layers.27', 'layers.28', 'layers.29', 'layers.30', 'layers.31', 'norm', 'output']
[rank3]:2025-10-24 18:10:19,181 - INFO - Using pipeline schedule GPipe with 8 microbatches and 4 stages.
[rank0]:2025-10-24 18:10:19,423 - INFO - Checkpointing active. Checkpoints will be loaded from and saved to /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/checkpoint/1024_GPipe_apf_h200
[rank0]:2025-10-24 18:10:19,423 - WARNING - Mixed precision training with TP or PP is only supported when FSDP/HSDP/CP is enabled.
[rank0]:2025-10-24 18:10:19,423 - INFO - Mixed precision training is disabled
[rank0]:2025-10-24 18:10:19,423 - INFO - Trainer is initialized with local batch size 16, global batch size 64, gradient accumulation steps 4, sequence length 1024, total steps 2000 (warmup 100)
[rank0]:2025-10-24 18:10:19,423 - INFO - Loading the checkpoint from /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp.
[rank3]:2025-10-24 18:10:19,411 - INFO - Peak FLOPS used for computing MFU: 9.890e+14
[rank3]:2025-10-24 18:10:19,411 - INFO - CUDA memory usage for model: 7.66GiB(5.48%)
[rank0]:2025-10-24 18:10:28,595 - INFO - [GC] GC collection for checkpoint loading. 0.00 seconds
[rank0]:2025-10-24 18:10:28,595 - INFO - Finished loading the checkpoint in 9.17 seconds.
[rank0]:2025-10-24 18:10:28,595 - INFO - Training starts at step 1
[rank0]:/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/autograd/graph.py:849: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)
[rank0]:  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:2025-10-24 18:10:52,049 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 58.03GiB(41.50%)  tps: 468  tflops: 21.81  mfu: 2.20%
[rank2]:2025-10-24 18:10:52,050 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank1]:2025-10-24 18:10:52,050 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 65.20GiB(46.63%)  tps: 468  tflops: 21.80  mfu: 2.20%
[rank1]:2025-10-24 18:10:52,050 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank3]:2025-10-24 18:10:52,053 - INFO -  step:  1  loss:  1.5658  grad_norm:  3.0754  memory: 77.01GiB(55.08%)  tps: 498  tflops: 23.23  mfu: 2.35%
[rank3]:2025-10-24 18:10:52,053 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank0]:2025-10-24 18:10:52,069 - INFO -  step:  1  loss: -4.0000  grad_norm:  3.0754  memory: 61.95GiB(44.31%)  tps: 469  tflops: 21.89  mfu: 2.21%
[rank0]:2025-10-24 18:10:52,070 - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:05:00
[rank2]:2025-10-24 18:17:53,928 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank3]:2025-10-24 18:17:53,934 - INFO -  step: 20  loss:  1.1858  grad_norm:  1.1848  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank0]:2025-10-24 18:17:53,934 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.42  mfu: 3.48%
[rank1]:2025-10-24 18:17:53,930 - INFO -  step: 20  loss: -4.0000  grad_norm:  1.1848  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank1]:2025-10-24 18:25:18,293 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5188  memory: 79.85GiB(57.11%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank3]:2025-10-24 18:25:18,296 - INFO -  step: 40  loss:  1.0795  grad_norm:  0.5188  memory: 92.28GiB(66.01%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank2]:2025-10-24 18:25:18,290 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5188  memory: 71.04GiB(50.81%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank0]:2025-10-24 18:25:18,297 - INFO -  step: 40  loss: -4.0000  grad_norm:  0.5188  memory: 78.87GiB(56.41%)  tps: 737  tflops: 34.39  mfu: 3.48%
[rank0]:2025-10-24 18:28:38,122 - INFO - [GC] Peforming periodical GC collection 0.00 seconds
[rank2]:2025-10-24 18:32:42,482 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4485  memory: 71.04GiB(50.81%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank3]:2025-10-24 18:32:42,488 - INFO -  step: 60  loss:  1.0643  grad_norm:  0.4485  memory: 92.28GiB(66.01%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank0]:2025-10-24 18:32:42,489 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4485  memory: 78.87GiB(56.41%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank1]:2025-10-24 18:32:42,485 - INFO -  step: 60  loss: -4.0000  grad_norm:  0.4485  memory: 79.85GiB(57.11%)  tps: 738  tflops: 34.41  mfu: 3.48%
[rank2]:2025-10-24 18:40:07,035 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 71.04GiB(50.81%)  tps: 737  tflops: 34.38  mfu: 3.48%
[rank0]:2025-10-24 18:40:07,041 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 78.87GiB(56.41%)  tps: 737  tflops: 34.38  mfu: 3.48%
[rank3]:2025-10-24 18:40:07,041 - INFO -  step: 80  loss:  1.0452  grad_norm:  0.4310  memory: 92.28GiB(66.01%)  tps: 737  tflops: 34.38  mfu: 3.48%
[rank1]:2025-10-24 18:40:07,037 - INFO -  step: 80  loss: -4.0000  grad_norm:  0.4310  memory: 79.85GiB(57.11%)  tps: 737  tflops: 34.38  mfu: 3.48%
[rank3]:2025-10-24 18:40:55,184 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-24 18:40:56,397 - WARNING - Dataset alpaca is being re-looped
[rank1]:2025-10-24 18:40:56,731 - WARNING - Dataset alpaca is being re-looped
[rank0]:2025-10-24 18:40:57,029 - WARNING - Dataset alpaca is being re-looped
[rank2]:2025-10-24 18:43:49,128 - INFO - Destroying the purge thread.
[rank2]:[rank2]: Traceback (most recent call last):
[rank2]:[rank2]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank2]:[rank2]:   File "<frozen runpy>", line 88, in _run_code
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank2]:[rank2]:     trainer.train()
[rank2]:[rank2]:     ~~~~~~~~~~~~~^^
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank2]:[rank2]:     return f(*args, **kwargs)
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank2]:[rank2]:     draw_charts(self.freezer, self.step, job_config)
[rank2]:[rank2]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:2025-10-24 18:43:49,128 - INFO - Destroying the purge thread.
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank0]:2025-10-24 18:43:49,128 - INFO - Destroying the purge thread.
[rank2]:[rank2]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank0]:[rank0]: Traceback (most recent call last):
[rank0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank2]:[rank2]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank2]:[rank2]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank0]:[rank0]:     trainer.train()
[rank0]:[rank0]:     ~~~~~~~~~~~~~^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank0]:[rank0]:     return f(*args, **kwargs)
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank0]:[rank0]:     draw_charts(self.freezer, self.step, job_config)
[rank0]:[rank0]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank0]:[rank0]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank0]:[rank0]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:[rank2]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank0]:[rank0]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank0]:[rank0]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank0]:[rank0]:                         ~~~~~~~~~~~~^^^
[rank0]:[rank0]: IndexError: list index out of range
[rank2]:[rank2]:                         ~~~~~~~~~~~~^^^
[rank2]:[rank2]: IndexError: list index out of range
[rank1]:2025-10-24 18:43:49,128 - INFO - Destroying the purge thread.
[rank1]:[rank1]: Traceback (most recent call last):
[rank1]:[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:[rank1]:   File "<frozen runpy>", line 88, in _run_code
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank1]:[rank1]:     trainer.train()
[rank1]:[rank1]:     ~~~~~~~~~~~~~^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank1]:[rank1]:     return f(*args, **kwargs)
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank1]:[rank1]:     draw_charts(self.freezer, self.step, job_config)
[rank1]:[rank1]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank1]:[rank1]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank1]:[rank1]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:[rank1]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank1]:[rank1]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank1]:[rank1]:                         ~~~~~~~~~~~~^^^
[rank1]:[rank1]: IndexError: list index out of range
[rank0]:[rank0]:[W1024 18:43:49.152749635 ProcessGroupNCCL.cpp:1552] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:wandb: uploading output.log; uploading config.yaml
[rank3]:wandb: 
[rank3]:wandb: Run history:
[rank3]:wandb:                    grad_norm ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_avg_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb: loss_metrics/global_max_loss ‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           lr ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñà
[rank3]:wandb:         memory/max_active(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_active(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:       memory/max_reserved(%) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/max_reserved(GiB) ‚ñÅ‚ñà‚ñà‚ñà‚ñà
[rank3]:wandb:     memory/num_alloc_retries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:              memory/num_ooms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: Run summary:
[rank3]:wandb:                    grad_norm 0.431
[rank3]:wandb: loss_metrics/global_avg_loss 1.04522
[rank3]:wandb: loss_metrics/global_max_loss 1.04522
[rank3]:wandb:                           lr 1e-05
[rank3]:wandb:         memory/max_active(%) 62.44231
[rank3]:wandb:       memory/max_active(GiB) 87.30143
[rank3]:wandb:       memory/max_reserved(%) 66.00552
[rank3]:wandb:     memory/max_reserved(GiB) 92.2832
[rank3]:wandb:     memory/num_alloc_retries 0
[rank3]:wandb:              memory/num_ooms 0
[rank3]:wandb:                           +7 ...
[rank3]:wandb: 
[rank3]:wandb: üöÄ View run 1024_GPipe_apf_h200 at: https://wandb.ai/orangingq/torchtitan/runs/mtwtry8u
[rank3]:wandb: ‚≠êÔ∏è View project at: https://wandb.ai/orangingq/torchtitan
[rank3]:wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[rank3]:wandb: Find logs at: ._data/tb/1024_GPipe_apf_h200/20251024-1810/wandb/run-20251024_181018-mtwtry8u/logs
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:    trainer.train()
[rank3]:    ~~~~~~~~~~~~~^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:    draw_charts(self.freezer, self.step, job_config)
[rank3]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:    pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:                                                                      ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:    schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:                        ~~~~~~~~~~~~^^^
[rank3]:IndexError: list index out of range
[rank3]:[rank3]: Traceback (most recent call last):
[rank3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 720, in <module>
[rank3]:[rank3]:     trainer.train()
[rank3]:[rank3]:     ~~~~~~~~~~~~~^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:[rank3]:     return f(*args, **kwargs)
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
[rank3]:[rank3]:     draw_charts(self.freezer, self.step, job_config)
[rank3]:[rank3]:     ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
[rank3]:[rank3]:     pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
[rank3]:[rank3]:                                                                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:[rank3]:   File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
[rank3]:[rank3]:     schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
[rank3]:[rank3]:                         ~~~~~~~~~~~~^^^
[rank3]:[rank3]: IndexError: list index out of range
W1024 18:43:52.020000 992521 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 992595 closing signal SIGTERM
W1024 18:43:52.021000 992521 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 992597 closing signal SIGTERM
W1024 18:43:52.021000 992521 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 992598 closing signal SIGTERM
E1024 18:43:54.071000 992521 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 1 (pid: 992596) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1024 18:43:54.088000 992521 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_hti7sech/4cc4fc4b-84fe-4c63-aa8b-e8ceb29642e5_ok2fnjvh/attempt_0/1/error.json)
[rank1]:Stage 1: Modules to keep: {'layers.12', 'layers.14', 'layers.10', 'layers.9', 'layers.16', 'layers.15', 'layers.13', 'layers.8', 'layers.11'}
[rank2]:Stage 2: Modules to keep: {'layers.19', 'layers.24', 'layers.17', 'layers.20', 'layers.18', 'layers.22', 'layers.21', 'layers.23'}
[rank0]:Stage 0: Modules to keep: {'layers.1', 'layers.5', 'tok_embeddings', 'layers.6', 'layers.4', 'layers.7', 'layers.0', 'layers.2', 'layers.3'}
[rank3]:Stage 3: Modules to keep: {'layers.30', 'output', 'layers.28', 'layers.29', 'layers.25', 'norm', 'layers.31', 'layers.26', 'layers.27'}
[rank2]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank3]:----- TimelyFreeze‚è∞ Configuration:
[rank3]:	- job:
[rank3]:		- config_file: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b/config.toml
[rank3]:		- dump_folder: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data
[rank3]:		- description: "Llama 3.1 8B Instruct Experiment"
[rank3]:		- use_for_integration_test: False
[rank3]:		- print_args: False
[rank3]:		- basename: 1024_GPipe_apf_h200
[rank3]:	- profiling:
[rank3]:		- enable_profiling: False
[rank3]:		- save_traces_folder: profile_trace/1024_GPipe_apf_h200
[rank3]:		- profile_freq: 100
[rank3]:		- enable_memory_snapshot: False
[rank3]:		- save_memory_snapshot_folder: memory_snapshot/1024_GPipe_apf_h200
[rank3]:	- metrics:
[rank3]:		- log_freq: 20
[rank3]:		- enable_tensorboard: False
[rank3]:		- disable_color_printing: True
[rank3]:		- save_tb_folder: tb/1024_GPipe_apf_h200
[rank3]:		- save_for_all_ranks: False
[rank3]:		- enable_wandb: True
[rank3]:		- pplog_freq: 80
[rank3]:		- draw_freq: 100
[rank3]:		- wandb_name: 1024_GPipe_apf_h200
[rank3]:		- draw_graph: True
[rank3]:	- model:
[rank3]:		- name: llama3
[rank3]:		- flavor: 8B
[rank3]:		- tokenizer_path: ./assets/tokenizer/Llama-3.1-8B-Instruct
[rank3]:		- converters: []
[rank3]:		- print_after_conversion: False
[rank3]:	- optimizer:
[rank3]:		- name: AdamW
[rank3]:		- lr: 1e-05
[rank3]:		- beta1: 0.9
[rank3]:		- beta2: 0.95
[rank3]:		- eps: 1e-08
[rank3]:		- weight_decay: 0.1
[rank3]:		- implementation: fused
[rank3]:		- early_step_in_backward: False
[rank3]:	- lr_scheduler:
[rank3]:		- warmup_steps: 100
[rank3]:		- decay_ratio: 0.8
[rank3]:		- decay_type: cosine
[rank3]:		- min_lr_factor: 0.0
[rank3]:	- training:
[rank3]:		- dataset: alpaca
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 16
[rank3]:		- global_batch_size: 64
[rank3]:		- seq_len: 1024
[rank3]:		- max_norm: 1.0
[rank3]:		- steps: 2000
[rank3]:		- enable_cpu_offload: False
[rank3]:		- mixed_precision_param: bfloat16
[rank3]:		- mixed_precision_reduce: float32
[rank3]:		- compile: False
[rank3]:		- gc_freq: 50
[rank3]:		- gc_debug: False
[rank3]:		- seed: None
[rank3]:		- deterministic: False
[rank3]:	- parallelism:
[rank3]:		- data_parallel_replicate_degree: 1
[rank3]:		- enable_compiled_autograd: False
[rank3]:		- data_parallel_shard_degree: -1
[rank3]:		- fsdp_reshard_after_forward: default
[rank3]:		- tensor_parallel_degree: 1
[rank3]:		- disable_loss_parallel: False
[rank3]:		- enable_async_tensor_parallel: False
[rank3]:		- pipeline_parallel_degree: 4
[rank3]:		- pipeline_parallel_split_points: []
[rank3]:		- module_fqns_per_model_part: None
[rank3]:		- pipeline_parallel_first_stage_less_layers: 1
[rank3]:		- pipeline_parallel_last_stage_less_layers: 1
[rank3]:		- pipeline_parallel_layers_per_stage: None
[rank3]:		- pipeline_parallel_schedule: GPipe
[rank3]:		- pipeline_parallel_schedule_csv: 
[rank3]:		- pipeline_parallel_microbatch_size: 2
[rank3]:		- context_parallel_degree: 1
[rank3]:		- context_parallel_rotate_method: allgather
[rank3]:		- expert_parallel_degree: 1
[rank3]:		- pp: 4
[rank3]:		- stages_per_rank: 1
[rank3]:		- stages_list: [3]
[rank3]:		- microbatches: 8
[rank3]:	- checkpoint:
[rank3]:		- enable_checkpoint: True
[rank3]:		- folder: checkpoint/1024_GPipe_apf_h200
[rank3]:		- interval: 500
[rank3]:		- initial_load_path: /opt/dlami/nvme/DMLAB/shcho/torchtitan_data/base_model/Llama-3.1-8B-Instruct/original_dcp
[rank3]:		- initial_load_model_only: True
[rank3]:		- initial_load_in_hf: False
[rank3]:		- last_save_model_only: True
[rank3]:		- last_save_in_hf: False
[rank3]:		- export_dtype: float16
[rank3]:		- async_mode: async
[rank3]:		- keep_latest_k: 2
[rank3]:		- load_step: -1
[rank3]:		- exclude_from_loading: ['dataloader', 'lr_scheduler', 'optimizer', 'train_state']
[rank3]:		- enable_first_step_checkpoint: False
[rank3]:		- create_seed_checkpoint: False
[rank3]:	- activation_checkpoint:
[rank3]:		- mode: none
[rank3]:		- selective_ac_option: op
[rank3]:		- per_op_sac_force_recompute_mm_shapes_by_fqns: ['moe.router.gate']
[rank3]:	- float8:
[rank3]:		- enable_fsdp_float8_all_gather: False
[rank3]:		- precompute_float8_dynamic_scale_for_fsdp: False
[rank3]:		- recipe_name: None
[rank3]:		- filter_fqns: ['output']
[rank3]:		- emulate: False
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- mx:
[rank3]:		- mxfp8_dim1_cast_kernel_choice: triton
[rank3]:		- recipe_name: mxfp8
[rank3]:		- filter_fqns: ['output']
[rank3]:		- moe_fqns_prototype: []
[rank3]:	- comm:
[rank3]:		- init_timeout_seconds: 300
[rank3]:		- train_timeout_seconds: 300
[rank3]:		- trace_buf_size: 20000
[rank3]:		- save_traces_folder: comm_traces/1024_GPipe_apf_h200
[rank3]:		- world_size: 4
[rank3]:		- global_rank: 3
[rank3]:		- local_rank: 3
[rank3]:		- dp: 1
[rank3]:		- pp: 4
[rank3]:		- pp_rank: 3
[rank3]:		- dp_rank: 0
[rank3]:		- master_dp_rank: 0
[rank3]:	- memory_estimation:
[rank3]:		- enabled: False
[rank3]:		- disable_fake_mode: False
[rank3]:	- fault_tolerance:
[rank3]:		- enable: False
[rank3]:		- process_group: gloo
[rank3]:		- process_group_timeout_ms: 10000
[rank3]:		- replica_id: 0
[rank3]:		- group_size: 0
[rank3]:		- min_replica_size: 1
[rank3]:		- semi_sync_method: None
[rank3]:		- sync_steps: 5
[rank3]:		- should_quantize: False
[rank3]:		- fragment_sync_delay: 0
[rank3]:		- fragment_update_alpha: 0.0
[rank3]:	- experimental:
[rank3]:		- custom_import: 
[rank3]:		- custom_args_module: 
[rank3]:	- validation:
[rank3]:		- enabled: False
[rank3]:		- dataset: c4_validation
[rank3]:		- dataset_path: None
[rank3]:		- local_batch_size: 8
[rank3]:		- seq_len: 2048
[rank3]:		- freq: 10
[rank3]:		- steps: -1
[rank3]:	- freezing:
[rank3]:		- freeze: True
[rank3]:		- metric_type: apf
[rank3]:		- phase_unit: 10
[rank3]:		- stability_check_freq: 5
[rank3]:		- aggressiveness: 0
[rank3]:
[rank3]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[rank0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-24_18:43:49
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 992595)
  error_file: /tmp/torchelastic_hti7sech/4cc4fc4b-84fe-4c63-aa8b-e8ceb29642e5_ok2fnjvh/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[2]:
  time      : 2025-10-24_18:43:49
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 992597)
  error_file: /tmp/torchelastic_hti7sech/4cc4fc4b-84fe-4c63-aa8b-e8ceb29642e5_ok2fnjvh/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
[3]:
  time      : 2025-10-24_18:43:49
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 992598)
  error_file: /tmp/torchelastic_hti7sech/4cc4fc4b-84fe-4c63-aa8b-e8ceb29642e5_ok2fnjvh/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-24_18:43:49
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 992596)
  error_file: /tmp/torchelastic_hti7sech/4cc4fc4b-84fe-4c63-aa8b-e8ceb29642e5_ok2fnjvh/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 619, in train
      draw_charts(self.freezer, self.step, job_config)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 651, in draw_charts
      pipeline_schedule :List[List[ActionWithTime]] = schedule_pipeline(gather_pipeline_schedule(pplog.pipeline_log.log_schedule, config.comm))
                                                                        ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/core/schedule.py", line 31, in gather_pipeline_schedule
      schedule_info = len(log_schedule[0].to_tensor(with_median=True)) # 5
                          ~~~~~~~~~~~~^^^
  IndexError: list index out of range
  
============================================================
‚úÖ All runs completed. Logs saved in /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1024_llama8b.
