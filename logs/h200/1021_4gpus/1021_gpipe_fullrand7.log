
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
âœ”ï¸Current Timestamp: Mon Oct 20 16:12:00 UTC 2025
âœ”ï¸SERVER: wbl-kaist-gpu-2 (172.16.131.118),  GPUs: 4,5,6,7
âœ”ï¸SCRIPT: 
âœ”ï¸OUTPUT: /opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1021_4gpus/1021_gpipe_fullrand7.log
âœ”ï¸Main Table Experiment
âœ”ï¸Running with fullrand7 x gpipe ... 
â˜‘ï¸> torchrun --standalone --nnodes=1 --nproc_per_node=4 --local_addr=127.0.0.1 --local-ranks-filter=0,1,2,3 --role=rank --tee=3 -m timelyfreeze.train --job.config_file=/opt/dlami/nvme/DMLAB/shcho/torchtitan/logs/h200/1021_4gpus/config.toml --job.description="Main Table Experiment" --training.global_batch_size=512 --training.local_batch_size=16 --parallelism.pipeline_parallel_microbatch_size=1 --training.seq_len=1024 --training.steps=500 --parallelism.pipeline_parallel_degree=4  --freezing.freeze --freezing.metric_type=fullrand7 --freezing.stability_check_freq=50 --freezing.aggressiveness=0.0
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
2025-10-20 16:12:07,885 - Starting job: "Main Table Experiment"
2025-10-20 16:12:07,938 - Starting job: "Main Table Experiment"
[rank3]:[titan] 2025-10-20 16:12:07,938 - root - INFO - Starting job: "Main Table Experiment"
[rank0]:[titan] 2025-10-20 16:12:07,885 - root - INFO - Starting job: "Main Table Experiment"
2025-10-20 16:12:08,013 - Starting job: "Main Table Experiment"
2025-10-20 16:12:08,037 - Starting job: "Main Table Experiment"
[rank2]:[titan] 2025-10-20 16:12:08,013 - root - INFO - Starting job: "Main Table Experiment"
[rank1]:[titan] 2025-10-20 16:12:08,037 - root - INFO - Starting job: "Main Table Experiment"
2025-10-20 16:12:08,646 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:[titan] 2025-10-20 16:12:08,646 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank0]:Traceback (most recent call last):
[rank0]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:  File "<frozen runpy>", line 88, in _run_code
[rank0]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 714, in <module>
[rank0]:    trainer = TrainerWithFreezer(config)
[rank0]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank0]:    return f(*args, **kwargs)
[rank0]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
[rank0]:    dist_utils.init_distributed(
[rank0]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank0]:        job_config.comm,
[rank0]:        ^^^^^^^^^^^^^^^^
[rank0]:        enable_cpu_backend=job_config.training.enable_cpu_offload,
[rank0]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:        base_folder=job_config.job.dump_folder,
[rank0]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:    )
[rank0]:    ^
[rank0]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
[rank0]:    os.makedirs(dump_dir, exist_ok=True)
[rank0]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:  File "<frozen os>", line 218, in makedirs
[rank0]:  File "<frozen os>", line 218, in makedirs
[rank0]:  File "<frozen os>", line 218, in makedirs
[rank0]:  [Previous line repeated 1 more time]
[rank0]:  File "<frozen os>", line 228, in makedirs
[rank0]:PermissionError: [Errno 13] Permission denied: '/data2'
2025-10-20 16:12:08,782 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:[titan] 2025-10-20 16:12:08,782 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank3]:Traceback (most recent call last):
[rank3]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank3]:  File "<frozen runpy>", line 88, in _run_code
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 714, in <module>
[rank3]:    trainer = TrainerWithFreezer(config)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank3]:    return f(*args, **kwargs)
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
[rank3]:    dist_utils.init_distributed(
[rank3]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank3]:        job_config.comm,
[rank3]:        ^^^^^^^^^^^^^^^^
[rank3]:        enable_cpu_backend=job_config.training.enable_cpu_offload,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:        base_folder=job_config.job.dump_folder,
[rank3]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:    )
[rank3]:    ^
[rank3]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
[rank3]:    os.makedirs(dump_dir, exist_ok=True)
[rank3]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:  File "<frozen os>", line 218, in makedirs
[rank3]:  File "<frozen os>", line 218, in makedirs
[rank3]:  File "<frozen os>", line 218, in makedirs
[rank3]:  [Previous line repeated 1 more time]
[rank3]:  File "<frozen os>", line 228, in makedirs
[rank3]:PermissionError: [Errno 13] Permission denied: '/data2'
2025-10-20 16:12:08,884 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2025-10-20 16:12:08,893 - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:[titan] 2025-10-20 16:12:08,884 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank2]:Traceback (most recent call last):
[rank2]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank2]:  File "<frozen runpy>", line 88, in _run_code
[rank2]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 714, in <module>
[rank2]:    trainer = TrainerWithFreezer(config)
[rank2]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank2]:    return f(*args, **kwargs)
[rank2]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
[rank2]:    dist_utils.init_distributed(
[rank2]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank2]:        job_config.comm,
[rank2]:        ^^^^^^^^^^^^^^^^
[rank2]:        enable_cpu_backend=job_config.training.enable_cpu_offload,
[rank2]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:        base_folder=job_config.job.dump_folder,
[rank2]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:    )
[rank2]:    ^
[rank2]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
[rank2]:    os.makedirs(dump_dir, exist_ok=True)
[rank2]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:  File "<frozen os>", line 218, in makedirs
[rank2]:  File "<frozen os>", line 218, in makedirs
[rank2]:  File "<frozen os>", line 218, in makedirs
[rank2]:  [Previous line repeated 1 more time]
[rank2]:  File "<frozen os>", line 228, in makedirs
[rank2]:PermissionError: [Errno 13] Permission denied: '/data2'
[rank1]:[titan] 2025-10-20 16:12:08,893 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
[rank1]:Traceback (most recent call last):
[rank1]:  File "<frozen runpy>", line 198, in _run_module_as_main
[rank1]:  File "<frozen runpy>", line 88, in _run_code
[rank1]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 714, in <module>
[rank1]:    trainer = TrainerWithFreezer(config)
[rank1]:  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
[rank1]:    return f(*args, **kwargs)
[rank1]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
[rank1]:    dist_utils.init_distributed(
[rank1]:    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
[rank1]:        job_config.comm,
[rank1]:        ^^^^^^^^^^^^^^^^
[rank1]:        enable_cpu_backend=job_config.training.enable_cpu_offload,
[rank1]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:        base_folder=job_config.job.dump_folder,
[rank1]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:    )
[rank1]:    ^
[rank1]:  File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
[rank1]:    os.makedirs(dump_dir, exist_ok=True)
[rank1]:    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:  File "<frozen os>", line 218, in makedirs
[rank1]:  File "<frozen os>", line 218, in makedirs
[rank1]:  File "<frozen os>", line 218, in makedirs
[rank1]:  [Previous line repeated 1 more time]
[rank1]:  File "<frozen os>", line 228, in makedirs
[rank1]:PermissionError: [Errno 13] Permission denied: '/data2'
W1020 16:12:10.969000 3908255 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 3908320 closing signal SIGTERM
W1020 16:12:10.970000 3908255 site-packages/torch/distributed/elastic/multiprocessing/api.py:940] Sending process 3908321 closing signal SIGTERM
E1020 16:12:11.362000 3908255 site-packages/torch/distributed/elastic/multiprocessing/api.py:914] failed (exitcode: 1) local_rank: 0 (pid: 3908319) of binary: /opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/python3.13
E1020 16:12:11.377000 3908255 site-packages/torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_sliw4j82/f7e3566e-1054-40fa-9ca9-294a179e3a4e_hszv84ng/attempt_0/0/error.json)
Traceback (most recent call last):
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ~~~~^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 946, in main
    run(args)
    ~~~^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/run.py", line 937, in run
    elastic_launch(
    ~~~~~~~~~~~~~~~
        config=config,
        ~~~~~~~~~~~~~~
        entrypoint=cmd,
        ~~~~~~~~~~~~~~~
    )(*cmd_args)
    ~^^^^^^^^^^^
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 159, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/launcher/api.py", line 300, in launch_agent
    raise ChildFailedError(
    ...<2 lines>...
    )
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
timelyfreeze.train FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-10-20_16:12:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3908322)
  error_file: /tmp/torchelastic_sliw4j82/f7e3566e-1054-40fa-9ca9-294a179e3a4e_hszv84ng/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
      dist_utils.init_distributed(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          job_config.comm,
          ^^^^^^^^^^^^^^^^
          enable_cpu_backend=job_config.training.enable_cpu_offload,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          base_folder=job_config.job.dump_folder,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
      os.makedirs(dump_dir, exist_ok=True)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    [Previous line repeated 1 more time]
    File "<frozen os>", line 228, in makedirs
  PermissionError: [Errno 13] Permission denied: '/data2'
  
[2]:
  time      : 2025-10-20_16:12:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3908320)
  error_file: /tmp/torchelastic_sliw4j82/f7e3566e-1054-40fa-9ca9-294a179e3a4e_hszv84ng/attempt_0/1/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
      dist_utils.init_distributed(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          job_config.comm,
          ^^^^^^^^^^^^^^^^
          enable_cpu_backend=job_config.training.enable_cpu_offload,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          base_folder=job_config.job.dump_folder,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
      os.makedirs(dump_dir, exist_ok=True)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    [Previous line repeated 1 more time]
    File "<frozen os>", line 228, in makedirs
  PermissionError: [Errno 13] Permission denied: '/data2'
  
[3]:
  time      : 2025-10-20_16:12:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3908321)
  error_file: /tmp/torchelastic_sliw4j82/f7e3566e-1054-40fa-9ca9-294a179e3a4e_hszv84ng/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
      dist_utils.init_distributed(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          job_config.comm,
          ^^^^^^^^^^^^^^^^
          enable_cpu_backend=job_config.training.enable_cpu_offload,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          base_folder=job_config.job.dump_folder,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
      os.makedirs(dump_dir, exist_ok=True)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    [Previous line repeated 1 more time]
    File "<frozen os>", line 228, in makedirs
  PermissionError: [Errno 13] Permission denied: '/data2'
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-20_16:12:08
  host      : ip-172-16-131-118.sa-east-1.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3908319)
  error_file: /tmp/torchelastic_sliw4j82/f7e3566e-1054-40fa-9ca9-294a179e3a4e_hszv84ng/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/opt/dlami/nvme/DMLAB/shcho/miniforge3/envs/llm/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
      return f(*args, **kwargs)
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/timelyfreeze/train.py", line 96, in __init__
      dist_utils.init_distributed(
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
          job_config.comm,
          ^^^^^^^^^^^^^^^^
          enable_cpu_backend=job_config.training.enable_cpu_offload,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          base_folder=job_config.job.dump_folder,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      )
      ^
    File "/opt/dlami/nvme/DMLAB/shcho/torchtitan/torchtitan/distributed/utils.py", line 269, in init_distributed
      os.makedirs(dump_dir, exist_ok=True)
      ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    File "<frozen os>", line 218, in makedirs
    [Previous line repeated 1 more time]
    File "<frozen os>", line 228, in makedirs
  PermissionError: [Errno 13] Permission denied: '/data2'
  
============================================================
